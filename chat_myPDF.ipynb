{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RAG with LangChain, Ollama, and FAISS Vector Store",
   "id": "a36efa837ee4a49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PDF Dataset: \n",
    "https://github.com/aydiegithub/rag-system-ollama/tree/dac3b4563a66b8b11962aaa08349ba5138be396a/rag-dataset-main"
   ],
   "id": "b0255993f2c3ba04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Document Ingestion](flowcharts/Flowchart.png)",
   "id": "f86ee17a06bbf6e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T15:58:16.405750Z",
     "start_time": "2025-07-05T15:58:16.403040Z"
    }
   },
   "cell_type": "code",
   "source": "# pip install -U langchain-community faiss-cpu langchain-huggingface pymupdf tiktoken langchain-ollama python-dotenv",
   "id": "19cfa02030611dfa",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T15:58:16.417723Z",
     "start_time": "2025-07-05T15:58:16.415697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "9e8bf279a90db4f2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T15:58:16.437330Z",
     "start_time": "2025-07-05T15:58:16.429293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "load_dotenv()"
   ],
   "id": "a53ba1b5de63fef6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Document loader",
   "id": "59a42dfe45d02b65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T15:58:17.234079Z",
     "start_time": "2025-07-05T15:58:16.438724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf\")\n",
    "\n",
    "docs = loader.load()"
   ],
   "id": "3b0e5b2ac78a13f8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T15:58:17.236466Z",
     "start_time": "2025-07-05T15:58:17.234738Z"
    }
   },
   "cell_type": "code",
   "source": "doc = docs[10]",
   "id": "99a2a7f29c0c59c8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T15:58:17.240255Z",
     "start_time": "2025-07-05T15:58:17.238209Z"
    }
   },
   "cell_type": "code",
   "source": "print(doc.page_content)",
   "id": "6e0257c598125717",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 \n",
      " \n",
      "covers less distance physically than by train because a plane is unrestricted. Similarly, in chess, the \n",
      "concept of distance depends on the piece used – for example, a Bishop can move diagonally.   Thus, \n",
      "depending on the entity and the mode of travel, the concept of distance can be experienced differently. \n",
      "The distance metrics commonly used are Euclidean, Minkowski, Manhattan, and Mahalanobis. \n",
      " \n",
      " \n",
      "Distance is applied through the concept of neighbours and exemplars. Neighbours are points in \n",
      "proximity with respect to the distance measure expressed through exemplars. Exemplars are \n",
      "either centroids that ﬁnd a centre of mass according to a chosen distance metric or medoids that ﬁnd \n",
      "the most centrally located data point. The most commonly used centroid is the arithmetic mean, which \n",
      "minimises squared Euclidean distance to all other points. \n",
      " \n",
      "Notes: \n",
      " \n",
      "The centroid represents the geometric centre of a plane figure, i.e., the arithmetic mean \n",
      "position of all the points in the figure from the centroid point. This definition extends to any \n",
      "object in n-dimensional space: its centroid is the mean position of all the points. \n",
      " \n",
      "Medoids are similar in concept to means or centroids. Medoids are most commonly used on \n",
      "data when a mean or centroid cannot be defined. They are used in contexts where the centroid \n",
      "is not representative of the dataset, such as in image data. \n",
      " \n",
      "Examples of distance-based models include the nearest-neighbour models, which use the training data \n",
      "as exemplars – for example, in classification. The K-means clustering algorithm also uses exemplars to \n",
      "create clusters of similar data points. \n",
      " \n",
      "1.3.3 Probabilistic models \n",
      "The third family of machine learning algorithms is the probabilistic models. We have seen \n",
      "before that the k-nearest neighbour algorithm uses the idea of distance (e.g., Euclidian distance) to \n",
      "classify entities, and logical models use a logical expression to partition the instance space. In this \n",
      "section, we see how the probabilistic models use the idea of probability to classify new entities. \n",
      " \n",
      "Probabilistic models see features and target variables as random variables. The process of modelling \n",
      "represents and manipulates the level of uncertainty with respect to these variables. There are two \n",
      "types of probabilistic models: Predictive and Generative. Predictive probability models use the idea of \n",
      "a conditional probability distribution P (Y |X) from which Y can be predicted from X.  Generative models \n",
      "estimate the joint distribution P (Y, X).  Once we know the joint distribution for the generative models, \n",
      "we can derive any conditional or marginal distribution involving the same variables. Thus, the \n",
      "generative model is capable of creating new data points and their labels, knowing the joint probability \n",
      "distribution. The joint distribution looks for a relationship between two variables. Once this relationship \n",
      "is inferred, it is possible to infer new data points. \n",
      "Naïve Bayes is an example of a probabilistic classifier. \n",
      " \n",
      "We can do this using the Bayes rule defined as\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load all the pdfs",
   "id": "c8c58c78e91d79f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T15:58:17.244961Z",
     "start_time": "2025-07-05T15:58:17.241400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "pdfs = []\n",
    "\n",
    "for root, dirs, files in os.walk('rag-dataset-main'):\n",
    "    # print(root, dirs, files)\n",
    "    for file in files:\n",
    "        if file.endswith('.pdf'):\n",
    "            pdfs.append(os.path.join(root, file))\n",
    "\n",
    "pdfs"
   ],
   "id": "aacc5938f676b89b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf',\n",
       " 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf',\n",
       " 'rag-dataset-main/machine-learning/ML_notes_22.pdf',\n",
       " 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf',\n",
       " 'rag-dataset-main/machine-learning/2505.03861v1.pdf',\n",
       " 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T15:58:19.101683Z",
     "start_time": "2025-07-05T15:58:17.245555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "docs = []\n",
    "\n",
    "for pdf in pdfs:\n",
    "    loader = PyMuPDFLoader(pdf)\n",
    "    pages = loader.load()\n",
    "    \n",
    "    docs.extend(pages)"
   ],
   "id": "a274ddeb1a1a0511",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T15:58:19.105603Z",
     "start_time": "2025-07-05T15:58:19.102621Z"
    }
   },
   "cell_type": "code",
   "source": "len(docs) # number of pages in document",
   "id": "955ba197a95796bb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1060"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Document Chuncking",
   "id": "4f4312cff059543c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T15:58:19.107738Z",
     "start_time": "2025-07-05T15:58:19.106358Z"
    }
   },
   "cell_type": "code",
   "source": "# pip install -qU langchain-text-splitters",
   "id": "cd4d49d8432ea0f",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T16:06:29.814732Z",
     "start_time": "2025-07-05T16:06:29.765167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)"
   ],
   "id": "7580167cec397a17",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T16:06:30.134869Z",
     "start_time": "2025-07-05T16:06:30.132355Z"
    }
   },
   "cell_type": "code",
   "source": "len(docs), len(chunks)",
   "id": "67a1aa2d159b2cde",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1060, 2440)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:03:25.800122Z",
     "start_time": "2025-07-05T17:03:25.797249Z"
    }
   },
   "cell_type": "code",
   "source": "print(chunks[100].page_content)",
   "id": "1b79bdb07429ed5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSIT DEPT-R22-MACHINE LEARNING \n",
      "48 \n",
      " \n",
      "The multi-layer perceptron model is also known as the Backpropagation algorithm, which \n",
      "executes in two stages as follows: \n",
      " \n",
      "Forward Stage: Activation functions start from the input layer in the forward stage \n",
      "and terminate on the output layer. \n",
      " \n",
      "Backward Stage: In the backward stage, weight and bias values are modified as per \n",
      "the model's requirement. In this stage, the error between actual output and demanded \n",
      "originated backward on the output layer and ended on the input layer. \n",
      "Hence, a multi-layered perceptron model has considered as multiple artificial neural networks \n",
      "having various layers in which activation function does not remain linear, similar to a single \n",
      "layer perceptron model. Instead of linear, activation function can be executed as sigmoid, \n",
      "TanH, ReLU, etc., for deployment. \n",
      "A multi-layer perceptron model has greater processing power and can process linear and non-\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:04:03.526979Z",
     "start_time": "2025-07-05T17:04:03.524209Z"
    }
   },
   "cell_type": "code",
   "source": "len(chunks[100].page_content)",
   "id": "5013e7c46b9f3974",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:05:11.577223Z",
     "start_time": "2025-07-05T17:05:03.186226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")"
   ],
   "id": "59ed901a792f87a4",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:06:11.421272Z",
     "start_time": "2025-07-05T17:06:11.418350Z"
    }
   },
   "cell_type": "code",
   "source": "len(encoding.encode(chunks[2].page_content))",
   "id": "a6d342538d7cde06",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:06:16.904931Z",
     "start_time": "2025-07-05T17:06:16.902236Z"
    }
   },
   "cell_type": "code",
   "source": "len(encoding.encode(docs[2].page_content))",
   "id": "b3d873ca9d589066",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Document Vector Embedding",
   "id": "a36d320c39f66392"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:26:31.865779Z",
     "start_time": "2025-07-05T17:26:31.863738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore # used to load the vectors in the ram"
   ],
   "id": "23822ebeadba7c22",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:29:59.782303Z",
     "start_time": "2025-07-05T17:29:59.750327Z"
    }
   },
   "cell_type": "code",
   "source": "embeddings = OllamaEmbeddings(model = 'nomic-embed-text', base_url = \"http://localhost:11434\")",
   "id": "8f98e103b5cdfd1c",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:32:16.078877Z",
     "start_time": "2025-07-05T17:32:16.026939Z"
    }
   },
   "cell_type": "code",
   "source": "single_vector = embeddings.embed_query(\"Hello there I am Aydie.\")",
   "id": "c129979c13b7ea6d",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:32:39.924632Z",
     "start_time": "2025-07-05T17:32:39.921231Z"
    }
   },
   "cell_type": "code",
   "source": "len(single_vector), single_vector[:10]",
   "id": "dc88963df6256512",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,\n",
       " [-0.0056480016,\n",
       "  0.012251444,\n",
       "  -0.1503848,\n",
       "  -0.051890645,\n",
       "  0.02039635,\n",
       "  0.020038703,\n",
       "  -0.015681038,\n",
       "  -0.038975507,\n",
       "  -0.020349437,\n",
       "  0.017903127])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:34:51.430631Z",
     "start_time": "2025-07-05T17:34:51.427665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index = faiss.IndexFlatL2(len(single_vector))\n",
    "index, index.ntotal, index.d"
   ],
   "id": "ba8588eab4e10ae7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<faiss.swigfaiss.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x135bad4d0> >,\n",
       " 0,\n",
       " 768)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:36:53.463643Z",
     "start_time": "2025-07-05T17:36:53.461464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vector_store = FAISS(\n",
    "    embedding_function = embeddings,\n",
    "    index = index,\n",
    "    docstore = InMemoryDocstore(),\n",
    "    index_to_docstore_id = {}\n",
    ")"
   ],
   "id": "8c986d76197ef01f",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:37:38.562733Z",
     "start_time": "2025-07-05T17:37:38.559518Z"
    }
   },
   "cell_type": "code",
   "source": "vector_store",
   "id": "3fd2c208c2e75c4b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x135bacec0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:37:52.379841Z",
     "start_time": "2025-07-05T17:37:52.330161Z"
    }
   },
   "cell_type": "code",
   "source": "docs",
   "id": "5f7735dcf91c13d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 0}, page_content='DIGITAL NOTES \\nOF \\nMACHINE LEARNING \\n[R22A6602] \\n \\n                            B. TECH III YEAR - II SEM \\n(2024-2025) \\n \\n \\n \\n \\n \\n \\n   PREPARED BY            \\n                                           P.HARIKRISHNA \\n \\n             DEPARTMENT OF COMPUTER SCIENCE & INFORMATION TECHNOLOGY \\n \\n           MALLA REDDY COLLEGE OF ENGINEERING & TECHNOLOGY \\n(Autonomous Institution – UGC, Govt. of India) \\nRecognized under 2(f) and 12 (B) of UGC ACT 195 \\n(Affiliated to JNTUH, Hyderabad, Approved by AICTE - Accredited by NBA & NAAC – ‘A’ Grade - ISO 9001:2015 Certified) \\nMaisammaguda, Dhulapally (Post Via. Hakimpet), Secunderabad – 500100, Telangana State, India'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 1}, page_content='MALLA REDDY COLLEGE OF ENGINEERING AND TECHNOLOGY \\n    B.TECH - III- YEAR II-SEM-CSIT \\n  L/T/P/C \\n                              3/-/-/3 \\n          (R22A6602) MACHINE LEARNING \\nCOURSE OBJECTIVES: \\n1. Recognize the basic terminology and fundamental concepts of machine learning. \\n2. Understand the concepts of Supervised Learning models with a focus on recent \\nadvancements. \\n3. Relate the Concepts of Neural Networks Models of supervised Learning \\n4. Discover Unsupervised learning paradigms of machine learning \\n5. Understand the concepts of Reinforcement learning and Ensemble methods. \\n \\nUNIT – I \\nIntroduction: Introduction to Machine learning , Supervised learning, Unsupervised learning \\nReinforcement learning. Deep learning. \\nFeature Selection: Filter, Wrapper , Embedded methods. \\nFeature Normalization:- min-max normalization, z-score normalization, and constant factor \\nnormalization \\nIntroduction to Dimensionality Reduction : Principal Component Analysis(PCA), Linear \\nDiscriminant Analysis(LDA) \\nUNIT – II \\nSupervised Learning – I (Regression/Classification) \\nRegression models: Simple Linear Regression, multiple linear Regression. Cost Function, \\nGradient Descent, Performance Metrics: Mean Absolute Error(MAE),Mean Squared \\nError(MSE) R-Squared error, Adjusted R Square. \\nClassification models: Decision Trees-ID3,CART, Naive Bayes, K-Nearest-Neighbours \\n(KNN), Logistic Regression, Multinomial Logistic Regression \\nSupport Vector Machines (SVM) - Nonlinearity and Kernel Methods \\nUNIT – III \\nSupervised Learning – II (Neural Networks) \\nNeural Network Representation – Problems – Perceptrons , Activation Functions, Artificial'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 2}, page_content='Neural Networks (ANN) , Back Propagation Algorithm. \\n \\nClassification Metrics: Confusion matrix, Precision, Recall, Accuracy, F-Score, ROC curves. \\n \\nUNIT - IV \\nModel Validation in Classification : Cross Validation - Holdout Method, K-Fold, Stratified K- \\nfold,Leave-One-Out Cross Validation. Bias-Variance tradeoff, Regularization Overfitting, \\nUnder fitting. Ensemble Methods: Boosting, Bagging, Random Forest. \\n \\nUNIT – V \\nUnsupervised Learning : Clustering-K-means, K-Modes, K-Prototypes, Gaussian Mixture \\nModels, Expectation-Maximization. \\nReinforcement Learning: Exploration and exploitation trade-offs, non-associative learning, \\nMarkov decision processes, Q-learning. \\n \\nTEXT BOOKS: \\n \\n1. Machine Learning –Tom M.Mitchel,MGH. \\n2. Kevin Murphy, Machine Learning: A Probabilistic Perspective, MIT Press, 2012. \\n3. Richard S. Sutton and Andrew G. Barto , Reinforcement Learning: An Introduction, MIT Press,1998. \\n \\n \\nREFERENCE BOOKS: \\n1. Trevor Hastie, Robert Tibshirani, Jerome Friedman, The Elements of Statistical \\nLearning, Springer2009. \\n2. Christopher Bishop, Pattern Recognition and Machine Learning, Springer,2007. \\n3. Machine Learning Yearning, AndrewNg \\n4. DataMining–Concepts and \\nTechniques-Jiawei Han and Micheline Kamber,Morgan \\nKaufmann.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 3}, page_content='INDEX \\nS.NO \\nTitle \\nPage No \\nUNIT-I \\n1 \\nIntroduction to Machine learning \\n1 \\n2 \\nFeature Selection \\n7 \\n3 \\nFeature Normalization \\n13 \\n4 \\nDimensionality Reduction \\n15 \\n5 \\nUNIT-II \\n6 \\nRegression Models \\n21 \\n7 \\nGradient Descent \\n23 \\n8 \\nPerformance Metrics \\n25 \\n9 \\nDecision Trees \\n26 \\n10 \\nNaïve Bayes Classifier \\n29 \\n11 \\nK-Nearest Neighbor(KNN) Algorithm \\n31 \\n12 \\nLogistic Regression \\n34 \\n13 \\nSupport Vector Machines \\n37 \\n14 \\nUNIT –III \\n15 \\nNeural Network Representation \\n41 \\n16 \\nArtificialNeural Networks \\n57 \\n17 \\nBack propagation Algorithm \\n62 \\n18 \\nClassification Metrics \\n67 \\n19 \\nUNIT-IV \\n20 \\nCross validation Techniques \\n68 \\n21 \\nBias-Variance Trade off \\n76 \\n22 \\nRegularization \\n77 \\n23 \\nOverfitting, Underfitting \\n80 \\n24 \\nEnsemble Methods \\n82 \\n25 \\nUNIT-V \\n26 \\nClustering Techniques \\n88 \\n27 \\nGaussian Mixture Models \\n102 \\n28 \\nExpectation-Maximization \\n103 \\n29 \\nExploration and exploitation trade-offs \\n105 \\n30 \\nNon-associative Learning \\n107 \\n31 \\nMarkov Decision Process \\n108 \\n32 \\nQ-learning \\n111'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 4}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n1 \\n \\nUNIT-I \\n \\n \\n \\nIntroduction \\n \\nDefinition of Machine Learning \\n \\nArthur Samuel, an early American leader in the field of computer gaming and artificial \\nintelligence, coined the term “Machine Learning” in 1959 while at IBM. He defined machine \\nlearning as “the field of study that gives computers the ability to learn without being explicitly \\nprogrammed.” However, there is no universally accepted definition for machine learning. \\nDifferent authors define the term differently. We give below two more definitions. \\nMachine learning is programming computers to optimize a performance criterion using \\nexample data or past experience. We have a model defined up to some parameters, and learning \\nis the execution of a computer program to optimize the parameters of the model using the \\ntraining data or past experience. \\nThe field of study known as machine learning is concerned with the question of how to \\nconstruct computer programs that automatically improve with experience. \\nIn the above definitions we have used the term “model” and we will be using this term at \\nseveral contexts later. It appears that there is no universally accepted one sentence definition \\nof this term. Loosely, it may be understood as some mathematical expression or equation, or \\nsome mathematical structures such as graphs and trees, or a division of sets into disjoint \\nsubsets, or a set of logical “if . . . then . . . else . . .” rules, or some such thing. It may be noted \\nthat this is not an exhaustive list. \\nDefinition of learning \\nIntroduction: Introduction to Machine learning, Supervised learning, Unsupervised \\nlearning, Reinforcement learning. Deep learning. Feature Selection: Filter, Wrapper \\nEmbedded \\nmethods. \\nFeature \\nNormalization:- \\nmin-max \\nnormalization, \\nz-score \\nnormalization, and constant factor normalization, Introduction to Dimensionality \\nReduction : Principal Component Analysis(PCA), Linear Discriminant Analysis(LDA)'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 5}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n2 \\n \\nA computer program is said to learn from experience E with respect to some class of tasks T and \\nperformance measure P, if its performance at tasks T, as measured by P, improves with experience \\nE. \\nExamples: \\n \\ni) Handwriting recognition learning problem \\n \\n• Task T: Recognising and classifying handwritten words within images \\n \\n• Performance P: Percent of words correctly classified \\n \\n• Training experience E: A dataset of handwritten words with given classifications \\n \\nii) A robot driving learning problem \\n \\n• Task T: Driving on highways using vision sensors \\n \\n• Performance measure P: Average distance traveled before an error \\n \\n• training experience: A sequence of images and steering commands recorded while \\nobserving a human driver \\niii) A chess learning problem \\n \\n• Task T: Playing chess \\n \\n• Performance measure P: Percent of games won against opponents \\n \\n• Training experience E: Playing practice games against itself \\n \\nA computer program which learns from experience is called a machine learning program or \\nsimply a learning program. Such a program is sometimes also referred to as a learner. \\nHow machines learn \\n \\nBasic components of learning process: \\n \\nThe learning process, whether by a human or a machine, can be divided into four components, \\nnamely, data storage, abstraction, generalization and evaluation. Figure 1.1 illustrates the \\nvarious components and the steps involved in the learning process.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 6}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n3 \\n \\n \\n \\n \\n \\n \\n \\nData storage: Facilities for storing and retrieving huge amounts of data are an important \\ncomponent of the learning process. Humans and computers alike utilize data storage as a \\nfoundation for advanced reasoning. \\n• In a human being, the data is stored in the brain and data is retrieved using electrochemical \\nsignals. \\n• Computers use hard disk drives, flash memory, random access memory and similar devices \\nto store data and use cables and other technology to retrieve data. \\nAbstraction \\n \\nThe second component of the learning process is known as abstraction. Abstraction is the \\nprocess of extracting knowledge about stored data. This involves creating general concepts \\nabout the data as a whole. The creation of knowledge involves application of known models \\nand creation of new models. \\nThe process of fitting a model to a dataset is known as training. When the model has been \\ntrained, the data is transformed into an abstract form that summarizes the original information. \\nGeneralization \\n \\nThe third component of the learning process is known as generalisation. \\n \\nThe term generalization describes the process of turning the knowledge about stored data into \\na form that can be utilized for future action. These actions are to be carried out on tasks that \\nare similar, but not identical, to those what have been seen before. In generalization, the goal \\nis to discover those properties of the data that will be most relevant to future tasks. \\nEvaluation'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 7}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n4 \\n \\nEvaluation is the last component of the learning process. It is the process of giving feedback to \\nthe user to measure the utility of the learned knowledge. This feedback is then utilised to effect \\nimprovements in the whole learning process. \\nApplications of machine learning \\n \\nApplication of machine learning methods to large databases is called data mining. In data \\nmining, a large volume of data is processed to construct a simple model with valuable use, for \\nexample, having high predictive accuracy. \\nThe following is a list of some of the typical applications of machine learning. \\n \\n1. In retail business, machine learning is used to study consumer behaviour. \\n \\n2. In finance, banks analyze their past data to build models to use in credit applications, fraud \\ndetection, and the stock market. \\n3. In manufacturing, learning models are used for optimization, control, and troubleshooting. \\n \\n4. In medicine, learning programs are used for medical diagnosis. \\n \\n5. In telecommunications, call patterns are analyzed for network optimization and \\nmaximizing the quality of service. \\n6. In science, large amounts of data in physics, astronomy, and biology can only be analyzed \\nfast enough by computers. The World Wide Web is huge; it is constantly growing and \\nsearching for relevant information cannot be done manually. \\n7. In artificial intelligence, it is used to teach a system to learn and adapt to changes so that \\nthe system designer need not foresee and provide solutions for all possible situations. \\n8. It is used to find solutions to many problems in vision, speech recognition, and robotics. \\n \\n9. Machine learning methods are applied in the design of computer-controlled vehicles to \\nsteer correctly when driving on a variety of roads. \\n10. Machine learning methods have been used to develop programmes for playing games \\nsuch as chess, backgammon and Go. \\nDifferent types of learning'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 8}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n5 \\n \\nIn general, machine learning algorithms can be classified into three types. \\n \\nSupervised learning: \\n \\nSupervised learning is the machine learning task of learning a function that maps an input to \\nan output based on example input-output pairs. \\nIn supervised learning, each example in the training set is a pair consisting of an input object \\n(typically a vector) and an output value. A supervised learning algorithm analyzes the training \\ndata and produces a function, which can be used for mapping new examples. In the optimal \\ncase, the function will correctly determine the class labels for unseen instances. Both \\nclassification and regression problems are supervised learning problems. \\nA wide range of supervised learning algorithms are available, each with its strengths and \\nweaknesses. There is no single learning algorithm that works best on all supervised learning \\nproblems. \\nA “supervised learning” is so called because the process of algorithm learning from the training \\ndataset can be thought of as a teacher supervising the learning process. We know the correct \\nanswers (that is, the correct outputs), the algorithm iteratively makes predictions on the training \\ndata and is corrected by the teacher. Learning stops when the algorithm achieves an acceptable \\nlevel of performance. \\nExample : \\n \\nConsider the following data regarding patients entering a clinic. The data consists of the gender \\nand age of the patients and each patient is labelled as “healthy” or “sick”. \\n \\n \\n \\n \\n \\n \\n \\n \\nUnsupervised learning \\n \\nUnsupervised learning is a type of machine learning algorithm used to draw inferences from \\ndatasets consisting of input data without labeled responses.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 9}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n6 \\n \\nIn unsupervised learning algorithms, a classification or categorization is not included in the \\nobservations. There are no output values and so there is no estimation of functions. Since the \\nexamples given to the learner are unlabeled, the accuracy of the structure that is output by the \\nalgorithm cannot be evaluated. \\nThe most common unsupervised learning method is cluster analysis, which is used for \\nexploratory data analysis to find hidden patterns or grouping in data. \\nExample : \\n \\nConsider the following data regarding patients entering a clinic. The data consists of the \\ngender and age of the patients. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nBased on this data, can we infer anything regarding the patients entering the clinic? \\n \\nReinforcement learning \\n \\nReinforcement learning is the problem of getting an agent to act in the world so as to maximize \\nits rewards. \\nA learner (the program) is not told what actions to take as in most forms of machine learning, \\nbut instead must discover which actions yield the most reward by trying them. In the most \\ninteresting and challenging cases, actions may affect not only the immediate reward but also \\nthe next situations and, through that, all subsequent rewards. \\nFor example, consider teaching a dog a new trick: we cannot tell it what to do, but we can \\nreward/punish it if it does the right/wrong thing. It has to find out what it did that made it get \\nthe reward/punishment. We can use a similar method to train computers to do many tasks, such \\nas playing backgammon or chess, scheduling jobs, and controlling robot limbs. Reinforcement \\nlearning is different from supervised learning. Supervised learning is learning from examples \\nprovided by a knowledgeable expert.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 10}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n7 \\n \\nDeep learning \\n \\nDeep learning is the subset of machine learning methods based on neural networks with \\nrepresentation learning. The adjective \"deep\" refers to the use of multiple layers in the network. \\nMethods used can be either supervised, semi-supervised or unsupervised. \\nFeature Selection \\nFeature selection is a way of selecting the subset of the most relevant features from the original \\nfeatures set by removing the redundant, irrelevant, or noisy features.” \\nWhile developing the machine learning model, only a few variables in the dataset are useful \\nfor building the model, and the rest features are either redundant or irrelevant. If we input the \\ndataset with all these redundant and irrelevant features, it may negatively impact and reduce \\nthe overall performance and accuracy of the model. Hence it is very important to identify and \\nselect the most appropriate features from the data and remove the irrelevant or less important \\nfeatures, which is done with the help of feature selection in machine learning. \\n \\nFeature selection is one of the important concepts of machine learning, which highly impacts \\nthe performance of the model. As machine learning works on the concept of \"Garbage In \\nGarbage Out\", so we always need to input the most appropriate and relevant dataset to the \\nmodel in order to get a better result. \\nIn this topic, we will discuss different feature selection techniques for machine learning. But \\nbefore that, let\\'s first understand some basics of feature selection. \\nWhat is Feature Selection? \\n \\nA feature is an attribute that has an impact on a problem or is useful for the problem, and \\nchoosing the important features for the model is known as feature selection. Each machine \\nlearning process depends on feature engineering, which mainly contains two processes;which \\nare Feature Selection and Feature Extraction. \\nAlthough feature selection and extraction processes may have the same objective, both are \\ncompletely different from each other. The main difference between them is that feature \\nselection is about selecting the subset of the original feature set, whereas feature extraction \\ncreates new features.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 11}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n8 \\n \\nFeature selection is a way of reducing the input variable for the model by using only relevant \\ndata in order to reduce over fitting in the model. \\nSo, we can define feature Selection as, \"It is a process of automatically or manually selecting \\nthe subset of most appropriate and relevant features to be used in model building.\" Feature \\nselection is performed by either including the important features or excluding the irrelevant \\nfeatures in the dataset without changing them. \\n \\nNeed for Feature Selection: \\n \\nBefore implementing any technique, it is really important to understand, need for the technique \\nand so for the Feature Selection. As we know, in machine learning, it is necessary to provide a \\npre-processed and good input dataset in order to get better outcomes. We collecta huge amount \\nof data to train our model and help it to learn better. Generally, the dataset consists of noisy \\ndata, irrelevant data, and some part of useful data. Moreover, the huge amount of data also \\nslows down the training process of the model, and with noise and irrelevant data, the model \\nmay not predict and perform well. So, it is very necessary to remove such noises and less- \\nimportant data from the dataset and to do this, and Feature selection techniques are used. \\nSelecting the best features helps the model to perform well. For example, Suppose we want to \\ncreate a model that automatically decides which car should be crushed for a spare part, and to \\ndo this, we have a dataset. This dataset contains a Model of the car, Year, Owner\\'s name, Miles. \\nSo, in this dataset, the name of the owner does not contribute to the model performance as it \\ndoes not decide if the car should be crushed or not, so we can remove this column and select \\nthe rest of the features(column) for the model building. \\nBelow are some benefits of using feature selection in machine learning: \\nIt helps in avoiding the curse of dimensionality. \\nIt helps in the simplification of the model so that it can be easily interpreted by the \\nresearchers. \\nIt reduces the training time. \\n \\nIt reduces over fitting hence enhance the generalization.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 12}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n9 \\n \\nFeature Selection Techniques: \\n \\nThere are mainly two types of Feature Selection techniques, which are: \\n \\nSupervised Feature selection techniques consider the target variable and can be used for the \\nlabelled dataset. \\nUnsupervised Feature selection techniques ignore the target variable and can be used for \\nthe unlabelled dataset. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFilter Methods: \\n \\nIn Filter Method, features are selected on the basis of statistics measures. This method does \\nnot depend on the learning algorithm and chooses the features as a pre-processing step. \\nThe filter method filters out the irrelevant feature and redundant columns from the model by \\nusing different metrics through ranking. \\nThe advantage of using filter methods is that it needs low computational time and does not \\nover fit the data.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 13}, page_content=\"CSIT DEPT-R22-MACHINE LEARNING \\n10 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSome common techniques of Filter methods are as follows:: \\n \\ninformation Gain \\nChi-square Test \\nFisher's Score \\nMissing Value Ratio \\nInformation Gain: Information gain determines the reduction in entropy while transforming \\nthe dataset. It can be used as a feature selection technique by calculating the information gain \\nof each variable with respect to the target variable. \\nChi-square Test: Chi-square test is a technique to determine the relationship between the \\ncategorical variables. The chi-square value is calculated between each feature and the target \\nvariable, and the desired number of features with the best chi-square value is selected. \\nFisher's Score: \\n \\nFisher's score is one of the popular supervised techniques of features selection. It returns the \\nrank of the variable on the fisher's criteria in descending order. Then we can select the variables \\nwith a large fisher's score. \\n \\nMissing Value Ratio: \\n \\nThe value of the missing value ratio can be used for evaluating the feature set against the \\nthreshold value. The formula for obtaining the missing value ratio is the number of missing\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 14}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n11 \\n \\nvalues in each column divided by the total number of observations. The variable is having \\nmore than the threshold value can be dropped. \\n \\n \\n \\n \\n \\nWrapper Methods: \\nIn wrapper methodology, selection of features is done by considering it as a search problem, in \\nwhich different combinations are made, evaluated, and compared with other combinations. It \\ntrains the algorithm by using the subset of features iteratively. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nOn the basis of the output of the model, features are added or subtracted, and with this feature \\nset, the model has trained again. \\nSome techniques of wrapper methods are: \\n \\nForward selection - Forward selection is an iterative process, which begins with an empty set \\nof features. After each iteration, it keeps adding on a feature and evaluates the performance to \\ncheck whether it is improving the performance or not. The process continues until the addition \\nof a new variable/feature does not improve the performance of the model. \\nBackward elimination - Backward elimination is also an iterative approach, but it is the \\nopposite of forward selection. This technique begins the process by considering all the features \\nand removes the least significant feature. This elimination process continues until removing \\nthe features does not improve the performance of the model.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 15}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n12 \\n \\nExhaustive Feature Selection- Exhaustive feature selection is one of the best feature selection \\nmethods, which evaluates each feature set as brute-force. It means this method tries & make \\neach possible combination of features and return the best performing feature set. \\nRecursive feature elimination \\n \\nRecursive feature elimination is a recursive greedy optimization approach, where features are \\nselected by recursively taking a smaller and smaller subset of features. Now, an estimator is \\ntrained with each set of features, and the importance of each feature is determined using \\ncoef_attribute or through a feature_importances_attribute. \\nEmbedded Methods \\n \\nEmbedded methods combined the advantages of both filter and wrapper methods by \\nconsidering the interaction of features along with low computational cost. These are fast \\nprocessing methods similar to the filter method but more accurate than the filter method. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThese methods are also iterative, which evaluates each iteration, and optimally finds the most \\nimportant features that contribute the most to training in a particular iteration. Some techniques \\nof embedded methods are: \\nRegularization- Regularization adds a penalty term to different parameters of the machine \\nlearning model for avoiding overfitting in the model. This penalty term is added to the \\ncoefficients; hence it shrinks some coefficients to zero. Those features with zero coefficients \\ncan be removed from the dataset. The types of regularization techniques are L1 Regularization \\n(Lasso Regularization) or Elastic Nets (L1 and L2 regularization).'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 16}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n13 \\n \\nRandom Forest Importance - Different tree-based methods of feature selection help us with \\nfeature importance to provide a way of selecting features. Here, feature importance specifies \\nwhich feature has more importance in model building or has a great impact on the target \\nvariable. Random Forest is such a tree-based method, which is a type of bagging algorithm that \\naggregates a different number of decision trees. It automatically ranks the nodes by their \\nperformance or decrease in the impurity (Gini impurity) over all the trees. Nodes are arranged \\nas per the impurity values, and thus it allows to pruning of trees below a specific node. The \\nremaining nodes create a subset of the most important features. \\nFeature normalization: \\n \\nNormalization is a scaling technique in Machine Learning applied during data preparation to \\nchange the values of numeric columns in the dataset to use a common scale. It is not necessary \\nfor all datasets in a model. It is required only when features of machine learning models have \\ndifferent ranges. \\nAlthough there are so many feature normalization techniques in Machine Learning, few of \\nthem are most frequently used. These are as follows: \\nMin-max normalization \\n \\nMin-max normalization (usually called feature scaling) performs a linear transformation on \\nthe original data. This technique gets all the scaled data in the range (0, 1). The formula to \\nachieve this is the following: \\n \\n \\n \\n \\n \\n \\nFor the three example values, min = 28 and max = 46. Therefore, the min-max normalized \\nvalues are:0.00,1.00,0.33.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 17}, page_content=\"CSIT DEPT-R22-MACHINE LEARNING \\n14 \\n \\nThe min-max technique results in values between 0.0 and 1.0 where the smallest value is \\nnormalized to 0.0 and the largest value is normalized to 1.0. \\nZ-score normalization refers to the process of normalizing every value in a dataset such \\nthat the mean of all of the values is 0 and the standard deviation is 1. \\nWe use the following formula to perform a z-score normalization on every value in a dataset: \\n \\nNew value = (x – μ) / σ \\n \\nwhere: \\n \\nx: Original value \\n \\nμ: Mean of data \\n \\nσ: Standard deviation of data \\n \\nFor the three example values, mean(μ) = (28 + 46 + 34) / 3 = 108 / 3 = 36.0. The standard \\ndeviation of a set of values is the square root of the sum of the squared difference of each value \\nand the mean, divided by the number of values, and so is 7.48. \\nTherefore, the z-score normalized values are:-1.07, 1.34,-0.27. \\nA z-score normalized value that is positive corresponds to an x value that is greater than the \\nmean value, and a z-score that is negative corresponds to an x value that is less than the mean. \\nConstant Factor Normalization: \\n \\nThe simplest normalization technique is constant factor normalization. Expressed as a math \\nequation constant factor normalization is x' = x / k, where x is a raw value, x' is the normalized \\nvalue, and k is a numeric constant. If k = 100, the constant factor normalized values are: \\n0.28.0.46,0.34.\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 18}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n15 \\n \\nDimensionality Reduction \\n \\nDimensionality reduction or dimension reduction is the process of reducing the number of \\nvariables under consideration by obtaining a smaller set of principal variables. \\nDimensionality reduction may be implemented in two ways. \\n• Feature selection \\nIn feature selection, we are interested in finding k of the total of n features that give us the \\nmost information and we discard the other (n−k) dimensions. We are going to discuss subset \\nselection as a feature selection method. \\n• Feature extraction \\nIn feature extraction, we are interested in finding a new set of k features that are the combination of \\nthe original n features. These methods may be supervised or unsupervised depending on whether or \\nnot they use the output information. The best known and most widely used feature extraction methods \\nare Principal Components Analysis (PCA) and LinearDiscriminant Analysis (LDA), which are both \\nlinear projection methods, unsupervised and supervised respectively. \\n \\nWhy dimensionality reduction is useful? \\nThere are several reasons why we are interested in reducing dimensionality. \\n\\uf0d8 In most learning algorithms, the complexity depends on the number of input \\ndimensions, d, as well as on the size of the data sample, N, and for reduced memory \\nand computation, we are interested in reducing the dimensionality of the problem. \\nDecreasing d also decreases the complexity of the inference algorithm during testing. \\n\\uf0d8 When an input is decided to be unnecessary, we save the cost of extracting it. \\n\\uf0d8 Simpler models are more robust on small datasets. Simpler models have less variance, \\nthat is, they vary less depending on the particulars of a sample, including noise, \\noutliers, and so forth. \\n\\uf0d8 When data can be explained with fewer features, we get a better idea about the process \\nthat underlies the data, which allows knowledge extraction. \\n\\uf0d8 When data can be represented in a few dimensions without loss of information, it can \\nbe plotted and analyzed visually for structure and outliers.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 19}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n16 \\n \\nPrincipal component analysis: \\n \\nPrincipal component analysis (PCA) is a statistical procedure that uses an orthogonal \\ntransformation to convert a set of observations of possibly correlated variables into a set of \\nvalues of linearly uncorrelated variables called principal components. The number of principal \\ncomponents is less than or equal to the smaller of the number of original variables or the \\nnumber of observations. This transformation is defined in such a way that the first principal \\ncomponent has the largest possible variance (that is, accounts for as much of the variability in \\nthe data as possible), and each succeeding component in turn has the highest variance possible \\nunder the constraint that it is orthogonal to the preceding components. \\nHow PCA Works (Simple Steps): \\n1. Normalize the Data: \\no Ensure the dataset has a mean of 0 and a standard deviation of 1. This standardization \\nremoves scale effects between features. \\n2. Compute the Covariance Matrix: \\no Create a covariance matrix to capture relationships (variances and covariances) \\nbetween features. \\n3. Find Eigenvalues and Eigenvectors: \\no Compute the eigenvalues (indicating the magnitude of variation) and eigenvectors \\n(indicating the direction of variation) from the covariance matrix. \\n4. Select Principal Components: \\no Rank the eigenvalues in descending order. \\no Choose the top eigenvectors (corresponding to the largest eigenvalues) as the \\nprincipal components. \\n5. Project Data onto Principal Components: \\no Transform the original data by projecting it onto the selected principal components. \\nAdvantages of Dimensionality Reduction \\n\\uf0b7 \\nIt helps in data compression, and hence reduced storage space. \\n\\uf0b7 \\nIt reduces computation time. \\n\\uf0b7 \\nIt also helps remove redundant features, if any.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 20}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n17 \\n \\n\\uf0b7 \\n \\nImproved Visualization: High dimensional data is difficult to visualize, and \\ndimensionality reduction techniques can help in visualizing the data in 2D or 3D, which \\ncan help in better understanding and analysis. \\n\\uf0b7 \\n \\nOverfitting Prevention: High dimensional data may lead to overfitting in machine \\nlearning models, which can lead to poor generalization performance. Dimensionality \\nreduction can help in reducing the complexity of the data, and hence prevent overfitting. \\n\\uf0b7 \\n \\nFeature Extraction: Dimensionality reduction can help in extracting important features \\nfrom high dimensional data, which can be useful in feature selection for machine learning \\nmodels. \\n\\uf0b7 \\n \\nData Preprocessing: Dimensionality reduction can be used as a preprocessing step \\nbefore applying machine learning algorithms to reduce the dimensionality of the data and \\nhence improve the performance of the model. \\n\\uf0b7 \\n \\nImproved Performance: Dimensionality reduction can help in improving the \\nperformance of machine learning models by reducing the complexity of the data, and hence \\nreducing the noise and irrelevant information in the data. \\nDisadvantages of Dimensionality Reduction \\n\\uf0b7 \\nIt may lead to some amount of data loss. \\n\\uf0b7 \\n \\nPCA tends to find linear correlations between variables, which is sometimes \\nundesirable. \\n\\uf0b7 \\nPCA fails in cases where mean and covariance are not enough to define datasets. \\n\\uf0b7 \\n \\nWe may not know how many principal components to keep- in practice, some thumb \\nrules are applied. \\n\\uf0b7 \\n \\nInterpretability: The reduced dimensions may not be easily interpretable, and it may \\nbe difficult to understand the relationship between the original features and the reduced \\ndimensions. \\n\\uf0b7 \\n \\nOverfitting: In some cases, dimensionality reduction may lead to overfitting, especially \\nwhen the number of components is chosen based on the training data. \\n\\uf0b7 \\n \\nSensitivity to outliers: Some dimensionality reduction techniques are sensitive to \\noutliers, which can result in a biased representation of the data. \\n\\uf0b7 \\n \\nComputational complexity: Some dimensionality reduction techniques, such as \\nmanifold learning, can be computationally intensive, especially when dealing with large \\ndatasets.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 21}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n18 \\n \\nLinear Discriminant Analysis (LDA): \\nLinear Discriminant Analysis (LDA) is one of the commonly used dimensionality reduction \\ntechniques in machine learning to solve more than two-class classification problems. It is also \\nknown as Normal Discriminant Analysis (NDA) or Discriminant Function Analysis (DFA). \\nLinear Discriminant analysis is one of the most popular dimensionality reduction techniques \\nused for supervised classification problems in machine learning. It is also considered a pre- \\nprocessing step for modeling differences in ML and applications of pattern classification \\nWhenever there is a requirement to separate two or more classes having multiple features \\nefficiently, the Linear Discriminant Analysis model is considered the most common technique \\nto solve such classification problems. For e.g., if we have two classes with multiplefeatures \\nand need to separate them efficiently. When we classify them using a single feature, then it \\nmay show overlapping. \\n \\nConsider a situation where you have plotted the relationship between two variables where each \\ncolor represents a different class. One is shown with a red color and the other with blue. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIf you are willing to reduce the number of dimensions to 1, you can just project everything \\nto the x-axis as shown below:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 22}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n19 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThis approach neglects any helpful information provided by the second feature. However, you \\ncan use LDA to plot it. The advantage of LDA is that it uses information from both the features \\nto create a new axis which in turn minimizes the variance and maximizes the class distance \\nof the two variables. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nDrawbacks of Linear Discriminant Analysis (LDA) \\n \\nAlthough, LDA is specifically used to solve supervised classification problems for two or \\nmore classes which are not possible using logistic regression in machine learning. But LDA'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 23}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n20 \\n \\nalso fails in some cases where the Mean of the distributions is shared. In this case, LDA fails \\nto create a new axis that makes both the classes linearly separable. Real-world \\nApplications of LDA \\nSome of the common real-world applications of Linear discriminant Analysis are given below: \\no FaceRecognition \\nFace recognition is the popular application of computer vision, where each face is \\nrepresented as the combination of a number of pixel values. In this case, LDA is used \\nto minimize the number of features to a manageable number before going through the \\nclassification process. It generates a new template in which each dimension consists of \\na linear combination of pixel values. If a linear combination is generated using Fisher\\'s \\nlinear discriminant, then it is called Fisher\\'s face. \\no Medical \\nIn the medical field, LDA has a great application in classifying the patient disease on \\nthe basis of various parameters of patient health and the medical treatment which is \\ngoing on. On such parameters, it classifies disease as mild, moderate, or severe. This \\nclassification helps the doctors in either increasing or decreasing the pace of the \\ntreatment. \\no Customer Identification \\nIn customer identification, LDA is currently being applied. It means with the help of \\nLDA; we can easily identify and select the features that can specify the group of \\ncustomers who are likely to purchase a specific product in a shopping mall. This can be \\nhelpful when we want to identify a group of customers who mostly purchase a product \\nin a shopping mall. \\no For Predictions \\nLDA can also be used for making predictions and so in decision making. For example, \\n\"will you buy this product” will give a predicted result of either one or two possible \\nclasses as a buying or not.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 24}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n21 \\n \\nUNIT – II \\n \\n \\n \\n \\nLinear regression: \\nLinear regression algorithm shows a linear relationship between a dependent (y) and one or \\nmore independent (y) variables, hence called as linear regression. \\nThe linear regression model provides a sloped straight line representing the relationship \\nbetween the variables. Consider the below image: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nMathematically, we can represent a linear regression as: \\ny= a0+a1x+ ε \\nHere, \\nY= Dependent Variable (Target Variable) \\nX= Independent Variable (predictor Variable) \\na0= intercept of the line (Gives an additional degree of freedom) \\nSupervised Learning – I (Regression/Classification) Regression models: Simple Linear \\nRegression, multiple linear Regression. Cost Function, Gradient Descent, Performance \\nMetrics: Mean Absolute Error(MAE),Mean Squared Error(MSE) R-Squared error, \\nAdjusted R Square. Classification models: Decision Trees-ID3,CART, Naive Bayes, K- \\nNearest-Neighbours (KNN), Logistic Regression, Multinomial Logistic Regression \\nSupport Vector Machines (SVM) - Nonlinearity and Kernel Methods'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 25}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n22 \\n \\na1 = Linear regression coefficient (scale factor to each input value). \\nε = random error \\nThe values for x and y variables are training datasets for Linear Regression model \\nrepresentation. \\nRegression Models \\nLinear regression can be further divided into two types of the algorithm: \\no Simple Linear Regression: \\nIf a single independent variable is used to predict the value of a numerical dependent \\nvariable, then such a Linear Regression algorithm is called Simple Linear Regression. \\no Multiple Linear regression: \\nIf more than one independent variable is used to predict the value of a numerical \\ndependent variable, then such a Linear Regression algorithm is called Multiple Linear \\nRegression.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 26}, page_content=\"cost function \\n \\nThe cost function is defined as the measurement of difference or error between actual values \\nand expected values at the current position and present in the form of a single real number. \\nGradient Descent \\nIt is known as one of the most commonly used optimization algorithms to train machine \\nlearning models by means of minimizing errors between actual and expected results. Further, \\ngradient descent is also used to train Neural Networks. \\n \\nTypes of Gradient Descent \\nBased on the error in various training models, the Gradient Descent learning algorithm can be \\ndivided into Batch gradient descent, stochastic gradient descent, and mini-batch gradient \\ndescent. Let's understand these different types of gradient descent: \\n1. Batch Gradient Descent: \\nBatch gradient descent (BGD) is used to find the error for each point in the training set and \\nupdate the model after evaluating all training examples. This procedure is known as the training \\nepoch. In simple words, it is a greedy approach where we have to sum over all examples for \\neach update. \\nAdvantages of Batch gradient descent: \\no It produces less noise in comparison to other gradient descent. \\no It produces stable gradient descent convergence. \\no It is Computationally efficient as all resources are used for all training samples. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nCSIT DEPT-R22-MACHINE LEARNING \\n23\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 27}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n24 \\n \\n2. Stochastic gradient descent \\nStochastic gradient descent (SGD) is a type of gradient descent that runs one training \\nexample per iteration. \\n3. MiniBatch Gradient Descent: \\nMini Batch gradient descent is the combination of both batch gradient descent and stochastic \\ngradient descent. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\uf0b7 \\n \\nMean Squared Error represents the average of the squared difference between the \\noriginal and predicted values in the data set. It measures the variance of the residuals. \\n \\n \\n \\n \\n \\n \\n\\uf0b7 \\n \\nRoot Mean Squared Error is the square root of Mean Squared error. It measures the \\nstandard deviation of residuals. \\n \\n \\n \\n \\n \\n \\n \\n \\n\\uf0b7 \\n \\nThe coefficient of determination or R-squared represents the proportion of the variance \\nin the dependent variable which is explained by the linear regression model. It is a scale- \\nfree score i.e. irrespective of the values being small or large, the value of R square will be \\nless than one.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 28}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n25 \\n \\n \\n \\n \\n \\n\\uf0b7 \\n \\nAdjusted R squared is a modified version of R square, and it is adjusted for the number \\nof independent variables in the model, and it will always be less than or equal to R².In the \\nformula below n is the number of observations in the data and k is the number of the \\nindependent variables in the data. \\n \\n \\n \\n \\n \\n \\nEvaluation Metrics \\n\\uf0b7 \\n \\nMean Squared Error(MSE) and Root Mean Square Error penalizes the large prediction \\nerrors vi-a-vis Mean Absolute Error (MAE). However, RMSE is widely used than MSE \\nto evaluate the performance of the regression model with other random models as it has \\nthe same units as the dependent variable (Y-axis). \\n\\uf0b7 \\n \\nMSE is a differentiable function that makes it easy to perform mathematical operations \\nin comparison to a non-differentiable function like MAE. Therefore, in many models, \\nRMSE is used as a default metric for calculating Loss Function despite being harder to \\ninterpret than MAE. \\n\\uf0b7 \\n \\nThe lower value of MAE, MSE, and RMSE implies higher accuracy of a regression \\nmodel. However, a higher value of R square is considered desirable. \\n\\uf0b7 \\n \\nR Squared & Adjusted R Squared are used for explaining how well the independent \\nvariables in the linear regression model explains the variability in the dependent variable. \\nR Squared value always increases with the addition of the independent variables which \\nmight lead to the addition of the redundant variables in our model. However, the adjusted \\nR-squared solves this problem. \\n\\uf0b7 \\n \\nAdjusted R squared takes into account the number of predictor variables, and it is used \\nto determine the number of independent variables in our model. The value of Adjusted R \\nsquared decreases if the increase in the R square by the additional variable isn’t significant \\nenough.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 29}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n26 \\n \\n\\uf0b7 \\n \\nFor comparing the accuracy among different linear regression models, RMSE is a \\nbetter choice than R Squared. \\n \\nDecision Trees \\nIn simple words, a decision tree is a structure that contains nodes (rectangular boxes) and \\nedges(arrows) and is built from a dataset (table of columns representing features/attributes and \\nrows corresponds to records). Each node is either used to make a decision (known as decision \\nnode) or represent an outcome (known as leaf node). \\nDecision tree Example \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThe picture above depicts a decision tree that  is  used to  classify whether  a person is \\nFit or Unfit. \\nThe decision nodes here are questions like ‘’‘Is the person less than 30 years of age?’, ‘Does \\nthe person eat junk?’, etc. and the leaves are one of the two possible outcomesviz. \\nFit and Unfit. \\nLooking  at  the  Decision  Tree  we  can  say  make  the  following  decisions: \\nif a person is less than 30 years of age and doesn’t eat junk food then he is Fit, if a person is \\nless than 30 years of age and eats junk food then he is Unfit and so on. \\nThe initial node is called the root node (colored in blue), the final nodes are called the leaf \\nnodes (colored in green) and the rest of the nodes are called intermediate or internal nodes. \\nThe root and intermediate nodes represent the decisions while the leaf nodes represent the \\noutcomes. \\nID3 \\nID3 stands for Iterative Dichotomiser 3 and is named such because the algorithm iteratively \\n(repeatedly) dichotomizes(divides) features into two or more groups at each step.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 30}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n27 \\n \\nInvented by Ross Quinlan, ID3 uses a top-down greedy approach to build a decision tree. In \\nsimple words, the top-down approach means that we start building the tree from the top and \\nthe greedy approach means that at each iteration we select the best feature at the present \\nmoment to create a node. \\nMost generally ID3 is only used for classification problems with nominal features only. \\nID3 Steps \\n1. \\nCalculate the Information Gain of each feature. \\n2. Considering that all rows don’t belong to the same class, split the dataset S into subsets \\nusing the feature for which the Information Gain is maximum. \\n3. \\nMake a decision tree node using the feature with the maximum Information gain. \\n4. If all rows belong to the same class, make the current node as a leaf node with the classas \\nits label. \\n5. Repeat for the remaining features until we run out of all features, or the decision tree \\nhas all leaf nodes. \\nCART Algorithm \\nThe CART algorithm works via the following process: \\n\\uf0b7 \\nThe best split point of each input is obtained. \\n\\uf0b7 \\n \\nBased on the best split points of each input in Step 1, the new “best” split point is \\nidentified. \\n\\uf0b7 \\nSplit the chosen input according to the “best” split point. \\n\\uf0b7 \\n \\nContinue splitting until a stopping rule is satisfied or no further desirable splitting is \\navailable. \\n \\nCART algorithm uses Gini Impurity to split the dataset into a decision tree .It does that by \\nsearching for the best homogeneity for the sub nodes, with the help of the Gini index criterion. \\nGini index/Gini impurity \\nThe Gini index is a metric for the classification tasks in CART. It stores the sum of squared \\nprobabilities of each class. It computes the degree of probability of a specific variable that is \\nwrongly being classified when chosen randomly and a variation of the Gini coefficient. It works \\non categorical variables, provides outcomes either “successful” or “failure” and hence conducts \\nbinary splitting only. \\nThe degree of the Gini index varies from 0 to 1,'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 31}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n28 \\n \\n\\uf0b7 \\n \\nWhere 0 depicts that all the elements are allied to a certain class, or only one class exists \\nthere. \\n\\uf0b7 \\n \\nThe Gini index of value 1 signifies that all the elements are randomly distributed across \\nvarious classes, and \\n\\uf0b7 \\nA value of 0.5 denotes the elements are uniformly distributed into some classes. \\nClassification tree \\nA classification tree is an algorithm where the target variable is categorical. The algorithm is \\nthen used to identify the “Class” within which the target variable is most likely to fall. \\nClassification trees are used when the dataset needs to be split into classes that belong to the \\nresponse variable(like yes or no) \\nRegression tree \\nA Regression tree is an algorithm where the target variable is continuous and the tree is used \\nto predict its value. Regression trees are used when the response variable is continuous. For \\nexample, if the response variable is the temperature of the day. \\nPseudo-code of the CART algorithm \\nd = 0, endtree = 0 \\nNote(0) = 1, Node(1) = 0, Node(2) = 0 \\nwhile endtree < 1 \\nif Node(2d-1) + Node(2d) + ......+ Node(2d+1-2) = 2 - 2d+1 \\nendtree = 1 \\nelse \\ndo i = 2d-1, 2d, ......, 2d+1-2 \\nif Node(i) > -1 \\nSplit tree \\nelse \\nNode(2i+1) = -1 \\nNode(2i+2) = -1 \\nend if \\nend do \\nend if \\nd = d + 1 \\nend while \\nCART model representation'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 32}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n29 \\n \\nCART models are formed by picking input variables and evaluating split points on those \\nvariables until an appropriate tree is produced. \\nSteps to create a Decision Tree using the CART algorithm: \\n\\uf0b7 \\n \\nGreedy algorithm: In this The input space is divided using the Greedy method which \\nis known as a recursive binary spitting. This is a numerical method within which all of the \\nvalues are aligned and several other split points are tried and assessed using a cost function. \\n\\uf0b7 \\n \\nStopping Criterion: As it works its way down the tree with the training data, the \\nrecursive binary splitting method described above must know when to stop splitting. The \\nmost frequent halting method is to utilize a minimum amount of training data allocated to \\nevery leaf node. If the count is smaller than the specified threshold, the split is rejected and \\nalso the node is considered the last leaf node. \\n\\uf0b7 \\n \\nTree pruning: Decision tree’s complexity is defined as the number of splits in the tree. \\nTrees with fewer branches are recommended as they are simple to grasp and less prone to \\ncluster the data. Working through each leaf node in the tree and evaluating the effect of \\ndeleting it using a hold-out test set is the quickest and simplest pruning approach. \\n\\uf0b7 \\n \\nData preparation for the CART: No special data preparation is required for the CART \\nalgorithm. \\nNaïve Bayes Classifier Algorithm \\no Naïve Bayes algorithm is a supervised learning algorithm, which is based on Bayes \\ntheorem and used for solving classification problems. \\no It is mainly used in text classification that includes a high-dimensional training dataset. \\no Naïve Bayes Classifier is one of the simple and most effective Classification algorithms \\nwhich helps in building the fast machine learning models that can make quick \\npredictions. \\no It is a probabilistic classifier, which means it predicts on the basis of the \\nprobability of an object. \\no Some popular examples of Naïve Bayes Algorithm are spam filtration, Sentimental \\nanalysis, and classifying articles. \\nWhy is it called Naïve Bayes?'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 33}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n30 \\n \\nThe Naïve Bayes algorithm is comprised of two words Naïve and Bayes, Which can be \\ndescribed as: \\no Naïve: It is called Naïve because it assumes that the occurrence of a certain feature is \\nindependent of the occurrence of other features. Such as if the fruit is identified on the \\nbases of color, shape, and taste, then red, spherical, and sweet fruit is recognized as an \\napple. Hence each feature individually contributes to identify that it is an apple without \\ndepending on each other. \\no Bayes: It is called Bayes because it depends on the principle of Bayes\\' Theorem. \\nBayes\\' Theorem: \\no Bayes\\' theorem is also known as Bayes\\' Rule or Bayes\\' law, which is used to determine \\nthe probability of a hypothesis with prior knowledge. It depends on the conditional \\nprobability. \\no The formula for Bayes\\' theorem is given as: \\n \\n \\n \\nWhere, \\nP(A|B) is Posterior probability: Probability of hypothesis A on the observed event B. \\nP(B|A) is Likelihood probability: Probability of the evidence given that the probability of a \\nhypothesis is true. \\nP(A) is Prior Probability: Probability of hypothesis before observing the evidence. \\nP(B) is Marginal Probability: Probability of Evidence. \\nWorking of Naïve Bayes\\' Classifier: \\nWorking of Naïve Bayes\\' Classifier can be understood with the help of the below example: \\nSuppose we have a dataset of weather conditions and corresponding target variable \"Play\". \\nSo using this dataset we need to decide that whether we should play or not on a particular day \\naccording to the weather conditions. So to solve this problem, we need to follow the below \\nsteps: \\n1. Convert the given dataset into frequency tables. \\n2. Generate Likelihood table by finding the probabilities of given features. \\n3. Now, use Bayes theorem to calculate the posterior probability.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 34}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n31 \\n \\nK-Nearest Neighbor(KNN) Algorithm \\no K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on \\nSupervised Learning technique. \\no K-NN algorithm assumes the similarity between the new case/data and available cases \\nand put the new case into the category that is most similar to the available categories. \\no K-NN algorithm stores all the available data and classifies a new data point based on \\nthe similarity. This means when new data appears then it can be easily classified into \\na well suite category by using K- NN algorithm. \\no K-NN algorithm can be used for Regression as well as for Classification but mostly it \\nis used for the Classification problems. \\no K-NN is a non-parametric algorithm, which means it does not make any assumption \\non underlying data. \\no It is also called a lazy learner algorithm because it does not learn from the training set \\nimmediately instead it stores the dataset and at the time of classification, it performs an \\naction on the dataset. \\no KNN algorithm at the training phase just stores the dataset and when it gets new data, \\nthen it classifies that data into a category that is much similar to the new data. \\nWhy do we need a K-NN Algorithm? \\nSuppose there are two categories, i.e., Category A and Category B, and we have a new data \\npoint x1, so this data point will lie in which of these categories. To solve this type of problem, \\nwe need a K-NN algorithm. With the help of K-NN, we can easily identify the category or class \\nof a particular dataset. Consider the below diagram:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 35}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n32 \\n \\nHow does K-NN work? \\nThe K-NN working can be explained on the basis of the below algorithm: \\no Step-1: Select the number K of the neighbors \\no Step-2: Calculate the Euclidean distance of K number of neighbors \\no Step-3: Take the K nearest neighbors as per the calculated Euclidean distance. \\no Step-4: Among these k neighbors, count the number of the data points in each \\ncategory. \\no Step-5: Assign the new data points to that category for which the number of the \\nneighbor is maximum. \\no Step-6: Our model is ready. \\nSuppose we have a new data point and we need to put it in the required category. Consider \\nthe below image: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\no Firstly, we will choose the number of neighbors, so we will choose the k=5. \\no Next, we will calculate the Euclidean distance between the data points. The Euclidean \\ndistance is the distance between two points, which we have already studied in geometry. \\nIt can be calculated as:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 36}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n33 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\no By calculating the Euclidean distance we got the nearest neighbors, as three nearest \\nneighbors in category A and two nearest neighbors in category B. Consider the below \\nimage: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\no As we can see the 3 nearest neighbors are from category A, hence this new data point \\nmust belong to category A. \\nHow to select the value of K in the K-NN Algorithm? \\nBelow are some points to remember while selecting the value of K in the K-NN algorithm: \\no There is no particular way to determine the best value for \"K\", so we need to try some \\nvalues to find the best out of them. The most preferred value for K is 5.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 37}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n34 \\n \\no A very low value for K such as K=1 or K=2, can be noisy and lead to the effects of \\noutliers in the model. \\no Large values for K are good, but it may find some difficulties. \\nLogistic Regression \\no Logistic regression is one of the most popular Machine Learning algorithms, which \\ncomes under the Supervised Learning technique. It is used for predicting the categorical \\ndependent variable using a given set of independent variables. \\no Logistic regression predicts the output of a categorical dependent variable. Therefore \\nthe outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, \\ntrue or False, etc. but instead of giving the exact value as 0 and 1, it gives the \\nprobabilistic values which lie between 0 and 1. \\no Logistic Regression is much similar to the Linear Regression except that how they are \\nused. Linear Regression is used for solving Regression problems, whereas Logistic \\nregression is used for solving the classification problems. \\no In Logistic regression, instead of fitting a regression line, we fit an \"S\" shaped logistic \\nfunction, which predicts two maximum values (0 or 1). \\no The curve from the logistic function indicates the likelihood of something such as \\nwhether the cells are cancerous or not, a mouse is obese or not based on its weight, etc. \\no Logistic Regression is a significant machine learning algorithm because it has the \\nability to provide probabilities and classify new data using continuous and discrete \\ndatasets. \\no Logistic Regression can be used to classify the observations using different types of \\ndata and can easily determine the most effective variables used for the classification. \\nThe below image is showing the logistic function:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 38}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n35 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nLogistic Function (Sigmoid Function): \\no The sigmoid function is a mathematical function used to map the predicted values to \\nprobabilities. \\no It maps any real value into another value within a range of 0 and 1. \\no The value of the logistic regression must be between 0 and 1, which cannot go beyond \\nthis limit, so it forms a curve like the \"S\" form. The S-form curve is called the Sigmoid \\nfunction or the logistic function. \\no In logistic regression, we use the concept of the threshold value, which defines the \\nprobability of either 0 or 1. Such as values above the threshold value tends to 1, and a \\nvalue below the threshold values tends to 0. \\nAssumptions for Logistic Regression: \\no The dependent variable must be categorical in nature. \\no The independent variable should not have multi-collinearity. \\nLogistic Regression Equation: \\nThe Logistic regression equation can be obtained from the Linear Regression equation. The \\nmathematical steps to get Logistic Regression equations are given below: \\no We know the equation of the straight line can be written as: \\n \\n \\no In Logistic Regression y can be between 0 and 1 only, so for this let\\'s divide the above \\nequation by (1-y):'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 39}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n36 \\n \\no But we need range between -[infinity] to +[infinity], then take logarithm of the \\nequation it will become: \\n \\n \\n \\nThe above equation is the final equation for Logistic Regression. \\nType of Logistic Regression: \\nOn the basis of the categories, Logistic Regression can be classified into three types: \\no Binomial: In binomial Logistic regression, there can be only two possible types of the \\ndependent variables, such as 0 or 1, Pass or Fail, etc. \\no Multinomial: In multinomial Logistic regression, there can be 3 or more possible \\nunordered types of the dependent variable, such as \"cat\", \"dogs\", or \"sheep\" \\no Ordinal: In ordinal Logistic regression, there can be 3 or more possible ordered types \\nof dependent variables, such as \"low\", \"Medium\", or \"High\". \\nMultinomial Logistic Regression \\nMultinomial Logistic Regression is a classification technique that extends the logistic \\nregression algorithm to solve multiclass possible outcome problems, given one or more \\nindependent variables. \\nExample for Multinomial Logistic Regression: \\n(a) Which Flavor of ice cream will a person choose? \\nDependent Variable: \\n\\uf0b7 \\nVanilla \\n\\uf0b7 \\nChocolate \\n\\uf0b7 \\nButterscotch \\n\\uf0b7 \\nBlack Current \\nIndependent Variables: \\n\\uf0b7 \\nGender \\n\\uf0b7 \\nAge \\n\\uf0b7 \\nOccasion \\n\\uf0b7 \\nHappiness \\n\\uf0b7 \\nEtc. \\nMultinomial Logistic Regression is also known as multiclass logistic regression, softmax \\nregression, polytomous logistic regression, multinomial logit, maximum entropy (MaxEnt) \\nclassifier and conditional maximum entropy model.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 40}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n37 \\n \\nDependent Variable: \\nThe dependent Variable can have two or more possible outcomes/classes. \\nThe dependent variables are nominal in nature means there is no any kind of ordering in \\ntarget dependent classes i.e. these classes cannot be meaningfully ordered. \\nThe dependent variable to be predicted belongs to a limited set of items defined. \\nBasic Steps \\nThe basic steps of the SVM are: \\n1.  \\nselect two hyperplanes (in 2D) which separates the data with no points between \\nthem (red lines) \\n2. \\nmaximize their distance (the margin) \\n3.  \\nthe average line (here the line half way between the two red lines) will be the decision \\nboundary \\nThis is very nice and easy, but finding the best margin, the optimization problem is not trivial \\n(it is easy in 2D, when we have only two attributes, but what if we have N dimensions with N \\na very big number). \\nNon-Linear SVM: \\nIf data is linearly arranged, then we can separate it by using a straight line, but for non-linear \\ndata, we cannot draw a single straight line. Consider the below image:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 41}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n38 \\n \\nSo to separate these data points, we need to add one more dimension. For linear data, we have \\nused two dimensions x and y, so for non-linear data, we will add a third dimension z. It can \\nbe calculated as: \\nz=x2 +y2 \\nBy adding the third dimension, the sample space will become as below image: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSo now, SVM will divide the datasets into classes in the following way. Consider the below \\nimage:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 42}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n39 \\n \\nSince we are in 3-d Space, hence it is looking like a plane parallel to the x-axis. If we convert \\nit in 2d space with z=1, then it will become as: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nHence we get a circumference of radius 1 in case of non-linear data. \\nKernel Methods \\nKernels or kernel methods (also called Kernel functions) are sets of different types of \\nalgorithms that are being used for pattern analysis. They are used to solve a non-linear problem \\nby using a linear classifier. Kernels Methods are employed in SVM (Support Vector Machines) \\nwhich are used in classification and regression problems. The SVM uses what is called a \\n“Kernel Trick” where the data is transformed and an optimal boundary is found for the possible \\noutputs. \\nThe Need for Kernel Method and its Working \\nBefore we get into the working of the Kernel Methods, it is more important to understand \\nsupport vector machines or the SVMs because kernels are implemented in SVM models. So, \\nSupport Vector Machines are supervised machine learning algorithms that are used in \\nclassification and regression problems such as classifying an apple to class fruit while \\nclassifying a Lion to the class animal. \\nwe have 2 dimension which represents the ambient space but the lone which divides or \\nclassifies the space is one dimension less than the ambient space and is called hyperplane. \\nBut what if we have input like this: \\nIt is very difficult to solve this classification using a linear classifier as there is no good linear \\nline that should be able to classify the red and the green dots as the points are randomly'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 43}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n40 \\n \\ndistributed. Here comes the use of kernel function which takes the points to higher dimensions, \\nsolves the problem over there and returns the output. Think of this in this way, we can see that \\nthe green dots are enclosed in some perimeter area while the red one lies outside it, likewise, \\nthere could be other scenarios where green dots might be distributed in a trapezoid-shaped area. \\nSo what we do is to convert the two-dimensional plane which was first classified by one- \\ndimensional hyperplane (“or a straight line”) to the three-dimensional area and here our \\nclassifier i.e. hyperplane will not be a straight line but a two-dimensional plane which will cut \\nthe area. \\nIn order to get a mathematical understanding of kernel, let us understand the Lili Jiang’s \\nequation of kernel which is: \\nK(x, y)=<f(x), f(y)> where, \\nK is the kernel function, \\nX and Y are the dimensional inputs, \\nf is the map from n-dimensional to m-dimensional space and, \\n< x, y > is the dot product.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 44}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n41 \\n \\nUNIT – III \\n \\n \\nNeural Network Representation \\n \\nThe term \" Neural Network\" is derived from Biological neural networks that develop the \\nstructure of a human brain. Similar to the human brain that has neurons interconnected to one \\nanother, artificial neural networks also have neurons that are interconnected to one another in \\nvarious layers of the networks. These neurons are known as nodes. \\nDendrites from Biological Neural Network represent inputs in Artificial Neural Networks, \\ncell nucleus represents Nodes, synapse represents Weights, and Axon represents Output. \\nRelationship between Biological neural network and artificial neural network: \\n \\nAn Artificial Neural Network in the field of Artificial intelligence where it attempts to \\nmimic the network of neurons makes up a human brain so that computers will have an option \\nto understand things and make decisions in a human-like manner. The artificial neural \\nnetwork is designed by programming computers to behave simply like interconnected brain \\ncells. \\n \\nThere are around 1000 billion neurons in the human brain. Each neuron has an association \\npoint somewhere in the range of 1,000 and 100,000. In the human brain, data is stored in such \\na manner as to be distributed, and we can extract more than one piece of this data when \\nnecessary from our memory parallelly. We can say that the human brain is made up of \\nincredibly amazing parallel processors. \\n \\nWe can understand the artificial neural network with an example, consider an example of a \\ndigital logic gate that takes an input and gives an output. \"OR\" gate, which takes two inputs. \\nIf one or both the inputs are \"On,\" then we get \"On\" in output. If both the inputs are \"Off,\" \\nthen we get \"Off\" in output. Here the output depends upon input. Our brain does not perform \\nSupervised Learning – II (Neural Networks) \\nNeural Network Representation – Problems – Perceptrons , Activation \\nFunctions, ArtificialNeural Networks (ANN) , Back Propagation Algorithm. \\nClassification Metrics: Confusion matrix, Precision, Recall, Accuracy, F-Score, \\nROC curves.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 45}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n42 \\n \\nthe same task. The outputs to inputs relationship keep changing because of the neurons in our \\nbrain, which are \"learning.\" \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThe architecture of an artificial neural network: \\n \\nTo understand the concept of the architecture of an artificial neural network, we have to \\nunderstand what a neural network consists of. In order to define a neural network that consists \\nof a large number of artificial neurons, which are termed units arranged in a sequence of layers. \\nLets us look at various types of layers available in an artificial neural network. \\n \\nArtificial Neural Network primarily consists of three layers: \\n \\nInput Layer: \\n \\nAs the name suggests, it accepts inputs in several different formats provided by the \\nprogrammer. \\n \\nHidden Layer: \\n \\nThe hidden layer presents in-between input and output layers. It performs all the calculations \\nto find hidden features and patterns. \\nOutput Layer:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 46}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n43 \\n \\nThe input goes through a series of transformations using the hidden layer, which finally \\nresults in output that is conveyed using this layer. \\nThe artificial neural network takes input and computes the weighted sum of the inputs and \\nincludes a bias. This computation is represented in the form of a transfer function. \\n \\n \\n \\n \\n \\nIt determines weighted total is passed as an input to an activation function to produce the \\noutput. Activation functions choose whether a node should fire or not. Only those who are \\nfired make it to the output layer. There are distinctive activation functions available that can \\nbe applied upon the sort of task we are performing. \\nPerceptrons \\n \\nPerceptron is a building block of an Artificial Neural Network. Initially, in the mid of 19th \\ncentury, Mr. Frank Rosenblatt invented the Perceptron for performing certain calculations \\nto detect input data capabilities or business intelligence. Perceptron is a linear Machine \\nLearning algorithm used for supervised learning for various binary classifiers. This algorithm \\nenables neurons to learn elements and processes them one by one during preparation. In this \\ntutorial, \"Perceptron in Machine Learning,\" we will discuss in-depth knowledge of Perceptron \\nand its basic functions in brief. Let\\'s start with the basic introduction of Perceptron.. \\n \\nPerceptron is Machine Learning algorithm for supervised learning of various binary \\nclassification tasks. Further, Perceptron is also understood as an Artificial Neuron or neural \\nnetwork unit that helps to detect certain input data computations in businessintelligence. \\n \\nPerceptron model is also treated as one of the best and simplest types of Artificial Neural \\nnetworks. However, it is a supervised learning algorithm of binary classifiers. Hence, we can \\nconsider it as a single-layer neural network with four main parameters, i.e., input values, \\nweights and Bias, net sum, and an activation function. \\n \\nBasic Components of Perceptron'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 47}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n44 \\n \\nMr. Frank Rosenblatt invented the perceptron model as a binary classifier which contains \\nthree main components. These are as follows: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\uf0b7 \\nInput Nodes or Input Layer: \\n \\nThis is the primary component of Perceptron which accepts the initial data into the system for \\nfurther processing. Each input node contains a real numerical value. \\n\\uf0b7 \\nWight and Bias: \\n \\nWeight parameter represents the strength of the connection between units. This is another \\nmost important parameter of Perceptron components. Weight is directly proportional to the \\nstrength of the associated input neuron in deciding the output. Further, Bias can be considered \\nas the line of intercept in a linear equation. \\n \\n\\uf0b7 \\nActivation Function: \\n \\nThese are the final and important components that help to determine whether the neuron will \\nfire or not. Activation Function can be considered primarily as a step function. \\nTypes of Activation functions: \\n \\n\\uf0b7 \\nSign function \\n\\uf0b7 \\nStep function, and'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 48}, page_content=\"CSIT DEPT-R22-MACHINE LEARNING \\n45 \\n \\n\\uf0b7 \\nSigmoid function \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThe data scientist uses the activation function to take a subjective decision based on various \\nproblem statements and forms the desired outputs. Activation function may differ (e.g., Sign, \\nStep, and Sigmoid) in perceptron models by checking whether the learning process is slow or \\nhas vanishing or exploding gradients. \\n \\nHow does Perceptron work? \\n \\nIn Machine Learning, Perceptron is considered as a single-layer neural network that consists \\nof four main parameters named input values (Input nodes), weights and Bias, net sum, and an \\nactivation function. The perceptron model begins with the multiplication of all input values and \\ntheir weights, then adds these values together to create the weighted sum. Then this weighted \\nsum is applied to the activation function 'f' to obtain the desired output. This activation function \\nis also known as the step function and is represented by 'f'.\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 49}, page_content=\"CSIT DEPT-R22-MACHINE LEARNING \\n46 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThis step function or Activation function plays a vital role in ensuring that output is mapped \\nbetween required values (0,1) or (-1,1). It is important to note that the weight of input is \\nindicative of the strength of a node. Similarly, an input's bias value gives the ability to shift \\nthe activation function curve up or down. \\nPerceptron model works in two important steps as follows: \\n \\nStep-1 \\n \\nIn the first step first, multiply all input values with corresponding weight values and then add \\nthem to determine the weighted sum. Mathematically, we can calculate the weighted sum as \\nfollows: \\n∑wi*xi = x1*w1 + x2*w2 +…wn*xn \\n \\nAdd a special term called bias 'b' to this weighted sum to improve the model's performance. \\n \\n∑wi*xi + b \\nStep-2 \\nIn the second step, an activation function is applied with the above-mentioned weighted sum, \\nwhich gives us output either in binary form or a continuous value as follows:\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 50}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n47 \\n \\nY = f(∑wi*xi + b) \\n \\n \\n \\n \\nTypes of Perceptron Models \\n \\nBased on the layers, Perceptron models are divided into two types. These are as follows: \\n \\n1. Single-layer Perceptron Model \\n2. Multi-layer Perceptron model \\n \\nSingle Layer Perceptron Model: \\n \\nThis is one of the easiest Artificial neural networks (ANN) types. A single-layered perceptron \\nmodel consists feed-forward network and also includes a threshold transfer function inside \\nthe model. The main objective of the single-layer perceptron model is to analyze the linearly \\nseparable objects with binary outcomes. \\n \\nIn a single layer perceptron model, its algorithms do not contain recorded data, so it begins \\nwith inconstantly allocated input for weight parameters. Further, it sums up all inputs \\n(weight). After adding all inputs, if the total sum of all inputs is more than a pre-determined \\nvalue, the model gets activated and shows the output value as +1. \\nIf the outcome is same as pre-determined or threshold value, then the performance of this \\nmodel is stated as satisfied, and weight demand does not change. However, this model \\nconsists of a few discrepancies triggered when multiple weight inputs values are fed into the \\nmodel. Hence, to find desired output and minimize errors, some changes should be necessary \\nfor the weights input. \\n\"Single-layer perceptron can learn only linearly separable patterns.\". \\n \\nMulti-Layered Perceptron Model: \\n \\nLike a single-layer perceptron model, a multi-layer perceptron model also has the same \\nmodel structure but has a greater number of hidden layers.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 51}, page_content=\"CSIT DEPT-R22-MACHINE LEARNING \\n48 \\n \\nThe multi-layer perceptron model is also known as the Backpropagation algorithm, which \\nexecutes in two stages as follows: \\n\\uf0b7 \\nForward Stage: Activation functions start from the input layer in the forward stage \\nand terminate on the output layer. \\n\\uf0b7 \\nBackward Stage: In the backward stage, weight and bias values are modified as per \\nthe model's requirement. In this stage, the error between actual output and demanded \\noriginated backward on the output layer and ended on the input layer. \\nHence, a multi-layered perceptron model has considered as multiple artificial neural networks \\nhaving various layers in which activation function does not remain linear, similar to a single \\nlayer perceptron model. Instead of linear, activation function can be executed as sigmoid, \\nTanH, ReLU, etc., for deployment. \\nA multi-layer perceptron model has greater processing power and can process linear and non- \\nlinear patterns. Further, it can also implement logic gates such as AND, OR, XOR, NAND, \\nNOT, XNOR, NOR. \\n \\nAdvantages of Multi-Layer Perceptron: \\n \\n\\uf0b7 \\nA multi-layered perceptron model can be used to solve complex non-linear problems. \\n\\uf0b7 \\nIt works well with both small and large input data. \\n\\uf0b7 \\nIt helps us to obtain quick predictions after the training. \\n\\uf0b7 \\nIt helps to obtain the same accuracy ratio with large as well as small data. \\n \\nDisadvantages of Multi-Layer Perceptron: \\n \\n\\uf0b7 \\nIn Multi-layer perceptron, computations are difficult and time-consuming. \\n\\uf0b7 \\nIn multi-layer Perceptron, it is difficult to predict how much the dependent variable \\naffects each independent variable. \\n\\uf0b7 \\nThe model functioning depends on the quality of the training. \\n \\nPerceptron Function \\n \\nPerceptron function ''f(x)'' can be achieved as output by multiplying the input 'x' with the \\nlearned weight coefficient 'w'.\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 52}, page_content=\"CSIT DEPT-R22-MACHINE LEARNING \\n49 \\n \\nMathematically, we can express it as follows: \\n \\nf(x)=1; if w.x+b>0 \\notherwise, f(x)=0 \\n\\uf0b7 \\n'w' represents real-valued weights vector \\n\\uf0b7 \\n'b' represents the bias \\n\\uf0b7 \\n'x' represents a vector of input x values. \\n \\nCharacteristics of Perceptron \\n \\nThe perceptron model has the following characteristics. \\n \\n1. Perceptron is a machine learning algorithm for supervised learning of binary \\nclassifiers. \\n2. In Perceptron, the weight coefficient is automatically learned. \\n3. Initially, weights are multiplied with input features, and the decision is made whether \\nthe neuron is fired or not. \\n4. The activation function applies a step rule to check whether the weight function is \\ngreater than zero. \\n5. The linear decision boundary is drawn, enabling the distinction between the two \\nlinearly separable classes +1 and -1. \\n6. If the added sum of all input values is more than the threshold value, it must have an \\noutput signal; otherwise, no output will be shown. \\n \\nLimitations of Perceptron Model \\n \\nA perceptron model has limitations as follows: \\n \\n\\uf0b7 \\nThe output of a perceptron can only be a binary number (0 or 1) due to the hard limit \\ntransfer function. \\n\\uf0b7 \\nPerceptron can only be used to classify the linearly separable sets of input vectors. If \\ninput vectors are non-linear, it is not easy to classify them properly. \\nActivation Functions\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 53}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n50 \\n \\nActivation function also helps to normalize the output of any input in the range between 1 to \\n-1. Activation function must be efficient and it should reduce the computation time because \\nthe neural network sometimes trained on millions of data points. \\n \\nWithout an activation function, a neural network will become a linear regression model. But \\nintroducing the activation function the neural network will perform a non-linear \\ntransformation to the input and will be suitable to solve problems like image classification, \\nsentence prediction, or langue translation. \\nThe neuron is basically is a weighted average of input, then this sum is passed through an \\nactivation function to get an output. \\nY = ∑ (weights*input + bias) \\n \\nHere Y can be anything for a neuron between range -infinity to +infinity. So, we have to \\nbound our output to get the desired prediction or generalized results. \\n \\n \\n \\nY = Activation function(∑ (weights*input + bias)) \\n \\n \\n \\nSo, we pass that neuron to activation function to bound output values. \\nWhy do we need Activation Functions \\nWithout activation function, weight and bias would only have a linear transformation, or neural \\nnetwork is just a linear regression model, a linear equation is polynomial of one degreeonly \\nwhich is simple to solve but limited in terms of ability to solve complex problems or higher \\ndegree polynomials. \\nBut opposite to that, the addition of activation function to neural network executes the non- \\nlinear transformation to input and make it capable to solve complex problems such as language \\ntranslations and image classifications.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 54}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n51 \\n \\nIn addition to that, Activation functions are differentiable due to which they can easily \\nimplement back propagations, optimized strategy while performing backpropagations to \\nmeasure gradient loss functions in the neural networks. \\nTypes of Activation Functions \\n \\nThe two main categories of activation functions are: \\n \\n\\uf0b7 \\nLinear Activation Function \\n\\uf0b7 \\nNon-linear Activation Functions \\n \\n Linear Neural Network Activation Function \\n \\nLinear Activation Function \\n \\nEquation: A linear function\\'s equation, which is y = x, is similar to the eqn of a single \\ndirection. \\nThe ultimate activation function of the last layer is nothing more than a linear function of \\ninput from the first layer, regardless of how many levels we have if they are all linear in \\nnature. -inf to +inf is the range. \\n \\nUses: The output layer is the only location where the activation function\\'s function is applied. \\n \\nIf we separate a linear function to add non-linearity, the outcome will no longer depend on \\nthe input \"x,\" the function will become fixed, and our algorithm won\\'t exhibit any novel \\nbehaviour. \\nA good example of a regression problem is determining the cost of a house. We can use \\nlinear activation at the output layer since the price of a house may have any huge or little \\nvalue. The neural network\\'s hidden layers must perform some sort of non-linear function \\neven in this circumstance.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 55}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n52 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nEquation : f(x) = x \\n \\nRange : (-infinity to infinity) \\n \\nIt doesn’t help with the complexity or various parameters of usual data that is fed to the \\nneural networks. \\n Non Linear Neural Network Activation Function \\n \\n1. Sigmoid or Logistic Activation Function'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 56}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n53 \\n \\nIt is a functional that is graphed in a \"S\" shape. \\nA is equal to 1/(1 + e-x). \\nNon-linear in nature. Observe that while Y values are fairly steep, X values range from -2 to \\n2. To put it another way, small changes in x also would cause significant shifts in the value of \\nY. spans from 0 to 1. \\n \\nUses: Sigmoid function is typically employed in the output nodes of a classi?cation, where \\nthe result may only be either 0 or 1. Since the value for the sigmoid function only ranges \\nfrom 0 to 1, the result can be easily anticipated to be 1 if the value is more than 0.5 and 0 if it \\nis not. \\n\\uf0b7 \\nIt is a function which is plotted as ‘S’ shaped graph. \\n\\uf0b7 \\nEquation : A = 1/(1 + e-x) \\n\\uf0b7 \\nNature : Non-linear. Notice that X values lies between -2 to 2, Y values are very \\nsteep. This means, small changes in x would also bring about large changes in the \\nvalue of Y. \\n\\uf0b7 \\nValue Range : 0 to 1 \\n\\uf0b7 \\nUses : Usually used in output layer of a binary classification, where result is either 0 \\nor 1, as value for sigmoid function lies between 0 and 1 only so, result can be \\npredicted easily to be 1 if value is greater than 0.5 and 0 otherwise. \\n \\nTanh Function \\n \\nThe activation that consistently outperforms sigmoid function is known as tangent hyperbolic \\nfunction. It\\'s actually a sigmoid function that has been mathematically adjusted. Both are \\ncomparable to and derivable from one another. \\n \\n \\n \\n \\n \\n \\n \\nRange of values: -1 to +1. non-linear nature'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 57}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n54 \\n \\nUses: - Since its values typically range from -1 to 1, the mean again for hidden layer of a \\nneural network will be 0 or very near to it. This helps to centre the data by getting the mean \\nclose to 0. This greatly facilitates learning for the following layer. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\uf0b7 \\nThe activation that works almost always better than sigmoid function is Tanh function \\nalso known as Tangent Hyperbolic function. It’s actually mathematically shifted \\nversion of the sigmoid function. Both are similar and can be derived from each other. \\n\\uf0b7 \\nEquation :- \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\uf0b7 \\nValue Range :- -1 to +1 \\n\\uf0b7 \\nNature :- non-linear \\n\\uf0b7 \\nUses :- Usually used in hidden layers of a neural network as it’s values lies between - \\n1 to 1 hence the mean for the hidden layer comes out be 0 or very close to it, hence \\nhelps in centering the data by bringing mean close to 0. This makes learning for the \\nnext layer much easier.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 58}, page_content=\"CSIT DEPT-R22-MACHINE LEARNING \\n55 \\n \\nReLU (Rectified Linear Unit) Activation Function \\n \\nCurrently, the ReLU is the activation function that is employed the most globally. Since \\npractically all convolutional neural networks and deep learning systems employ it. \\nThe derivative and the function are both monotonic. \\n \\nHowever, the problem is that all negative values instantly become zero, which reduces the \\nmodel's capacity to effectively fit or learn from the data. This means that any negative input \\nto a ReLU activation function immediately becomes zero in the graph, which has an impact \\non the final graph by improperly mapping the negative values. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\uf0b7 \\nIt Stands for Rectified linear unit. It is the most widely used activation function. \\nChiefly implemented in hidden layers of Neural network. \\n\\uf0b7 \\nEquation :- A(x) = max(0,x). It gives an output x if x is positive and 0 otherwise. \\n\\uf0b7 \\nValue Range :- [0, inf) \\n\\uf0b7 \\nNature :- non-linear, which means we can easily backpropagate the errors and have \\nmultiple layers of neurons being activated by the ReLU function. \\n\\uf0b7 \\nUses :- ReLu is less computationally expensive than tanh and sigmoid because it \\ninvolves simpler mathematical operations. At a time only a few neurons are activated \\nmaking the network sparse making it efficient and easy for computation. \\nIn simple words, RELU learns much faster than sigmoid and Tanh function. \\n \\nSoftmax Function\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 59}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n56 \\n \\nAlthough it is a subclass of the sigmoid function, the softmax function comes in handy when \\ndealing with multiclass classification issues. \\nUsed frequently when managing several classes. In the output nodes of image classification \\nissues, the softmax was typically present. The softmax function would split by the sum of the \\noutputs and squeeze all outputs for each category between 0 and 1. \\n \\nThe output unit of the classifier, where we are actually attempting to obtain the probabilities \\nto determine the class of each input, is where the softmax function is best applied. \\nThe usual rule of thumb is to utilise RELU, which is a usual perceptron in hidden layers and \\nis employed in the majority of cases these days, if we really are unsure of what encoder to \\napply. \\nA very logical choice for the output layer is the sigmoid function if your input is for binary \\nclassification. If our output involves multiple classes, Softmax can be quite helpful in \\npredicting the odds for each class. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThe softmax function is also a type of sigmoid function but is handy when we are trying to \\nhandle multi- class classification problems. \\n\\uf0b7 \\nNature :- non-linear \\n\\uf0b7 \\nUses :- Usually used when trying to handle multiple classes. the softmax function was \\ncommonly found in the output layer of image classification problems.The softmax'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 60}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n57 \\n \\nfunction would squeeze the outputs for each class between 0 and 1 and would also \\ndivide by the sum of the outputs. \\n\\uf0b7 \\nOutput:- The softmax function is ideally used in the output layer of the classifier \\nwhere we are actually trying to attain the probabilities to define the class of each \\ninput. \\n\\uf0b7 \\nThe basic rule of thumb is if you really don’t know what activation function to use, \\nthen simply use RELU as it is a general activation function in hidden layers and is \\nused in most cases these days. \\n\\uf0b7 \\nIf your output is for binary classification then, sigmoid function is very natural choice \\nfor output layer. \\n\\uf0b7 \\nIf your output is for multi-class classification then, Softmax is very useful to predict \\nthe probabilities of each classes. \\nArtificial Neural Networks (ANN) \\n \\nArtificial Neural Networks (ANN) are algorithms based on brain function and are used to \\nmodel complicated patterns and forecast issues. The Artificial Neural Network (ANN) is a deep \\nlearning method that arose from the concept of the human brain Biological Neural Networks. \\nThe development of ANN was the result of an attempt to replicate the workings of the human \\nbrain. The workings of ANN are extremely similar to those of biological neural networks, \\nalthough they are not identical. ANN algorithm accepts only numeric and structured data. \\n \\nArtificial Neural Networks Architecture \\n \\n1. There are three layers in the network architecture: the input layer, the hidden layer (more \\nthan one), and the output layer. Because of the numerous layers are sometimes referred to as \\nthe MLP (Multi-Layer Perceptron).'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 61}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n58 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n2. It is possible to think of the hidden layer as a “distillation layer,” which extracts some of \\nthe most relevant patterns from the inputs and sends them on to the next layer for further \\nanalysis. It accelerates and improves the efficiency of the network by recognizing just the \\nmost important information from the inputs and discarding the redundant information. \\n \\n3. The activation function is important for two reasons: \\n \\n\\uf0b7 \\nThis model captures the presence of non-linear relationships between the inputs. \\n\\uf0b7 \\nIt contributes to the conversion of the input into a more usable output.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 62}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n59 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n4. Finding the “optimal values of W — weights” that minimize prediction error is critical to \\nbuilding a successful model. The “backpropagation algorithm” does this by converting ANN \\ninto a learning algorithm by learning from mistakes. \\n5. The optimization approach uses a “gradient descent” technique to quantify prediction \\nerrors. To find the optimum value for W, small adjustments in W are tried, and the impact on \\nprediction errors is examined. Finally, those W values are chosen as ideal since further W \\nchanges do not reduce mistakes. \\nHow artificial neural networks functions \\n \\nThe core component of ANNs is artificial neurons. Each neuron receives inputs from several \\nother neurons, multiplies them by assigned weights, adds them and passes the sum to one or \\nmore neurons. Some artificial neurons might apply an activation function to the output before \\npassing it to the next variable.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 63}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n60 \\n \\nAt its core, this might sound like a very trivial math operation. But when you place hundreds, \\nthousands and millions of neurons in multiple layers and stack them up on top of each other, \\nyou’ll obtain an artificial neural network that can perform very complicated tasks, such as \\nclassifying images or recognizing speech. \\nArtificial neural networks are composed of an input layer, which receives data from outside \\nsources (data files, images, hardware sensors, microphone…), one or more hidden layers that \\nprocess the data, and an output layer that provides one or more data points based on the function \\nof the network. For instance, a neural network that detects persons, cars and animals will have \\nan output layer with three nodes. A network that classifies bank transactions between safe and \\nfraudulent will have a single output. \\nTraining artificial neural networks \\n \\nArtificial neural networks start by assigning random values to the weights of the connections \\nbetween neurons. The key for the ANN to perform its task correctly and accurately is to adjust \\nthese weights to the right numbers. But finding the right weights is not very easy, especially \\nwhen you’re dealing with multiple layers and thousands of neurons. \\n \\nThis calibration is done by “training” the network with annotated examples. For instance, if \\nyou want to train the image classifier mentioned above, you provide it with multiple photos, \\neach labeled with its corresponding class (person, car or animal). As you provide it with more \\nand more training examples, the neural network gradually adjusts its weights to map each input \\nto the correct outputs. \\n \\nBasically, what happens during training is the network adjust itself to glean specific patterns \\nfrom the data. Again, in the case of an image classifier network, when you train the AI model \\nwith quality examples, each layer detects a specific class of features. For instance, the first \\nlayer might detect horizontal and vertical edges, the next layers might detect corners and round \\nshapes. Further down the network, deeper layers will start to pick out more advanced features \\nsuch as faces and objects.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 64}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n61 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nWhen you run a new image through a well-trained neural network, the adjusted weights of \\nthe neurons will be able to extract the right features and determine with accuracy to which \\noutput class the image belongs. \\n \\nOne of the challenges of training neural networks is to find the right amount and quality of \\ntraining examples. Also, training large AI models requires vast amounts of computing \\nresources. To overcome this challenge, many engineers use “transfer learning,” a training \\ntechnique where you take a pre-trained model and fine-tune it with new, domain-specific \\nexamples. Transfer learning is especially efficient when there’s already an AI model that is \\nclose to your use case.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 65}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n62 \\n \\nBackpropagation Algorithm \\n \\nBackpropagation is an algorithm that backpropagates the errors from the output nodes to the \\ninput nodes. Therefore, it is simply referred to as the backward propagation of errors. It uses in \\nthe vast applications of neural networks in data mining like Character recognition, Signature \\nverification, etc. \\nBackpropagation is the essence of neural network training. It is the method of fine-tuning \\nthe weights of a neural network based on the error rate obtained in the previous epoch (i.e., \\niteration). Proper tuning of the weights allows you to reduce error rates and make the model \\nreliable by increasing its generalization. \\n \\nHow Backpropagation Algorithm Works \\n \\nThe Back propagation algorithm in neural network computes the gradient of the loss function \\nfor a single weight by the chain rule. It efficiently computes one layer at a time, unlike a \\nnative direct computation. It computes the gradient, but it does not define how the gradient is \\nused. It generalizes the computation in the delta rule. \\nConsider the following Back propagation neural network example diagram to understand:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 66}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n63 \\n \\nBackpropagation Algorithm: \\nStep 1: Inputs X, arrive through the preconnected path. \\nStep 2: The input is modeled using true weights W. Weights are usually chosen randomly. \\nStep 3: Calculate the output of each neuron from the input layer to the hidden layer to the \\noutput layer. \\nStep 4: Calculate the error in the outputs \\nBackpropagation Error= Actual Output – Desired Output \\nStep 5: From the output layer, go back to the hidden layer to adjust the weights to reduce the \\nerror. \\nStep 6: Repeat the process until the desired output is achieved. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTraining Algorithm : \\n \\nStep 1: Initialize weight to small random values.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 67}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n64 \\n \\nStep 2: While the stepsstopping condition is to be false do step 3 to 10. \\n \\nStep 3: For each training pair do step 4 to 9 (Feed-Forward). \\n \\nStep 4: Each input unit receives the signal unit and transmitsthe signal xi signal to all the \\nunits. \\nStep 5 : Each hidden unit Zj (z=1 to a) sums its weighted input signal to calculate its net \\ninput \\n \\nzinj = v0j + Σxivij \\n( i=1 to n) \\n \\nApplying activation function zj = f(zinj) and sends this signals to all units in the layer \\nabout i.e output units \\n \\nFor each output l=unit yk = (k=1 to m) sums its weighted input signals. \\nyink = w0k + Σ ziwjk (j=1 to a) \\nand applies its activation function to calculate the output signals. \\nyk = f(yink) \\nBackpropagation Error : \\n \\nStep 6: Each output unit yk (k=1 to n) receives a target pattern corresponding to an input \\npattern then error is calculated as: \\nδk = ( tk – yk ) + yink \\n \\nStep 7: Each hidden unit Zj (j=1 to a) sums its input from all units in the layer above \\nδinj = Σ δj wjk \\nThe error information term is calculated as : \\nδj = δinj + zinj \\nUpdation of weight and bias :'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 68}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n65 \\n \\nStep 8: Each output unit yk (k=1 to m) updates its bias and weight (j=1 to a). The weight \\ncorrection term is given by : \\nΔ wjk = α δk zj \\n \\nand the bias correction term is given by Δwk = α δk. \\ntherefore \\nwjk(new) = wjk(old) + Δ wjk \\nw0k(new) = wok(old) + Δ wok \\n \\n \\nfor each hidden unit zj (j=1 to a) update its bias and weights (i=0 to n) the weight \\nconnection term \\n \\nΔ vij = α δj xi \\n \\nand the bias connection on term \\n \\nΔ v0j = α δj \\nTherefore vij(new) = vij(old) + Δvij \\nv0j(new) = v0j(old) + Δv0j \\n \\n \\nStep 9: Test the stopping condition. The stopping condition can be the minimization of error, \\nnumber of epochs. \\n \\nNeed for Backpropagation: \\nBackpropagation is “backpropagation of errors” and is very useful for training neural networks. \\nIt’s fast, easy to implement, and simple. Backpropagation does not require any parameters to \\nbe set, except the number of inputs. Backpropagation is a flexible method because no prior \\nknowledge of the network is required. \\nTypes of Backpropagation \\n \\nThere are two types of backpropagation networks.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 69}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n66 \\n \\n\\uf0b7 \\nStatic backpropagation: Static backpropagation is a network designed to map static \\ninputs for static outputs. These types of networks are capable of solving static \\nclassification problems such as OCR (Optical Character Recognition). \\n\\uf0b7 \\nRecurrent backpropagation: Recursive backpropagation is another network used for \\nfixed-point learning. Activation in recurrent backpropagation is feed-forward until a \\nfixed value is reached. Static backpropagation provides an instant mapping, while \\nrecurrent backpropagation does not provide an instant mapping. \\n \\nAdvantages: \\n \\n\\uf0b7 \\nIt is simple, fast, and easy to program. \\n\\uf0b7 \\nOnly numbers of the input are tuned, not any other parameter. \\n\\uf0b7 \\nIt is Flexible and efficient. \\n\\uf0b7 \\nNo need for users to learn any special functions. \\n \\nDisadvantages: \\n \\n\\uf0b7 \\nIt is sensitive to noisy data and irregularities. Noisy data can lead to inaccurate results. \\n\\uf0b7 \\nPerformance is highly dependent on input data. \\n\\uf0b7 \\nSpending too much time training. \\n\\uf0b7 \\nThe matrix-based approach is preferred over a mini-batch.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 70}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n67 \\n \\nClassification metrics: A confusion matrix is a matrix that summarizes the \\nperformance of a machine learningmodel on a set of test data. It is a means of displaying \\nthe number of accurate and inaccurate instances based on the model’s predictions. It is \\noften used to measure the performance of classification models, which aim to predict a \\ncategorical label for each input instance. \\nThe matrix displays the number of instances produced by the model on the test data. \\n\\uf0b7 \\n \\nTrue positives (TP): occur when the model accurately predicts a positive data \\npoint. \\n\\uf0b7 \\n \\nTrue negatives (TN): occur when the model accurately predicts a negative data \\npoint. \\n\\uf0b7 \\n \\nFalse positives (FP): occur when the model predicts a positive data point \\nincorrectly. \\n\\uf0b7 \\nFalse negatives (FN): occur when the model mispredicts a negative data point. \\n \\nPrecision \\n\\uf0b7 \\n \\nThe ratio of correctly predicted positive observations to all predicted positives is \\nknown as precision. \\n\\uf0b7 \\nIt gauges how well the model forecasts the positive outcomes. \\nRecall \\n\\uf0b7 \\n \\nThe ratio of correctly predicted positive observations to the total number of actual \\npositive observations is known as recall. \\n\\uf0b7 \\nIt gauges how well the model can capture each pertinent instance. \\nAccuracy: One of the more obvious metrics, it is the measure of all the correctly identified \\ncases. It is most used when all the classes are equally important. \\n \\n \\n \\nF1-score: This is the harmonic mean of Precision and Recall and gives a better measure of the \\nincorrectly classified cases than the Accuracy Metric. \\n \\n \\n \\n \\nThe Receiver Operating Characteristic (ROC) Curve is a graphical representation used to evaluate \\nthe performance of a binary classification model. It plots the trade-off between sensitivity (True \\nPositive Rate) and 1-specificity (False Positive Rate) at different threshold settings.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 71}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n68 \\n \\n \\nUNIT-IV \\n \\n \\n         CROSS VALIDATION \\nTo test the performance of a classifier, we need to have a number of training/validation set \\npairs from a dataset X. To get them, if the sample X is large enough, we can randomly divide \\nit then divide each part randomly into two and use one half for training and the other half for \\nvalidation. Unfortunately, datasets are never large enough to do this. So, we use the same data \\nsplit differently; this is called cross-validation. \\nCross-validation is a technique to evaluate predictive models by partitioning the original \\nsample into a training set to train the model, and a test set to evaluate it. \\n \\nDuring the evaluation of machine learning (ML) models, the following question might arise: \\n \\n\\uf0b7 \\n \\nIs this model the best one available from the hypothesis space of the algorithm in terms \\nof generalization error on an unknown/future data set? \\n\\uf0b7 \\nWhat training and testing techniques are used for the model? \\n \\n\\uf0b7 \\nWhat model should be selected from the available ones? \\n \\nMethods used for Cross-Validation: \\n Holdout Method \\nConsider training a model using an algorithm on a given dataset. Using the same training \\ndata, you determine that the trained model has an accuracy of 95% or even 100%. What \\ndoes this mean? Can this model be used for prediction? \\nNo. This is because your model has been trained on the given data, i.e. it knows the data \\nand has generalized over it very well. In contrast, when you try to predict over a new set \\nModel Validation in Classification : Cross Validation - Holdout Method, K-Fold, \\nStratified K-Fold, Leave-One-Out Cross Validation. Bias-Variance tradeoff, \\nRegularization , Overfitting, Underfitting. Ensemble Methods: Boosting, Bagging, \\nRandom Forest.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 72}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n69 \\n \\nof data, it will most likely give you very bad accuracy because it has never seen the data \\n \\n \\nbefore and thus cannot generalize well over it. To deal with such problems, hold-out \\nmethods can be employed. \\n \\nThe hold-out method involves splitting the data into multiple parts and using one part \\nfor training the model and the rest for validating and testing it. It can be used for both \\nmodel evaluation and selection. \\nIn cases where every piece of data is used for training the model, there remains the \\nproblem of selecting the best model from a list of possible models. Primarily, we want \\nto identify which model has the lowest generalization error or which model makes a \\nbetter prediction on future or unseen datasets than all of the others. There is a need to \\nhave a mechanism that allows the model to be trained on one set of data and tested on \\nanother set of data. This is where hold-out comes into play. \\n \\n \\nHold-Out Method for Model Evaluation \\n \\nModel evaluation using the hold-out method entails splitting the dataset into training and test \\ndatasets, evaluating model performance, and determining the most optimal model. This \\ndiagram illustrates the hold-out method for model evaluation. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThere are two parts to the dataset in the diagram above. One split is held aside as a training set.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 73}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n70 \\n \\nAnother set is held back for testing or evaluation of the model. The percentage of the split is \\n \\n \\ndetermined based on the amount of training data available. A typical split of 70–30% is used \\nin which 70% of the dataset is used for training and 30% is used for testing the model. \\n \\nThe objective of this technique is to select the best model based on its accuracy on the testing \\ndataset and compare it with other models. There is, however, the possibility that the model can \\nbe well fitted to the test data using this technique. In other words, models are trained to improve \\nmodel accuracy on test datasets based on the assumption that the test dataset represents the \\npopulation. As a result, the test error becomes an optimistic estimation of the generalization \\nerror. Obviously, this is not what we want. Since the final model is trained to fitwell (or overfit) \\nthe test data, it won’t generalize well to unknowns or future datasets. \\n \\nFollow the steps below for using the hold-out method for model evaluation: \\n \\n \\n1.  \\nSplit the dataset in two (preferably 70–30%; however, the split percentage can vary \\nand should be random). \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n2. Now, we train the model on the training dataset by selecting some fixed set of \\nhyperparameters while training the model.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 74}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n71 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n3. Use the hold-out test dataset to evaluate the model. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n4. Use the entire dataset to train the final model so that it can generalize better on future \\ndatasets. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIn this process, the dataset is split into training and test sets, and a fixed set of hyperparameters \\nis used to evaluate the model. There is another process in which data can also be split into three \\nsets, and these sets can be used to select a model or to tune hyperparameters. We will discuss \\nthat technique next.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 75}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n72 \\n \\nHold-Out Method for Model Selection \\n \\nSometimes the model selection process is referred to as hyperparameter tuning. During the \\nhold-out method of selecting a model, the dataset is separated into three sets — training, \\nvalidation, and test. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFollow the steps below for using the hold-out method for model selection: \\n1. \\nDivide the dataset into three parts: training dataset, validation dataset, and test dataset. \\n2. Now, different machine learning algorithms can be used to train different models. Youcan \\ntrain your classification model, for example, using logistic regression, random forest, and \\nXGBoost. \\n3. Tune the hyperparameters for models trained with different algorithms. Change the \\nhyperparameter settings for each algorithm mentioned in step 2 and come up withmultiple \\nmodels. \\n4. On the validation dataset, test the performance of each of these models (associating with \\neach of the algorithms). \\n5. Choose the most optimal model from those tested on the validation dataset. The most \\noptimal model will be set up with the most optimal hyperparameters. Using the example \\nabove, let’s suppose the model trained with XGBoost with the most optimal \\nhyperparameters is selected. \\n6. \\nFinally, on the test dataset, test the performance of the most optimal model. \\n \\n \\nK-Fold Cross-Validation \\nK-fold cross-validation approach divides the input dataset into K groups of samples of equal \\nsizes. These samples are called folds. For each learning set, the prediction function uses k-1'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 76}, page_content=\"CSIT DEPT-R22-MACHINE LEARNING \\n73 \\n \\nfolds, and the rest of the folds are used for the test set. This approach is a very popular CV \\napproach because it is easy to understand, and the output is less biased than other methods. \\nThe steps for k-fold cross-validation are: \\n \\no Split the input dataset into K groups \\no For each group: \\no Take one group as the reserve or test data set. \\no Use remaining groups as the training dataset \\no Fit the model on the training set and evaluate the performance of the model \\nusing the test set. \\n \\nLet's take an example of 5-folds cross-validation. So, the dataset is grouped into 5 folds. On 1st \\niteration, the first fold is reserved for test the model , and rest are used to train the model. On \\n2nd iteration, the second fold is used to test the model, and rest are used to train the model.This \\nprocess will continue until each fold is not used for the test fold. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nStratified k-fold cross-validation: \\n \\nThis technique is similar to k-fold cross-validation with some little changes. This approach \\nworks on stratification concept, it is a process of rearranging the data to ensure that each fold \\nor group is a good representative of the complete dataset. To deal with the bias and variance, it \\nis one of the best approaches.\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 77}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n74 \\n \\nIt can be understood with an example of housing prices, such that the price of some houses can \\nbe much high than other houses. To tackle such situations, a stratified k-fold cross- validation \\ntechnique is useful. \\n \\nLeave one out cross-validation \\nThis method is similar to the leave-p-out cross-validation, but instead of p, we need to take 1 \\ndataset out of training. It means, in this approach, for each learning set, only one data point is \\nreserved, and the remaining dataset is used to train the model. This process repeats for each \\ndata point. Hence for n samples, we get n different training set and n test set. It has the following \\nfeatures: \\n \\no In this approach, the bias is minimum as all the data points are used. \\no The process is executed for n times; hence execution time is high. \\no This approach leads to high variation in testing the effectiveness of the model as we \\niteratively check against one data point. \\n \\nBias-Variance Trade off \\n \\nIt is important to understand prediction errors (bias and variance) when it comes to accuracy \\nin any machine learning algorithm. There is a tradeoff between a model’s ability to minimize \\nbias and variance which is referred to as the best solution for selecting a value of \\nRegularization constant. Proper understanding of these errors would help to avoid the \\noverfitting and underfitting of a data set while training the algorithm \\nBias \\nThe bias is known as the difference between the prediction of the values by the ML model and \\nthe correct value. Being high in biasing gives a large error in training as well as testing data. \\nIts recommended that an algorithm should always be low biased to avoid the problemof \\nunderfitting.By high bias, the data predicted is in a straight line format, thus not fitting \\naccurately in the data in the data set. Such fitting is known as Underfitting of Data. This \\nhappens when the hypothesis is too simple or linear in nature. Refer to the graph given below \\nfor an example of such a situation.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 78}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n75 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIn such a problem, a hypothesis looks like follows. \\n \\n \\n \\n \\n \\n \\nVariance \\nThe variability of model prediction for a given data point which tells us spread of our datais \\ncalled the variance of the model. The model with high variance has a very complex fit to the \\ntraining data and thus is not able to fit accurately on the data which it hasn’t seen before. As \\na result, such models perform very well on training data but has high error rateson test \\ndata.When a model is high on variance, it is then said to as Overfitting of Data. Overfitting is \\nfitting the training set accurately via complex curve and high order hypothesis but is not \\nthe solution as the error with unseen data is  high. While training a data model \\nvariance should be kept low. \\nThe high variance data looks like follows.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 79}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n76 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIn such a problem, a hypothesis looks like follows. \\n \\n \\n \\n \\n \\n \\nBias Variance Tradeoff \\nIf the algorithm is too simple (hypothesis with linear eq.) then it may be on high bias and low \\nvariance condition and thus is error-prone. If algorithms fit too complex ( hypothesis with high \\ndegree eq.) then it may be on high variance and low bias. In the latter condition, the new \\nentries will not perform well. Well, there is something between both of these conditions, known \\nas Trade-off or Bias Variance Trade-off. \\nThis tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm \\ncan’t be more complex and less complex at the same time. For the graph, the perfect tradeoff \\nwill be like.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 80}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n77 \\n \\nThe best fit will be given by hypothesis on the tradeoff point. \\nThe error to complexity graph to show trade-off is given as – \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThis is referred to as the best point chosen for the training of the algorithm which gives low \\nerror in training as well as testing data. \\n \\nRegularization : \\nRegularization is one of the most important concepts of machine learning. It is a technique to \\nprevent the model from overfitting by adding extra information to it. Sometimes the machine \\nlearning model performs well with the training data but does not perform well with the test \\ndata. It means the model is not able to predict the output when deals with unseen data by \\nintroducing noise in the output, and hence the model is called overfitted. This problem can be \\ndeal with the help of a regularization technique. \\n \\nThis technique can be used in such a way that it will allow to maintain all variables or features \\nin the model by reducing the magnitude of the variables. Hence, it maintains accuracy as well \\nas a generalization of the model. \\nit mainly regularizes or reduces the coefficient of features toward zero. In simple words, \"In \\nregularization technique, we reduce the magnitude of the features by keeping the same number \\nof features.\"'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 81}, page_content=\"CSIT DEPT-R22-MACHINE LEARNING \\n78 \\n \\nRegularization works by adding a penalty or complexity term to the complex model. Let's \\nconsider the simple linear regression equation: \\ny= β0+β1x1+β2x2+β3x3+⋯+βnxn +b \\n \\nIn the above equation, Y represents the value to be predicted \\nX1, X2, …Xn are the features for Y. \\nβ0,β1,…..βn are the weights or magnitude attached to the features, respectively. Here \\nrepresents the bias of the model, and b represents the intercept. \\nLinear regression models try to optimize the β0 and b to minimize the cost function. The \\nequation for the cost function for the linear model is given below: \\n \\n \\n \\n \\n \\nNow, we will add a loss function and optimize parameter to make the model that can predict \\nthe accurate value of Y. The loss function for the linear regression is called as RSS or Residual \\nsum of squares. \\n \\nTechniques of Regularization \\n \\nThere are mainly two types of regularization techniques, which are given below: \\n \\no Ridge Regression \\no Lasso Regression \\n \\nRidge Regression \\no Ridge regression is one of the types of linear regression in which a small amount of \\nbias is introduced so that we can get better long-term predictions. \\no Ridge regression is a regularization technique, which is used to reduce the complexity \\nof the model. It is also called as L2 regularization. \\no In this technique, the cost function is altered by adding the penalty term to it. The \\namount of bias added to the model is called Ridge Regression penalty. We can\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 82}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n79 \\n \\ncalculate it by multiplying with the lambda to the squared weight of each individual \\nfeature. \\no The equation for the cost function in ridge regression will be: \\n \\n \\n \\n \\n \\n \\n \\no In the above equation, the penalty term regularizes the coefficients of the model, and \\nhence ridge regression reduces the amplitudes of the coefficients that decreases the \\ncomplexity of the model. \\no As we can see from the above equation, if the values of λ tend to zero, the equation \\nbecomes the cost function of the linear regression model. Hence, for the minimum \\nvalue of λ, the model will resemble the linear regression model. \\no A general linear or polynomial regression will fail if there is high collinearity between \\nthe independent variables, so to solve such problems, Ridge regression can be used. \\no It helps to solve the problems if we have more parameters than samples. \\n \\nLasso Regression: \\no Lasso regression is another regularization technique to reduce the complexity of the \\nmodel. It stands for Least Absolute and Selection Operator. \\no It is similar to the Ridge Regression except that the penalty term contains only the \\nabsolute weights instead of a square of weights. \\no Since it takes absolute values, hence, it can shrink the slope to 0, whereas Ridge \\nRegression can only shrink it near to 0. \\no It is also called as L1 regularization. The equation for the cost function of Lasso \\nregression will be:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 83}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n80 \\n \\no Some of the features in this technique are completely neglected for model \\nevaluation. \\no Hence, the Lasso regression can help us to reduce the overfitting in the model \\nas well as the feature selection. \\nOverfitting and Under fitting: \\nTo train our machine learning model, we give it some data to learn from. The process of plotting \\na series of data points and drawing the best fit line to understand the relationship between the \\nvariables is called Data Fitting. Our model is the best fit when it can find all necessary patterns \\nin our data and avoid the random data points and unnecessary patterns called Noise. \\nOverfitting \\nWhen a model performs very well for training data but has poor performance with test data \\n(new data), it is known as overfitting. In this case, the machine learning model learns the details \\nand noise in the training data such that it negatively affects the performance of the model on \\ntest data. Overfitting can happen due to low bias and high variance.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 84}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n81 \\n \\nReasons for Overfitting \\n \\n\\uf0b7 \\nData used for training is not cleaned and contains noise (garbage values) in it \\n\\uf0b7 \\nThe model has a high variance \\n\\uf0b7 \\nThe size of the training dataset used is not enough \\n\\uf0b7 \\nThe model is too complex \\n \\nWays to Tackle Overfitting \\n\\uf0b7 \\nUsing K-fold cross-validation \\n\\uf0b7 \\nUsing Regularization techniques such as Lasso and Ridge \\n\\uf0b7 \\nTraining model with sufficient data \\n\\uf0b7 \\nAdopting ensembling techniques \\n \\nUnderfitting: \\nWhen a model has not learned the patterns in the training data well and is unable to generalize \\nwell on the new data, it is known as underfitting. An underfit model has poor performance on \\nthe training data and will result in unreliable predictions. Underfitting occurs due to high bias \\nand low variance. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nReasons for Underfitting \\n\\uf0b7 \\nData used for training is not cleaned and contains noise (garbage values) in it \\n\\uf0b7 \\nThe model has a high bias \\n\\uf0b7 \\nThe size of the training dataset used is not enough'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 85}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n82 \\n \\n\\uf0b7 \\nThe model is too simple \\n \\n \\nWays to Tackle Underfitting \\n\\uf0b7 \\nIncrease the number of features in the dataset \\n\\uf0b7 \\nIncrease model complexity \\n\\uf0b7 \\nReduce noise in the data \\n\\uf0b7 \\nIncrease the duration of training the data \\nEnsemble Methods: \\nWhen you want to purchase a new car, will you walk up to the first car shop and purchase \\none based on the advice of the dealer? It’s highly unlikely. \\nYou would likely browser a few web portals where people have posted their reviews and \\ncompare different car models, checking for their features and prices. You will also probably \\nask your friends and colleagues for their opinion. In short, you wouldn’t directly reach a \\nconclusion, but will instead make a decision considering the opinions of other people as well. \\nEnsemble models in machine learning operate on a similar idea. They combine the decisions \\nfrom multiple models to improve the overall performance. \\nAdvantage : Improvement in predictive accuracy. \\n \\n \\nDisadvantage : It is difficult to understand an ensemble of classifiers. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nEnsembles overcome three problems – \\n \\n\\uf0b7 Statistical Problem –'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 86}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n83 \\n \\nThe Statistical Problem arises when the hypothesis space is too large for the amount \\nof available data. Hence, there are many hypotheses with the same accuracy on the \\ndata and the learning algorithm chooses only one of them! There is a risk that the \\naccuracy of the chosen hypothesis is low on unseen data! \\n\\uf0b7 Computational Problem – \\nThe Computational Problem arises when the learning algorithm cannot guarantees \\nfinding the best hypothesis. \\n\\uf0b7 Representational Problem – \\nThe Representational Problem arises when the hypothesis space does not contain \\nany good approximation of the target class(es). \\n \\nTypes of Ensemble Classifier – \\n1)Bagging \\n2)Boosting \\n3)Random Forest \\n \\nBagging: \\nBAGGing, or Bootstrap AGGregating. BAGGing gets its name because it combines \\nBootstrapping and Aggregation to form one ensemble model. Given a sample of data, \\nmultiple bootstrapped subsamples are pulled. A Decision Tree is formed on each of the \\nbootstrapped subsamples. After each subsample Decision Tree has been formed, an \\nalgorithm is used to aggregate over the Decision Trees to form the most efficient \\npredictor. The image below will help explain:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 87}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n84 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nBoosting : \\nUnlike bagging, which aggregates prediction results at the end, boosting aggregates the \\nresults at each step. They are aggregated using weighted averaging. \\nWeighted averaging involves giving all models different weights depending on their \\npredictive power. In other words, it gives more weight to the model with the highest \\npredictive power. This is because the learner with the highest predictive power is \\nconsidered the most important. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nBoosting works with the following steps: \\n \\n \\n1. We sample m-number of subsets from an initial training dataset. \\n2. Using the first subset, we train the first weak learner.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 88}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n85 \\n \\n3. We test the trained weak learner using the training data. As a result of the testing, \\nsome data points will be incorrectly predicted. \\n4. Each data point with the wrong prediction is sent into the second subset of data, and \\nthis subset is updated. \\n5. Using this updated subset, we train and test the second weak learner. \\n6. We continue with the following subset until the total number of subsets is reached. \\n7. We now have the total prediction. The overall prediction has already been aggregated \\nat each step, so there is no need to calculate it. \\nRandom Forest Models. \\nRandom Forest Models can be thought of as BAGGing, with a slight tweak. When deciding \\nwhere to split and how to make decisions, BAGGed Decision Trees have the full disposal of \\nfeatures to choose from. Therefore, although the bootstrapped samples may be slightly \\ndifferent, the data is largely going to break off at the same features throughout each model. In \\ncontrary, Random Forest models decide where to split based on a random selection of features. \\nRather than splitting at similar features at each node throughout, Random Forest models \\nimplement a level of differentiation because each tree will split based on different features. \\nThis level of differentiation provides a greater ensemble to aggregate over, ergo producing a \\nmore accurate predictor. Refer to the image for a better understanding.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 89}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n86 \\n \\nUNIT-V \\n \\nUnsupervised Learning: \\nUnsupervised learning is a machine learning technique in which models are not supervised \\nusing training dataset. Instead, models itself find the hidden patterns and insights from the \\ngiven data. It can be compared to learning which takes place in the human brain while learning \\nnew things. It can be defined as: \\n \\nUnsupervised learning cannot be directly applied to a regression or classification problem \\nbecause unlike supervised learning, we have the input data but no corresponding output data. \\nThe goal of unsupervised learning is to find the underlying structure of dataset, group that \\ndata according to similarities, and represent that dataset in a compressed format. \\nExample: Suppose the unsupervised learning algorithm is given an input dataset containing \\nimages of different types of cats and dogs. The algorithm is never trained upon the given \\ndataset, which means it does not have any idea about the features of the dataset. The task of the \\nunsupervised learning algorithm is to identify the image features on their own. Unsupervised \\nlearning algorithm will perform this task by clustering the image dataset into the groups \\naccording to similarities between images. \\nUnsupervised Learning : Clustering-K-means, K-Modes, K-Prototypes, \\nGaussian MixtureModels, Expectation-Maximization. \\nReinforcement Learning: Exploration and exploitation trade-offs, non- \\nassociative learning,Markov decision processes, Q-learning.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 90}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n87 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nBelow are some main reasons which describe the importance of Unsupervised Learning: \\n \\no \\nUnsupervised learning is helpful for finding useful insights from the data. \\no \\nUnsupervised learning is much similar as a human learns to think by their own \\nexperiences, which makes it closer to the real AI. \\no \\nUnsupervised learning works on unlabeled and uncategorized data which make \\nunsupervised learning more important. \\no \\nIn real-world, we do not always have input data with the corresponding output so to \\nsolve such cases, we need unsupervised learning. \\nWorking of unsupervised learning can be understood by the below diagram: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\no'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 91}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n88 \\n \\nHere, we have taken an unlabeled input data, which means it is not categorized and \\ncorresponding outputs are also not given. Now, this unlabeled input data is fed to the machine \\nlearning model in order to train it. Firstly, it will interpret the raw data to find the hidden \\npatterns from the data and then will apply suitable algorithms such as k-means clustering, \\nDecision tree, etc. \\nOnce it applies the suitable algorithm, the algorithm divides the data objects into groups \\naccording to the similarities and difference between the objects. \\nThe unsupervised learning algorithm can be further categorized into two types of problems: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\no \\nClustering: Clustering is a method of grouping the objects into clusters such that \\nobjects with most similarities remains into a group and has less or no similarities with \\nthe objects of another group. Cluster analysis finds the commonalities between the data \\nobjects and categorizes them as per the presence and absence of those commonalities. \\no \\nAssociation: An association rule is an unsupervised learning method which is used for \\nfinding the relationships between variables in the large database. It determines the set \\nof items that occurs together in the dataset. Association rule makes marketing strategy \\nmore effective. Such as people who buy X item (suppose a bread) are also tend to \\npurchase Y (Butter/Jam) item. A typical example of Association rule is MarketBasket \\nAnalysis \\n \\nK-Means Clustering \\n \\n\\uf0b7 \\nK-Means clustering is an unsupervised iterative clustering technique. \\n\\uf0b7 \\nIt partitions the given data set into k predefined distinct clusters.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 92}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n89 \\n \\n\\uf0b7 \\nA cluster is defined as a collection of data points exhibiting certain similarities. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\uf0b7 It partitions the data set such that-\\uf020\\n \\n\\uf0b7 Each data point belongs to a cluster with the nearest mean.\\uf020\\n\\uf0b7 Data points belonging to one cluster have high degree of similarity.\\uf020\\n\\uf0b7 Data points belonging to different clusters have high degree of dissimilarity.\\uf020\\n \\n \\nK-Means Clustering Algorithm- \\n \\n\\uf0b7 K-Means Clustering Algorithm involves the following steps- \\nStep-01:\\uf020\\n1. Choose the number of clusters K. \\nStep-02: \\n1. Randomly select any K data points as cluster centers. \\n2. Select cluster centers in such a way that they are as farther as possible from each \\nother. \\nStep-03: \\n1. Calculate the distance between each data point and each cluster center. \\n2. The distance may be calculated either by using given distance function or by using \\neuclidean distance formula. \\nStep-04: \\n1. Assign each data point to some cluster. \\n2. A data point is assigned to that cluster whose center is nearest to that data point. \\nStep-05: \\n1. Re-compute the center of newly formed clusters.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 93}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n90 \\n \\n2. The center of a cluster is computed by taking mean of all the data points contained in \\nthat cluster. \\n \\nStep-06: \\nKeep repeating the procedure from Step-03 to Step-05 until any of the following stopping \\ncriteria is met- \\n1. Center of newly formed clusters do not change \\n2. Data points remain present in the same cluster \\n3. Maximum number of iterations are reached \\nProblem: \\nCluster the following eight points (with (x, y) representing locations) into three clusters: \\nA1(2, 10), A2(2, 5), A3(8, 4), A4(5, 8), A5(7, 5), A6(6, 4), A7(1, 2), A8(4, 9) \\n \\nInitial cluster centers are: A1(2, 10), A4(5, 8) and A7(1, 2). \\nThe distance function between two points a = (x1, y1) and b = (x2, y2) is defined as- \\nΡ(a, b) = |x2 – x1| + |y2 – y1| \\n \\n \\nUse K-Means Algorithm to find the three cluster centers after the second iteration. \\nSolution- \\nWe follow the above discussed K-Means Clustering Algorithm- \\nIteration-01: \\n \\n\\uf0b7 \\nWe calculate the distance of each point from each of the center of the three clusters.\\uf020\\n\\uf0b7 \\nThe distance is calculated by using the given distance function.\\uf020\\n \\nThe following illustration shows the calculation of distance between point A1(2, 10) and each \\nof the center of the three clusters- \\nCalculating Distance Between A1(2, 10) and C1(2, 10)- \\nΡ(A1, C1) \\n= |x2 – x1| + |y2 – y1| \\n= |2 – 2| + |10 – 10| \\n= 0 \\nCalculating Distance Between A1(2, 10) and C2(5, 8)-'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 94}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n91 \\n \\nΡ(A1, C2) \\n= |x2 – x1| + |y2 – y1| \\n= |5 – 2| + |8 – 10| \\n= 3 + 2 \\n= 5 \\nCalculating Distance Between A1(2, 10) and C3(1, 2)- \\nΡ(A1, C3) \\n= |x2 – x1| + |y2 – y1| \\n= |1 – 2| + |2 – 10| \\n= 1 + 8 \\n= 9 \\n \\n \\nIn the similar manner, we calculate the distance of other points from each of the center of the \\nthree clusters. \\nNext, \\n \\n\\uf0b7 \\nWe draw a table showing all the results.\\uf020\\n\\uf0b7 \\nUsing the table, we decide which point belongs to which cluster.\\uf020\\n\\uf0b7 \\nThe given point belongs to that cluster whose center is nearest to it.\\uf020'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 95}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n92 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFrom here, New clusters are- \\n \\n \\nCluster-01: \\nFirst cluster contains points- \\n\\uf0b7 \\nA1(2, 10)\\uf020\\n \\nCluster-02: \\nSecond cluster contains points- \\n\\uf0b7 \\nA3(8, 4)\\uf020\\n\\uf0b7 \\nA4(5, 8)\\uf020\\n\\uf0b7 \\nA5(7, 5)\\uf020\\n\\uf0b7 \\nA6(6, 4)\\uf020\\n\\uf0b7 \\nA8(4, 9)\\uf020\\n \\nCluster-03: \\nThird cluster contains points- \\n\\uf0b7 \\nA2(2, 5)\\uf020\\n\\uf0b7 \\nA7(1, 2)\\uf020'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 96}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n93 \\n \\nNow, \\n\\uf0b7 \\nWe re-compute the new cluster clusters.\\uf020\\n\\uf0b7 \\n \\nThe new cluster center is computed by taking mean of all the points contained in that \\ncluster.\\uf020\\n \\nFor Cluster-01: \\n\\uf0b7 We have only one point A1(2, 10) in Cluster-01. \\n\\uf0b7 So, cluster center remains the same. \\n \\nFor Cluster-02: \\nCenter of Cluster-02 \\n= ((8 + 5 + 7 + 6 + 4)/5, (4 + 8 + 5 + 4 + 9)/5) \\n= (6, 6) \\nFor Cluster-03: \\nCenter of Cluster-03 \\n= ((2 + 1)/2, (5 + 2)/2) \\n= (1.5, 3.5) \\n \\n \\nThis is completion of Iteration-01. \\nIteration-02: \\n \\n\\uf0b7 \\nWe calculate the distance of each point from each of the center of the three clusters.\\uf020\\n\\uf0b7 \\nThe distance is calculated by using the given distance function.\\uf020\\n \\nThe following illustration shows the calculation of distance between point A1(2, 10) and each \\nof the center of the three clusters- \\n \\n \\nCalculating Distance Between A1(2, 10) and C1(2, 10)- \\n \\nΡ(A1, C1) \\n= |x2 – x1| + |y2 – y1| \\n= |2 – 2| + |10 – 10|'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 97}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n94 \\n \\n= 0 \\n \\n \\nCalculating Distance Between A1(2, 10) and C2(6, 6)- \\n \\nΡ(A1, C2) \\n= |x2 – x1| + |y2 – y1| \\n= |6 – 2| + |6 – 10| \\n= 4 + 4 \\n= 8 \\nCalculating Distance Between A1(2, 10) and C3(1.5, 3.5)- \\n \\nΡ(A1, C3) \\n= |x2 – x1| + |y2 – y1| \\n= |1.5 – 2| + |3.5 – 10| \\n= 0.5 + 6.5 \\n= 7 \\n \\n \\nIn the similar manner, we calculate the distance of other points from each of the center of the \\nthree clusters. \\nNext, \\n\\uf0b7 \\nWe draw a table showing all the results.\\uf020\\n\\uf0b7 \\nUsing the table, we decide which point belongs to which cluster.\\uf020\\n\\uf0b7 \\nThe given point belongs to that cluster whose center is nearest to it.\\uf020'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 98}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n95 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFrom here, New clusters are- \\nCluster-01: \\nFirst cluster contains points- \\n\\uf0b7 \\nA1(2, 10)\\uf020\\n\\uf0b7 \\nA8(4, 9)\\uf020\\n \\nCluster-02: \\nSecond cluster contains points- \\n\\uf0b7 \\nA3(8, 4)\\uf020\\n\\uf0b7 \\nA4(5, 8)\\uf020\\n\\uf0b7 \\nA5(7, 5)\\uf020\\n\\uf0b7 \\nA6(6, 4)\\uf020\\n \\nCluster-03: \\n \\nThird cluster contains points- \\n\\uf0b7 \\nA2(2, 5)\\uf020\\n\\uf0b7 \\nA7(1, 2)\\uf020'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 99}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n96 \\n \\nNow, \\n\\uf0b7 \\nWe re-compute the new cluster clusters.\\uf020\\n\\uf0b7 \\n \\nThe new cluster center is computed by taking mean of all the points contained in that \\ncluster.\\uf020\\n \\nFor Cluster-01: \\nCenter of Cluster-01 \\n= ((2 + 4)/2, (10 + 9)/2) \\n= (3, 9.5) \\n \\n \\nFor Cluster-02: \\n \\nCenter of Cluster-02 \\n= ((8 + 5 + 7 + 6)/4, (4 + 8 + 5 + 4)/4) \\n= (6.5, 5.25) \\n \\n \\nFor Cluster-03: \\nCenter of Cluster-03 \\n= ((2 + 1)/2, (5 + 2)/2) \\n= (1.5, 3.5) \\n \\n \\nThis is completion of Iteration-02. \\n \\n \\nAfter second iteration, the center of the three clusters are- \\n\\uf0b7 \\nC1(3, 9.5)\\uf020\\n\\uf0b7 \\nC2(6.5, 5.25)\\uf020\\n\\uf0b7 \\nC3(1.5, 3.5)\\uf020\\n \\nK MODES CLUSTERING \\nKModes is a clustering algorithm used to group similar data points into clusters based on their \\ncategorical attributes. Unlike traditional clustering algorithms that use distance metrics, \\nKModes works by identifying the modes or most frequent values within each cluster to'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 100}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n97 \\n \\ndetermine its centroid. KModes is ideal for clustering categorical data such as customer \\ndemographics, market segments, or survey responses \\nIn K-means clustering when we used categorical data after converting it into a numerical form. \\nit  doesn’t  give  a  good  result  for  high-dimensional  data. So, Some \\nchanges are made for categorical data t. \\n\\uf0b7 \\nReplace Euclidean distance with Dissimilarity metric \\n\\uf0b7 \\nReplace Mean by Mode for cluster centers. \\n\\uf0b7 \\nApply a frequency-based method in each iteration to update the mode. \\nAnd then this is called K-MODE Clustering because of MODE. \\nSimilarity and dissimilarity measurements are used to determine the distance between the data \\nobjects in the dataset. In the case of K-modes, these distances are calculated using a \\ndissimilarity measure called the Hamming distance. The Hamming distance between two data \\nobjects is the number of categorical attributes that differ between the two objects. \\nLet x and y be two categorical data objects defined by m features or attributes. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFor example, consider the following dataset with three categorical attributes:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 101}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n98 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTo calculate the Hamming distance between data objects 1 and 2, we compare their valuesfor \\neach attribute and count the number of differences. In this case, there is one difference \\n(Attribute 3 is C for object 1 and D for object 2), so the Hamming distance between objects \\n1 and 2 is 1. \\nTo calculate the Hamming distance between objects 1 and 3, we again compare their values \\nfor each attribute and count the number of differences. In this case, there are two differences \\n(Attribute 2 is B for object 1 and C for object 3, and Attribute 3 is C for object 1 and E for \\nobject 3), so the Hamming distance between objects 1 and 3 is 2. \\nTo calculate the Hamming distance between objects 1 and 4, we again compare their values \\nfor each attribute and count the number of differences. In this case, there are three differences \\n(Attribute 1 is A for objects 1 and B for object 4, Attribute 2 is B for object 1 and C for object \\n4, and Attribute 3 is C for objects 1 and E for object 4), so the Hamming distance between \\nobjects 1 and 4 is 3. \\nData objects with a smaller Hamming distance are considered more similar, while objects with \\na larger Hamming distance is considered more dissimilar.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 102}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n99 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThe main task for K-Modes algorithm is to minimize this C(Q) cost function. \\nIt consists of the following steps. \\n1. \\nSelect K data objects for each cluster. \\n2. \\nCalculate dissimilarities D(X,Q) and allocate each data object to \\nnearest cluster. \\n3. \\nCalculate the new modes for all clusters. \\n4. \\nRepeat step 2 and 3 until the cluster will become stable. \\n \\n \\nSome variations of the K-modes algorithm may use different methods for updating the \\ncentroids (modes) of the clusters, such as taking the weighted mode or the median of the \\nobjects within each cluster. \\nOverall, the goal of K-modes clustering is to minimize the dissimilarities between the data \\nobjects and the centroids (modes) of the clusters, using a measure of categorical similarity \\nsuch as the Hamming distance.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 103}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n100 \\n \\nK PROTOTYPE CLUSTERING \\n \\nOne of the conventional clustering methods commonly used in clustering techniques and \\nefficiently used for large data is the K-Means algorithm. However, its method is not good and \\nsuitable for data that contains categorical variables. This problem happens when the cost \\nfunction in K-Means is calculated using the Euclidian distance that is only suitable for \\nnumerical data. While K-Mode is only suitable for categorical data only, not mixed data types. \\n \\nFacing these problems, Huang proposed an algorithm called K-Prototype which is created in \\norder to handle clustering algorithms with the mixed data types (numerical and categorical \\nvariables). K-Prototype is a clustering method based on partitioning. Its algorithm is an \\nimprovement of the K-Means and K-Mode clustering algorithm to handle clustering with the \\nmixed data types. \\n \\nK-Prototype has an advantage because it’s not too complex and is able to handle large data and \\nis better than hierarchical based algorithms'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 104}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n101'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 105}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n102 \\n \\nNormal or Gaussian Distribution \\n \\nIn real life, many datasets can be modeled by Gaussian Distribution (Univariate or \\nMultivariate). So it is quite natural and intuitive to assume that the clusters come from \\ndifferent Gaussian Distributions. Or in other words, it tried to model the dataset as a \\nmixture of several Gaussian Distributions. This is the core idea of this model. \\nIn one dimension the probability density function of a Gaussian Distribution is given by \\n \\n \\n \\nwhere  and \\n are respectively the mean and variance of the distribution. For \\nMultivariate ( let us say d-variate) Gaussian Distribution, the probability density function is \\ngiven by \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nGaussian Mixture Model \\n \\nSuppose there are K clusters (For the sake of simplicity here it is assumed that the number \\nof clusters is known and it is K). So and are also estimated for each k. Had it been only one \\ndistribution, they would have been estimated by the maximum-likelihood method. But \\nsince there are K such clusters and the probability density is defined as a linear function of \\ndensities of all these K distributions, i.e.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 106}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n103 \\n \\nFrom Bayes theorem, \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSo it can be clearly seen that the parameters cannot be estimated in closed form. This is \\nwhere the Expectation-Maximization algorithm is beneficial. \\n \\nExpectation-Maximization (EM) Algorithm \\n \\nThe Expectation-Maximization (EM) algorithm is an iterative way to find maximum- \\nlikelihood estimates for model parameters when the data is incomplete or has some missing \\ndata points or has some hidden variables. EM chooses some random values for the missing'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 107}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n104 \\n \\ndata points and estimates a new set of data. These new values are then recursively used to \\nestimate a better first date, by filling up missing points, until the values get fixed. \\nThese are the two basic steps of the EM algorithm, namely the E Step, or Expectation Step \\nor Estimation Step, and M Step, or Maximization Step. \\n \\n \\nEstimation step \\nInitialize \\n by some random values, or by K means clustering results or by \\nhierarchical clustering results. Then for those given parameter values, estimate the value of \\nthe latent variables \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nReinforcement Learning \\n \\nUnlike supervised and unsupervised learning, reinforcement learning is a feedback-based \\napproach in which agent learns by performing some actions as well as their outcomes. Based \\non action status (good or bad), the agent gets positive or negative feedback. Further, for each \\npositive feedback, they get rewarded, whereas, for each negative feedback, they also get \\npenalized. \\nDef: “Reinforcement learning is a type of machine learning technique, where an intelligent \\nagent (computer program) interacts with the environment, explore it by itself, and makes \\nactions within that.\" \\n \\n \\no Reinforcement learning does not require any labeled data for the learning process. It \\nlearns through the feedback of action performed by the agent. Moreover, in \\nreinforcement learning, agents also learn from past experiences.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 108}, page_content=\"CSIT DEPT-R22-MACHINE LEARNING \\n105 \\n \\no Reinforcement learning methods are used to solve tasks where decision-making is \\nsequential and the goal is long-term, e.g., robotics, online chess, etc. \\no Reinforcement learning aims to get maximum positive feedback so that they can \\nimprove their performance. \\no Reinforcement learning involves various actions, which include taking action, \\nchanging/unchanged state, and getting feedback. And based on these actions, agents \\nlearn and explore the environment. \\n \\nExploration and Exploitation in Reinforcement Learning: \\nBefore going to a brief description of exploration and exploitation in machine learning, let's \\nfirst understand these terms in simple words. In reinforcement learning, whenever agents get \\na situation in which they have to make a difficult choice between whether to continue the same \\nwork or explore something new at a specific time, then, this situation results in Exploration- \\nExploitation Dilemma because the knowledge of an agent about the state, actions, rewards and \\nresulting states is always partial. \\nNow we will discuss exploitation and exploration in technical terms. \\n \\nExploitation in Reinforcement Learning \\n \\nExploitation is defined as a greedy approach in which agents try to get more rewards by using \\nestimated value but not the actual value. So, in this technique, agents make the best decision \\nbased on current information. \\nExploration in Reinforcement Learning \\n \\nUnlike exploitation, in exploration techniques, agents primarily focus on improving their \\nknowledge about each action instead of getting more rewards so that they can get long-term \\nbenefits. So, in this technique, agents work on gathering more information to make the best \\noverall decision. \\nExamples of Exploitation and Exploration in Machine Learning \\n \\nLet's understand exploitation and exploration with some interesting real-world examples.\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 109}, page_content=\"CSIT DEPT-R22-MACHINE LEARNING \\n106 \\n \\nCoal mining: \\n \\nLet's suppose people A and B are digging in a coal mine in the hope of getting a diamond inside \\nit. Person B got success in finding the diamond before person A and walks off happily. After \\nseeing him, person A gets a bit greedy and thinks he too might get success in finding diamond \\nat the same place where person B was digging coal. This action performed by person A is \\ncalled greedy action, and this policy is known as a greedy policy. But person A was unknown \\nbecause a bigger diamond was buried in that place where he was initially digging the coal, and \\nthis greedy policy would fail in this situation. \\n \\nIn this example, person A only got knowledge of the place where person B was digging but \\nhad no knowledge of what lies beyond that depth. But in the actual scenario, the diamond can \\nalso be buried in the same place where he was digging initially or some completely another \\nplace. Hence, with this partial knowledge about getting more rewards, our reinforcement \\nlearning agent will be in a dilemma on whether to exploit the partial knowledge to receive some \\nrewards or it should explore unknown actions which could result in many rewards. \\n \\nHowever, both these techniques are not feasible simultaneously, but this issue can be resolved \\nby using Epsilon Greedy Policy (Explained below). \\nhere are a few other examples of Exploitation and Exploration in Machine Learning as follows: \\n \\nExample 1: Let's say we have a scenario of online restaurant selection for food orders, where \\nyou have two options to select the restaurant. In the first option, you can choose your favorite \\nrestaurant from where you ordered food in the past; this is called exploitation because here, \\nyou only know information about a specific restaurant. And for other options, you can try a \\nnew restaurant to explore new varieties and tastes of food, and it is called exploration. However, \\nfood quality might be better in the first option, but it is also possible that it is more delicious in \\nanother restaurant. \\n \\nExample 2: Suppose there is a game-playing platform where you can play chess with robots. \\nTo win this game, you have two choices either play the move that you believe is best, and for \\nthe other choice, you can play an experimental move. However, you are playing the best \\npossible move, but who knows new move might be more strategic to win this game. Here, the\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 110}, page_content=\"CSIT DEPT-R22-MACHINE LEARNING \\n107 \\n \\nfirst choice is called exploitation, where you know about your game strategy, and the second \\nchoice is called exploration, where you are exploring your knowledge and playing a new move \\nto win the game. \\n \\nNon-Associative Learning \\n \\nIn reinforcement learning, non-associative learning refers to a type of learning that does not \\ninvolve forming associations or relationships between different stimuli or actions. It isa \\nsimpler form of learning compared to associative learning, which involves linking different \\nstimuli or actions together. \\nNon-associative learning is typically observed in situations where an agent's behavior \\nchanges in response to a single stimulus or repeated exposure to the same stimulus. There \\nare two common types of non-associative learning: habituation and sensitization. \\n1. Habituation: Habituation occurs when an agent's response to a particular stimulus \\ndecreases over time with repeated exposure. It is a form of adaptive behavior where the \\nagent learns to ignore irrelevant or harmless stimuli. For example, if a robot is repeatedly \\nexposed to a loud noise that is not associated with any reward or punishment, it may \\ngradually stop reacting to the noise and become habituated to it. \\n \\n2. Sensitization: Sensitization is the opposite of habituation and occurs when an agent's \\nresponse to a stimulus increases over time with repeated exposure. It involves an increased \\nsensitivity or responsiveness to a stimulus. For example, if a robot is repeatedly exposed to \\na painful stimulus, it may become more sensitive to that stimulus and show an increased \\nresponse. \\nNon-associative learning is not directly related to reinforcement learning, as reinforcement \\nlearning primarily focuses on associative learning, where agents learn to associate their \\nactions with rewards or punishments. However, non-associative learning mechanisms can \\nplay a role in shaping an agent's behavior and influencing its responsesto stimuli, which \\ncan indirectly impact the learning process in reinforcement learning scenarios. \\nExamples\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 111}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n108 \\n \\n\\uf0b7 \\nLiving near an airport for a year and getting used to the sound of airplanes passing \\noverhead ─ Habituation \\n\\uf0b7 \\nHearing loud thunder when at home alone at night and then becoming easily startled \\nby bright flashes of light ─ Sensitization \\n \\n \\nMarkov-Decision  Process \\n \\nReinforcement Learning is a type of Machine Learning. It allows machines and software agents \\nto automatically determine the ideal behavior within a specific context, in order to maximize \\nits performance. Simple reward feedback is required for the agent to learn its behavior; this is \\nknown as the reinforcement signal. \\nThere are many different algorithms that tackle this issue. As a matter of fact, Reinforcement \\nLearning is defined by a specific type of problem, and all its solutions are classed as \\nReinforcement Learning algorithms. In the problem, an agent is supposed to decide the best \\naction to select based on his current state. When this step is repeated, the problem is known as \\na Markov Decision Process. \\nA Markov Decision Process (MDP) model contains: \\n \\nA set of possible world states S. \\n\\uf0b7 \\nA set of Models. \\n\\uf0b7 \\nA set of possible actions A. \\n\\uf0b7 \\nA real-valued reward function R(s,a). \\n\\uf0b7 \\nA policy the solution of Markov Decision Process.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 112}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n109 \\n \\nState: \\n \\nA State is a set of tokens that represent every state that the agent can be in. \\n \\n \\nModel: \\nA Model (sometimes called Transition Model) gives an action’s effect in a state. In \\nparticular, T(S, a, S’) defines a transition T where being in state S and taking an action ‘a’ \\ntakes us to state S’ (S and S’ may be the same). For stochastic actions (noisy, non- \\ndeterministic) we also define a probability P(S’|S,a) which represents the probability of \\nreaching a state S’ if action ‘a’ is taken in state S. Note Markov property states that the \\neffects of an action taken in a state depend only on that state and not on the prior history. \\n \\nActions \\nAn Action A is a set of all possible actions. A(s) defines the set of actions that can be taken \\nbeing in state S. \\n \\nReward \\nA Reward is a real-valued reward function. R(s) indicates the reward for simply being in the \\nstate S. R(S,a) indicates the reward for being in a state S and taking an action ‘a’. R(S,a,S’) \\nindicates the reward for being in a state S, taking an action ‘a’ and ending up in a state S’. \\nPolicy \\nA Policy is a solution to the Markov Decision Process. A policy is a mapping from S to a. \\nIt indicates the action ‘a’ to be taken while in state S. \\n \\nLet us take the example of a grid world:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 113}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n110 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAn agent lives in the grid. The above example is a 3*4 grid. The grid has a START state(grid \\nno 1,1). The purpose of the agent is to wander around the grid to finally reach the Blue Diamond \\n(grid no 4,3). Under all circumstances, the agent should avoid the Fire grid (orange color, grid \\nno 4,2). Also the grid no 2,2 is a blocked grid, it acts as a wall hence the agent cannot enter it. \\nThe agent can take any one of these actions: UP, DOWN, LEFT, RIGHT \\nWalls block the agent path, i.e., if there is a wall in the direction the agent would have taken, \\nthe agent stays in the same place. So for example, if the agent says LEFT in the START grid \\nhe would stay put in the START grid. \\nFirst Aim: To find the shortest sequence getting from START to the Diamond. Two such \\nsequences can be found: \\n\\uf0b7 \\nRIGHT RIGHT UP UPRIGHT \\n\\uf0b7 \\nUP UP RIGHT RIGHT RIGHT \\nLet us take the second one (UP UP RIGHT RIGHT RIGHT) for the subsequent discussion. \\nThe move is now noisy. 80% of the time the intended action works correctly. 20% of the time \\nthe action agent takes causes it to move at right angles. For example, if the agent says UP the \\nprobability of going UP is 0.8 whereas the probability of going LEFT is 0.1, and the probability \\nof going RIGHT is 0.1 (since LEFT and RIGHT are right angles to UP). \\nThe agent receives rewards each time step:- \\n \\n\\uf0b7 \\n \\nSmall reward each step (can be negative when can also be term as punishment, in \\nthe above example entering the Fire can have a reward of -1). \\n\\uf0b7 \\nBig rewards come at the end (good or bad). \\n\\uf0b7 \\nThe goal is to Maximize the sum of rewards.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 114}, page_content=\"CSIT DEPT-R22-MACHINE LEARNING \\n111 \\n \\nQ-learning \\nQ-learning is a model-free, value-based, off-policy algorithm that will find the best series of \\nactions based on the agent's current state. The “Q” stands for quality. Quality represents how \\nvaluable the action is in maximizing future rewards. \\nThe model-based algorithms use transition and reward functions to estimate the optimal policy \\nand create the model. In contrast, model-free algorithms learn the consequences of their actions \\nthrough the experience without transition and reward function. \\nThe value-based method trains the value function to learn which state is more valuable and \\ntake action. On the other hand, policy-based methods train the policy directly to learn which \\naction to take in a given state. \\nIn the off-policy, the algorithm evaluates and updates a policy that differs from the policy used \\nto take an action. Conversely, the on-policy algorithm evaluates and improves the same policy \\nused to take an action \\n \\nBefore we jump into how Q-learning works, we need to learn a few useful terminologies to \\nunderstand Q-learning's fundamentals. \\n \\n\\uf0b7 \\nStates(s): the current position of the agent in the environment. \\n \\n\\uf0b7 \\nAction(a): a step taken by the agent in a particular state. \\n \\n\\uf0b7 \\nRewards: for every action, the agent receives a reward and penalty. \\n \\n\\uf0b7 \\n \\nEpisodes: the end of the stage, where agents can’t take new action. It happens when \\nthe agent has achieved the goal or failed. \\n\\uf0b7 \\nQ(St+1, a): expected optimal Q-value of doing the action in a particular \\nstate. \\n \\n\\uf0b7 \\nQ(St, At): it is the current estimation of Q(St+1, a). \\n \\n\\uf0b7 \\nQ-Table: the agent maintains the Q-table of sets of states and actions. \\n \\n\\uf0b7 \\n \\nTemporal Differences(TD): used to estimate the expected value of Q(St+1, a) by using \\nthe current state and action and previous state and action.\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 115}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n112 \\n \\nWe will learn in detail how Q-learning works by using the example of a frozen lake. In this \\nenvironment, the agent must cross the frozen lake from the start to the goal, without falling into \\nthe holes. The best strategy is to reach goals by taking the shortest path \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nQ-Table \\n \\nThe agent will use a Q-table to take the best possible action based on the expected reward for \\neach state in the environment. In simple words, a Q-table is a data structure of sets of actions \\nand states, and we use the Q-learning algorithm to update the values in the table. \\n \\nQ-Function \\n \\nThe Q-function uses the Bellman equation and takes state(s) and action(a) as input. The \\nequation simplifies the state values and state-action value calculation.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 116}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n113 \\n \\nQ-learning algorithm \\n \\n \\nInitialize Q-Table \\n \\nWe will first initialize the Q-table. We will build the table with columns based on the number \\nof actions and rows based on the number of states. \\n \\nIn our example, the character can move up, down, left, and right. We have four possible actions \\nand four states(start, Idle, wrong path, and end). You can also consider the wrongpath for \\nfalling into the hole. We will initialize the Q-Table with values at 0.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 117}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n114 \\n \\nChoose an Action \\n \\nThe second step is quite simple. At the start, the agent will choose to take the random \\naction(down or right), and on the second run, it will use an updated Q-Table to select the action. \\n \\nPerform an Action \\n \\nChoosing an action and performing the action will repeat multiple times until the training loop \\nstops. The first action and state are selected using the Q-Table. In our case, all values of the Q- \\nTable are zero. \\n \\nThen, the agent will move down and update the Q-Table using the Bellman equation. With \\nevery move, we will be updating values in the Q-Table and also using it for determining the \\nbest course of action. \\n \\nInitially, the agent is in exploration mode and chooses a random action to explore the \\nenvironment. The Epsilon Greedy Strategy is a simple method to balance exploration and \\nexploitation. The epsilon stands for the probability of choosing to explore and exploits when \\nthere are smaller chances of exploring. \\n \\nAt the start, the epsilon rate is higher, meaning the agent is in exploration mode. While \\nexploring the environment, the epsilon decreases, and agents start to exploit the environment. \\nDuring exploration, with every iteration, the agent becomes more confident in estimating Q- \\nvalues'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 118}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n115 \\n \\nIn the frozen lake example, the agent is unaware of the environment, so it takes random action \\n(move down) to start. As we can see in the above image, the Q-Table is updated using the \\nBellman equation. \\n \\nMeasuring the Rewards \\n \\nAfter taking the action, we will measure the outcome and the reward. \\n \\n\\uf0b7 \\nThe reward for reaching the goal is +1 \\n \\n\\uf0b7 \\nThe reward for taking the wrong path (falling into the hole) is 0 \\n \\n\\uf0b7 \\nThe reward for Idle or moving on the frozen lake is also 0. \\n \\nUpdate Q-Table \\n \\nWe will update the function Q(St, At) using the equation. It uses the previous episode’s \\nestimated Q-values, learning rate, and Temporal Differences error. Temporal Differences error \\nis calculated using Immediate reward, the discounted maximum expected future reward, and \\nthe former estimation Q-value. \\nThe process is repeated multiple times until the Q-Table is updated and the Q-value function is \\nmaximized.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-12-06T07:02:22+00:00', 'source': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'file_path': 'rag-dataset-main/machine-learning/CSIT_(R22)_3-2_MACHINE LEARNING_DIGITAL NOTES.pdf', 'total_pages': 120, 'format': 'PDF 1.5', 'title': '', 'author': 'MRCETIT', 'subject': '', 'keywords': '', 'moddate': '2024-12-06T07:02:22+00:00', 'trapped': '', 'modDate': 'D:20241206070222Z', 'creationDate': \"D:20241206070222+00'00'\", 'page': 119}, page_content='CSIT DEPT-R22-MACHINE LEARNING \\n116 \\n \\nAt the start, the agent is exploring the environment to update the Q-table. And when the Q- \\nTable is ready, the agent will start exploiting and start taking better decisions. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIn the case of a frozen lake, the agent will learn to take the shortest path to reach the goal and \\navoid jumping into the holes.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 0}, page_content='Lecture Notes in \\nMACHINE LEARNING \\n \\nDr V N Krishnachandran \\n \\n \\nVidya Centre for Artificial Intelligence Research'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 1}, page_content='This page is intentionally left blank.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 2}, page_content='LECTURE NOTES IN\\nMACHINE LEARNING\\nDr V N Krishnachandran\\nVidya Centre for Artiﬁcial Intelligence Research\\nVidya Academy of Science & Technology\\nThrissur - 680501'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 3}, page_content='Copyright © 2018 V. N. Krishnachandran\\nPublished by\\nVidya Centre for Artiﬁcial Intelligence Research\\nVidya Academy of Science & Technology\\nThrissur - 680501, Kerala, India\\nThe book was typeset by the author using the LATEX document preparation system.\\nCover design: Author\\nLicensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License. You may\\nnot use this ﬁle except in compliance with the License. You may obtain a copy of the License at\\nhttps://creativecommons.org/licenses/by/4.0/.\\nPrice: Rs 0.00.\\nFirst printing: July 2018'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 4}, page_content='Preface\\nThe book is exactly what its title claims it to be: lecture notes; nothing more, nothing less!\\nA reader looking for elaborate descriptive expositions of the concepts and tools of machine\\nlearning will be disappointed with this book. There are plenty of books out there in the market\\nwith different styles of exposition. Some of them give a lot of emphasis on the mathematical theory\\nbehind the algorithms. In some others the emphasis is on the verbal descriptions of algorithms\\navoiding the use of mathematical notations and concepts to the maximum extent possible. There is\\none book the author of which is so afraid of introducing mathematical symbols that he introduces\\nσ as “the Greek letter sigma similar to a b turned sideways\". But among these books, the author of\\nthese Notes could not spot a book that would give complete worked out examples illustrating the\\nvarious algorithms. These notes are expected to ﬁll this gap.\\nThe focus of this book is on giving a quick and fast introduction to the basic concepts and im-\\nportant algorithms in machine learning. In nearly all cases, whenever a new concept is introduced\\nit has been illustrated with “toy examples” and also with examples from real life situations. In the\\ncase of algorithms, wherever possible, the working of the algorithm has been illustrated with con-\\ncrete numerical examples. In some cases, the full algorithm may contain heavy use of mathematical\\nnotations and concepts. Practitioners of machine learning sometimes treat such algorithms as “black\\nbox algorithms”. Student readers of this book may skip these details on a ﬁrst reading.\\nThe book is written primarily for the students pursuing the B Tech programme in Computer\\nScience and Engineering of the APJ Abdul Kalam Technological University. The Curriculum for\\nthe programme offers a course on machine learning as an elective course in the Seventh Semester\\nwith code and name “CS 467 Machine Learning”. The selection of topics in the book was guided\\nby the contents of the syllabus for the course. The book will also be useful to faculty members who\\nteach the course.\\nThough the syllabus for CS 467 Machine Learning is reasonably well structured and covers most\\nof the basic concepts of machine learning, there is some lack of clarity on the depth to which the\\nvarious topics are to be covered. This ambiguity has been compounded by the lack of any mention\\nof a single textbook for the course and unfortunately the books cited as references treat machine\\nlearning at varying levels. The guiding principle the author has adopted in the selection of materials\\nin the preparation of these notes is that, at the end of the course, the student must acquire enough\\nunderstanding about the methodologies and concepts underlying the various topics mentioned in the\\nsyllabus.\\nAny study of machine learning algorithms without studying their implementations in software\\npackages is deﬁnitely incomplete. There are implementations of these algorithms available in the\\nR and Python programming languages. Two or three lines of code may be sufﬁcient to implement\\nan algorithm. Since the syllabus for CS 467 Machine Learning does not mandate the study of such\\nimplementations, this aspect of machine learning has not been included in this book. The students\\nare well advised to refer to any good book or the resources available in the internet to acquire a\\nworking knowledge of these implementations.\\nEvidently, there are no original material in this book. The readers can see shadows of everything\\npresented here in other sources which include the reference books listed in the syllabus of the course\\nreferred to earlier, other books on machine learning, published research/review papers and also\\nseveral open sources accessible through the internet. However, care has been taken to present the\\nmaterial borrowed from other sources in a format digestible to the targeted audience. There are\\niii'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 5}, page_content='iv\\nmore than a hundred ﬁgures in the book. Nearly all of them were drawn using the TikZ package for\\nLATEX. A few of the ﬁgures were created using the R programming language. A small number of\\nﬁgures are reproductions of images available in various websites. There surely will be many errors\\n– conceptual, technical and printing – in these notes. The readers are earnestly requested to point\\nout such errors to the author so that an error free book can be brought up in the future.\\nThe author wishes to put on record his thankfulness to Vidya Centre for Artiﬁcial Intelligence\\nResearch (V-CAIR) for agreeing to be the publisher of this book. V-CAIR is a research centre func-\\ntioning in Vidya Academy of Science & Technology, Thrissur, Kerala, established as part of the\\n“AI and Deep Learning: Skilling and Research” project launched by Royal Academy of Engineer-\\ning, UK, in collaboration with University College, London, Brunel University, London and Bennett\\nUniversity, India.\\nVAST Campus\\nDr V N Krishnachandran\\nJuly 2018\\nDepartment of Computer Applications\\nVidya Academy of Science & Technology, Thrissur - 680501\\n(email: krishnachandran.vn@vidyaacademy.ac.in)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 6}, page_content='Syllabus\\nCourse code\\nCourse Name\\nL - T - P - Credits\\nYear of introduction\\nCS467\\nMachine Learning\\n3 - 0 - 0 - 3\\n2016\\nCourse Objectives\\n• To introduce the prominent methods for machine learning\\n• To study the basics of supervised and unsupervised learning\\n• To study the basics of connectionist and other architectures\\nSyllabus\\nIntroduction to Machine Learning, Learning in Artiﬁcial Neural Networks, Decision trees, HMM,\\nSVM, and other Supervised and Unsupervised learning methods.\\nExpected Outcome\\nThe students will be able to\\ni) differentiate various learning approaches, and to interpret the concepts of supervised learn-\\ning\\nii) compare the different dimensionality reduction techniques\\niii) apply theoretical foundations of decision trees to identify best split and Bayesian classiﬁer\\nto label data points\\niv) illustrate the working of classiﬁer models like SVM, Neural Networks and identify classiﬁer\\nmodel for typical machine learning applications\\nv) identify the state sequence and evaluate a sequence emission probability from a given HMM\\nvi) illustrate and apply clustering algorithms and identify its applicability in real life problems\\nReferences\\n1. Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006.\\n2. Ethem Alpayidin, Introduction to Machine Learning (Adaptive Computation and machine\\nLearning), MIT Press, 2004.\\n3. Margaret H. Dunham, Data Mining: Introductory and Advanced Topics, Pearson, 2006.\\nv'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 7}, page_content='vi\\n4. Mitchell T., Machine Learning, McGraw Hill.\\n5. Ryszard S. Michalski, Jaime G. Carbonell, and Tom M. Mitchell, Machine Learning : An\\nArtiﬁcial Intelligence Approach, Tioga Publishing Company.\\nCourse Plan\\nModule I. Introduction to Machine Learning, Examples of Machine Learning applications -\\nLearning associations, Classiﬁcation, Regression, Unsupervised Learning, Reinforce-\\nment Learning. Supervised learning- Input representation, Hypothesis class, Version\\nspace, Vapnik-Chervonenkis (VC) Dimension\\nHours: 6. Semester exam marks: 15%\\nModule II. Probably Approximately Learning (PAC), Noise, Learning Multiple classes, Model\\nSelection and Generalization, Dimensionality reduction- Subset selection, Principle\\nComponent Analysis\\nHours: 8. Semester exam marks: 15%\\nFIRST INTERNAL EXAMINATION\\nModule III. Classiﬁcation- Cross validation and re-sampling methods- Kfold cross validation,\\nBoot strapping, Measuring classiﬁer performance- Precision, recall, ROC curves.\\nBayes Theorem, Bayesian classiﬁer, Maximum Likelihood estimation, Density func-\\ntions, Regression\\nHours: 8. Semester exam marks: 20%\\nModule IV. Decision Trees- Entropy, Information Gain, Tree construction, ID3, Issues in Decision\\nTree learning- Avoiding Over-ﬁtting, Reduced Error Pruning, The problem of Missing\\nAttributes, Gain Ratio, Classiﬁcation by Regression (CART), Neural Networks- The\\nPerceptron, Activation Functions, Training Feed Forward Network by Back Propaga-\\ntion.\\nHours: 6. Semester exam marks: 15%\\nSECOND INTERNAL EXAMINATION\\nModule V. Kernel Machines - Support Vector Machine - Optimal Separating hyper plane, Soft-\\nmargin hyperplane, Kernel trick, Kernel functions. Discrete Markov Processes, Hid-\\nden Markov models, Three basic problems of HMMs - Evaluation problem, ﬁnding\\nstate sequence, Learning model parameters. Combining multiple learners, Ways to\\nachieve diversity, Model combination schemes, Voting, Bagging, Booting\\nHours: 8. Semester exam marks: 20%\\nModule VI. Unsupervised Learning - Clustering Methods - K-means, Expect-ation-Maxi-mization\\nAlgorithm, Hierarchical Clustering Methods, Density based clustering\\nHours: 6. Semester exam marks: 15%\\nEND SEMESTER EXAMINATION\\nQuestion paper pattern\\n1. There will be FOUR parts in the question paper: A, B, C, D.\\n2. Part A\\na) Total marks: 40\\nb) TEN questions, each have 4 marks, covering all the SIX modules (THREE questions\\nfrom modules I & II; THREE questions from modules III & IV; FOUR questions from\\nmodules V & VI).'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 8}, page_content='vii\\nc) All the TEN questions have to be answered.\\n3. Part B\\na) Total marks: 18\\nb) THREE questions, each having 9 marks. One question is from module I; one question\\nis from module II; one question uniformly covers modules I & II.\\nc) Any TWO questions have to be answered.\\nd) Each question can have maximum THREE subparts.\\n4. Part C\\na) Total marks: 18\\nb) THREE questions, each having 9 marks. One question is from module III; one question\\nis from module IV; one question uniformly covers modules III & IV.\\nc) Any TWO questions have to be answered.\\nd) Each question can have maximum THREE subparts.\\n5. Part D\\na) Total marks: 24\\nb) THREE questions, each having 12 marks. One question is from module V; one question\\nis from module VI; one question uniformly covers modules V & VI.\\nc) Any TWO questions have to be answered.\\nd) Each question can have maximum THREE subparts.\\n6. There will be AT LEAST 60% analytical/numerical questions in all possible combinations of\\nquestion choices.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 9}, page_content='Contents\\nIntroduction\\niii\\nSyllabus\\nv\\n1\\nIntroduction to machine learning\\n1\\n1.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n1\\n1.2\\nHow machines learn\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n2\\n1.3\\nApplications of machine learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.4\\nUnderstanding data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n4\\n1.5\\nGeneral classes of machine learning problems . . . . . . . . . . . . . . . . . . . . . .\\n6\\n1.6\\nDifferent types of learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n1.7\\nSample questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n2\\nSome general concepts\\n15\\n2.1\\nInput representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2.2\\nHypothesis space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2.3\\nOrdering of hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n2.4\\nVersion space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n2.5\\nNoise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n2.6\\nLearning multiple classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n2.7\\nModel selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n2.8\\nGeneralisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n2.9\\nSample questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n3\\nVC dimension and PAC learning\\n27\\n3.1\\nVapnik-Chervonenkis dimension\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n3.2\\nProbably approximately correct learning . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n3.3\\nSample questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n4\\nDimensionality reduction\\n35\\n4.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n4.2\\nWhy dimensionality reduction is useful . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n4.3\\nSubset selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n4.4\\nPrincipal component analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n4.5\\nSample questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n5\\nEvaluation of classiﬁers\\n48\\n5.1\\nMethods of evaluation\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\n5.2\\nCross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\n5.3\\nK-fold cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\n5.4\\nMeasuring error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n5.5\\nReceiver Operating Characteristic (ROC)\\n. . . . . . . . . . . . . . . . . . . . . . . . 54\\n5.6\\nSample questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\nviii'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 10}, page_content='CONTENTS\\nix\\n6\\nBayesian classiﬁer and ML estimation\\n61\\n6.1\\nConditional probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\\n6.2\\nBayes’ theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n6.3\\nNaive Bayes algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\\n6.4\\nUsing numeric features with naive Bayes algorithm . . . . . . . . . . . . . . . . . . . 67\\n6.5\\nMaximum likelihood estimation (ML estimation) . . . . . . . . . . . . . . . . . . . . 68\\n6.6\\nSample questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\\n7\\nRegression\\n72\\n7.1\\nDeﬁnition\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\\n7.2\\nCriterion for minimisation of error . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\\n7.3\\nSimple linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\\n7.4\\nPolynomial regression\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\\n7.5\\nMultiple linear regression\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\\n7.6\\nSample questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\\n8\\nDecision trees\\n83\\n8.1\\nDecision tree: Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\\n8.2\\nTwo types of decision trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\\n8.3\\nClassiﬁcation trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\\n8.4\\nFeature selection measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\\n8.5\\nEntropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\\n8.6\\nInformation gain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\\n8.7\\nGini indices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\\n8.8\\nGain ratio\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\\n8.9\\nDecision tree algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\\n8.10\\nThe ID3 algorithm\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\\n8.11\\nRegression trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\\n8.12\\nCART algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\\n8.13\\nOther decision tree algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\\n8.14\\nIssues in decision tree learning\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\\n8.15\\nAvoiding overﬁtting of data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\\n8.16\\nProblem of missing attributes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\\n8.17\\nSample questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\\n9\\nNeural networks\\n111\\n9.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\\n9.2\\nBiological motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\\n9.3\\nArtiﬁcial neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\\n9.4\\nActivation function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\\n9.5\\nPerceptron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\\n9.6\\nArtiﬁcial neural networks\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\\n9.7\\nCharacteristics of an ANN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\\n9.8\\nBackpropagation\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\\n9.9\\nIntroduction to deep learning\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\\n9.10\\nSample questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\\n10 Support vector machines\\n133\\n10.1\\nAn example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\\n10.2\\nFinite dimensional vector spaces\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\\n10.3\\nHyperplanes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\\n10.4\\nTwo-class data sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\\n10.5\\nLinearly separable data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\\n10.6\\nMaximal margin hyperplanes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\\n10.7\\nMathematical formulation of the SVM problem . . . . . . . . . . . . . . . . . . . . . 147'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 11}, page_content='CONTENTS\\nx\\n10.8\\nSolution of the SVM problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\\n10.9\\nSoft margin hyperlanes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\\n10.10 Kernel functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\\n10.11 The kernel method (kernel trick)\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\\n10.12 Multiclass SVM’s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\\n10.13 Sample questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\\n11 Hidden Markov models\\n161\\n11.1\\nDiscrete Markov processes: Examples . . . . . . . . . . . . . . . . . . . . . . . . . . 161\\n11.2\\nDiscrete Markov processes: General case\\n. . . . . . . . . . . . . . . . . . . . . . . . 163\\n11.3\\nHidden Markov models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\\n11.4\\nThree basic problems of HMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\\n11.5\\nHMM application: Isolated word recognition . . . . . . . . . . . . . . . . . . . . . . 170\\n11.6\\nSample questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\\n12 Combining multiple learners\\n173\\n12.1\\nWhy combine many learners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\\n12.2\\nWays to achieve diversity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\\n12.3\\nModel combination schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\\n12.4\\nEnsemble learning⋆. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176\\n12.5\\nRandom forest⋆. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176\\n12.6\\nSample questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178\\n13 Clustering methods\\n179\\n13.1\\nClustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\\n13.2\\nk-means clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\\n13.3\\nMulti-modal distributions\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\\n13.4\\nMixture of normal distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\\n13.5\\nMixtures in terms of latent variables\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 188\\n13.6\\nExpectation-maximisation algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\\n13.7\\nThe EM algorithm for Gaussian mixtures\\n. . . . . . . . . . . . . . . . . . . . . . . . 190\\n13.8\\nHierarchical clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191\\n13.9\\nMeasures of dissimilarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194\\n13.10 Algorithm for agglomerative hierarchical clustering\\n. . . . . . . . . . . . . . . . . . 196\\n13.11 Algorithm for divisive hierarchical clustering . . . . . . . . . . . . . . . . . . . . . . 200\\n13.12 Density-based clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\\n13.13 Sample questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204\\nBibliography\\n206\\nIndex\\n207'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 12}, page_content='List of Figures\\n1.1\\nComponents of learning process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n2\\n1.2\\nExample for “examples” and “features” collected in a matrix format (data relates\\nto automobiles and their features) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5\\n1.3\\nGraphical representation of data in Table 1.1. Solid dots represent data in “Pass”\\nclass and hollow dots data in “Fail” class. The class label of the square dot is to be\\ndetermined.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n1.4\\nSupervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n2.1\\nData in Table 2.1 with hollow dots representing positive examples and solid dots\\nrepresenting negative examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n2.2\\nAn example hypothesis deﬁned by Eq. (2.5) . . . . . . . . . . . . . . . . . . . . . . . 17\\n2.3\\nHypothesis h′ is more general than hypothesis h′′ if and only if S′′ ⊆S′ . . . . . . . 18\\n2.4\\nValues of m which deﬁne the version space with data in Table 2.1 and hypothesis\\nspace deﬁned by Eq.(2.4) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n2.5\\nScatter plot of price-power data (hollow circles indicate positive examples and\\nsolid dots indicate negative examples)\\n. . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n2.6\\nThe version space consists of hypotheses corresponding to axis-aligned rectangles\\ncontained in the shaded region . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n2.7\\nExamples for overﬁtting and overﬁtting models . . . . . . . . . . . . . . . . . . . . . 24\\n2.8\\nFitting a classiﬁcation boundary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n3.1\\nDifferent forms of the set {x ∈S ∶h(x) = 1} for D = {a,b,c} . . . . . . . . . . . . . 28\\n3.2\\nGeometrical representation of the hypothesis ha,b,c . . . . . . . . . . . . . . . . . . . 30\\n3.3\\nA hypothesis ha,b,c consistent with the dichotomy deﬁned by the subset {A,C} of\\n{A,B,C} . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n3.4\\nThere is no hypothesis ha,b,c consistent with the dichotomy deﬁned by the subset\\n{A,C} of {A,B,C,D} . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n3.5\\nAn axis-aligned rectangle in the Euclidean plane . . . . . . . . . . . . . . . . . . . . 32\\n3.6\\nAxis-aligned rectangle which gives the tightest ﬁt to the positive examples\\n. . . . . 33\\n4.1\\nPrincipal components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n4.2\\nScatter plot of data in Table 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n4.3\\nCoordinate system for principal components . . . . . . . . . . . . . . . . . . . . . . . 45\\n4.4\\nProjections of data points on the axis of the ﬁrst principal component\\n. . . . . . . . 46\\n4.5\\nGeometrical representation of one-dimensional approximation to the data in Table\\n4.2\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n5.1\\nOne iteration in a 5-fold cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . 50\\n5.2\\nThe ROC space and some special points in the space . . . . . . . . . . . . . . . . . . 56\\n5.3\\nROC curves of three different classiﬁers A, B, C\\n. . . . . . . . . . . . . . . . . . . . 57\\n5.4\\nROC curve of data in Table 5.3 showing the points closest to the perfect prediction\\npoint (0,1)\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\n6.1\\nEvents A,B,C which are not mutually independent: Eqs.(6.1)–(6.3) are satisﬁed,\\nbut Eq.(6.4) is not satisﬁed.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\nxi'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 13}, page_content='LIST OF FIGURES\\nxii\\n6.2\\nEvents A,B,C which are not mutually independent: Eq.(6.4) is satisﬁed but Eqs.(6.1)–\\n(6.2) are not satisﬁed. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n6.3\\nDiscretization of numeric data: Example . . . . . . . . . . . . . . . . . . . . . . . . . 68\\n7.1\\nErrors in observed values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\\n7.2\\nRegression model for Table 7.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\\n7.3\\nPlot of quadratic polynomial model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\\n7.4\\nThe regression plane for the data in Table 7.4 . . . . . . . . . . . . . . . . . . . . . . 80\\n8.1\\nExample for a decision tree\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\\n8.2\\nThe graph-theoretical representation of the decision tree in Figure 8.6 . . . . . . . . 84\\n8.3\\nClassiﬁcation tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\\n8.4\\nClassiﬁcation tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\\n8.5\\nClassiﬁcation tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\\n8.6\\nPlot of p vs. Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\\n8.7\\nRoot node of the decision tree for data in Table 8.9 . . . . . . . . . . . . . . . . . . . 97\\n8.8\\nDecision tree for data in Table 8.9, after selecting the branching feature at root node\\n99\\n8.9\\nDecision tree for data in Table 8.9, after selecting the branching feature at Node 1 . 100\\n8.10\\nDecision tree for data in Table 8.9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\\n8.11\\nPart of a regression tree for Table 8.11 . . . . . . . . . . . . . . . . . . . . . . . . . . 102\\n8.12\\nPart of regression tree for Table 8.11 . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\\n8.13\\nA regression tree for Table 8.11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\\n8.14\\nImpact of overﬁtting in decision tree learning . . . . . . . . . . . . . . . . . . . . . . 107\\n9.1\\nAnatomy of a neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\\n9.2\\nFlow of signals in a biological neuron\\n. . . . . . . . . . . . . . . . . . . . . . . . . . 112\\n9.3\\nSchematic representation of an artiﬁcial neuron . . . . . . . . . . . . . . . . . . . . . 112\\n9.4\\nSimpliﬁed representation of an artiﬁcial neuron . . . . . . . . . . . . . . . . . . . . . 113\\n9.5\\nThreshold activation function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\\n9.6\\nUnit step activation function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\\n9.7\\nThe sigmoid activation function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\\n9.8\\nLinear activation function\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\\n9.9\\nPiecewise linear activation function . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\\n9.10\\nGaussian activation function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\\n9.11\\nHyperbolic tangent activation function . . . . . . . . . . . . . . . . . . . . . . . . . . 116\\n9.12\\nSchematic representation of a perceptrn\\n. . . . . . . . . . . . . . . . . . . . . . . . . 116\\n9.13\\nRepresentation of x1 ANDx2 by a perceptron . . . . . . . . . . . . . . . . . . . . . . 117\\n9.14\\nAn ANN with only one layer\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\\n9.15\\nAn ANN with two layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\\n9.16\\nExamples of different topologies of networks . . . . . . . . . . . . . . . . . . . . . . 122\\n9.17\\nA simpliﬁed model of the error surface showing the direction of gradient\\n. . . . . . 123\\n9.18\\nANN for illustrating backpropagation algorithm . . . . . . . . . . . . . . . . . . . . . 124\\n9.19\\nANN for illustrating backpropagation algorithm with initial values for weights . . . 124\\n9.20\\nNotations of backpropagation algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 128\\n9.21\\nNotations of backpropagation algorithm: The i-th node in layer j . . . . . . . . . . . 128\\n9.22\\nA shallow neural network\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\\n9.23\\nA deep neural network with three hidden layers . . . . . . . . . . . . . . . . . . . . . 130\\n10.1\\nScatter plot of data in Table 10.1 (ﬁlled circles represent “yes” and unﬁlled circles\\n“no”) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\\n10.2\\nScatter plot of data in Table 10.1 with a separating line . . . . . . . . . . . . . . . . . 135\\n10.3\\nTwo separating lines for the data in Table 10.1 . . . . . . . . . . . . . . . . . . . . . . 135\\n10.4\\nShortest perpendicular distance of a separating line from data points . . . . . . . . . 136\\n10.5\\nMaximum margin line for data in Table 10.1 . . . . . . . . . . . . . . . . . . . . . . . 136\\n10.6\\nSupport vectors for data in Table 10.1\\n. . . . . . . . . . . . . . . . . . . . . . . . . . 137'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 14}, page_content='LIST OF FIGURES\\nxiii\\n10.7\\nBoundaries of “street” of maximum width separating “yes” points and “no” points\\nin Table 10.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\\n10.8\\nPlot of the maximum margin line of data in Table 10.1 produced by the R program-\\nming language\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\\n10.9\\nHalf planes deﬁned by a line . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\\n10.10 Perpendicular distance of a point from a plane . . . . . . . . . . . . . . . . . . . . . . 143\\n10.11 Scatterplot of data in Table 10.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\\n10.12 Maximal separating hyperplane, margin and support vectors . . . . . . . . . . . . . . 146\\n10.13 Maximal margin hyperplane of a 2-sample set in 2-dimensional space . . . . . . . . 147\\n10.14 Maximal margin hyperplane of a 3-sample set in 2-dimensional space . . . . . . . . 147\\n10.15 Soft margin hyperplanes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\\n10.16 One-against all\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\\n10.17 One-against-one . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\\n11.1\\nA state diagram showing state transition probabilities . . . . . . . . . . . . . . . . . . 162\\n11.2\\nA two-coin model of an HMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\\n11.3\\nAn N-state urn and ball model which illustrates the general case of a discrete\\nsymbol HMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\\n11.4\\nBlock diagram of an isolated word HMM recogniser . . . . . . . . . . . . . . . . . . 171\\n12.1\\nExample of random forest with majority voting . . . . . . . . . . . . . . . . . . . . . 177\\n13.1\\nScatter diagram of data in Table 13.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\\n13.2\\nInitial choice of cluster centres and the resulting clusters . . . . . . . . . . . . . . . . 181\\n13.3\\nCluster centres after ﬁrst iteration and the corresponding clusters . . . . . . . . . . . 182\\n13.4\\nNew cluster centres and the corresponding clusters . . . . . . . . . . . . . . . . . . . 183\\n13.5\\nProbability distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\\n13.6\\nGraph of pdf deﬁned by Eq.(13.9) superimposed on the histogram of the data in\\nTable 13.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188\\n13.7\\nA dendrogram of the dataset {a,b,c,d,e} . . . . . . . . . . . . . . . . . . . . . . . . 192\\n13.8\\nDifferent ways of drawing dendrogram . . . . . . . . . . . . . . . . . . . . . . . . . . 192\\n13.9\\nA dendrogram of the dataset {a,b,c,d,e} showing the distances (heights) of the\\nclusters at different levels\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\\n13.10 Hierarchical clustering using agglomerative method\\n. . . . . . . . . . . . . . . . . . 193\\n13.11 Hierarchical clustering using divisive method . . . . . . . . . . . . . . . . . . . . . . 195\\n13.12 Length of the solid line “ae” is max{d(x,y) ∶x ∈A,y ∈B} . . . . . . . . . . . . . . 196\\n13.13 Length of the solid line “bc” is min{d(x,y) ∶x ∈A,y ∈B} . . . . . . . . . . . . . . 196\\n13.14 Dendrogram for the data given in Table 13.4 (complete linkage clustering)\\n. . . . . 199\\n13.15 Dendrogram for the data given in Table 13.4 (single linkage clustering) . . . . . . . 200\\n13.16 Dx= (average of dashed lines) −(average of solid lines) . . . . . . . . . . . . . . . . 201\\n13.17 Clusters of points and noise points not belonging to any of those clusters\\n. . . . . . 203\\n13.18 With m0 = 4: (a) p a point of high density (b) p a core point (c) p a border point\\n(d) r a noise point . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\\n13.19 With m0 = 4: (a) q is directly density-reachable from p (b) q is indirectly density-\\nreachable from p\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 15}, page_content='Chapter 1\\nIntroduction to machine learning\\nIn this chapter, we consider different deﬁnitions of the term “machine learning” and explain what\\nis meant by “learning” in the context of machine learning. We also discuss the various components\\nof the machine learning process. There are also brief discussions about different types learning like\\nsupervised learning, unsupervised learning and reinforcement learning.\\n1.1\\nIntroduction\\n1.1.1\\nDeﬁnition of machine learning\\nArthur Samuel, an early American leader in the ﬁeld of computer gaming and artiﬁcial intelligence,\\ncoined the term “Machine Learning” in 1959 while at IBM. He deﬁned machine learning as “the ﬁeld\\nof study that gives computers the ability to learn without being explicitly programmed.” However,\\nthere is no universally accepted deﬁnition for machine learning. Different authors deﬁne the term\\ndifferently. We give below two more deﬁnitions.\\n1. Machine learning is programming computers to optimize a performance criterion using exam-\\nple data or past experience. We have a model deﬁned up to some parameters, and learning is\\nthe execution of a computer program to optimize the parameters of the model using the train-\\ning data or past experience. The model may be predictive to make predictions in the future, or\\ndescriptive to gain knowledge from data, or both (see [2] p.3).\\n2. The ﬁeld of study known as machine learning is concerned with the question of how to con-\\nstruct computer programs that automatically improve with experience (see [4], Preface.).\\nRemarks\\nIn the above deﬁnitions we have used the term “model” and we will be using this term at several\\ncontexts later in this book. It appears that there is no universally accepted one sentence deﬁnition\\nof this term. Loosely, it may be understood as some mathematical expression or equation, or some\\nmathematical structures such as graphs and trees, or a division of sets into disjoint subsets, or a set\\nof logical “if ... then ... else ...” rules, or some such thing. It may be noted that this is not an\\nexhaustive list.\\n1.1.2\\nDeﬁnition of learning\\nDeﬁnition\\nA computer program is said to learn from experience E with respect to some class of tasks T and\\nperformance measure P, if its performance at tasks T, as measured by P, improves with experience\\nE.\\n1'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 16}, page_content='CHAPTER 1. INTRODUCTION TO MACHINE LEARNING\\n2\\nExamples\\ni) Handwriting recognition learning problem\\n• Task T: Recognising and classifying handwritten words within images\\n• Performance P: Percent of words correctly classiﬁed\\n• Training experience E: A dataset of handwritten words with given classiﬁcations\\nii) A robot driving learning problem\\n• Task T: Driving on highways using vision sensors\\n• Performance measure P: Average distance traveled before an error\\n• training experience: A sequence of images and steering commands recorded while\\nobserving a human driver\\niii) A chess learning problem\\n• Task T: Playing chess\\n• Performance measure P: Percent of games won against opponents\\n• Training experience E: Playing practice games against itself\\nDeﬁnition\\nA computer program which learns from experience is called a machine learning program or simply\\na learning program. Such a program is sometimes also referred to as a learner.\\n1.2\\nHow machines learn\\n1.2.1\\nBasic components of learning process\\nThe learning process, whether by a human or a machine, can be divided into four components,\\nnamely, data storage, abstraction, generalization and evaluation. Figure 1.1 illustrates the various\\ncomponents and the steps involved in the learning process.\\nData\\nConcepts\\nInferences\\nData storage\\nAbstraction\\nGeneralization\\nEvaluation\\nFigure 1.1: Components of learning process\\n1. Data storage\\nFacilities for storing and retrieving huge amounts of data are an important component of\\nthe learning process. Humans and computers alike utilize data storage as a foundation for\\nadvanced reasoning.\\n• In a human being, the data is stored in the brain and data is retrieved using electrochem-\\nical signals.\\n• Computers use hard disk drives, ﬂash memory, random access memory and similar de-\\nvices to store data and use cables and other technology to retrieve data.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 17}, page_content='CHAPTER 1. INTRODUCTION TO MACHINE LEARNING\\n3\\n2. Abstraction\\nThe second component of the learning process is known as abstraction.\\nAbstraction is the process of extracting knowledge about stored data. This involves creating\\ngeneral concepts about the data as a whole. The creation of knowledge involves application\\nof known models and creation of new models.\\nThe process of ﬁtting a model to a dataset is known as training. When the model has been\\ntrained, the data is transformed into an abstract form that summarizes the original information.\\n3. Generalization\\nThe third component of the learning process is known as generalisation.\\nThe term generalization describes the process of turning the knowledge about stored data into\\na form that can be utilized for future action. These actions are to be carried out on tasks that\\nare similar, but not identical, to those what have been seen before. In generalization, the goal\\nis to discover those properties of the data that will be most relevant to future tasks.\\n4. Evaluation\\nEvaluation is the last component of the learning process.\\nIt is the process of giving feedback to the user to measure the utility of the learned knowledge.\\nThis feedback is then utilised to effect improvements in the whole learning process.\\n1.3\\nApplications of machine learning\\nApplication of machine learning methods to large databases is called data mining. In data mining, a\\nlarge volume of data is processed to construct a simple model with valuable use, for example, having\\nhigh predictive accuracy.\\nThe following is a list of some of the typical applications of machine learning.\\n1. In retail business, machine learning is used to study consumer behaviour.\\n2. In ﬁnance, banks analyze their past data to build models to use in credit applications, fraud\\ndetection, and the stock market.\\n3. In manufacturing, learning models are used for optimization, control, and troubleshooting.\\n4. In medicine, learning programs are used for medical diagnosis.\\n5. In telecommunications, call patterns are analyzed for network optimization and maximizing\\nthe quality of service.\\n6. In science, large amounts of data in physics, astronomy, and biology can only be analyzed fast\\nenough by computers. The World Wide Web is huge; it is constantly growing and searching\\nfor relevant information cannot be done manually.\\n7. In artiﬁcial intelligence, it is used to teach a system to learn and adapt to changes so that the\\nsystem designer need not foresee and provide solutions for all possible situations.\\n8. It is used to ﬁnd solutions to many problems in vision, speech recognition, and robotics.\\n9. Machine learning methods are applied in the design of computer-controlled vehicles to steer\\ncorrectly when driving on a variety of roads.\\n10. Machine learning methods have been used to develop programmes for playing games such as\\nchess, backgammon and Go.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 18}, page_content='CHAPTER 1. INTRODUCTION TO MACHINE LEARNING\\n4\\n1.4\\nUnderstanding data\\nSince an important component of the machine learning process is data storage, we brieﬂy consider\\nin this section the different types and forms of data that are encountered in the machine learning\\nprocess.\\n1.4.1\\nUnit of observation\\nBy a unit of observation we mean the smallest entity with measured properties of interest for a study.\\nExamples\\n• A person, an object or a thing\\n• A time point\\n• A geographic region\\n• A measurement\\nSometimes, units of observation are combined to form units such as person-years.\\n1.4.2\\nExamples and features\\nDatasets that store the units of observation and their properties can be imagined as collections of\\ndata consisting of the following:\\n• Examples\\nAn “example” is an instance of the unit of observation for which properties have been recorded.\\nAn “example” is also referred to as an “instance”, or “case” or “record.” (It may be noted that\\nthe word “example” has been used here in a technical sense.)\\n• Features\\nA “feature” is a recorded property or a characteristic of examples. It is also referred to as\\n“attribute”, or “variable” or “feature.”\\nExamples for “examples” and “features”\\n1. Cancer detection\\nConsider the problem of developing an algorithm for detecting cancer. In this study we note\\nthe following.\\n(a) The units of observation are the patients.\\n(b) The examples are members of a sample of cancer patients.\\n(c) The following attributes of the patients may be chosen as the features:\\n• gender\\n• age\\n• blood pressure\\n• the ﬁndings of the pathology report after a biopsy\\n2. Pet selection\\nSuppose we want to predict the type of pet a person will choose.\\n(a) The units are the persons.\\n(b) The examples are members of a sample of persons who own pets.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 19}, page_content='CHAPTER 1. INTRODUCTION TO MACHINE LEARNING\\n5\\nFigure 1.2: Example for “examples” and “features” collected in a matrix format (data relates to\\nautomobiles and their features)\\n(c) The features might include age, home region, family income, etc. of persons who own\\npets.\\n3. Spam e-mail\\nLet it be required to build a learning algorithm to identify spam e-mail.\\n(a) The unit of observation could be an e-mail messages.\\n(b) The examples would be speciﬁc messages.\\n(c) The features might consist of the words used in the messages.\\nExamples and features are generally collected in a “matrix format”. Fig. 1.2 shows such a data\\nset.\\n1.4.3\\nDifferent forms of data\\n1. Numeric data\\nIf a feature represents a characteristic measured in numbers, it is called a numeric feature.\\n2. Categorical or nominal\\nA categorical feature is an attribute that can take on one of a limited, and usually ﬁxed, number\\nof possible values on the basis of some qualitative property. A categorical feature is also called\\na nominal feature.\\n3. Ordinal data\\nThis denotes a nominal variable with categories falling in an ordered list. Examples include\\nclothing sizes such as small, medium, and large, or a measurement of customer satisfaction\\non a scale from “not at all happy” to “very happy.”\\nExamples\\nIn the data given in Fig.1.2, the features “year”, “price” and “mileage” are numeric and the features\\n“model”, “color” and “transmission” are categorical.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 20}, page_content='CHAPTER 1. INTRODUCTION TO MACHINE LEARNING\\n6\\n1.5\\nGeneral classes of machine learning problems\\n1.5.1\\nLearning associations\\n1. Association rule learning\\nAssociation rule learning is a machine learning method for discovering interesting relations, called\\n“association rules”, between variables in large databases using some measures of “interestingness”.\\n2. Example\\nConsider a supermarket chain. The management of the chain is interested in knowing whether\\nthere are any patterns in the purchases of products by customers like the following:\\n“If a customer buys onions and potatoes together, then he/she is likely to also buy\\nhamburger.”\\nFrom the standpoint of customer behaviour, this deﬁnes an association between the set of\\nproducts {onion, potato} and the set {burger}. This association is represented in the form of\\na rule as follows:\\n{onion, potato} ⇒{burger}\\nThe measure of how likely a customer, who has bought onion and potato, to buy burger also\\nis given by the conditional probability\\nP({onion, potato}∣{burger}).\\nIf this conditional probability is 0.8, then the rule may be stated more precisely as follows:\\n“80% of customers who buy onion and potato also buy burger.”\\n3. How association rules are made use of\\nConsider an association rule of the form\\nX ⇒Y,\\nthat is, if people buy X then they are also likely to buy Y .\\nSuppose there is a customer who buys X and does not buy Y . Then that customer is a potential\\nY customer. Once we ﬁnd such customers, we can target them for cross-selling. A knowledge of\\nsuch rules can be used for promotional pricing or product placements.\\n4. General case\\nIn ﬁnding an association rule X ⇒Y , we are interested in learning a conditional probability of\\nthe form P(Y ∣X) where Y is the product the customer may buy and X is the product or the set of\\nproducts the customer has already purchased.\\nIf we may want to make a distinction among customers, we may estimate P(Y ∣X,D) where\\nD is a set of customer attributes, like gender, age, marital status, and so on, assuming that we have\\naccess to this information.\\n5. Algorithms\\nThere are several algorithms for generating association rules. Some of the well-known algorithms\\nare listed below:\\na) Apriori algorithm\\nb) Eclat algorithm\\nc) FP-Growth Algorithm (FP stands for Frequency Pattern)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 21}, page_content='CHAPTER 1. INTRODUCTION TO MACHINE LEARNING\\n7\\n1.5.2\\nClassiﬁcation\\n1. Deﬁnition\\nIn machine learning, classiﬁcation is the problem of identifying to which of a set of categories a\\nnew observation belongs, on the basis of a training set of data containing observations (or instances)\\nwhose category membership is known.\\n2. Example\\nConsider the following data:\\nScore1\\n29\\n22\\n10\\n31\\n17\\n33\\n32\\n20\\nScore2\\n43\\n29\\n47\\n55\\n18\\n54\\n40\\n41\\nResult\\nPass\\nFail\\nFail\\nPass\\nFail\\nPass\\nPass\\nPass\\nTable 1.1: Example data for a classiﬁcation problem\\nData in Table 1.1 is the training set of data. There are two attributes “Score1” and “Score2”. The\\nclass label is called “Result”. The class label has two possible values “Pass” and “Fail”. The data\\ncan be divided into two categories or classes: The set of data for which the class label is “Pass” and\\nthe set of data for which the class label is“Fail”.\\nLet us assume that we have no knowledge about the data other than what is given in the table.\\nNow, the problem can be posed as follows: If we have some new data, say “Score1 = 25” and\\n“Score2 = 36”, what value should be assigned to “Result” corresponding to the new data; in other\\nwords, to which of the two categories or classes the new observation should be assigned? See Figure\\n1.3 for a graphical representation of the problem.\\nScore1\\nScore2\\n?\\n0\\n10\\n20\\n30\\n40\\n10\\n20\\n30\\n40\\n50\\n60\\nFigure 1.3: Graphical representation of data in Table 1.1. Solid dots represent data in “Pass” class\\nand hollow dots data in “Fail” class. The class label of the square dot is to be determined.\\nTo answer this question, using the given data alone we need to ﬁnd the rule, or the formula, or\\nthe method that has been used in assigning the values to the class label “Result”. The problem of\\nﬁnding this rule or formula or the method is the classiﬁcation problem. In general, even the general\\nform of the rule or function or method will not be known. So several different rules, etc. may have\\nto be tested to obtain the correct rule or function or method.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 22}, page_content='CHAPTER 1. INTRODUCTION TO MACHINE LEARNING\\n8\\n3. Real life examples\\ni) Optical character recognition\\nOptical character recognition problem, which is the problem of recognizing character codes\\nfrom their images, is an example of classiﬁcation problem. This is an example where there\\nare multiple classes, as many as there are characters we would like to recognize. Especially\\ninteresting is the case when the characters are handwritten. People have different handwrit-\\ning styles; characters may be written small or large, slanted, with a pen or pencil, and there\\nare many possible images corresponding to the same character.\\nii) Face recognition\\nIn the case of face recognition, the input is an image, the classes are people to be recognized,\\nand the learning program should learn to associate the face images to identities. This prob-\\nlem is more difﬁcult than optical character recognition because there are more classes, input\\nimage is larger, and a face is three-dimensional and differences in pose and lighting cause\\nsigniﬁcant changes in the image.\\niii) Speech recognition\\nIn speech recognition, the input is acoustic and the classes are words that can be uttered.\\niv) Medical diagnosis\\nIn medical diagnosis, the inputs are the relevant information we have about the patient and\\nthe classes are the illnesses. The inputs contain the patient’s age, gender, past medical\\nhistory, and current symptoms. Some tests may not have been applied to the patient, and\\nthus these inputs would be missing.\\nv) Knowledge extraction\\nClassiﬁcation rules can also be used for knowledge extraction. The rule is a simple model\\nthat explains the data, and looking at this model we have an explanation about the process\\nunderlying the data.\\nvi) Compression\\nClassiﬁcation rules can be used for compression. By ﬁtting a rule to the data, we get an\\nexplanation that is simpler than the data, requiring less memory to store and less computation\\nto process.\\nvii) More examples\\nHere are some further examples of classiﬁcation problems.\\n(a) An emergency room in a hospital measures 17 variables like blood pressure, age, etc.\\nof newly admitted patients. A decision has to be made whether to put the patient in an\\nICU. Due to the high cost of ICU, only patients who may survive a month or more are\\ngiven higher priority. Such patients are labeled as “low-risk patients” and others are\\nlabeled “high-risk patients”. The problem is to device a rule to classify a patient as a\\n“low-risk patient” or a “high-risk patient”.\\n(b) A credit card company receives hundreds of thousands of applications for new cards.\\nThe applications contain information regarding several attributes like annual salary,\\nage, etc. The problem is to devise a rule to classify the applicants to those who are\\ncredit-worthy, who are not credit-worthy or to those who require further analysis.\\n(c) Astronomers have been cataloguing distant objects in the sky using digital images cre-\\nated using special devices. The objects are to be labeled as star, galaxy, nebula, etc.\\nThe data is highly noisy and are very faint. The problem is to device a rule using which\\na distant object can be correctly labeled.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 23}, page_content='CHAPTER 1. INTRODUCTION TO MACHINE LEARNING\\n9\\n4. Discriminant\\nA discriminant of a classiﬁcation problem is a rule or a function that is used to assign labels to new\\nobservations.\\nExamples\\ni) Consider the data given in Table 1.1 and the associated classiﬁcation problem. We may\\nconsider the following rules for the classiﬁcation of the new data:\\nIF Score1 + Score2 ≥60, THEN “Pass” ELSE “Fail”.\\nIF Score1 ≥20 AND Score2 ≥40 THEN “Pass” ELSE “Fail”.\\nOr, we may consider the following rules with unspeciﬁed values for M,m1,m2 and then by\\nsome method estimate their values.\\nIF Score1 + Score2 ≥M, THEN “Pass” ELSE “Fail”.\\nIF Score1 ≥m1 AND Score2 ≥m2 THEN “Pass” ELSE “Fail”.\\nii) Consider a ﬁnance company which lends money to customers. Before lending money, the\\ncompany would like to assess the risk associated with the loan. For simplicity, let us assume\\nthat the company assesses the risk based on two variables, namely, the annual income and\\nthe annual savings of the customers.\\nLet x1 be the annual income and x2 be the annual savings of a customer.\\n• After using the past data, a rule of the following form with suitable values for θ1 and\\nθ2 may be formulated:\\nIF x1 > θ1 AND x2 > θ2 THEN “low-risk” ELSE “high-risk”.\\nThis rule is an example of a discriminant.\\n• Based on the past data, a rule of the following form may also be formulated:\\nIF x2 −0.2x1 > 0 THEN “low-risk” ELSE “high-risk”.\\nIn this case the rule may be thought of as the discriminant. The function f(x1,x2) =\\nx2 −0,2x1 can also be considered as the discriminant.\\n5. Algorithms\\nThere are several machine learning algorithms for classiﬁcation. The following are some of the\\nwell-known algorithms.\\na) Logistic regression\\nb) Naive Bayes algorithm\\nc) k-NN algorithm\\nd) Decision tree algorithm\\ne) Support vector machine algorithm\\nf) Random forest algorithm'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 24}, page_content='CHAPTER 1. INTRODUCTION TO MACHINE LEARNING\\n10\\nRemarks\\n• A classiﬁcation problem requires that examples be classiﬁed into one of two or more classes.\\n• A classiﬁcation can have real-valued or discrete input variables.\\n• A problem with two classes is often called a two-class or binary classiﬁcation problem.\\n• A problem with more than two classes is often called a multi-class classiﬁcation problem.\\n• A problem where an example is assigned multiple classes is called a multi-label classiﬁcation\\nproblem.\\n1.5.3\\nRegression\\n1. Deﬁnition\\nIn machine learning, a regression problem is the problem of predicting the value of a numeric vari-\\nable based on observed values of the variable. The value of the output variable may be a number,\\nsuch as an integer or a ﬂoating point value. These are often quantities, such as amounts and sizes.\\nThe input variables may be discrete or real-valued.\\n2. Example\\nConsider the data on car prices given in Table 1.2.\\nPrice\\nAge\\nDistance\\nWeight\\n(US$)\\n(years)\\n(KM)\\n(pounds)\\n13500\\n23\\n46986\\n1165\\n13750\\n23\\n72937\\n1165\\n13950\\n24\\n41711\\n1165\\n14950\\n26\\n48000\\n1165\\n13750\\n30\\n38500\\n1170\\n12950\\n32\\n61000\\n1170\\n16900\\n27\\n94612\\n1245\\n18600\\n30\\n75889\\n1245\\n21500\\n27\\n19700\\n1185\\n12950\\n23\\n71138\\n1105\\nTable 1.2: Prices of used cars: example data for regression\\nSuppose we are required to estimate the price of a car aged 25 years with distance 53240 KM\\nand weight 1200 pounds. This is an example of a regression problem beause we have to predict the\\nvalue of the numeric variable “Price”.\\n3. General approach\\nLet x denote the set of input variables and y the output variable. In machine learning, the general\\napproach to regression is to assume a model, that is, some mathematical relation between x and y,\\ninvolving some parameters say, θ, in the following form:\\ny = f(x,θ)\\nThe function f(x,θ) is called the regression function. The machine learning algorithm optimizes\\nthe parameters in the set θ such that the approximation error is minimized; that is, the estimates\\nof the values of the dependent variable y are as close as possible to the correct values given in the\\ntraining set.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 25}, page_content='CHAPTER 1. INTRODUCTION TO MACHINE LEARNING\\n11\\nExample\\nFor example, if the input variables are “Age”, “Distance” and “Weight” and the output variable\\nis “Price”, the model may be\\ny = f(x,θ)\\nPrice = a0 + a1 × (Age) + a2 × (Distance) + a3 × (Weight)\\nwhere x = (Age, Distance, Weight) denotes the the set of input variables and θ = (a0,a1,a2,a3)\\ndenotes the set of parameters of the model.\\n4. Different regression models\\nThere are various types of regression techniques available to make predictions. These techniques\\nmostly differ in three aspects, namely, the number and type of independent variables, the type of\\ndependent variables and the shape of regression line. Some of these are listed below.\\n• Simple linear regression: There is only one continuous independent variable x and the as-\\nsumed relation between the independent variable and the dependent variable y is\\ny = a + bx.\\n• Multivariate linear regression: There are more than one independent variable, say x1,...,xn,\\nand the assumed relation between the independent variables and the dependent variable is\\ny = a0 + a1x1 + ⋯+ anxn.\\n• Polynomial regression: There is only one continuous independent variable x and the assumed\\nmodel is\\ny = a0 + a1x + ⋯+ anxn.\\n• Logistic regression: The dependent variable is binary, that is, a variable which takes only the\\nvalues 0 and 1. The assumed model involves certain probability distributions.\\n1.6\\nDifferent types of learning\\nIn general, machine learning algorithms can be classiﬁed into three types.\\n1.6.1\\nSupervised learning\\nSupervised learning is the machine learning task of learning a function that maps an input to an\\noutput based on example input-output pairs.\\nIn supervised learning, each example in the training set is a pair consisting of an input object\\n(typically a vector) and an output value. A supervised learning algorithm analyzes the training\\ndata and produces a function, which can be used for mapping new examples. In the optimal case,\\nthe function will correctly determine the class labels for unseen instances. Both classiﬁcation and\\nregression problems are supervised learning problems.\\nA wide range of supervised learning algorithms are available, each with its strengths and weak-\\nnesses. There is no single learning algorithm that works best on all supervised learning problems.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 26}, page_content='CHAPTER 1. INTRODUCTION TO MACHINE LEARNING\\n12\\nFigure 1.4: Supervised learning\\nRemarks\\nA “supervised learning” is so called because the process of an algorithm learning from the training\\ndataset can be thought of as a teacher supervising the learning process. We know the correct answers\\n(that is, the correct outputs), the algorithm iteratively makes predictions on the training data and\\nis corrected by the teacher. Learning stops when the algorithm achieves an acceptable level of\\nperformance.\\nExample\\nConsider the following data regarding patients entering a clinic. The data consists of the\\ngender and age of the patients and each patient is labeled as “healthy” or “sick”.\\ngender\\nage\\nlabel\\nM\\n48\\nsick\\nM\\n67\\nsick\\nF\\n53\\nhealthy\\nM\\n49\\nhealthy\\nF\\n34\\nsick\\nM\\n21\\nhealthy\\nBased on this data, when a new patient enters the clinic, how can one predict whether he/she\\nis healthy or sick?\\n1.6.2\\nUnsupervised learning\\nUnsupervised learning is a type of machine learning algorithm used to draw inferences from datasets\\nconsisting of input data without labeled responses.\\nIn unsupervised learning algorithms, a classiﬁcation or categorization is not included in the\\nobservations. There are no output values and so there is no estimation of functions. Since the\\nexamples given to the learner are unlabeled, the accuracy of the structure that is output by the\\nalgorithm cannot be evaluated.\\nThe most common unsupervised learning method is cluster analysis, which is used for ex-\\nploratory data analysis to ﬁnd hidden patterns or grouping in data.\\nExample\\nConsider the following data regarding patients entering a clinic. The data consists of the\\ngender and age of the patients.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 27}, page_content='CHAPTER 1. INTRODUCTION TO MACHINE LEARNING\\n13\\ngender\\nage\\nM\\n48\\nM\\n67\\nF\\n53\\nM\\n49\\nF\\n34\\nM\\n21\\nBased on this data, can we infer anything regarding the patients entering the clinic?\\n1.6.3\\nReinforcement learning\\nReinforcement learning is the problem of getting an agent to act in the world so as to maximize its\\nrewards.\\nA learner (the program) is not told what actions to take as in most forms of machine learning, but\\ninstead must discover which actions yield the most reward by trying them. In the most interesting\\nand challenging cases, actions may affect not only the immediate reward but also the next situations\\nand, through that, all subsequent rewards.\\nFor example, consider teaching a dog a new trick: we cannot tell it what to do, but we can\\nreward/punish it if it does the right/wrong thing. It has to ﬁnd out what it did that made it get the\\nreward/punishment. We can use a similar method to train computers to do many tasks, such as\\nplaying backgammon or chess, scheduling jobs, and controlling robot limbs.\\nReinforcement learning is different from supervised learning. Supervised learning is learning\\nfrom examples provided by a knowledgeable expert.\\n1.7\\nSample questions\\n(a) Short answer questions\\n1. What is meant by “learning” in the context of machine learning?\\n2. List out the types of machine learning.\\n3. Distinguish between classiﬁcation and regression.\\n4. What are the differences between supervised and unsupervised learning?\\n5. What is meant by supervised classiﬁcation?\\n6. Explain supervised learning with an example.\\n7. What do you mean by reinforcement learning?\\n8. What is an association rule?\\n9. Explain the concept of Association rule learning. Give the names of two algorithms for gen-\\nerating association rules.\\n10. What is a classiﬁcation problem in machine learning. Illustrate with an example.\\n11. Give three examples of classiﬁcation problems from real life situations.\\n12. What is a discriminant in a classiﬁcation problem?\\n13. List three machine learning algorithms for solving classiﬁcation problems.\\n14. What is a binary classiﬁcation problem? Explain with an example. Give also an example for\\na classiﬁcation problem which is not binary.\\n15. What is regression problem. What are the different types of regression?'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 28}, page_content='CHAPTER 1. INTRODUCTION TO MACHINE LEARNING\\n14\\n(b) Long answer questions\\n1. Give a deﬁnition of the term “machine learning”. Explain with an example the concept of\\nlearning in the context of machine learning.\\n2. Describe the basic components of the machine learning process.\\n3. Describe in detail applications of machine learning in any three different knowledge domains.\\n4. Describe with an example the concept of association rule learning. Explain how it is made\\nuse of in real life situations.\\n5. What is the classiﬁcation problem in machine learning? Describe three real life situations in\\ndifferent domains where such problems arise.\\n6. What is meant by a discriminant of a classiﬁcation problem? Illustrate the idea with examples.\\n7. Describe in detail with examples the different types of learning like the supervised learning,\\netc.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 29}, page_content='Chapter 2\\nSome general concepts\\nIn this chapter we introduce some general concepts related to one of the simplest examples of su-\\npervised learning, namely, the classiﬁcation problem. We consider mainly binary classiﬁcation\\nproblems. In this context we introduce the concepts of hypothesis, hypothesis space and version\\nspace. We conclude the chapter with a brief discussion on how to select hypothesis models and how\\nto evaluate the performance of a model.\\n2.1\\nInput representation\\nThe general classiﬁcation problem is concerned with assigning a class label to an unknown instance\\nfrom instances of known assignments of labels. In a real world problem, a given situation or an\\nobject will have large number of features which may contribute to the assignment of the labels.\\nBut in practice, not all these features may be equally relevant or important. Only those which are\\nsigniﬁcant need be considered as inputs for assigning the class labels. These features are referred to\\nas the “input features” for the problem. They are also said to constitute an “input representation”\\nfor the problem.\\nExample\\nConsider the problem of assigning the label “family car” or “not family car” to cars. Let us\\nassume that the features that separate a family car from other cars are the price and engine\\npower. These attributes or features constitute the input representation for the problem. While\\ndeciding on this input representation, we are ignoring various other attributes like seating\\ncapacity or colour as irrelevant.\\n2.2\\nHypothesis space\\nIn the following discussions we consider only “binary classiﬁcation” problems; that is, classiﬁcation\\nproblems with only two class labels. The class labels are usually taken as “1” and “0”. The label “1”\\nmay indicate “True”, or “Yes”, or “Pass”, or any such label. The label “0” may indicate “False”, or\\n“No” or “Fail”, or any such label. The examples with class labels 1 are called “positive examples”\\nand examples with labels “0” are called “negative examples”.\\n2.2.1\\nDeﬁnition\\n1. Hypothesis\\nIn a binary classiﬁcation problem, a hypothesis is a statement or a proposition purporting to\\nexplain a given set of facts or observations.\\n15'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 30}, page_content='CHAPTER 2. SOME GENERAL CONCEPTS\\n16\\n2. Hypothesis space\\nThe hypothesis space for a binary classiﬁcation problem is a set of hypotheses for the problem\\nthat might possibly be returned by it.\\n3. Consistency and satisfying\\nLet x be an example in a binary classiﬁcation problem and let c(x) denote the class label\\nassigned to x (c(x) is 1 or 0). Let D be a set of training examples for the problem. Let h be a\\nhypothesis for the problem and h(x) be the class label assigned to x by the hypothesis h.\\n(a) We say that the hypothesis h is consistent with the set of training examples D if h(x) =\\nc(x) for all x ∈D.\\n(b) We say that an example x satisﬁes the hypothesis h if h(x) = 1.\\n2.2.2\\nExamples\\n1. Consider the set of observations of a variable x with the associated class labels given in Table\\n2.1:\\nx\\n27\\n15\\n23\\n20\\n25\\n17\\n12\\n30\\n6\\n10\\nClass\\n1\\n0\\n1\\n1\\n1\\n0\\n0\\n1\\n0\\n0\\nTable 2.1: Sample data to illustrate the concept of hypotheses\\nFigure 2.1 shows the data plotted on the x-axis.\\nx\\n0\\n27\\n23\\n20\\n25\\n30\\n10\\n17\\n12\\n15\\n6\\nFigure 2.1: Data in Table 2.1 with hollow dots representing positive examples and solid dots repre-\\nsenting negative examples\\nLooking at Figure 2.1, it appears that the class labeling has been done based on the following\\nrule.\\nh′\\n:\\nIF x ≥20 THEN “1” ELSE “0”.\\n(2.1)\\nNote that h′ is consistent with the training examples in Table 2.1. For example, we have:\\nh′(27) = 1,\\nc(27) = 1,\\nh′(27) = c(27)\\nh′(15) = 0,\\nc(15) = 0,\\nh′(15) = c(15)\\nNote also that, for x = 5 and x = 28 (not in training data),\\nh′(5) = 0,\\nh′(28) = 1.\\nThe hypothesis h′ explains the data. The following proposition also explains the data:\\nh′′\\n:\\nIF x ≥19 THEN “0” ELSE “1”.\\n(2.2)\\nIt is not enough that the hypothesis explains the given data; it must also predict correctly the\\nclass label of future observations. So we consider a set of such hypotheses and choose the\\n“best” one. The set of hypotheses can be deﬁned using a parameter, say m, as given below:\\nhm\\n:\\nIF x ≥m THEN “1” ELSE ”0”.\\n(2.3)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 31}, page_content='CHAPTER 2. SOME GENERAL CONCEPTS\\n17\\nThe set of all hypotheses obtained by assigning different values to m constitutes the hypothesis\\nspace H; that is,\\nH = {hm ∶m is a real number}.\\n(2.4)\\nFor the same data, we can have different hypothesis spaces. For example, for the data in Table\\n2.1, we may also consider the hypothesis space deﬁned by the following proposition:\\nh′\\nm\\n:\\nIF x ≤m THEN “0” ELSE “1”.\\n2. Consider a situation with four binary variables x1, x2, x3, x4 and one binary output variable\\ny. Suppose we have the following observations.\\nx1\\nx2\\nx3\\nx4\\ny\\n0\\n0\\n0\\n1\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n1\\n0\\n0\\n1\\n0\\n0\\n1\\n0\\n0\\nThe problem is to predict a function f of x1, x2, x3, x4 which predicts the value of y for any\\ncombination of values of x1, x2, x3, x4. In this problem, the hypothesis space is the set of all\\npossible functions f. It can be shown that the size of the hypothesis space is 2(24) = 65536.\\n3. Consider the problem of assigning the label “family car” or “not family car” to cars. For\\nconvenience, we shall replace the label “family car” by “1” and “not family car” by “0”.\\nSuppose we choose the features “price (’000 $)” and “power (hp)” as the input representation\\nfor the problem. Further, suppose that there is some reason to believe that for a car to be a\\nfamily car, its price and power should be in certain ranges. This supposition can be formulated\\nin the form of the following proposition:\\nIF (p1 < price < p2) AND (e1 < power < e2) THEN “1” ELSE ”0”\\n(2.5)\\nfor suitable values of p1, p2, e1 and e2. Since a solution to the problem is a proposition of the\\nform Eq.(2.5) with speciﬁc values for p1, p2, e1 and e2, the hypothesis space for the problem\\nis the set of all such propositions obtained by assigning all possible values for p1, p2, e1 and\\ne2.\\npower (hp)\\nprice (’000 $)\\np1\\np2\\ne1\\ne2\\nh(x1,x2) = 1\\nx1\\nx2\\nhypothesis h\\nFigure 2.2: An example hypothesis deﬁned by Eq. (2.5)\\nIt is interesting to observe that the set of points in the power–price plane which satisﬁes the\\ncondition\\n(p1 < price < p2) AND (e1 < power < e2)\\ndeﬁnes a rectangular region (minus the boundary) in the price–power space as shown in Figure\\n2.2. The sides of this rectangular region are parallel to the coordinate axes. Such a rectangle'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 32}, page_content='CHAPTER 2. SOME GENERAL CONCEPTS\\n18\\nis called an axis-aligned rectangle If h is the hypothesis deﬁned by Eq.(2.5), and (x1,x2)\\nis any point in the price–power plane, then h(x1,x2) = 1 if and only if (x1,x2) is within\\nthe rectangular region. Hence we may identify the hypothesis h with the rectangular region.\\nThus, the hypothesis space for the problem can be thought of as the set of all axis-aligned\\nrectangles in the price–power plane.\\n4. Consider the trading agent trying to infer which books or articles the user reads based on\\nkeywords supplied in the article. Suppose the learning agent has the following data (“1\"\\nindicates “True” and “0” indicates “False”):\\narticle\\ncrime\\nacademic\\nlocal\\nmusic\\nreads\\na1\\ntrue\\nfalse\\nfalse\\ntrue\\n1\\na2\\ntrue\\nfalse\\nfalse\\nfalse\\n1\\na3\\nfalse\\ntrue\\nfalse\\nfalse\\n0\\na4\\nfalse\\nfalse\\ntrue\\nfalse\\n0\\na5\\ntrue\\ntrue\\nfalse\\nfalse\\n1\\nThe aim is to learn which articles the user reads. The aim is to ﬁnd a deﬁnition such as\\nIF (crime OR (academic AND (NOT music))) THEN ”1” ELSE ”0”.\\nThe hypothesis space H could be all boolean combinations of the input features or could be\\nmore restricted, such as conjunctions or propositions deﬁned in terms of fewer than three\\nfeatures.\\n2.3\\nOrdering of hypotheses\\nDeﬁnition\\nLet X be the set of all possible examples for a binary classiﬁcation problem and let h′ and h′′ be\\ntwo hypotheses for the problem.\\nS′ = {x ∈X ∶h′(x) = 1}\\nS′′ = {x ∈X ∶h′′(x) = 1}\\nFigure 2.3: Hypothesis h′ is more general than hypothesis h′′ if and only if S′′ ⊆S′\\n1. We say that h′ is more general than h′′ if and only if for every x ∈X, if x satisﬁes h′′ then x\\nsatisﬁes h′ also; that is, if h′′(x) = 1 then h′(x) = 1 also. The relation “is more general than”\\ndeﬁnes a partial ordering relation in hypothesis space.\\n2. We say that h′ is more speciﬁc than h′′, if h′′ is more general than h′.\\n3. We say that h′ is strictly more general than h′′ if h′ is more general than h′′ and h′′ is not\\nmore general than h′.\\n4. We say that h′ is strictly more speciﬁc than h′′ if h′ is more speciﬁc than h′′ and h′′ is not\\nmore speciﬁc than h′.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 33}, page_content='CHAPTER 2. SOME GENERAL CONCEPTS\\n19\\nExample\\nConsider the hypotheses h′ and h′′ deﬁned in Eqs.(2.1),(2.2). Then it is easy to check that if\\nh′(x) = 1 then h′′(x) = 1 also. So, h′′ is more general than h′. But, h′ is not more general\\nthan h′′ and so h′′ is strictly more general than h′.\\n2.4\\nVersion space\\nDeﬁnition\\nConsider a binary classiﬁcation problem. Let D be a set of training examples and H a hypothesis\\nspace for the problem. The version space for the problem with respect to the set D and the space H\\nis the set of hypotheses from H consistent with D; that is, it is the set\\nVSD,H = {h ∈H ∶h(x) = c(x) for all x ∈D}.\\n2.4.1\\nExamples\\nExample 1\\nConsider the data D given in Table 2.1 and the hypothesis space deﬁned by Eqs.(2.3)-(2.4).\\nx\\n0\\n27\\n23\\n20\\n25\\n30\\n10\\n17\\n12\\n15\\n6\\nm\\nFigure 2.4: Values of m which deﬁne the version space with data in Table 2.1 and hypothesis space\\ndeﬁned by Eq.(2.4)\\nFrom Figure 2.4 we can easily see that the hypothesis space with respect this dataset D and\\nhypothesis space H is as given below:\\nVSD,H = {hm ∶17 < m ≤20}.\\nExample 2\\nConsider the problem of assigning the label “family car” (indicated by “1”) or “not family car”\\n(indicated by “0”) to cars. Given the following examples for the problem and assuming that the\\nhypothesis space is as deﬁned by Eq. (2.5), the version space for the problem.\\nx1: Price in ’000 ($)\\n32\\n82\\n44\\n34\\n43\\n80\\n38\\nx2: Power (hp)\\n170\\n333\\n220\\n235\\n245\\n315\\n215\\nClass\\n0\\n0\\n1\\n1\\n1\\n0\\n1\\nx1\\n47\\n27\\n56\\n28\\n20\\n25\\n66\\n75\\nx2\\n260\\n290\\n320\\n305\\n160\\n300\\n250\\n340\\nClass\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\nSolution\\nFigure 2.5 shows a scatter plot of the given data. In the ﬁgure, the data with class label “1” (family\\ncar) is shown as hollow circles and the data with class labels “0” (not family car) are shown as solid\\ndots.\\nA hypothesis as given by Eq.(2.5) with speciﬁc values for the parameters p1, p2, e1 and e2\\nspeciﬁes an axis-aligned rectangle as shown in Figure 2.2. So the hypothesis space for the problem\\ncan be thought as the set of axis-aligned rectangles in the price-power plane.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 34}, page_content='CHAPTER 2. SOME GENERAL CONCEPTS\\n20\\npower (hp)\\nprice (’000 $)\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n150\\n200\\n250\\n300\\n350\\nFigure 2.5: Scatter plot of price-power data (hollow circles indicate positive examples and solid dots\\nindicate negative examples)\\npower (hp)\\nprice (’000 $)\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n150\\n200\\n250\\n300\\n350\\n(32,170)\\n(66,250)\\n(27,290)\\n(34,235)\\n(38,215)\\n(47,260)\\nFigure 2.6: The version space consists of hypotheses corresponding to axis-aligned rectangles con-\\ntained in the shaded region\\nThe version space consists of all hypotheses speciﬁed by axis-aligned rectangles contained in\\nthe shaded region in Figure 2.6. The inner rectangle is deﬁned by\\n(34 < price < 47) AND (215 < power < 260)\\nand the outer rectangle is deﬁned by\\n(27 < price < 66) AND (170 < power < 290).\\nExample 3\\nConsider the problem of ﬁnding a rule for determining days on which one can enjoy water sport. The\\nrule is to depend on a few attributes like “temp”, ”humidity”, etc. Suppose we have the following\\ndata to help us devise the rule. In the data, a value of “1” for “enjoy” means “yes” and a value of\\n“0” indicates ”no”.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 35}, page_content='CHAPTER 2. SOME GENERAL CONCEPTS\\n21\\nExample\\nsky\\ntemp\\nhumidity\\nwind\\nwater\\nforecast\\nenjoy\\n1\\nsunny\\nwarm\\nnormal\\nstrong\\nwarm\\nsame\\n1\\n2\\nsunny\\nwarm\\nhigh\\nstrong\\nwarm\\nsame\\n1\\n3\\nrainy\\ncold\\nhigh\\nstrong\\nwarm\\nchange\\n0\\n4\\nsunny\\nwarm\\nhigh\\nstrong\\ncool\\nchange\\n1\\nFind the hypothesis space and the version space for the problem. (For a detailed discussion of this\\nproblem see [4] Chapter2.)\\nSolution\\nWe are required to ﬁnd a rule of the following form, consistent with the data, as a solution of the\\nproblem.\\n(sky = x1) ∧(temp = x2) ∧(humidity = x3)∧\\n(wind = x4) ∧(water = x5) ∧(forecast = x6) ↔yes\\n(2.6)\\nwhere\\nx1 = sunny, warm, ⋆\\nx2 = warm, cold, ⋆\\nx3 = normal, high, ⋆\\nx4 = strong, ⋆\\nx5 = warm, cool, ⋆\\nx6 = same, change, ⋆\\n(Here a “⋆” indicates other possible values of the attributes.) The hypothesis may be represented\\ncompactly as a vector\\n(a1,a2,a3,a4,a5,a6)\\nwhere, in the positions of a1,...,a6, we write\\n• a “?” to indicate that any value is acceptable for the corresponding attribute,\\n• a ”∅” to indicate that no value is acceptable for the corresponding attribute,\\n• some speciﬁc single required value for the corresponding attribute\\nFor example, the vector\\n(?, cold, high, ?, ?, ?)\\nindicates the hypothesis that one enjoys the sport only if “temp” is “cold” and “humidity” is “high”\\nwhatever be the values of the other attributes.\\nIt can be shown that the version space for the problem consists of the following six hypotheses\\nonly:\\n(sunny,\\nwarm,\\n?,\\nstrong,\\n?,\\n?)\\n(sunny,\\n?,\\n?,\\nstrong,\\n?,\\n?)\\n(sunny,\\nwarm,\\n?,\\n?,\\n?,\\n?)\\n(?,\\nwarm,\\n?,\\nstrong,\\n?,\\n?)\\n(sunny,\\n?,\\n?,\\n?,\\n?,\\n?)\\n(?,\\nwarm,\\n?,\\n?,\\n?,\\n?)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 36}, page_content='CHAPTER 2. SOME GENERAL CONCEPTS\\n22\\n2.5\\nNoise\\n2.5.1\\nNoise and their sources\\nNoise is any unwanted anomaly in the data ([2] p.25). Noise may arise due to several factors:\\n1. There may be imprecision in recording the input attributes, which may shift the data points in\\nthe input space.\\n2. There may be errors in labeling the data points, which may relabel positive instances as nega-\\ntive and vice versa. This is sometimes called teacher noise.\\n3. There may be additional attributes, which we have not taken into account, that affect the label\\nof an instance. Such attributes may be hidden or latent in that they may be unobservable. The\\neffect of these neglected attributes is thus modeled as a random component and is included in\\n“noise.”\\n2.5.2\\nEffect of noise\\nNoise distorts data. When there is noise in data, learning problems may not produce accurate results.\\nAlso, simple hypotheses may not be sufﬁcient to explain the data and so complicated hypotheses\\nmay have to be formulated. This leads to the use of additional computing resources and the needless\\nwastage of such resources.\\nFor example, in a binary classiﬁcation problem with two variables, when there is noise, there\\nmay not be a simple boundary between the positive and negative instances and to separate them. A\\nrectangle can be deﬁned by four numbers, but to deﬁne a more complicated shape one needs a more\\ncomplex model with a much larger number of parameters. So, when there is noise, we may make a\\ncomplex model which makes a perfect ﬁt to the data and attain zero error; or, we may use a simple\\nmodel and allow some error.\\n2.6\\nLearning multiple classes\\nSo far we have been discussing binary classiﬁcation problems. In a general case there may be more\\nthan two classes. Two methods are generally used to handle such cases. These methods are known\\nby the names “one-against-all\" and “one-against-one”.\\n2.6.1\\nProcedures for learning multiple classes\\n“One-against all” method\\nConsider the case where there are K classes denoted by C1,...,CK. Each input instance belongs\\nto exactly one of them.\\nWe view a K-class classiﬁcation problem as K two-class problems. In the i-th two-class prob-\\nlem, the training examples belonging to Ci are taken as the positive examples and the examples of\\nall other classes are taken as the negative examples. So, we have to ﬁnd K hypotheses h1,...,hK\\nwhere hi is deﬁned by\\nhi(x) =\\n⎧⎪⎪⎨⎪⎪⎩\\n1\\nif x is in class Ci\\n0\\notherwise\\nFor a given x, ideally only one of hi(x) is 1 and then we assign the class Ci to x. But, when\\nno, or, two or more, hi(x) is 1, we cannot choose a class. In such a case, we say that the classiﬁer\\nrejects such cases.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 37}, page_content='CHAPTER 2. SOME GENERAL CONCEPTS\\n23\\n“One-against-one” method\\nIn the one-against-one (OAO) (also called one-vs-one (OVO)) strategy, a classiﬁer is constructed\\nfor each pair of classes. If there are K different class labels, a total of K(K −1)/2 classiﬁers are\\nconstructed. An unknown instance is classiﬁed with the class getting the most votes. Ties are broken\\narbitrarily.\\nFor example, let there be three classes, A, B and C. In the OVO method we construct 3(3 −\\n1)/2 = 3 binary classiﬁers. Now, if any x is to be classiﬁed, we apply each of the three classiﬁers to\\nx. Let the three classiﬁers assign the classes A, B, B respectively to x. Since a label to x is assigned\\nby the majority voting, in this example, we assign the class label of B to x.\\n2.7\\nModel selection\\nAs we have pointed earlier in Section 1.1.1, there is no universally accepted deﬁnition of the term\\n“model”. It may be understood as some mathematical expression or equation, or some mathematical\\nstructures such as graphs and trees, or a division of sets into disjoint subsets, or a set of logical “if\\n... then ... else ...” rules, or some such thing.\\nIn order to formulate a hypothesis for a problem, we have to choose some model and the term\\n“model selection” has been used to refer to the process of choosing a model. However, the term has\\nbeen used to indicate several things. In some contexts it has been used to indicates the process of\\nchoosing one particular approach from among several different approaches. This may be choosing\\nan appropriate algorithms from a selection of possible algorithms, or choosing the sets of features\\nto be used for input, or choosing initial values for certain parameters. Sometimes “model selection”\\nrefers to the process of picking a particular mathematical model from among different mathematical\\nmodels which all purport to describe the same data set. It has also been described as the process of\\nchoosing the right inductive bias.\\n2.7.1\\nInductive bias\\nIn a learning problem we only have the data. But data by itself is not sufﬁcient to ﬁnd the solution.\\nWe should make some extra assumptions to have a solution with the data we have. The set of\\nassumptions we make to have learning possible is called the inductive bias of the learning algorithm.\\nOne way we introduce inductive bias is when we assume a hypothesis class.\\nExamples\\n• In learning the class of family car, there are inﬁnitely many ways of separating the positive\\nexamples from the negative examples. Assuming the shape of a rectangle is an inductive bias.\\n• In regression, assuming a linear function is an inductive bias.\\nThe model selection is about choosing the right inductive bias.\\n2.7.2\\nAdvantages of a simple model\\nEven though a complex model may not be making any errors in prediction, there are certain advan-\\ntages in using a simple model.\\n1. A simple model is easy to use.\\n2. A simple model is easy to train. It is likely to have fewer parameters.\\nIt is easier to ﬁnd the corner values of a rectangle than the control points of an arbitrary shape.\\n3. A simple model is easy to explain.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 38}, page_content='CHAPTER 2. SOME GENERAL CONCEPTS\\n24\\n4. A simple model would generalize better than a complex model. This principle is known as\\nOccam’s razor, which states that simpler explanations are more plausible and any unnecessary\\ncomplexity should be shaved off.\\nRemarks\\nA model should not be too simple! With a small training set when the training instances differ a\\nlittle bit, we expect the simpler model to change less than a complex model: A simple model is thus\\nsaid to have less variance. On the other hand, a too simple model assumes more, is more rigid, and\\nmay fail if indeed the underlying class is not that simple. A simpler model has more bias. Finding\\nthe optimal model corresponds to minimizing both the bias and the variance.\\n2.8\\nGeneralisation\\nHow well a model trained on the training set predicts the right output for new instances is called\\ngeneralization.\\nGeneralization refers to how well the concepts learned by a machine learning model apply to\\nspeciﬁc examples not seen by the model when it was learning. The goal of a good machine learning\\nmodel is to generalize well from the training data to any data from the problem domain. This allows\\nus to make predictions in the future on data the model has never seen. Overﬁtting and underﬁtting\\nare the two biggest causes for poor performance of machine learning algorithms. The model should\\nbe selected having the best generalisation. This is said to be the case if these problems are avoided.\\n• Underﬁtting\\nUnderﬁtting is the production of a machine learning model that is not complex enough to\\naccurately capture relationships between a datasetâ ˘A´Zs features and a target variable.\\n• Overﬁtting\\nOverﬁtting is the production of an analysis which corresponds too closely or exactly to a\\nparticular set of data, and may therefore fail to ﬁt additional data or predict future observations\\nreliably.\\nExample 1\\n(a) Given dataset\\n(b) “Just right” model\\n(c) Underﬁtting model\\n(d) Overﬁtting model\\nFigure 2.7: Examples for overﬁtting and overﬁtting models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 39}, page_content='CHAPTER 2. SOME GENERAL CONCEPTS\\n25\\nConsider a dataset shown in Figure 2.7(a). Let it be required to ﬁt a regression model to the data. The\\ngraph of a model which looks “just right” is shown in Figure 2.7(b). In Figure 2.7(c)we have a linear\\nregression model for the same dataset and this model does seem to capture the essential features of\\nthe dataset. So this model suffers from underﬁtting. In Figure 2.7(d) we have a regression model\\nwhich corresponds too closely to the given dataset and hence it does not account for small random\\nnoises in the dataset. Hence it suffers from overﬁtting.\\nExample 2\\n(a) Underﬁtting\\n(b) Right ﬁtting\\n(c) Overﬁtting\\nFigure 2.8: Fitting a classiﬁcation boundary\\nSuppose we have to determine the classiﬁcation boundary for a dataset two class labels. An example\\nsituation is shown in Figure 2.8 where the curved line is the classiﬁcation boundary. The three ﬁgures\\nillustrate the cases of underﬁtting, right ﬁtting and overﬁtting.\\n2.8.1\\nTesting generalisation: Cross-validation\\nWe can measure the generalization ability of a hypothesis, namely, the quality of its inductive bias,\\nif we have access to data outside the training set. We simulate this by dividing the training set we\\nhave into two parts. We use one part for training (that is, to ﬁnd a hypothesis), and the remaining\\npart is called the validation set and is used to test the generalization ability. Assuming large enough\\ntraining and validation sets, the hypothesis that is the most accurate on the validation set is the best\\none (the one that has the best inductive bias). This process is called cross-validation.\\n2.9\\nSample questions\\n(a) Short answer questions\\n1. Explain the general-to-speciﬁc ordering of hypotheses.\\n2. In the context of classiﬁcation problems explain with examples the following: (i) hypothesis\\n(ii) hypothesis space.\\n3. Deﬁne the version space of a binary classiﬁcation problem.\\n4. Explain the “one-against-all” method for learning multiple classes.\\n5. Describe the “one-against-one” method for learning multiple classes.\\n6. What is meant by inductive bias in machine learning? Give an example.\\n7. What is meant by overﬁtting of data? Explain with an example.\\n8. What is meant by overﬁtting and underﬁtting of data with examples.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 40}, page_content='CHAPTER 2. SOME GENERAL CONCEPTS\\n26\\n(b) Long answer questions\\n1. Deﬁne version space and illustrate it with an example.\\n2. Given the following data\\nx\\n0\\n3\\n5\\n9\\n12\\n18\\n23\\nLabel\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\nand the hypothesis space\\nH = {hm ∣m a real number}\\nwhere hm is deﬁned by\\nIF x ≤m THEN 1 ELSE 0,\\nﬁnd the version space the problem with respect to D and H.\\n3. What is meant by “noise” in data? What are its sources and how it is affecting results?\\n4. Consider the following data:\\nx\\n2\\n3\\n5\\n8\\n10\\n15\\n16\\n18\\n20\\ny\\n12\\n15\\n10\\n6\\n8\\n10\\n7\\n9\\n10\\nClass label\\n0\\n0\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\nDetermine the version space if the hypothesis space consists of all hypotheses of the form\\nIF (x1 < x < x2) AND (y1 < y < y2) THEN “1” ELSE ”0”.\\n5. For the date in problem 4, what would be the version space if the hypothesis space consists of\\nall hypotheses of the form\\nIF (x −x1)2 + (y −y1)2 ≤r2 THEN “1” ELSE ”0”.\\n6. What issues are to be considered while selecting a model for applying machine learning in a\\ngiven problem.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 41}, page_content='Chapter 3\\nVC dimension and PAC learning\\nThe concepts of Vapnik-Chervonenkis dimension (VC dimension) and probably approximate correct\\n(PAC) learning are two important concepts in the mathematical theory of learnability and hence are\\nmathematically oriented. The former is a measure of the capacity (complexity, expressive power,\\nrichness, or ﬂexibility) of a space of functions that can be learned by a classiﬁcation algorithm.\\nIt was originally deﬁned by Vladimir Vapnik and Alexey Chervonenkis in 1971. The latter is a\\nframework for the mathematical analysis of learning algorithms. The goal is to check whether the\\nprobability for a selected hypothesis to be approximately correct is very high. The notion of PAC\\nlearning was proposed by Leslie Valiant in 1984.\\n3.1\\nVapnik-Chervonenkis dimension\\nLet H be the hypothesis space for some machine learning problem. The Vapnik-Chervonenkis\\ndimension of H, also called the VC dimension of H, and denoted by V C(H), is a measure of the\\ncomplexity (or, capacity, expressive power, richness, or ﬂexibility) of the space H. To deﬁne the VC\\ndimension we require the notion of the shattering of a set of instances.\\n3.1.1\\nShattering of a set\\nLet D be a dataset containing N examples for a binary classiﬁcation problem with class labels 0\\nand 1. Let H be a hypothesis space for the problem. Each hypothesis h in H partitions D into two\\ndisjoint subsets as follows:\\n{x ∈D ∣h(x) = 0} and {x ∈D ∣h(x) = 1}.\\nSuch a partition of S is called a “dichotomy” in D. It can be shown that there are 2N possible\\ndichotomies in D. To each dichotomy of D there is a unique assignment of the labels “1” and “0”\\nto the elements of D. Conversely, if S is any subset of D then, S deﬁnes a unique hypothesis h as\\nfollows:\\nh(x) =\\n⎧⎪⎪⎨⎪⎪⎩\\n1\\nif x ∈S\\n0\\notherwise\\nThus to specify a hypothesis h, we need only specify the set {x ∈D ∣h(x) = 1}.\\nFigure 3.1 shows all possible dichotomies of D if D has three elements. In the ﬁgure, we have\\nshown only one of the two sets in a dichotomy, namely the set {x ∈D ∣h(x) = 1}. The circles and\\nellipses represent such sets.\\n27'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 42}, page_content='CHAPTER 3. VC DIMENSION AND PAC LEARNING\\n28\\na\\nb\\nc\\na\\nb\\nc\\na\\nb\\nc\\na\\nb\\nc\\n(i) Emty set\\n(ii)\\n(iii)\\n(iv)\\na\\nb\\nc\\na\\nb\\nc\\na\\nb\\nc\\na\\nb\\nc\\n(v)\\n(vi)\\n(vii)\\n(viii) Full set D\\nFigure 3.1: Different forms of the set {x ∈S ∶h(x) = 1} for D = {a,b,c}\\nWe require the notion of a hypothesis consistent with a set of examples introduced in Section 2.4\\nin the following deﬁnition.\\nDeﬁnition\\nA set of examples D is said to be shattered by a hypothesis space H if and only if for every di-\\nchotomy of D there exists some hypothesis in H consistent with the dichotomy of D.\\n3.1.2\\nVapnik-Chervonenkis dimension\\nThe following example illustrates the concept of Vapnik-Chervonenkis dimension.\\nExample\\nLet the instance space X be the set of all real numbers. Consider the hypothesis space deﬁned by\\nEqs.(2.3)-(2.4):\\nH = {hm ∶m is a real number},\\nwhere\\nhm\\n∶\\nIF x ≥m THEN ”1” ELSE “0”.\\ni) Let D be a subset of X containing only a single number, say, D = {3.5}. There are 2\\ndichotomies for this set. These correspond to the following assignment of class labels:\\nx\\n3.25\\nLabel\\n0\\nx\\n3.25\\nLabel\\n1\\nh4 ∈H is consistent with the former dichotomy and h3 ∈H is consistent with the latter. So,\\nto every dichotomy in D there is a hypothesis in H consistent with the dichotomy. Therefore,\\nthe set D is shattered by the hypothesis space H.\\nii) Let D be a subset of X containing two elements, say, D = {3.25,4.75}. There are 4 di-\\nchotomies in D and they correspond to the assignment of class labels shown in Table 3.1.\\nIn these dichotomies, h5 is consistent with (a), h4 is consistent with (b) and h3 is consistent\\nwith (d). But there is no hypothesis hm ∈H consistent with (c). Thus the two-element set D\\nis not shattered by H. In a similar way it can be shown that there is no two-element subset\\nof X which is shattered by H.\\nIt follows that the size of the largest ﬁnite subset of X shattered by H is 1. This number is the\\nVC dimension of H.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 43}, page_content='CHAPTER 3. VC DIMENSION AND PAC LEARNING\\n29\\nx\\n3.25\\n4.75\\nLabel\\n0\\n0\\nx\\n3.25\\n4.75\\nLabel\\n0\\n1\\n(a)\\n(b)\\nx\\n3.25\\n4.75\\nLabel\\n1\\n0\\nx\\n3.25\\n4.75\\nLabel\\n1\\n1\\n(c)\\n(d)\\nTable 3.1: Different assignments of class labels to the elements of {3.25,4.75}\\nDeﬁnition\\nThe Vapnik-Chervonenkis dimension (VC dimension) of a hypothesis space H deﬁned over an in-\\nstance space (that is, the set of all possible examples) X, denoted by V C(H), is the size of the\\nlargest ﬁnite subset of X shattered by H. If arbitrarily large subsets of X can be shattered by H,\\nthen we deﬁne V C(H) = ∞.\\nRemarks\\nIt can be shown that V C(H) ≤log2(∣H∣) where H is the number of hypotheses in H.\\n3.1.3\\nExamples\\n1. Let X be the set of all real numbers (say, for example, the set of heights of people). For any\\nreal numbers a and b deﬁne a hypothesis ha,b as follows:\\nha,b(x) =\\n⎧⎪⎪⎨⎪⎪⎩\\n1\\nif a < x < b\\n0\\notherwise\\nLet the hypothesis space H consist of all hypotheses of the form ha,b. We show that V C(H) =\\n2. We have to show that there is a subset of X of size 2 shattered by H and there is no subset\\nof size 3 shattered by H.\\n• Consider the two-element set D = {3.25,4.75}. The various dichotomies of D are\\ngiven in Table 3.1. It can be seen that the hypothesis h5,6 is consistent with (a), h4,5 is\\nconsistent with (b), h3,4 is consistent with (c) and h3,5 is consistent with (d). So the set\\nD is shattered by H.\\n• Consider a three-element subset D = {x1,x2,x3}. Let us assume that x1 < x2 < x3. H\\ncannot shatter this subset because the dichotomy represented by the set {x1,x3} cannot\\nbe represented by a hypothesis in H (any interval containing both x1 and x3 will contain\\nx2 also).\\nTherefore, the size of the largest subset of X shattered by H is 2 and so V C(H) = 2.\\n2. Let the instance space X be the set of all points (x,y) in a plane. For any three real numbers,\\na,b,c deﬁne a class labeling as follows:\\nha,b,c(x,y) =\\n⎧⎪⎪⎨⎪⎪⎩\\n1\\nif ax + by + c > 0\\n0\\notherwise'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 44}, page_content='CHAPTER 3. VC DIMENSION AND PAC LEARNING\\n30\\nO\\nx\\ny\\nha,b,c(x,y) = 0\\nax + by + c = 0\\n(assume c < 0)\\nha,b,c(x,y) = 0\\nax + by + c < 0\\nha,b,c(x,y) = 1\\nax + by + c > 0\\nFigure 3.2: Geometrical representation of the hypothesis ha,b,c\\nLet H be the set of all hypotheses of the form ha,b,c. We show that V C(H) = 3. We have\\nshow that there is a subset of size 3 shattered by H and there is no subset of size 4 shattered\\nby H.\\n• Consider a set D = {A,B,C} of three non-collinear points in the plane. There are 8 sub-\\nsets of D and each of these deﬁnes a dichotomy of D. We can easily ﬁnd 8 hypotheses\\ncorresponding to the dichotomies deﬁned by these subsets (see Figure 3.3).\\nA\\nB\\nC\\nFigure 3.3: A hypothesis ha,b,c consistent with the dichotomy deﬁned by the subset\\n{A,C} of {A,B,C}\\n• Consider a set S = {A,B,C,D} of four points in the plane. Let no three of these points\\nbe collinear. Then, the points form a quadrilateral. It can be easily seen that, in this case,\\nthere is no hypothesis for which the two element set formed by the ends of a diagonal is\\nthe corresponding dichotomy (see Figure 3.4).\\nA\\nB\\nC\\nD\\nFigure 3.4: There is no hypothesis ha,b,c consistent with the dichotomy deﬁned by the\\nsubset {A,C} of {A,B,C,D}\\nSo the set cannot be shattered by H. If any three of them are collinear, then by some\\ntrial and error, it can be seen that in this case also the set cannot be shattered by H. No\\nset with four elements cannot be shattered by H.\\nFrom the above discussion we conclude that V C(H) = 3.\\n3. Let X be set of all conjunctions of n boolean literals. Let the hypothesis space H consists of\\nconjunctions of up to n literals. It can be shown that V C(H) = n. (The full details of the\\nproof of this is beyond the scope of these notes.)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 45}, page_content='CHAPTER 3. VC DIMENSION AND PAC LEARNING\\n31\\n3.2\\nProbably approximately correct learning\\nIn computer science, computational learning theory (or just learning theory) is a subﬁeld of artiﬁcial\\nintelligence devoted to studying the design and analysis of machine learning algorithms. In compu-\\ntational learning theory, probably approximately correct learning (PAC learning) is a framework for\\nmathematical analysis of machine learning algorithms. It was proposed in 1984 by Leslie Valiant.\\nIn this framework, the learner (that is, the algorithm) receives samples and must select a hypoth-\\nesis from a certain class of hypotheses. The goal is that, with high probability (the “probably” part),\\nthe selected hypothesis will have low generalization error (the “approximately correct” part).\\nIn this section we ﬁrst give an informal deﬁnition of PAC-learnability. After introducing a few\\nnore notions, we give a more formal, mathematically oriented, deﬁnition of PAC-learnability. At the\\nend, we mention one of the applications of PAC-learnability.\\n3.2.1\\nPAC-learnability\\nTo deﬁne PAC-learnability we require some speciﬁc terminology and related notations.\\n• Let X be a set called the instance space which may be ﬁnite or inﬁnite. For example, X may\\nbe the set of all points in a plane.\\n• A concept class C for X is a family of functions c ∶X →{0,1}. A member of C is called a\\nconcept. A concept can also be thought of as a subset of X. If C is a subset of X, it deﬁnes a\\nunique function µC ∶X →{0,1} as follows:\\nµC(x) =\\n⎧⎪⎪⎨⎪⎪⎩\\n1\\nif x ∈C\\n0\\notherwise\\n• A hypothesis h is also a function h ∶X →{0,1}. So, as in the case of concepts, a hypothesis\\ncan also be thought of as a subset of X. H will denote a set of hypotheses.\\n• We assume that F is an arbitrary, but ﬁxed, probability distribution over X.\\n• Training examples are obtained by taking random samples from X. We assume that the\\nsamples are randomly generated from X according to the probability distribution F.\\nNow, we give below an informal deﬁnition of PAC-learnability.\\nDeﬁnition (informal)\\nLet X be an instance space, C a concept class for X, h a hypothesis in C and F an arbitrary,\\nbut ﬁxed, probability distribution. The concept class C is said to be PAC-learnable if there is an\\nalgorithm A which, for samples drawn with any probability distribution F and any concept c ∈C,\\nwill with high probability produce a hypothesis h ∈C whose error is small.\\nAdditional notions\\n• True error\\nTo formally deﬁne PAC-learnability, we require the concept of the true error of a hypothesis\\nh with respect to a target concept c denoted by errorF (h). It is deﬁned by\\nerrorF (h) = Px∈F (h(x) ≠c(x))\\nwhere the notation Px∈F indicates that the probability is taken for x drawn from X according\\nto the distribution F. This error is the probability that h will misclassify an instance x drawn\\nat random from X according to the distribution F. This error is not directly observable to the\\nlearner; it can only see the training error of each hypothesis (that is, how often h(x) ≠c(x)\\nover training instances).'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 46}, page_content='CHAPTER 3. VC DIMENSION AND PAC LEARNING\\n32\\n• Length or dimension of an instance\\nWe require the notion of the length or dimension or size of an instance in the instance space X.\\nIf the instance space X is the n-dimensional Euclidean space, then each example is speciﬁed\\nby n real numbers and so the length of the examples may be taken as n. Similarly, if X is the\\nspace of the conjunctions of n Boolean literals, then the length of the examples may be taken\\nas n. These are the commonly considered instance spaces in computational learning theory.\\n• Size of a concept\\nWe need the notion of the size of a concept c. For any concept c, we deﬁne size(c) to be the\\nsize of the smallest representation of c using some ﬁnite alphabet Σ.\\n(For a detailed discussion of these and related ideas, see [6] pp.7-15.)\\nDeﬁnition ([4] p.206)\\nConsider a concept class C deﬁned over a set of instances X of length n and a learner (algorithm) L\\nusing hypothesis space H. C is said to be PAC-learnable by L using H if for all c ∈C, distribution\\nF over X, ϵ such that 0 < ϵ < 1/2 and δ such that 0 < δ < 1/2, learner L will with probability at least\\n(1 −δ) output a hypothesis h such that errorF (h) ≤ϵ, in time that is polynomial in 1/ϵ, 1/δ, n and\\nsize(c).\\n3.2.2\\nExamples\\nTo illustrate the deﬁnition of PAC-learnability, let us consider some concrete examples.\\ny\\nx\\na\\nb\\nc\\nd\\n(x,y)\\nx\\ny\\nconcept/hypothesis\\nFigure 3.5: An axis-aligned rectangle in the Euclidean plane\\nExample 1\\n• Let the instance space be the set X of all points in the Euclidean plane. Each point is repre-\\nsented by its coordinates (x,y). So, the dimension or length of the instances is 2.\\n• Let the concept class C be the set of all “axis-aligned rectangles” in the plane; that is, the set\\nof all rectangles whose sides are parallel to the coordinate axes in the plane (see Figure 3.5).\\n• Since an axis-aligned rectangle can be deﬁned by a set of inequalities of the following form\\nhaving four parameters\\na ≤x ≤b,\\nc ≤y ≤d\\nthe size of a concept is 4.\\n• We take the set H of all hypotheses to be equal to the set C of concepts, H = C.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 47}, page_content='CHAPTER 3. VC DIMENSION AND PAC LEARNING\\n33\\n• Given a set of sample points labeled positive or negative, let L be the algorithm which outputs\\nthe hypothesis deﬁned by the axis-aligned rectangle which gives the tightest ﬁt to the posi-\\ntive examples (that is, that rectangle with the smallest area that includes all of the positive\\nexamples and none of the negative examples) (see Figure 3.6).\\ny\\nx\\nFigure 3.6: Axis-aligned rectangle which gives the tightest ﬁt to the positive examples\\nIt can be shown that, in the notations introduced above, the concept class C is PAC-learnable by\\nthe algorithm L using the hypothesis space H of all axis-aligned rectangles.\\nExample 2\\n• Let X the set of all n-bit strings. Each n-bit string may be represented by an ordered n-tuple\\n(a1,...,an) where each ai is either 0 or 1. This may be thought of as an assignment of 0 or\\n1 to n boolean variables x1,...,xn. The set X is sometimes denoted by {0,1}n.\\n• To deﬁne the concept class, we distinguish certain subsets of X in a special way. By a literal\\nwe mean, a Boolean variable xi or its negation /xi. We consider conjunctions of literals over\\nx1,...,xn. Each conjunction deﬁnes a subset of X. for example, the conjunction x1∧/x2 ∧x4\\ndeﬁnes the following subset of X:\\n{a = (a1,...,an) ∈X∣a1 = 1,a2 = 0,a4 = 1}\\nThe concept class C consists of all subsets of X deﬁned by conjunctions of Boolean literals\\nover x1,...,xn.\\n• The hypothesis class H is deﬁned as equal to the concept class C.\\n• Let L be a certain algorithm called “Find-S algorithm” used to ﬁnd a most speciﬁc hypothesis\\n(see [4] p.26).\\nThe concept class C of all subsets of X = {0,1}n deﬁned by conjunctions of Boolean literals\\nover x1,...,xn is PAC-learnable by the Find-S algorithm using the hypothesis space H = C.\\n3.2.3\\nApplications\\nTo make the discussions complete, we introduce one simple application of the PAC-learning theory.\\nThe application is the derivation of a mathematical expression to estimate the size of samples that\\nwould produce a hypothesis with a given high probability and which has a generalization error of\\ngiven low probability.\\nWe use the following assumptions and notations:\\n• We assume that the hypothesis space H is ﬁnite. Let ∣H∣denote the number of elements in H.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 48}, page_content='CHAPTER 3. VC DIMENSION AND PAC LEARNING\\n34\\n• We assume that the concept class C be equal to H.\\n• Let m be the number of elements in the set of samples.\\n• Let ϵ and δ be such that 0 < ϵ,δ < 1.\\n• The algorithm can be any consistent algorithm, that is, any algorithm which correctly classiﬁes\\nthe training examples.\\nIt can be shown that, if m is chosen such that\\nm ≥1\\nϵ (ln(∣H∣) + ln(1/δ))\\nthen any consistent algorithm will successfully produce any concept in H with probability (1 −δ)\\nand with an error having a maximum probability of ϵ.\\n3.3\\nSample questions\\n(a) Short answer questions\\n1. What is VC dimension?\\n2. Explain Vapnik-Chervonenkis dimension.\\n3. Give an informal deﬁnition of PAC learnability.\\n4. Give a precise deﬁnition of PAC learnability.\\n5. Give an application of PAC learnable algorithm.\\n(b) Long answer questions\\n1. Let X be the set of all real numbers. Describe a hypothesis for X for which the VC dimension\\nis 0.\\n2. Let X be the set of all real numbers. Describe a hypothesis for X for which the VC dimension\\nis 1.\\n3. Let X be the set of all real numbers. Describe a hypothesis for X for which the VC dimension\\nis 2. Describe an example for which the VC dimension is 3.\\n4. Describe an example of a PAC learnable concept class.\\n5. An open interval in R is deﬁned as (a,b) = {x ∈R ∣a < x < b}. It has two parameters a and b.\\nShow that the sets of all open intervals has a VC dimension of 2.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 49}, page_content='Chapter 4\\nDimensionality reduction\\nThe complexity of any classiﬁer or regressor depends on the number of inputs. This determines both\\nthe time and space complexity and the necessary number of training examples to train such a clas-\\nsiﬁer or regressor. In this chapter, we discuss various methods for decreasing input dimensionality\\nwithout losing accuracy.\\n4.1\\nIntroduction\\nIn many learning problems, the datasets have large number of variables. Sometimes, the number\\nof variables is more than the number of observations. For example, such situations have arisen in\\nmany scientiﬁc ﬁelds such as image processing, mass spectrometry, time series analysis, internet\\nsearch engines, and automatic text analysis among others. Statistical and machine learning methods\\nhave some difﬁculty when dealing with such high-dimensional data. Normally the number of input\\nvariables is reduced before the machine learning algorithms can be successfully applied.\\nIn statistical and machine learning, dimensionality reduction or dimension reduction is the pro-\\ncess of reducing the number of variables under consideration by obtaining a smaller set of principal\\nvariables.\\nDimensionality reduction may be implemented in two ways.\\n• Feature selection\\nIn feature selection, we are interested in ﬁnding k of the total of n features that give us the\\nmost information and we discard the other (n−k) dimensions. We are going to discuss subset\\nselection as a feature selection method.\\n• Feature extraction\\nIn feature extraction, we are interested in ﬁnding a new set of k features that are the combina-\\ntion of the original n features. These methods may be supervised or unsupervised depending\\non whether or not they use the output information. The best known and most widely used\\nfeature extraction methods are Principal Components Analysis (PCA) and Linear Discrimi-\\nnant Analysis (LDA), which are both linear projection methods, unsupervised and supervised\\nrespectively.\\nMeasures of error\\nIn both methods we require a measure of the error in the model.\\n• In regression problems, we may use the Mean Squared Error (MSE) or the Root Mean\\nSquared Error (RMSE) as the measure of error. MSE is the sum, over all the data points,\\nof the square of the difference between the predicted and actual target variables, divided by\\n35'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 50}, page_content='CHAPTER 4. DIMENSIONALITY REDUCTION\\n36\\nthe number of data points. If y1,...,yn are the observed values and ˆyi,..., ˆyn are the pre-\\ndicted values, then\\nMSE = 1\\nn\\nn\\n∑\\ni=1\\n(yi −ˆyi)2\\n• In classiﬁcation problems, we may use the misclassiﬁcation rate as a measure of the error.\\nThis is deﬁned as follows:\\nmisclassiﬁcation rate = no. of misclassiﬁed examples\\ntotal no. of examples\\n4.2\\nWhy dimensionality reduction is useful\\nThere are several reasons why we are interested in reducing dimensionality.\\n• In most learning algorithms, the complexity depends on the number of input dimensions, d,\\nas well as on the size of the data sample, N, and for reduced memory and computation, we\\nare interested in reducing the dimensionality of the problem. Decreasing d also decreases the\\ncomplexity of the inference algorithm during testing.\\n• When an input is decided to be unnecessary, we save the cost of extracting it.\\n• Simpler models are more robust on small datasets. Simpler models have less variance, that is,\\nthey vary less depending on the particulars of a sample, including noise, outliers, and so forth.\\n• When data can be explained with fewer features, we get a better idea about the process that\\nunderlies the data, which allows knowledge extraction.\\n• When data can be represented in a few dimensions without loss of information, it can be\\nplotted and analyzed visually for structure and outliers.\\n4.3\\nSubset selection\\nIn machine learning subset selection, sometimes also called feature selection, or variable selection,\\nor attribute selection, is the process of selecting a subset of relevant features (variables, predictors)\\nfor use in model construction.\\nFeature selection techniques are used for four reasons:\\n• simpliﬁcation of models to make them easier to interpret by researchers/users\\n• shorter training times,\\n• to avoid the curse of dimensionality\\n• enhanced generalization by reducing overﬁtting\\nThe central premise when using a feature selection technique is that the data contains many\\nfeatures that are either redundant or irrelevant, and can thus be removed without incurring much loss\\nof information.\\nThere are several approaches to subset selection. In these notes, we discuss two of the simplest\\napproaches known as forward selection and backward selection methods.\\n4.3.1\\nForward selection\\nIn forward selection, we start with no variables and add them one by one, at each step adding the one\\nthat decreases the error the most, until any further addition does not decrease the error (or decreases\\nit only sightly).'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 51}, page_content='CHAPTER 4. DIMENSIONALITY REDUCTION\\n37\\nProcedure\\nWe use the following notations:\\nn\\n:\\nnumber of input variables\\nx1,...,xn\\n:\\ninput variables\\nFi\\n:\\na subset of the set of input variables\\nE(Fi)\\n:\\nerror incurred on the validation sample when only the inputs\\nin Fi are used\\n1. Set F0 = ∅and E(F0) = ∞.\\n2. For i = 0,1,..., repeat the following until E(Fi+1) ≥E(Fi):\\n(a) For all possible input variables xj, train the model with the input variables Fi ∪{xj} and\\ncalculate E(Fi ∪{xj}) on the validation set.\\n(b) Choose that input variable xm that causes the least error E(Fi ∪{xj}):\\nm = arg min\\nj\\nE(Fi ∪{xj})\\n(c) Set Fi+1 = Fi ∪{xm}.\\n3. The set Fi is outputted as the best subset.\\nRemarks\\n1. In this procedure, we stop if adding any feature does not decrease the error E. We may\\neven decide to stop earlier if the decrease in error is too small, where there is a user-deﬁned\\nthreshold that depends on the application constraints.\\n2. This process may be costly because to decrease the dimensions from n to k, we need to train\\nand test the system\\nn + (n −l) + (n −2) + ⋯+ (n −k)\\ntimes, which is O(n2).\\n4.3.2\\nBackward selection\\nIn sequential backward selection, we start with the set containing all features and at each step remove\\nthe one feature that causes the least error.\\nProcedure\\nWe use the following notations:\\nn\\n:\\nnumber of input variables\\nx1,...,xn\\n:\\ninput variables\\nFi\\n:\\na subset of the set of input variables\\nE(Fi)\\n:\\nerror incurred on the validation sample when only the inputs\\nin Fi are used\\n1. Set F0 = {x1,...,xn} and E(F0) = ∞.\\n2. For i = 0,1,..., repeat the following until E(Fi+1) ≥E(Fi):\\n(a) For all possible input variables xj, train the model with the input variables Fi −{xj})\\nand calculate E(Fi −{xj}) on the validation set.\\n(b) Choose that input variable xm that causes the least error E(Fi −{xj}):\\nm = arg min\\nj\\nE(Fi −{xj})'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 52}, page_content='CHAPTER 4. DIMENSIONALITY REDUCTION\\n38\\n(c) Set Fi+1 = Fi −{xm}.\\n3. The set Fi is outputted as the best subset.\\n4.4\\nPrincipal component analysis\\nPrincipal component analysis (PCA) is a statistical procedure that uses an orthogonal transforma-\\ntion to convert a set of observations of possibly correlated variables into a set of values of linearly\\nuncorrelated variables called principal components. The number of principal components is less\\nthan or equal to the smaller of the number of original variables or the number of observations. This\\ntransformation is deﬁned in such a way that the ﬁrst principal component has the largest possible\\nvariance (that is, accounts for as much of the variability in the data as possible), and each succeeding\\ncomponent in turn has the highest variance possible under the constraint that it is orthogonal to the\\npreceding components.\\n4.4.1\\nGraphical illustration of the idea\\nConsider a two-dimensional data, that is, a dataset consisting of examples having two features. Let\\neach of the features be numeric data. So, each example can be plotted on a coordinate plane (x-\\ncoordinate indicating the ﬁrst feature and y-coordinate indicating the second feature). Plotting the\\nexample, we get a scatter diagram of the data. Now let us examine some typical scatter diagram\\nand make some observations regarding the directions in which the points in the scatter diagram are\\nspread out.\\nLet us examine the ﬁgures in Figure 4.1.\\n(i) Figure 4.1a shows a scatter diagram of a two-dimensional data.\\n(ii) Figure 4.1b shows spread of the data in the x direction and Figure 4.1c shows the spread of\\nthe data in the y-direction. We note that the spread in the x-direction is more than the spread\\nin the y direction.\\n(iii) Examining Figures 4.1d and 4.1e, we note that the maximum spread occurs in the direction\\nshown in Figure 4.1e. Figure 4.1e also shows the point whose coordinates are the mean\\nvalues of the two features in the dataset. This direction is called the direction of the ﬁrst\\nprincipal component of the given dataset.\\n(iv) The direction which is perpendicular (orthogonal) to the direction of the ﬁrst principal com-\\nponent is called the direction of the second principal component of the dataset. This direc-\\ntion is shown in Figure 4.1f. (This is only with reference to a two-dimensional dataset.)\\n(v) The unit vectors along the directions of principal components are called the principal com-\\nponent vectors, or simply, principal components. These are shown in Figure 4.1g.\\nRemark\\nlet us consider a dataset consisting of examples with three or more features. In such a case, we have\\nan n-dimensional dataset with n ≥3. In this case, the ﬁrst principal component is deﬁned exactly as\\nin item iii above. But, for the second component, it may be noted that there would be many directions\\nperpendicular to the direction of the ﬁrst principal component. The direction of the second principal\\ncomponent is that direction, which is perpendicular to the ﬁrst principal component, in which the\\nspread of data is largest. The third and higher order principal components are constructed in a similar\\nway.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 53}, page_content='CHAPTER 4. DIMENSIONALITY REDUCTION\\n39\\n(a) Scatter diagram\\n(b) Spread along x-direction\\n(c) Spread along y-direction\\n(d) Largest spread\\n(e) Direction of largest spread : Direction of the ﬁrst\\nprincipal component (solid dot is the point whose coor-\\ndinates are the means of x and y)\\n(f) Directions of principal components\\n(g) Principal component vectors (unit vectors in the di-\\nrections of principal components)\\nFigure 4.1: Principal components\\nA warning!\\nThe graphical illustration of the idea of PCA as explained above is slightly misleading. For the sake\\nof simplicity and easy geometrical representation, in the graphical illustration we have used range\\nas the measure of spread. The direction of the ﬁrst principal component was taken as the direction of\\nmaximum range. But, due to theoretical reasons, in the implementation of PCA in practice, it is the\\nvariance that is taken as as the measure of spread. The ﬁrst principal component is the the direction\\nin which the variance is maximum.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 54}, page_content='CHAPTER 4. DIMENSIONALITY REDUCTION\\n40\\n4.4.2\\nComputation of the principal component vectors\\n(PCA algorithm)\\nThe following is an outline of the procedure for performing a principal component analysis on a\\ngiven data. The procedure is heavily dependent on mathematical concepts. A knowledge of these\\nconcepts is essential to carry out this procedure.\\nStep 1. Data\\nWe consider a dataset having n features or variables denoted by X1,X2,...,Xn. Let there\\nbe N examples. Let the values of the i-th feature Xi be Xi1,Xi2,...,XiN (see Table 4.1).\\nFeatures\\nExample 1\\nExample 2\\n⋯\\nExample N\\nX1\\nX11\\nX12\\n⋯\\nX1N\\nX2\\nX21\\nX22\\n⋯\\nX2N\\n⋮\\nXi\\nXi1\\nXi2\\n⋯\\nXiN\\n⋮\\nXn\\nXn1\\nXn2\\n⋯\\nXnN\\nTable 4.1: Data for PCA algorithm\\nStep 2. Compute the means of the variables\\nWe compute the mean ¯Xi of the variable Xi:\\n¯Xi = 1\\nN (Xi1 + Xi2 + ⋯+ XiN).\\nStep 3. Calculate the covariance matrix\\nConsider the variables Xi and Xj (i and j need not be different). The covariance of the\\nordered pair (Xi,Xj) is deﬁned as1\\nCov(Xi,Xj) =\\n1\\nN −1\\nN\\n∑\\nk=1\\n(Xik −¯\\nXi)(Xjk −¯\\nXj).\\n(4.1)\\nWe calculate the following n × n matrix S called the covariance matrix of the data. The\\nelement in the i-th row j-th column is the covariance Cov(Xi,Xj):\\nS =\\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣\\nCov(X1,X1)\\nCov(X1,X2)\\n⋯\\nCov(X1,Xn)\\nCov(X2,X1)\\nCov(X2,X2)\\n⋯\\nCov(X2,Xn)\\n⋮\\nCov(Xn,X1)\\nCov(Xn,X2)\\n⋯\\nCov(Xn,Xn)\\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦\\nStep 4. Calculate the eigenvalues and eigenvectors of the covariance matrix\\nLet S be the covariance matrix and let I be the identity matrix having the same dimension\\nas the dimension of S.\\ni) Set up the equation:\\ndet(S −λI) = 0.\\n(4.2)\\nThis is a polynomial equation of degree n in λ. It has n real roots (some of the\\nroots may be repeated) and these roots are the eigenvalues of S. We ﬁnd the n roots\\nλ1,λ2,...,λn of Eq. (4.2).\\n1There is an alternative deﬁnition of covariance. In this deﬁnition, covariance is deﬁned as in Eq. (4.1) with N −1\\nreplaced by N. There are certain theoretical reasons for adopting the deﬁnition as given here.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 55}, page_content='CHAPTER 4. DIMENSIONALITY REDUCTION\\n41\\nii) If λ = λ′ is an eigenvalue, then the corresponding eigenvector is a vector\\nU =\\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣\\nu1\\nu2\\n⋮\\nun\\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦\\nsuch that\\n(S −λ′I)U = 0.\\n(This is a system of n homogeneous linear equations in u1, u2, ..., un and it al-\\nways has a nontrivial solution.) We next ﬁnd a set of n orthogonal eigenvectors\\nU1,U2,...,Un such that Ui is an eigenvector corresponding to λi.2\\niii) We now normalise the eigenvectors. Given any vector X we normalise it by dividing\\nX by its length. The length (or, the norm) of the vector\\nX =\\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣\\nx1\\nx2\\n⋮\\nxn\\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦\\nis deﬁned as\\n∣∣X∣∣=\\n√\\nx2\\n1 + x2\\n2 + ⋯+ x2n.\\nGiven any eigenvector U, the corresponding normalised eigenvector is computed as\\n1\\n∣∣U∣∣U.\\nWe compute the n normalised eigenvectors e1,e2,...,en by\\nei =\\n1\\n∣∣Ui∣∣Ui,\\ni = 1,2,...,n.\\nStep 5. Derive new data set\\nOrder the eigenvalues from highest to lowest. The unit eigenvector corresponding to the\\nlargest eigenvalue is the ﬁrst principal component. The unit eigenvector corresponding to\\nthe next highest eigenvalue is the second principal component, and so on.\\ni) Let the eigenvalues in descending order be λ1 ≥λ2 ≥... ≥λn and let the corre-\\nsponding unit eigenvectors be e1,e2,...,en.\\nii) Choose a positive integer p such that 1 ≤p ≤n.\\niii) Choose the eigenvectors corresponding to the eigenvalues λ1, λ2, ..., λp and form\\nthe following p × n matrix (we write the eigenvectors as row vectors):\\nF =\\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣\\neT\\n1\\neT\\n2\\n⋮\\neT\\np\\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦\\n,\\nwhere T in the superscript denotes the transpose.\\n2For i ≠j, the vectors Ui and Uj are orthogonal means UT\\ni Uj = 0 where T denotes the transpose.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 56}, page_content='CHAPTER 4. DIMENSIONALITY REDUCTION\\n42\\niv) We form the following n × N matrix:\\nX =\\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣\\nX11 −¯X1\\nX12 −¯X1\\n⋯\\nX1N −¯X1\\nX21 −¯X2\\nX22 −¯X2\\n⋯\\nX2N −¯X2\\n⋮\\nXn1 −¯Xn\\nXn2 −¯Xn\\n⋯\\nXnN −¯Xn\\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦\\nv) Next compute the matrix:\\nXnew = FX.\\nNote that this is a p × N matrix. This gives us a dataset of N samples having p\\nfeatures.\\nStep 6. New dataset\\nThe matrix Xnew is the new dataset. Each row of this matrix represents the values of a\\nfeature. Since there are only p rows, the new dataset has only features.\\nStep 7. Conclusion\\nThis is how the principal component analysis helps us in dimensional reduction of the\\ndataset. Note that it is not possible to get back the original n-dimensional dataset from\\nthe new dataset.\\n4.4.3\\nIllustrative example\\nWe illustrate the ideas of principal component analysis by considering a toy example. In the discus-\\nsions below, all the details of the computations are given. This is to give the reader an idea of the\\ncomplexity of computations and also to help the reader do a “worked example” by hand computa-\\ntions without recourse to software packages.\\nProblem\\nGiven the data in Table 4.2, use PCA to reduce the dimension from 2 to 1.\\nFeature\\nExample 1\\nExample 2\\nExample 3\\nExample 4\\nX1\\n4\\n8\\n13\\n7\\nX2\\n11\\n4\\n5\\n14\\nTable 4.2: Data for illustrating PCA\\nSolution\\n1. Scatter plot of data\\nWe have\\n¯X1 = 1\\n4(4 + 8 + 13 + 7) = 8,\\n¯X2 = 1\\n4(11 + 4 + 5 + 14) = 8.5.\\nFigure 4.2 shows the scatter plot of the data together with the point ( ¯X1, ¯X2).'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 57}, page_content='CHAPTER 4. DIMENSIONALITY REDUCTION\\n43\\nX1\\nX2\\n0\\n2\\n4\\n6\\n8 10 12 14\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n( ¯X1, ¯X2)\\nFigure 4.2: Scatter plot of data in Table 4.2\\n2. Calculation of the covariance matrix\\nThe covariances are calculated as follows:\\nCov(X1,X2) =\\n1\\nN −1\\nN\\n∑\\nk=1\\n(X1k −¯X1)2\\n= 1\\n3 ((4 −8)2 + (8 −8)2 + (13 −8)2 + (7 −8)2)\\n= 14\\nCov(X1,X2) =\\n1\\nN −1\\nN\\n∑\\nk=1\\n(X1k −¯X1)(X2k −¯X2)\\n= 1\\n3((4 −8)(11 −8.5) + (8 −8)(4 −8.5)\\n+ (13 −8)(5 −8.5) + (7 −8)(14 −8.5)\\n= −11\\nCov(X2,X1) = Cov(X1,X2)\\n= −11\\nCov(X2,X2) =\\n1\\nN −1\\nN\\n∑\\nk=1\\n(X2k −¯X2)2\\n= 1\\n3 ((11 −8.5)2 + (4 −8.5)2 + (5 −8.5)2 + (14 −8.5)2)\\n= 23\\nThe covariance matrix is\\nS = [Cov(X1,X1)\\nCov(X1,X2)\\nCov(X2,X1)\\nCov(X2,X2)]\\n= [ 14\\n−11\\n−11\\n23]\\n3. Eigenvalues of the covariance matrix\\nThe characteristic equation of the covariance matrix is\\n0 = det(S −λI)\\n= ∣14 −λ\\n−11\\n−11\\n23 −λ∣\\n= (14 −λ)(23 −λ) −(−11) × (−11)\\n= λ2 −37λ + 201'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 58}, page_content='CHAPTER 4. DIMENSIONALITY REDUCTION\\n44\\nSolving the characteristic equation we get\\nλ = 1\\n2(37 ±\\n√\\n565)\\n= 30.3849, 6.6151\\n= λ1, λ2\\n(say)\\n4. Computation of the eigenvectors\\nTo ﬁnd the ﬁrst principal components, we need only compute the eigenvector corresponding to the\\nlargest eigenvalue. In the present example, the largest eigenvalue is λ1 and so we compute the\\neigenvector corresponding to λ1.\\nThe eigenvector corresponding to λ = λ1 is a vector U = [u1\\nu2] satisfying the following equation:\\n[0\\n0] = (S −λ1I)X\\n= [14 −λ1\\n−11\\n−11\\n23 −λ1][u1\\nu2]\\n= [ (14 −λ1)u1 −11u2\\n−11u1 + (23 −λ1)u2]\\nThis is equivalent to the following two equations:\\n(14 −λ1)u1 −11u2 = 0\\n−11u1 + (23 −λ1)u2 = 0\\nUsing the theory of systems of linear equations, we note that these equations are not independent\\nand solutions are given by\\nu1\\n11 =\\nu2\\n14 −λ1\\n= t,\\nthat is\\nu1 = 11t,\\nu2 = (14 −λ1)t,\\nwhere t is any real number. Taking t = 1, we get an eigenvector corresponding to λ1 as\\nU1 = [\\n11\\n14 −λ1].\\nTo ﬁnd a unit eigenvector, we compute the length of X1 which is given by\\n∣∣U1∣∣=\\n√\\n112 + (14 −λ1)2\\n=\\n√\\n112 + (14 −30.3849)2\\n= 19.7348\\nTherefore, a unit eigenvector corresponding to lambda1 is\\ne1 = [\\n11/∣∣U1∣∣\\n(14 −λ1)/∣∣U1∣∣]\\n= [\\n11/19.7348\\n(14 −30.3849)/19.7348]\\n= [ 0.5574\\n−0.8303]\\nBy carrying out similar computations, the unit eigenvector e2 corresponding to the eigenvalue\\nλ = λ2 can be shown to be\\ne2 = [0.8303\\n0.5574].'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 59}, page_content='CHAPTER 4. DIMENSIONALITY REDUCTION\\n45\\nX1\\nX2\\n0\\n2\\n4\\n6\\n8 10 12 14\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\ne1\\ne2\\n( ¯X1, ¯X2)\\nFigure 4.3: Coordinate system for principal components\\n5. Computation of ﬁrst principal components\\nLet [X1k\\nX2k] be the k-th sample in Table 4.2. The ﬁrst principal component of this example is given\\nby (here “T” denotes the transpose of the matrix)\\neT\\n1 [X1k −¯X1\\nX2k −¯X2] = [0.5574\\n−0.8303][X1k −¯X1\\nX2k −¯X2]\\n= 0.5574(X1k −¯X1) −0.8303(X2k −¯X2).\\nFor example, the ﬁrst principal component corresponding to the ﬁrst example [X11\\nX21] = [ 4\\n11] is\\ncalculated as follows:\\n[0.5574\\n−0.8303][X11 −¯X1\\nX21 −¯X2] = 0.5574(X11 −¯X1) −0.8303(X21 −¯X2)\\n= 0.5574(4 −8) −0.8303(11 −8,5)\\n= −4.30535\\nThe results of calculations are summarised in Table 4.3.\\nX1\\n4\\n8\\n13\\n7\\nX2\\n11\\n4\\n5\\n14\\nFirst principal components\\n-4.3052\\n3.7361\\n5.6928\\n-5.1238\\nTable 4.3: First principal components for data in Table 4.2\\n6. Geometrical meaning of ﬁrst principal components\\nAs we have seen in Figure 4.1, we introduce new coordinate axes. First we shift the origin to\\nthe “center” ( ¯X1, ¯X2) and then change the directions of coordinate axes to the directions of the\\neigenvectors e1 and e2 (see Figure 4.3).\\nNext, we drop perpendiculars from the given data points to the e1-axis (see Figure 4.4). The ﬁrst\\nprincipal components are the e1-coordinates of the feet of perpendiculars, that is, the projections on\\nthe e1-axis. The projections of the data points on e1-axis may be taken as approximations of the\\ngiven data points hence we may replace the given data set with these points. Now, each of these'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 60}, page_content='CHAPTER 4. DIMENSIONALITY REDUCTION\\n46\\nX1\\nX2\\n0\\n2\\n4\\n6\\n8 10 12 14\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\ne1\\ne2\\n(4,11)\\n(8,4)\\n(13,5)\\n(7,14)\\n( ¯X1, ¯X2)\\nFigure 4.4: Projections of data points on the axis of the ﬁrst principal component\\nPC1 components\\n-4.305187\\n3.736129\\n5.692828\\n-5.123769\\nTable 4.4: One-dimensional approximation to the data in Table 4.2\\napproximations can be unambiguously speciﬁed by a single number, namely, the e1-coordinate of\\napproximation. Thus the two-dimensional data set given in Table 4.2 can be represented approxi-\\nmately by the following one-dimensional data set (see Figure 4.5):\\nX1\\nX2\\n0\\n2\\n4\\n6\\n8\\n10 12 14\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\ne1\\ne2\\n(4,11)\\n(8,4)\\n(13, 5)\\n(7,14)\\n( ¯X1, ¯X2)\\nX1\\nX2\\n0\\n2\\n4\\n6\\n8\\n10 12 14\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\ne1\\ne2\\n( ¯X1, ¯X2)\\nFigure 4.5: Geometrical representation of one-dimensional approximation to the data in Table 4.2\\n4.5\\nSample questions\\n(a) Short answer questions\\n1. What is dimensionality reduction? How is it implemented?\\n2. Explain why dimensionality reduction is useful in machine learning.\\n3. What are the commonly used dimensionality reduction techniques in machine learning?\\n4. How is the subset selection method used for dimensionality reduction?\\n5. Explain the method of principal component analysis in machine learning.\\n6. What are the ﬁrst principal components of a data?\\n7. Is subset selection problem an unsupervised learning problem? Why?'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 61}, page_content='CHAPTER 4. DIMENSIONALITY REDUCTION\\n47\\n8. Is principal component analysis a supervised learning problem? Why?\\n(b) Long answer questions\\n1. Describe the forward selection algorithm for implementing the subset selection procedure for\\ndimensionality reduction.\\n2. Describe the backward selection algorithm for implementing the subset selection procedure\\nfor dimensionality reduction.\\n3. What is the ﬁrst principal component of a data? How one can compute it?\\n4. Describe with the use of diagrams the basic principle of PCA.\\n5. Explain the procedure for the computation of the principal components of a given data.\\n6. Describe how principal component analysis is carried out to reduce dimensionality of data\\nsets.\\n7. Given the following data, compute the principal component vectors and the ﬁrst principal\\ncomponents:\\nx\\n2\\n3\\n7\\ny\\n11\\n14\\n26\\n8. Given the following data, compute the principal component vectors and the ﬁrst principal\\ncomponents:\\nx\\n-3\\n1\\n-2\\ny\\n2\\n-1\\n3'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 62}, page_content='Chapter 5\\nEvaluation of classiﬁers\\nIn machine learning, there are several classiﬁcation algorithms and, given a certain problem, more\\nthan one may be applicable. So there is a need to examine how we can assess how good a se-\\nlected algorithm is. Also, we need a method to compare the performance of two or more different\\nclassiﬁcation algorithms. These methods help us choose the right algorithm in a practical situation.\\n5.1\\nMethods of evaluation\\n5.1.1\\nNeed for multiple validation sets\\nWhen we apply a classiﬁcation algorithm in a practical situation, we always do a validation test.\\nWe keep a small sample of examples as validation set and the remaining set as the training set. The\\nclassiﬁer developed using the training set is applied to the examples in the validation set. Based on\\nthe performance on the validation set, the accuracy of the classiﬁer is assessed. But, the performance\\nmeasure obtained by a single validation set alone does not give a true picture of the performance of a\\nclassiﬁer. Also these measures alone cannot be meaningfully used to compare two algorithms. This\\nrequires us to have different validation sets.\\nCross-validation in general, and k-fold cross-validation in particular, are two common method\\nfor generating multiple training-validation sets from a given dataset.\\n5.1.2\\nStatistical distribution of errors\\nWe use a classiﬁcation algorithm on a dataset and generate a classiﬁer. If we do the training once,\\nwe have one classiﬁer and one validation error. To average over randomness (in training data, initial\\nweights, etc.), we use the same algorithm and generate multiple classiﬁers. We test these classiﬁers\\non multiple validation sets and record a sample of validation errors. We base our evaluation of the\\nclassiﬁcation algorithm on the statistical distribution of these validation errors. We can use this\\ndistribution for assessing the expected error rate of the classiﬁcation algorithm for that problem, or\\ncompare it with the error rate distribution of some other classiﬁcation algorithm.\\nA detailed discussion of these ideas is beyond the scope of these notes.\\n5.1.3\\nNo-free lunch theorem\\nWhatever conclusion we draw from our analysis is conditioned on the dataset we are given. We\\nare not comparing classiﬁcation algorithms in a domain-independent way but on some particular\\napplication. We are not saying anything about the expected error-rate of a learning algorithm, or\\ncomparing one learning algorithm with another algorithm, in general. Any result we have is only\\ntrue for the particular application. There is no such thing as the “best” learning algorithm. For any\\n48'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 63}, page_content='CHAPTER 5. EVALUATION OF CLASSIFIERS\\n49\\nlearning algorithm, there is a dataset where it is very accurate and another dataset where it is very\\npoor. This is called the No Free Lunch Theorem.1\\n5.1.4\\nOther factors\\nClassiﬁcation algorithms can be compared based not only on error rates but also on several other\\ncriteria like the following:\\n• risks when errors are generalized using loss functions\\n• training time and space complexity,\\n• testing time and space complexity,\\n• interpretability, namely, whether the method allows knowledge extraction which can be checked\\nand validated by experts, and\\n• easy programmability.\\n5.2\\nCross-validation\\nTo test the performance of a classiﬁer, we need to have a number of training/validation set pairs\\nfrom a dataset X. To get them, if the sample X is large enough, we can randomly divide it then\\ndivide each part randomly into two and use one half for training and the other half for validation.\\nUnfortunately, datasets are never large enough to do this. So, we use the same data split differently;\\nthis is called cross-validation.\\nCross-validation is a technique to evaluate predictive models by partitioning the original sample\\ninto a training set to train the model, and a test set to evaluate it.\\nThe holdout method is the simplest kind of cross validation. The data set is separated into two\\nsets, called the training set and the testing set. The algorithm ﬁts a function using the training set\\nonly. Then the function is used to predict the output values for the data in the testing set (it has never\\nseen these output values before). The errors it makes are used to evaluate the model.\\n5.3\\nK-fold cross-validation\\nIn K-fold cross-validation, the dataset X is divided randomly into K equal-sized parts, Xi, i =\\n1,...,K. To generate each pair, we keep one of the K parts out as the validation set Vi, and combine\\nthe remaining K −1 parts to form the training set Ti. Doing this K times, each time leaving out\\nanother one of the K parts out, we get K pairs (Vi,Ti):\\nV1 = X1,\\nT1 = X2 ∪X3 ∪... ∪XK\\nV2 = X2,\\nT2 = X1 ∪X3 ∪... ∪XK\\n⋯\\nVK = XK,\\nTK = X1 ∪X2 ∪... ∪XK−1\\nRemarks\\n1. There are two problems with this: First, to keep the training set large, we allow validation sets\\nthat are small. Second, the training sets overlap considerably, namely, any two training sets\\nshare K −2 parts.\\n1“We have dubbed the associated results NFL theorems because they demonstrate that if an algorithm performs well on\\na certain class of problems then it necessarily pays for that with degraded performance on the set of all remaining prob-\\nlems.”(David Wolpert and William Macready in [7])'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 64}, page_content='CHAPTER 5. EVALUATION OF CLASSIFIERS\\n50\\n2. K is typically 10 or 30. As K increases, the percentage of training instances increases and we\\nget more robust estimators, but the validation set becomes smaller. Furthermore, there is the\\ncost of training the classiﬁer K times, which increases as K is increased.\\ntest set\\ntest set\\ntest set\\ntest set\\ntest set\\ntraining set\\ntraining set\\ntraining set\\ntraining set\\ntraining set\\ntraining set\\ntraining set\\ntraining set\\n1-st fold\\n2-nd fold\\n3-rd fold\\n4-th fold\\n5-th fold\\nFigure 5.1: One iteration in a 5-fold cross-validation\\nLeave-one-out cross-validation\\nAn extreme case of K-fold cross-validation is leave-one-out where given a dataset of N instances,\\nonly one instance is left out as the validation set and training uses the remaining N −1 instances.\\nWe then get N separate pairs by leaving out a different instance at each iteration. This is typically\\nused in applications such as medical diagnosis, where labeled data is hard to ﬁnd.\\n5.3.1\\n5 × 2 cross-validation\\nIn this method, the dataset X is divided into two equal parts X(1)\\n1\\nand X(2)\\n1\\n. We take as the training\\nset and X(2)\\n1\\nas the validation set. We then swap the two sets and take X(2)\\n1\\nas the training set and\\nX(1)\\n1\\nas the validation set. This is the ﬁrst fold. the process id repeated four more times to get ten\\npairs of training sets and validation sets.\\nT1 = X(1)\\n1\\n,\\nV1 = X(2)\\n1\\nT2 = X(2)\\n1\\n,\\nV2 = X(1)\\n1\\nT3 = X(1)\\n2\\n,\\nV3 = X(2)\\n2\\nT4 = X(2)\\n2\\n,\\nV4 = X(1)\\n2\\n⋮\\nT9 = X(1)\\n5\\n,\\nV3 = X(2)\\n5\\nT10 = X(2)\\n5\\n,\\nV10 = X(1)\\n5\\nIt has been shown that after ﬁve folds, the validation error rates become too dependent and do\\nnot add new information. On the other hand, if there are fewer than ﬁve folds, we get fewer data\\n(fewer than ten) and will not have a large enough sample to ﬁt a distribution and test our hypothesis.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 65}, page_content='CHAPTER 5. EVALUATION OF CLASSIFIERS\\n51\\n5.3.2\\nBootstrapping\\nBootstrapping in statistics\\nIn statistics, the term “bootstrap sampling”, the “bootstrap” or “bootstrapping” for short, refers to\\nprocess of “random sampling with replacement”.\\nExample\\nFor example, let there be ﬁve balls labeled A, B, C, D, E in an urn. We wish to select different\\nsamples of balls from the urn each sample containing two balls. The following procedure may be\\nused to select the samples. This is an example for bootstrap sampling.\\n1. We select two balls from the basket. Let them be A and E. Record the labels.\\n2. Put the two balls back in the basket.\\n3. We select two balls from the basket. Let them be C and E. Record the labels.\\n4. Put the two balls back into the basket.\\nThis is repeated as often as required. So we get different samples of size 2, say, A, E; B, E; etc.\\nThese samples are obtained by sampling with replacement, that is, by bootstrapping.\\nBootstrapping in machine learning\\nIn machine learning, bootstrapping is the process of computing performance measures using several\\nrandomly selected training and test datasets which are selected through a precess of sampling with\\nreplacement, that is, through bootstrapping. Sample datasets are selected multiple times.\\nThe bootstrap procedure will create one or more new training datasets some of which are re-\\npeated. The corresponding test datasets are then constructed from the set of examples that were not\\nselected for the respective training datasets.\\n5.4\\nMeasuring error\\n5.4.1\\nTrue positive, false positive, etc.\\nDeﬁnitions\\nConsider a binary classiﬁcation model derived from a two-class dataset. Let the class labels be c and\\n¬c. Let x be a test instance.\\n1. True positive\\nLet the true class label of x be c. If the model predicts the class label of x as c, then we say\\nthat the classiﬁcation of x is true positive.\\n2. False negative\\nLet the true class label of x be c. If the model predicts the class label of x as ¬c, then we say\\nthat the classiﬁcation of x is false negative.\\n3. True negative\\nLet the true class label of x be ¬c. If the model predicts the class label of x as ¬c, then we say\\nthat the classiﬁcation of x is true negative.\\n4. False positive\\nLet the true class label of x be ¬c. If the model predicts the class label of x as c, then we say\\nthat the classiﬁcation of x is false positive.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 66}, page_content='CHAPTER 5. EVALUATION OF CLASSIFIERS\\n52\\nActual label of x is c\\nActual label of x is ¬c\\nPredicted label of x is c\\nTrue positive\\nFalse positive\\nPredicted label of x is ¬c\\nFalse negative\\nTrue negative\\n5.4.2\\nConfusion matrix\\nA confusion matrix is used to describe the performance of a classiﬁcation model (or “classiﬁer”) on\\na set of test data for which the true values are known. A confusion matrix is a table that categorizes\\npredictions according to whether they match the actual value.\\nTwo-class datasets\\nFor a two-class dataset, a confusion matrix is a table with two rows and two columns that reports the\\nnumber of false positives, false negatives, true positives, and true negatives.\\nAssume that a classiﬁer is applied to a two-class test dataset for which the true values are known.\\nLet TP denote the number of true positives in the predicted values, TN the number of true negatives,\\netc. Then the confusion matrix of the predicted values can be represented as follows:\\nActual condition\\nis true\\nActual condition\\nis false\\nPredicted condi-\\ntion is true\\nTP\\nFP\\nPredicted condi-\\ntion is false\\nFN\\nFN\\nTable 5.1: Confusion matrix for two-class dataset\\nMulticlass datasets\\nConfusion matrices can be constructed for multiclass datasets also.\\nExample\\nIf a classiﬁcation system has been trained to distinguish between cats, dogs and rabbits, a confusion\\nmatrix will summarize the results of testing the algorithm for further inspection. Assuming a sample\\nof 27 animals - 8 cats, 6 dogs, and 13 rabbits, the resulting confusion matrix could look like the table\\nbelow: This confusion matrix shows that, for example, of the 8 actual cats, the system predicted that\\nActual “cat”\\nActual “dog”\\nActual “rabbit”\\nPredicted “cat”\\n5\\n2\\n0\\nPredicted “dog”\\n3\\n3\\n2\\nPredicted “ rabbit”\\n0\\n1\\n11\\nthree were dogs, and of the six dogs, it predicted that one was a rabbit and two were cats.\\n5.4.3\\nPrecision and recall\\nIn machine learning, precision and recall are two measures used to assess the quality of results\\nproduced by a binary classiﬁer. They are formally deﬁned as follows.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 67}, page_content='CHAPTER 5. EVALUATION OF CLASSIFIERS\\n53\\nDeﬁnitions\\nLet a binary classiﬁer classify a collection of test data. Let\\nTP = Number of true positives\\nTN = Number of true negatives\\nFP = Number of false positives\\nFN = Number of false negatives\\nThe precision P is deﬁned as\\nP =\\nTP\\nTP + FP\\nThe recall R is deﬁned as\\nR =\\nTP\\nTP + FN\\nProblem 1\\nSuppose a computer program for recognizing dogs in photographs identiﬁes eight dogs in a picture\\ncontaining 12 dogs and some cats. Of the eight dogs identiﬁed, ﬁve actually are dogs while the rest\\nare cats. Compute the precision and recall of the computer program.\\nSolution\\nWe have:\\nTP = 5\\nFP = 3\\nFN = 7\\nThe precision P is\\nP =\\nTP\\nTP + FP =\\n5\\n5 + 3 = 5\\n8\\nThe recall R is\\nR =\\nTP\\nTP + FN =\\n5\\n5 + 7 = 5\\n12\\nProblem 2\\nLet there be 10 balls (6 white and 4 red balls) in a box and let it be required to pick up the red balls\\nfrom them. Suppose we pick up 7 balls as the red balls of which only 2 are actually red balls. What\\nare the values of precision and recall in picking red ball?\\nSolution\\nObviously we have:\\nTP = 2\\nFP = 7 −2 = 5\\nFN = 4 −2 = 2\\nThe precision P is\\nP =\\nTP\\nTP + FP =\\n2\\n2 + 5 = 2\\n7\\nThe recall R is\\nR =\\nTP\\nTP + FN =\\n2\\n2 + 2 = 1\\n2'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 68}, page_content='CHAPTER 5. EVALUATION OF CLASSIFIERS\\n54\\nProblem 3\\nAssume the following: A database contains 80 records on a particular topic of which 55 are relevant\\nto a certain investigation. A search was conducted on that topic and 50 records were retrieved. Of the\\n50 records retrieved, 40 were relevant. Construct the confusion matrix for the search and calculate\\nthe precision and recall scores for the search.\\nSolution\\nEach record may be assigned a class label “relevant\" or “not relevant”. All the 80 records were\\ntested for relevance. The test classiﬁed 50 records as “relevant”. But only 40 of them were actually\\nrelevant. Hence we have the following confusion matrix for the search:\\nActual ”relevant”\\nActual “not rele-\\nvant”\\nPredicted\\n“rele-\\nvant”\\n40\\n10\\nPredicted\\n“not\\nrelevant”\\n15\\n25\\nTable 5.2: Example for confusion matrix\\nTP = 40\\nFP = 10\\nFN = 15\\nThe precision P is\\nP =\\nTP\\nTP + FP =\\n40\\n40 + 10 = 4\\n5\\nThe recall R is\\nR =\\nTP\\nTP + FN =\\n40\\n40 + 15 = 40\\n55\\n5.4.4\\nOther measures of performance\\nUsing the data in the confusion matrix of a classiﬁer of two-class dataset, several measures of per-\\nformance have been deﬁned. A few of them are listed below.\\n1. Accuracy =\\nTP + TN\\nTP + TN + FP + FN\\n2. Error rate = 1−Accuracy\\n3. Sensitivity =\\nTP\\nTP + FN\\n4. Speciﬁcity =\\nTN\\nTN + FP\\n5. F-measure =\\n2 × TP\\n2 × TP + FP + FN\\n5.5\\nReceiver Operating Characteristic (ROC)\\nThe acronym ROC stands for Receiver Operating Characteristic, a terminology coming from signal\\ndetection theory. The ROC curve was ﬁrst developed by electrical engineers and radar engineers\\nduring World War II for detecting enemy objects in battleﬁelds. They are now increasingly used in\\nmachine learning and data mining research.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 69}, page_content='CHAPTER 5. EVALUATION OF CLASSIFIERS\\n55\\nTPR and FPR\\nLet a binary classiﬁer classify a collection of test data. Let, as before,\\nTP = Number of true positives\\nTN = Number of true negatives\\nFP = Number of false positives\\nFN = Number of false negatives\\nNow we introduce the following terminology:\\nTPR = True Positive Rate\\n=\\nTP\\nTP + FN\\n= Fraction of positive examples correctly classiﬁed\\n= Sensitivity\\nFPR = False Positive Rate\\n=\\nFP\\nFP + TN\\n= Fraction of negative examples incorrectly classiﬁed\\n= 1 −Speciﬁcity\\nROC space\\nWe plot the values of FPR along the horizontal axis (that is , x-axis) and the values of TPR along\\nthe vertical axis (that is, y-axis) in a plane. For each classiﬁer, there is a unique point in this plane\\nwith coordinates (FPR,TPR). The ROC space is the part of the plane whose points correspond to\\n(FPR,TPR). Each prediction result or instance of a confusion matrix represents one point in the\\nROC space.\\nThe position of the point (FPR,TPR) in the ROC space gives an indication of the performance\\nof the classiﬁer. For example, let us consider some special points in the space.\\nSpecial points in ROC space\\n1. The left bottom corner point (0,0): Always negative prediction\\nA classiﬁer which produces this point in the ROC space never classiﬁes an example as positive,\\nneither rightly nor wrongly, because for this point TP = 0 and FP = 0. It always makes\\nnegative predictions. All positive instances are wrongly predicted and all negative instances\\nare correctly predicted. It commits no false positive errors.\\n2. The right top corner point (1,1): Always positive prediction\\nA classiﬁer which produces this point in the ROC space always classiﬁes an example as posi-\\ntive because for this point FN = 0 and TN = 0. All positive instances are correctly predicted\\nand all negative instances are wrongly predicted. It commits no false negative errors.\\n3. The left top corner point (0,1): Perfect prediction\\nA classiﬁer which produces this point in the ROC space may be thought as a perfect classiﬁer.\\nIt produces no false positives and no false negatives.\\n4. Points along the diagonal: Random performance\\nConsider a classiﬁer where the class labels are randomly guessed, say by ﬂipping a coin. Then,\\nthe corresponding points in the ROC space will be lying very near the diagonal line joining\\nthe points (0,0) and (1,1).'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 70}, page_content='CHAPTER 5. EVALUATION OF CLASSIFIERS\\n56\\n.0\\n.0\\n.1\\n.1\\n.2\\n.2\\n.3\\n.3\\n.4\\n.4\\n.5\\n.5\\n.6\\n.6\\n.7\\n.7\\n.8\\n.8\\n.9\\n.9\\n1\\n1\\nFalse Positive Rate (FPR) →\\nTrue Positive Rate (TPR) →\\nROC space\\nAlways negative prediction\\nAlways positive prediction\\nPerfect prediction\\nPoint on diagonal\\n(Random performance)\\nFigure 5.2: The ROC space and some special points in the space\\nROC curve\\nIn the case of certain classiﬁcation algorithms, the classiﬁer may depend on a parameter. Different\\nvalues of the parameter will give different classiﬁers and these in turn give different values to TPR\\nand FPR. The ROC curve is the curve obtained by plotting in the ROC space the points (TPR , FPR)\\nobtained by assigning all possible values to the parameter in the classiﬁer.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 71}, page_content='CHAPTER 5. EVALUATION OF CLASSIFIERS\\n57\\n.0\\n.0\\n.1\\n.1\\n.2\\n.2\\n.3\\n.3\\n.4\\n.4\\n.5\\n.5\\n.6\\n.6\\n.7\\n.7\\n.8\\n.8\\n.9\\n.9\\n1\\n1\\nFalse Positive Rate (FPR) →\\nTrue Positive Rate (TPR) →\\nROC space\\nA\\nB\\nC\\nFigure 5.3: ROC curves of three different classiﬁers A, B, C\\nThe closer the ROC curve is to the top left corner (0,1) of the ROC space, the better the accuracy\\nof the classiﬁer. Among the three classiﬁers A, B, C with ROC curves as shown in Figure 5.3, the\\nclassiﬁer C is closest to the top left corner of the ROC space. Hence, among the three, it gives the\\nbest accuracy in predictions.\\nExample\\nCut-off value of BMI\\nBreast cancer\\nNormal persons\\nTPR\\nFPR\\nTP\\nFN\\nFP\\nTN\\n18\\n100\\n0\\n200\\n0\\n1.00\\n1.000\\n20\\n100\\n0\\n198\\n2\\n1.00\\n0.990\\n22\\n99\\n1\\n177\\n23\\n0.99\\n0.885\\n24\\n95\\n5\\n117\\n83\\n0.95\\n0.585\\n26\\n85\\n15\\n80\\n120\\n0.85\\n0.400\\n28\\n66\\n34\\n53\\n147\\n0.66\\n0.265\\n30\\n47\\n53\\n27\\n173\\n0.47\\n0.135\\n32\\n34\\n66\\n17\\n183\\n0.34\\n0.085\\n34\\n21\\n79\\n14\\n186\\n0.21\\n0.070\\n36\\n17\\n83\\n6\\n194\\n0.17\\n0.030\\n38\\n7\\n93\\n4\\n196\\n0.07\\n0.020\\n40\\n1\\n99\\n1\\n199\\n0.01\\n0.005\\nTable 5.3: Data on breast cancer for various values of BMI\\nThe body mass index (BMI) of a person is deﬁned as (weight(kg)/height(m)2). Researchers have\\nestablished a link between BMI and the risk of breast cancer among women. The higher the BMI\\nthe higher the risk of developing breast cancer. The critical threshold value of BMI may depend on\\nseveral parameters like food habits, socio-cultural-economic background, life-style, etc. Table 5.3'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 72}, page_content='CHAPTER 5. EVALUATION OF CLASSIFIERS\\n58\\ngives real data of a breast cancer study with a sample having 100 patients and 200 normal persons.2\\nThe table also shows the values of TPR and FPR for various cut-off values of BMI. The ROC curve\\nof the data in Table 5.3 is shown in Figure 5.4.\\n.0\\n.0\\n.1\\n.1\\n.2\\n.2\\n.3\\n.3\\n.4\\n.4\\n.5\\n.5\\n.6\\n.6\\n.7\\n.7\\n.8\\n.8\\n.9\\n.9\\n1\\n1\\nFalse Positive Rate (FPR) →\\nTrue Positive Rate (TPR) →\\nROC space\\nCut-off BMI = 26\\nCut-off BMI = 28\\nAUC = Area of shaded region\\nFigure 5.4: ROC curve of data in Table 5.3 showing the points closest to the perfect prediction point\\n(0,1)\\nArea under the ROC curve\\nThe measure of the area under the ROC curve is denoted by the acronym AUC (see Figure 5.4). The\\nvalue of AUC is a measure of the performance of a classiﬁer. For the perfect classiﬁer, AUC = 1.0.\\n5.6\\nSample questions\\n(a) Short answer questions\\n1. What is cross-validation in machine learning?\\n2. What is meant by 5 × 2 cross-validation?\\n3. What is meant by leave-one-out cross validation?\\n4. What is meant by the confusion matrix of a binary classiﬁcation problem.\\n5. Deﬁne the following terms: precision, recall, sensitivity, speciﬁcity.\\n6. What is ROC curve in machine learning?\\n7. What are true positive rates and false positive rates in machine learning?\\n8. What is AUC in relation to ROC curves?\\n2https://www.ncbi.nlm.nih.gov/ pmc/articles/PMC3755824/'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 73}, page_content='CHAPTER 5. EVALUATION OF CLASSIFIERS\\n59\\n(b) Long answer questions\\n1. Explain cross-validation in machine learning. Explain the different types of cross-validations.\\n2. What is meant by true positives etc.? What is meant by confusion matrix of a binary classiﬁ-\\ncation problem? Explain how this can be extended to multi-class problems.\\n3. What are ROC space and ROC curve in machine learning? In ROC space, which points\\ncorrespond to perfect prediction, always positive prediction and always negative prediction?\\nWhy?\\n4. Consider a two-class classiﬁcation problem of predicting whether a photograph contains a\\nman or a woman. Suppose we have a test dataset of 10 records with expected outcomes and a\\nset of predictions from our classiﬁcation algorithm.\\nExpected\\nPredicted\\n1\\nman\\nwoman\\n2\\nman\\nman\\n3\\nwoman\\nwoman\\n4\\nman\\nman\\n5\\nwoman\\nman\\n6\\nwoman\\nwoman\\n7\\nwoman\\nwoman\\n8\\nman\\nman\\n9\\nman\\nwoman\\n10\\nwoman\\nwoman\\n(a) Compute the confusion matrix for the data.\\n(b) Compute the accuracy, precision, recall, sensitivity and speciﬁcity of the data.\\n5. Suppose 10000 patients get tested for ﬂu; out of them, 9000 are actually healthy and 1000\\nare actually sick. For the sick people, a test was positive for 620 and negative for 380. For\\nthe healthy people, the same test was positive for 180 and negative for 8820. Construct a\\nconfusion matrix for the data and compute the accuracy, precision and recall for the data.\\n6. Given the following data, construct the ROC curve of the data. Compute the AUC.\\nThreshold\\nTP\\nTN\\nFP\\nFN\\n1\\n0\\n25\\n0\\n29\\n2\\n7\\n25\\n0\\n22\\n3\\n18\\n24\\n1\\n11\\n4\\n26\\n20\\n5\\n3\\n5\\n29\\n11\\n14\\n0\\n6\\n29\\n0\\n25\\n0\\n7\\n29\\n0\\n25\\n0\\n7. Given the following hypothetical data at various cut-off points of mid-arm circumference of\\nmid-arm circumference to detect low birth-weight construct the ROC curve for the data.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 74}, page_content='CHAPTER 5. EVALUATION OF CLASSIFIERS\\n60\\nMid-arm circumference (cm)\\nNormal birth-weight\\nLow birth-weight\\nTP\\nTN\\n≤8.3\\n13\\n867\\n≤8.4\\n24\\n844\\n≤8.5\\n73\\n826\\n≤8.6\\n90\\n800\\n≤8.7\\n113\\n783\\n≤8.8\\n119\\n735\\n≤8.9\\n121\\n626\\n≤9.0\\n125\\n505\\n≤9.1\\n127\\n435\\n≤9.2 and above\\n130\\n0'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 75}, page_content='Chapter 6\\nBayesian classiﬁer and ML\\nestimation\\nThe Bayesian classiﬁer is an algorithm for classifying multiclass datasets. This is based on the\\nBayes’ theorem in probability theory. Bayes in whose name the theorem is known was an English\\nstatistician who was known for having formulated a speciﬁc case of a theorem that bears his name.\\nThe classiﬁer is also known as “naive Bayes Algorithm” where the word “naive” is an English word\\nwith the following meanings: simple, unsophisticated, or primitive. We ﬁrst explain Bayes’ theorem\\nand then describe the algorithm. Of course, we require the notion of conditional probability.\\n6.1\\nConditional probability\\nThe probability of the occurrence of an event A given that an event B has already occurred is called\\nthe conditional probability of A given B and is denoted by P(A∣B). We have\\nP(A∣B) = P(A ∩B)\\nP(B)\\nif\\nP(B) ≠0.\\n6.1.1\\nIndependent events\\n1. Two events A and B are said to be independent if\\nP(A ∩B) = P(A)P(B).\\n2. Three events A,B,C are said to be pairwise independent if\\nP(B ∩C) = P(B)P(C)\\nP(C ∩A) = P(C)P(A)\\nP(A ∩B) = P(A)P(B)\\n3. Three events A,B,C are said to be mutually independent if\\nP(B ∩C) = P(B)P(C)\\n(6.1)\\nP(C ∩A) = P(C)P(A)\\n(6.2)\\nP(A ∩B) = P(A)P(B)\\n(6.3)\\nP(A ∩B ∩C) = P(A)P(B)P(C)\\n(6.4)\\n4. In general, a family of k events A1,A2,...,Ak is said to be mutually independent if for any\\nsubfamily consisting of Ai1,...Aim we have\\nP(Ai1 ∩... ∩Aim) = P(Ai1)...P(Aim).\\n61'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 76}, page_content='CHAPTER 6. BAYESIAN CLASSIFIER AND ML ESTIMATION\\n62\\nRemarks\\nConsider events and respective probabilities as shown in Figure 6.1. It can be seen that, in this case,\\nthe conditions Eqs.(6.1)–(6.3) are satisﬁed, but Eq.(6.4) is not satisﬁed. But if the probabilities are\\nas in Figure 6.2, then Eq.(6.4) is satisﬁed but all the conditions in Eqs.(6.1)–(6.2) are not satisﬁed.\\nFigure 6.1: Events A,B,C which are not mutually independent: Eqs.(6.1)–(6.3) are satisﬁed, but\\nEq.(6.4) is not satisﬁed.\\nFigure 6.2: Events A,B,C which are not mutually independent: Eq.(6.4) is satisﬁed but Eqs.(6.1)–\\n(6.2) are not satisﬁed.\\n6.2\\nBayes’ theorem\\n6.2.1\\nTheorem\\nLet A and B any two events in a random experiment. If P(A) ≠0, then\\nP(B∣A) = P(A∣B)P(B)\\nP(A)\\n.\\n6.2.2\\nRemarks\\n1. The importance of the result is that it helps us to “invert” conditional probabilities, that is, to\\nexpress the conditional probability P(A∣B) in terms of the conditional probability P(B∣A).\\n2. The following terminology is used in this context:\\n• A is called the proposition and B is called the evidence.\\n• P(A) is called the prior probability of proposition and P(B) is called the prior proba-\\nbility of evidence.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 77}, page_content='CHAPTER 6. BAYESIAN CLASSIFIER AND ML ESTIMATION\\n63\\n• P(A∣B) is called the posterior probability of A given B.\\n• P(B∣A) is called the likelihood of B given A.\\n6.2.3\\nGeneralisation\\nLet the sample space be divided into disjoint events B1,B2,...,Bn and A be any event. Then we\\nhave\\nP(Bk∣A) =\\nP(A∣Bk)P(Bk)\\n∑n\\ni=1 P(A∣Bi)P(Bi)\\n6.2.4\\nExamples\\nProblem 1\\nConsider a set of patients coming for treatment in a certain clinic. Let A denote the event that\\na “Patient has liver disease” and B the event that a “Patient is an alcoholic.” It is known from\\nexperience that 10% of the patients entering the clinic have liver disease and 5% of the patients are\\nalcoholics. Also, among those patients diagnosed with liver disease, 7% are alcoholics. Given that\\na patient is alcoholic, what is the probability that he will have liver disease?\\nSolution\\nUsing the notations of probability, we have\\nP(A) = 10% = 0.10\\nP(B) = 5% = 0.05\\nP(B∣A) = 7% = 0.07\\nP(A∣B) = P(B∣A)P(A)\\nP(B)\\n= 0.07 × 0.10\\n0.05\\n= 0.14\\nProblem 2\\nThree factories A, B, C of an electric bulb manufacturing company produce respectively 35%. 35%\\nand 30% of the total output. Approximately 1.5%, 1% and 2% of the bulbs produced by these\\nfactories are known to be defective. If a randomly selected bulb manufactured by the company was\\nfound to be defective, what is the probability that the bulb was manufactures in factory A?\\nSolution\\nLet A,B,C denote the events that a randomly selected bulb was manufactured in factory A, B, C\\nrespectively. Let D denote the event that a bulb is defective. We have the following data:\\nP(A) = 0.35,\\nP(B) = 0.35,\\nP(C) = 0.30\\nP(D∣A) = 0.015,\\nP(D∣B) = 0.010,\\nP(D∣C) = 0.020\\nWe are required to ﬁnd P(A∣D). By the generalisation of the Bayes’ theorem we have:\\nP(A∣D) =\\nP(D∣A)P(A)\\nP(D∣A)P(A) + P(D∣B)P(B) + P(D∣C)P(C)\\n=\\n0.015 × 0.35\\n0.015 × 0.35 + 0.010 × 0.35 + 0.020 × 0.30\\n= 0.356.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 78}, page_content='CHAPTER 6. BAYESIAN CLASSIFIER AND ML ESTIMATION\\n64\\n6.3\\nNaive Bayes algorithm\\n6.3.1\\nAssumption\\nThe naive Bayes algorithm is based on the following assumptions:\\n• All the features are independent and are unrelated to each other. Presence or absence of a\\nfeature does not inﬂuence the presence or absence of any other feature.\\n• The data has class-conditional independence, which means that events are independent so\\nlong as they are conditioned on the same class value.\\nThese assumptions are, in general, true in many real world problems. It is because of these assump-\\ntions, the algorithm is called a naive algorithm.\\n6.3.2\\nBasic idea\\nSuppose we have a training data set consisting of N examples having n features. Let the features\\nbe named as (F1,...,Fn). A feature vector is of the form (f1,f2,...,fn). Associated with each\\nexample, there is a certain class label. Let the set of class labels be {c1,c2,...,cp}.\\nSuppose we are given a test instance having the feature vector\\nX = (x1,x2,...,xn).\\nWe are required to determine the most appropriate class label that should be assigned to the test\\ninstance. For this purpose we compute the following conditional probabilities\\nP(c1∣X),P(c2∣X),...,P(cp∣X).\\n(6.5)\\nand choose the maximum among them. Let the maximum probability be P(ci∣X). Then, we choose\\nci as the most appropriate class label for the training instance having X as the feature vector.\\nThe direct computation of the probabilities given in Eq.(6.5) are difﬁcult for a number of reasons.\\nThe Bayes’ theorem can b applied to obtain a simpler method. This is explained below.\\n6.3.3\\nComputation of probabilities\\nUsing Bayes’ theorem, we have:\\nP(ck∣X) = P(X∣ck)P(ck)\\nP(X)\\n(6.6)\\nSince, by assumption, the data has class-conditional independence, we note that the events “x1∣ck”,\\n“x2∣ck”, ⋯, xn∣ck are independent (because they are all conditioned on the same class label ck).\\nHence we have\\nP(X∣ck) = P((x1,x2,...,xn)∣ck)\\n= P(x1∣ck)P(x2∣ck)⋯P(xn∣ck)\\nUsing this in Eq,(6.6) we get\\nP(ck∣X) = P(x1∣ck)P(x2∣ck)⋯P(xn∣ck)P(ck)\\nP(X)\\n.\\nSince the denominator P(X) is independent of the class labels, we have\\nP(ck∣X) ∝P(x1∣ck)P(x2∣ck)⋯P(xn∣ck)P(ck).\\nSo it is enough to ﬁnd the maximum among the following values:\\nP(x1∣ck)P(x2∣ck)⋯P(xn∣ck)P(ck),\\nk = 1,...,p.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 79}, page_content='CHAPTER 6. BAYESIAN CLASSIFIER AND ML ESTIMATION\\n65\\nRemarks\\nThe various probabilities in the above expression are computed as follows:\\nP(ck) = No. of examples with class label ck\\nTotal number of examples\\nP(xj∣ck) = No. of examples with jth feature equal to xj and class label ck\\nNo. of examples with class label ck\\n6.3.4\\nThe algorithm\\nAlgorithm: Naive Bayes\\nLet there be a training data set having n features F1,...,Fn. Let f1 denote an arbitrary value of F1,\\nf2 of F2, and so on. Let the set of class labels be {c1,c2,...,cp}. Let there be given a test instance\\nhaving the feature vector\\nX = (x1,x2,...,xn).\\nWe are required to determine the most appropriate class label that should be assigned to the test\\ninstance.\\nStep 1.\\nCompute the probabilities P(ck) for k = 1,...,p.\\nStep 2.\\nForm a table showing the conditional probabilities\\nP(f1∣ck),\\nP(f2∣ck),\\n...\\n,P(fn∣ck)\\nfor all values of f1,f2,...,fn and for k = 1,...,p.\\nStep 3.\\nCompute the products\\nqk = P(x1∣ck)P(x2∣ck)⋯P(xn∣ck)P(ck)\\nfor k = 1,...,p.\\nStep 4.\\nFind j such qj = max{q1,q2,...,qp}.\\nStep 5.\\nAssign the class label cj to the test instance X.\\nRemarks\\nIn the above algorithm, Steps 1 and 2 constitute the learning phase of the algorithm. The remaining\\nsteps constitute the testing phase. For testing purposes, only the table of probabilities is required;\\nthe original data set is not required.\\n6.3.5\\nExample\\nProblem\\nConsider a training data set consisting of the fauna of the world. Each unit has three features named\\n“Swim”, “Fly” and “Crawl”. Let the possible values of these features be as follows:\\nSwim\\nFast, Slow, No\\nFly\\nLong, Short, Rarely, No\\nCrawl\\nYes, No\\nFor simplicity, each unit is classiﬁed as “Animal”, “Bird” or “Fish”. Let the training data set be as in\\nTable 6.1. Use naive Bayes algorithm to classify a particular species if its features are (Slow, Rarely,\\nNo)?'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 80}, page_content='CHAPTER 6. BAYESIAN CLASSIFIER AND ML ESTIMATION\\n66\\nSl. No.\\nSwim\\nFly\\nCrawl\\nClass\\n1\\nFast\\nNo\\nNo\\nFish\\n2\\nFast\\nNo\\nYes\\nAnimal\\n3\\nSlow\\nNo\\nNo\\nAnimal\\n4\\nFast\\nNo\\nNo\\nAnimal\\n5\\nNo\\nShort\\nNo\\nBird\\n6\\nNo\\nShort\\nNo\\nBird\\n7\\nNo\\nRarely\\nNo\\nAnimal\\n8\\nSlow\\nNo\\nYes\\nAnimal\\n9\\nSlow\\nNo\\nNo\\nFish\\n10\\nSlow\\nNo\\nYes\\nFish\\n11\\nNo\\nLong\\nNo\\nBird\\n12\\nFast\\nNo\\nNo\\nBird\\nTable 6.1: Sample data set for naive Bayes algorithm\\nSolution\\nIn this example, the features are\\nF1 = “Swim”,\\nF2 = “Fly”,\\nF3 = “Crawl”.\\nThe class labels are\\nc1 = “Animal”,\\nc2 = “ Bird”,\\nc3 = “Fish”.\\nThe test instance is (Slow, Rarely, No) and so we have:\\nx1 = “Slow”,\\nx2 = “Rarely”,\\nx3 = “No”.\\nWe construct the frequency table shown in Table 6.2 which summarises the data. (It may be noted\\nthat the construction of the frequency table is not part of the algorithm.)\\nClass\\nFeatures\\nTotal\\nSwim (F1)\\nFly (F2)\\nCrawl (F3)\\nFast\\nSlow\\nNo\\nLong\\nShort\\nRarely\\nNo\\nYes\\nNo\\nAnimal (c1)\\n2\\n2\\n1\\n0\\n0\\n1\\n4\\n2\\n3\\n5\\nBird (c2)\\n1\\n0\\n3\\n1\\n2\\n0\\n1\\n1\\n3\\n4\\nFish (c3)\\n1\\n2\\n0\\n0\\n0\\n0\\n3\\n0\\n3\\n3\\nTotal\\n4\\n4\\n4\\n1\\n2\\n1\\n8\\n4\\n8\\n12\\nTable 6.2: Frequency table for the data in Table 6.1\\nStep 1.\\nWe compute following probabilities.\\nP(c1) = No. of records with class label “Animal”\\nTotal number of examples\\n= 5/12\\nP(c2) = No. of records with class label “Bird”\\nTotal number of examples\\n= 4/12\\nP(c3) = No of records with class label “Fish”\\nTotal number of examples\\n= 3/12'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 81}, page_content='CHAPTER 6. BAYESIAN CLASSIFIER AND ML ESTIMATION\\n67\\nStep 2.\\nWe construct the following table of conditional probabilities:\\nClass\\nFeatures\\nSwim (F1)\\nFly (F2)\\nCrawl (F3)\\nf1\\nf2\\nf3\\nFast\\nSlow\\nNo\\nLong\\nShort\\nRarely\\nNo\\nYes\\nNo\\nAnimal (c1)\\n2/5\\n2/5\\n1/5\\n0/5\\n0/5\\n1/5\\n4/5\\n2/5\\n3/5\\nBird (c2)\\n1/4\\n0/4\\n3/4\\n1/4\\n2/4\\n0/4\\n1/4\\n0/4\\n4/4\\nFish (c3)\\n1 3\\n2/3\\n0/3\\n0/3\\n0/3\\n0/3\\n3/3\\n0/3\\n3/3\\nTable 6.3: Table of the conditional probabilities P(fi∣ck)\\nNote: The conditional probabilities are calculated as follows:\\nP((F1 = Slow)∣c1) = No. of records with F1 = Slow and class label c1\\nNo. of records with class label c1\\n= 2/5.\\nStep 3.\\nWe now calculate the following numbers:\\nq1 = P(x1∣c1)P(x2∣c1)P(x3∣c1)P(c1)\\n= (2/5) × (1/5) × (3/5) × (5/12)\\n= 0.02\\nq2 = P(x1∣c2)P(x2∣c2)P(x3∣c2)P(c2)\\n= (0/4) × (0/4) × (3/4) × (4/12)\\n= 0\\nq3 = P(x1∣c3)P(x2∣c3)P(x3∣c3)P(c3)\\n= (2/3) × (0/3) × (3/3) × (3/12)\\n= 0\\nStep 4.\\nNow\\nmax{q1,q2,q3} = 0.05.\\nStep 5.\\nThe maximum is q1 an it corresponds to the class label\\nc1 = “ Animal”.\\nSo we assign the class label “Animal” to the test instance “(Slow, Rarely, No)”.\\n6.4\\nUsing numeric features with naive Bayes algorithm\\nThe naive Bayes algorithm can be applied to a data set only if the features are categorical. This is\\nso because, the various probabilities are computed using the various frequencies and the frequencies\\ncan be counted only if each feature has a limited set of values.\\nIf a feature is numeric, it has to be discretized before applying the algorithm. The discretization\\nis effected by putting the numeric values into categories known as bins. Because of this discretization\\nis also known as binning. This is ideal when there are large amounts of data.\\nThere are several different ways to discretize a numeric feature.\\n1. If there are natural categories or cut points in the distribution of values, use these cut points to\\ncreate the bins. For example, let the data consists of records of times when certain activities\\nwere carried out. The the categories, or bins, may be created as in Figure 6.3.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 82}, page_content='CHAPTER 6. BAYESIAN CLASSIFIER AND ML ESTIMATION\\n68\\nFigure 6.3: Discretization of numeric data: Example\\n2. If there are no obvious cut points, we may discretize the feature using quantiles. We may\\ndivide the data into three bins with tertiles, four bins with quartiles, or ﬁve bins with quintiles,\\netc.\\n6.5\\nMaximum likelihood estimation (ML estimation)\\nTo develop a Bayesian classiﬁer, we need the probabilities P(x∣ck) for the class labels c1,...,ck.\\nThese probabilities are estimated from the given data. There is need to know whether the sample\\nis truly random so that the computed probabilities are good approximations to true probabilities. If\\nthey are good approximations of true probabilities, then there would be an underlying probability\\ndistribution. Suppose we have reasons to believe that the underlying distribution has a particular\\nform, say binomial, Poisson or normal. These forms are deﬁned by probability functions or proba-\\nbility density functions. There are parameters which deﬁne these functions, and these parameters are\\nto be estimated to test whether a given data follow some particular distribution. Maximum likelihood\\nestimation is particular method to estimate the parameters of a probability distribution.\\nDeﬁnition\\nMaximum likelihood estimation (MLE) is a method of estimating the parameters of a statistical\\nmodel, given observations. MLE attempts to ﬁnd the parameter values that maximize the likelihood\\nfunction, given the observations. The resulting estimate is called a maximum likelihood estimate,\\nwhich is also abbreviated as MLE.\\n6.5.1\\nThe general MLE method\\nSuppose we have a random sample X = {x1,...,xn} taken from a probability distribution having\\nthe probability mass function or probability density function p(x∣θ) where x denotes a value of the\\nrandom variable and θ denotes the set of parameters that appear in the function.\\nThe likelihood of sample X is a function of the parameter θ and is deﬁned as\\nl(θ) = p(x1∣θ)p(x2∣θ)...p(xn∣θ).\\nIn maximum likelihood estimation, we ﬁnd the value of θ that makes the value of the likelihood\\nfunction maximum. For computation convenience, we deﬁne the log likelihood function as the\\nlogarithm of the likelihood function:\\nL(θ) = log l(θ)\\n= log p(x1∣θ) + log p(x2∣θ) + ⋯+ log p(xn∣θ).'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 83}, page_content='CHAPTER 6. BAYESIAN CLASSIFIER AND ML ESTIMATION\\n69\\nA value of θ that maximizes L(θ) will also maximise l(θ) and vice-versa. Hence, in maximum like-\\nlihood estimation, we ﬁnd θ that maximizes the log likelihood function. Sometimes the maximum\\nlikelihood estimate of θ is denoted by ˆθ.\\n6.5.2\\nSpecial cases\\n1. Bernoulli density\\nIn a Bernoulli distribution there are two outcomes: An event occurs or it does not, for example, an\\ninstance is a positive example of the class, or it is not. The event occurs and the Bernoulli random\\nvariable X takes the value 1 with probability p, and the nonoccurrence of the event has probability\\n1 −p and this is denoted by X taking the value 0.\\nThe probability function of X is given by\\nf(x∣p) = px(1 −p)1−x,\\nx = 0,1.\\nIn this function, the probability p is the only parameter.\\nEstimation of p\\nConsider a random sample X = {x1,...,xn} taken from a Bernoulli distribution with the probability\\nfunction f(x∣p). The log likelihood function is\\nL(p) = log f(x1∣p) + ⋯+ log f(xn∣p)\\n= log px1(1 −p)1−x1 + ⋯+ log pxn(1 −p)1−xn\\n= [x1 log p + (1 −x1)log(1 −p)] + ⋯+ [xn log p + (1 −xn)log(1 −p)]\\nTo ﬁnd the value of p that maximizes L(p) we set up the equation\\ndL\\ndp = 0,\\nthat is,\\n[x1\\np −1 −x1\\n1 −p ] + ⋯+ [xn\\np −1 −xn\\n1 −p ] = 0.\\nSolving this equation, we have the maximum likelihood estimate of p as\\nˆp = 1\\nn(x1 + ⋯+ xn).\\n2. Multinomial density\\nSuppose that the outcome of a random event is one of K classes, each of which has a probability of\\noccurring pi with\\np1 + ⋯+ pK = 1.\\nWe represent each outcome by an ordered K-tuple x = (x1,...,xK) where exactly one of x1,...,xK\\nis 1 and all others are 0. xi = 1 if the outcome in the i-th class occurs. The probability function can\\nbe expressed as\\nf(x∣p, ...,pK) = px1\\n1 ...pxK\\nK .\\nHere, p1,...,pK are the parameters.\\nWe choose n random samples. The i-the sample may be represented by\\nxi = (x1i,...,xKi).\\nThe values of the parameters that maximizes the likelihood function can be shown to be\\nˆpk = 1\\nn(xk1 + xk2 + ⋯+ xkn).\\n(We leave the details of the derivation as an exercise.)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 84}, page_content='CHAPTER 6. BAYESIAN CLASSIFIER AND ML ESTIMATION\\n70\\n3. Gaussian (normal) density\\nA continuous random variable X has the Gaussian or normal distribution if its density function is\\nf(x∣µ,σ) =\\n1\\nσ\\n√\\n2π\\nexp(−(x −µ)2\\n2σ2\\n),\\n−∞< x < ∞.\\nHere µ and σ are the parameters.\\nGiven a sample x1,x2,...,xn from the distribution. the log likelihood function is\\nL(µ,σ) = −n\\n2 log(2π) −nlog σ −\\n1\\n2σ2 [(x1 −µ)2 + ⋯+ (xn −µ)2].\\nSetting up the equations\\ndL\\ndµ = 0,\\ndL\\ndσ = 0\\nand solving for µ and σ we get the maximum likelihood estimates of µ and σ as\\nˆµ = 1\\nn(x1 + ⋯+ xn)\\nˆσ2 = 1\\nn((x1 −ˆµ)2 + ⋯+ (xn −ˆµ)2)\\n(We leave the details of the derivation as an exercise.)\\n6.6\\nSample questions\\n(a) Short answer questions\\n1. What are the assumptions under the naive Bayes algorithm?\\n2. Why is naive Bayes algorithm “naive”?\\n3. Given an instance X of a feature vector and a class label ck, explain how Bayes theorem is\\nused to compute the probability P(ck ∣X).\\n4. What does a naive Bayes classiﬁer do?\\n5. What is naive Bayes used for?\\n6. Is naive Bayes supervised or unsupervised? Why?\\n7. What is meant by the likelihood of a random sample taken from population?\\n8. How do we use numeric features in naive Bayes algorithm?\\n(b) Long answer questions\\n1. State Bayes theorem and illustrate it with an example.\\n2. Explain naive Bayes algorithm.\\n3. Use naive Bayes algorithm to determine whether a red domestic SUV car is a stolen car or not\\nusing the following data:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 85}, page_content='CHAPTER 6. BAYESIAN CLASSIFIER AND ML ESTIMATION\\n71\\nExample no.\\nColour\\nType\\nOrigin\\nWhether stolen\\n1\\nred\\nsports\\ndomestic\\nyes\\n2\\nred\\nsports\\ndomestic\\nno\\n3\\nred\\nsports\\ndomestic\\nyes\\n4\\nyellow\\nsports\\ndomestic\\nno\\n5\\nyellow\\nsports\\nimported\\nyes\\n6\\nyellow\\nSUV\\nimported\\nno\\n7\\nyellow\\nSUV\\nimported\\nyes\\n8\\nyellow\\nSUV\\ndomestic\\nno\\n9\\nred\\nSUV\\nimported\\nno\\n10\\nred\\nsports\\nimported\\nyes\\n4. Based on the following data determine the gender of a person having height 6 ft., weight 130\\nlbs. and foot size 8 in. (use naive Bayes algorithm).\\nperson\\nheight (feet)\\nweight (lbs)\\nfoot size (inches)\\nmale\\n6.00\\n180\\n10\\nmale\\n6.00\\n180\\n10\\nmale\\n5.50\\n170\\n8\\nmale\\n6.00\\n170\\n10\\nfemale\\n5.00\\n130\\n8\\nfemale\\n5.50\\n150\\n6\\nfemale\\n5.00\\n130\\n6\\nfemale\\n6.00\\n150\\n8\\n5. Given the following data on a certain set of patients seen by a doctor, can the doctor conclude\\nthat a person having chills, fever, mild headache and without running nose has the ﬂu?\\nchills\\nrunning nose\\nheadache\\nfever\\nhas ﬂu\\nY\\nN\\nmild\\nY\\nN\\nY\\nY\\nno\\nN\\nY\\nY\\nN\\nstrong\\nY\\nY\\nN\\nY\\nmild\\nY\\nY\\nN\\nN\\nno\\nN\\nN\\nN\\nY\\nstrong\\nY\\nY\\nN\\nY\\nstrong\\nN\\nN\\nY\\nY\\nmild\\nY\\nY\\n6. Explain the general MLE method for estimating the parameters of a probability distribution.\\n7. Find the ML estimate for the parameter p in the binomial distribution whose probability func-\\ntion is\\nf(x) = (n\\nx)px(1 −p)n−x,\\nx = 0,1,2,...,n\\n8. Compute the ML estimate for the parameter λ in the Poisson distribution whose probability\\nfunction is\\nf(x) = e−λ λx\\nx! ,\\nx = 0,1,2,...\\nFind the ML estimate of the parameter p in the geometric distribution deﬁned by the proba-\\nbility mass function\\nf(x) = (1 −p)px,\\nx = 1,2,3,...'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 86}, page_content='Chapter 7\\nRegression\\nWe have seen in Section 1.5.3 that regression is a supervised learning problem where there is an\\ninput x an output y and the task is to learn the mapping from the input to the output. We have also\\nseen that the approach in machine learning is that we assume a model, that is, a relation between x\\nand y containing a set of parameters, say, θ in the following form:\\ny = g(x,θ).\\ng(x,θ) is the regression function. The machine learning program optimizes the parameters θ such\\nthat the approximation error is minimized, that is, our estimates are as close as possible to the correct\\nvalues given in the training set. In this chapter we discuss a method, known as ordinary least squares\\nmethod, to estimate the parameters. In fact this method can be derived from the maximum likelihood\\nestimation method discussed in Section 6.5.\\n7.1\\nDeﬁnition\\nA regression problem is the problem of determining a relation between one or more independent\\nvariables and an output variable which is a real continuous variable, given a set of observed values\\nof the set of independent variables and the corresponding values of the output variable.\\n7.1.1\\nExamples\\n1. Let us say we want to have a system that can predict the price of a used car. Inputs are the\\ncar attributes â ˘AˇT brand, year, engine capacity, mileage, and other information â ˘AˇT that we\\nbelieve affect a car’s worth. The output is the price of the car.\\n2. Consider the navigation of a mobile robot, say an autonomous car. The output is the angle by\\nwhich the steering wheel should be turned at each time, to advance without hitting obstacles\\nand deviating from the route. Inputs are provided by sensors on the car like a video camera,\\nGPS, and so forth.\\n3. In ﬁnance, the capital asset pricing model uses regression for analyzing and quantifying the\\nsystematic risk of an investment.\\n4. In economics, regression is the predominant empirical tool. For example, it is used to predict\\nconsumption spending, inventory investment, purchases of a country’s exports, spending on\\nimports, labor demand, and labor supply.\\n7.1.2\\nDifferent regression models\\nThe different regression models are deﬁned based on type of functions used to represent the relation\\nbetween the dependent variable y and the independent variables.\\n72'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 87}, page_content='CHAPTER 7. REGRESSION\\n73\\n1. Simple linear regression\\nAssume that there is only one independent variable x. If the relation between x and y is\\nmodeled by the relation\\ny = a + bx\\nthen we have a simple linear regression.\\n2. Multiple regression\\nLet there be more than one independent variable, say x1, x2, ..., xn, and let the relation\\nbetween y and the independent variables be modeled as\\ny = α0 + α1x1 + ⋯+ αnxn\\nthen it is case of multiple linear regression or multiple regression.\\n3. Polynomial regression\\nLet there be only one variable x and let the relation between x y be modeled as\\ny = a0 + a1x + a2x2 + ⋯+ anxn\\nfor some positive integer n > 1, then we have a polynomial regression.\\n4. Logistic regression\\nLogistic regression is used when the dependent variable is binary (0/1, True/False, Yes/No)\\nin nature. Even though the output is a binary variable, what is being sought is a probability\\nfunction which may take any value from 0 to 1.\\n7.2\\nCriterion for minimisation of error\\nIn regression, we would like to write the numeric output y, called the dependent variable, as a\\nfunction of the input x, called the independent variable. We assume that the output is the sum of a\\nfunction f(x) of the input and some random error denoted by ϵ:\\ny = f(x) + ϵ.\\nHere the function f(x) is unknown and we would like to approximate it by some estimator g(x,θ)\\ncontaining a set of parameters θ. We assume that the random error ϵ follows normal distribution\\nwith mean 0.\\nLet x1,...,xn be a random sample of observations of the input variable x and y1,...,yn the\\ncorresponding observed values of the output variable y.\\nUsing the assumption that the error ϵ follows normal distribution, we can apply the method of\\nmaximum likelihood estimation to estimate the values of the parameter θ. It can be shown that the\\nvalues of θ which maximizes the likelihood function are the values of θ that minimizes the following\\nsum of squares:\\nE(θ) = (y1 −g(x1,θ))2 + ⋯+ (yn −g(xn,θ))2.\\nThe method of ﬁnding the value of θ as that value of θ that minimizes E(θ) is known as the ordinary\\nleast squares method.\\nThe full details of the derivation of the above result are beyond the scope of these notes.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 88}, page_content='CHAPTER 7. REGRESSION\\n74\\nx\\nx1\\nx2\\n⋯\\nxn\\ny\\ny1\\ny2\\n⋯\\nyn\\nTable 7.1: Data set for simple linear regression\\nx\\ny\\nActual value\\nPredicted value\\nError\\nRegression model\\nFigure 7.1: Errors in observed values\\n7.3\\nSimple linear regression\\nLet x be the independent predictor variable and y the dependent variable. Assume that we have a set\\nof observed values of x and y:\\nA simple linear regression model deﬁnes the relationship between x and y using a line deﬁned\\nby an equation in the following form:\\ny = α + βx\\nIn order to determine the optimal estimates of α and β, an estimation method known as Ordinary\\nLeast Squares (OLS) is used.\\nThe OLS method\\nIn the OLS method, the values of y-intercept and slope are chosen such that they minimize the sum\\nof the squared errors; that is, the sum of the squares of the vertical distance between the predicted\\ny-value and the actual y-value (see Figure 7.1). Let ˆyi be the predicted value of yi. Then the sum of\\nsquares of errors is given by\\nE =\\nn\\n∑\\ni=1\\n(yi −ˆyi)2\\n=\\nn\\n∑\\ni=1\\n[yi −(α + βxi)]2\\nSo we are required to ﬁnd the values of α and β such that E is minimum. Using methods of calculus,\\nwe can show that the values of a and b, which are respectively the values of α and β for which E is\\nminimum, can be obtained by solving the following equations.\\nn\\n∑\\ni=1\\nyi = na + b\\nn\\n∑\\ni=1\\nxi'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 89}, page_content='CHAPTER 7. REGRESSION\\n75\\nn\\n∑\\ni=1\\nxiyi = a\\nn\\n∑\\ni=1\\nxi + b\\nn\\n∑\\ni=1\\nx2\\ni\\nFormulas to ﬁnd a and b\\nRecall that the means of x and y are given by\\n¯x = 1\\nn ∑xi\\n¯y = 1\\nn ∑yi\\nand also that the variance of x is given by\\nVar(x) =\\n1\\nn −1 ∑(xi −¯xi)2.\\nThe covariance of x and y, denoted by Cov(x,y) is deﬁned as\\nCov(x,y) =\\n1\\nn −1 ∑(xi −¯x)(yi −¯y)\\nIt can be shown that the values of a and b can be computed using the following formulas:\\nb = Cov(x,y)\\nVar(x)\\na = ¯y −b¯x\\nRemarks\\nIt is interesting to note why the least squares method discussed above is christened as “ordinary”\\nleast squares method. Several different variants of the least squares method have been developed\\nover the years. For example, in the weighted least squares method, the coefﬁcients a and b are\\nestimated such that the weighted sum of squares of errors\\nE =\\nn\\n∑\\ni=1\\nwi[yi −(a + bxi)]2,\\nfor some positive constants w1,...,wn, is minimum. There are also methods known by the names\\ngeneralised least squares method, partial least squares method, total least squares method, etc. The\\nreader may refer to Wikipedia, a free online encyclopedia, to obtain further information about these\\nmethods.\\nThe OLS method has a long history. The method is usually credited to Carl Friedrich Gauss\\n(1795), but it was ﬁrst published by Adrien-Marie Legendre (1805).\\nExample\\nObtain a linear regression for the data in Table 7.2 assuming that y is the independent variable.\\nx\\n1.0\\n2.0\\n3.0\\n4.0\\n5.0\\ny\\n1.00\\n2.00\\n1.30\\n3.75\\n2.25\\nTable 7.2: Example data for simple linear regression'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 90}, page_content='CHAPTER 7. REGRESSION\\n76\\nFigure 7.2: Regression model for Table 7.2\\nSolution\\nIn the usual notations of simple linear regression, we have\\nn = 5\\n¯x = 1\\n5(1.0 + 2.0 + 3.0 + 4.0 + 5.0)\\n= 3.0\\n¯y = 1\\n5(1.00 + 2.00 + 1.30 + 3.75 + 2.25)\\n= 2.06\\nCov(x,y) = 1\\n4[(1.0 −3.0)(1.00 −2.06) + ⋯+ (5.0 −3.0)(2.25 −2.06)]\\n= 1.0625\\nVar(x) = 1\\n4[(1.0 −3.0)2 + ⋯+ (5.0 −3.0)2]\\n= 2.5\\nb = 1.0625\\n2.5\\n= 0.425\\na = 2.06 −0.425 × 3.0\\n= 0.785\\nTherefore, the linear regression model for the data is\\ny = 0.785 + 0.425x.\\n(7.1)\\nRemark\\nFigure 7.2 in page 76 shows the data in Table 7.2 and the line given by Eq. (7.1). The ﬁgure was\\ncreated using R.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 91}, page_content='CHAPTER 7. REGRESSION\\n77\\n7.4\\nPolynomial regression\\nLet x be the independent predictor variable and y the dependent variable. Assume that we have a set\\nof observed values of x and y as in Table 7.1 in page 74.\\nA polynomial regression model deﬁnes the relationship between x and y by an equation in the\\nfollowing form:\\ny = α0 + α1x + α2x2 + ⋯+ αkxk.\\nTo determine the optimal values of the parameters α0, α1, ..., αk the method of ordinary least\\nsquares is used. The values of the parameters are those values which minimizes the sum of squares:\\nE =\\nn\\n∑\\ni=1\\n[yi −(α0 + α1xi + α2x2\\ni + ⋯+ αkxk\\ni )]2.\\nThe optimal values of the parameters are obtained by solving the following system of equations:\\n∂E\\n∂αi\\n= 0,\\ni = 0,1,...,k.\\n(7.2)\\nLet the values of values of the parameters which minimizes E be\\nαi = ai,\\ni = 0,1,2,...,n.\\n(7.3)\\nSimplifying Eq. (7.2) and using Eq. (7.3), we can see that the values of ai can be obtained by\\nsolving the the following system of (k + 1) linear equations:\\n∑yi = α0n + α1(∑xi) + ⋯+ αk(∑xk\\ni )\\n∑yixi = α0(∑xi) + α1(∑x2\\ni ) + ⋯+ αk(∑xk+1\\ni\\n)\\n∑yix2\\ni = α0(∑x2\\ni ) + α1(∑x3\\ni ) + ⋯+ αk(∑xk+2\\ni\\n)\\n⋮\\n∑yixk\\ni = α0(∑xk\\ni ) + α1(∑xk+1\\ni\\n) + ⋯+ αk(∑x2k\\ni )\\nSolving this system of linear equations we get the optimal values for the parameters.\\nRemarks\\nThe linear system of equations to ﬁnd ai’s, has a compact matrix representation. We write:\\nD =\\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣\\n1\\nx1\\nx2\\n1\\n⋯\\nxk\\n1\\n1\\nx2\\nx2\\n2\\n⋯\\nxk\\n2\\n⋮\\n1\\nxn\\nx2\\nn\\n⋯\\nxk\\nn\\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦\\n,\\n⃗y =\\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣\\ny1\\ny2\\n⋮\\nyn\\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦\\n,\\n⃗a =\\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣\\na0\\na1\\n⋮\\nak\\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦\\nThen we have\\n⃗a = (DT D)−1DT ⃗y,\\nwhere the superscript T denotes the transpose of the matrix.\\n7.4.1\\nExample\\nFind a quadratic regression model for the following data:\\nx\\n3\\n4\\n5\\n6\\n7\\ny\\n2.5\\n3.2\\n3.8\\n6.5\\n11.5'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 92}, page_content='CHAPTER 7. REGRESSION\\n78\\nFigure 7.3: Plot of quadratic polynomial model\\nSolution\\nLet the quadratic regression model be\\ny = α0 + α1x + α2x2.\\nThe values of α0, α1 and α2 which minimises the sum of squares of errors are a0, a1 and a2 which\\nsatisfy the following system of equations:\\n∑yi = na0 + a1(∑xi) + a2(∑x2\\ni )\\n∑yixi = a0(∑xi) + a1(∑x2\\ni ) + a2(∑x3\\ni )\\n∑yix2\\ni = a0(∑x2\\ni ) + a1(∑x3\\ni ) + a2(∑x4\\ni )\\nUsing the given data we have\\n27.5 = 5a0 + 25a1 + 135a2\\n158.8 = 25a0 + 135a1 + 775a2\\n966.2 = 135a0 + 775a1 + 4659a2\\nSolving this system of equations we get\\na0 =\\n12.4285714\\na1 = −5.5128571\\na2 =\\n0.7642857\\nThe required quadratic polynomial model is\\ny = 12.4285714 −5.5128571x + 0.7642857x2.\\nFigure 7.3 shows plots of the data and the quadratic polynomial model.\\n7.5\\nMultiple linear regression\\nWe assume that there are N independent variables x1, x2, ⋯, xN. Let the dependent variable be y.\\nLet there also be n observed values of these variables:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 93}, page_content='CHAPTER 7. REGRESSION\\n79\\nVariables\\nValues (examples)\\n(features)\\nExample 1\\nExample 2\\n⋯\\nExample n\\nx1\\nx11\\nx12\\n⋯\\nx1n\\nx2\\nx21\\nx22\\n⋯\\nx2n\\n⋯\\nxN\\nxN1\\nxN2\\n⋯\\nxNn\\ny (outcomes)\\ny1\\ny2\\n⋯\\nyn\\nTable 7.3: Data for multiple linear regression\\nThe multiple linear regression model deﬁnes the relationship between the N independent vari-\\nables and the dependent variable by an equation of the following form:\\ny = β0 + β1x1 + ⋯+ βNxN\\nAs in simple linear regression, here also we use the ordinary least squares method to obtain the\\noptimal estimates of β0, β1, ⋯, βN. The method yields the following procedure for the computation\\nof these optimal estimates. Let\\nX =\\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣\\n1\\nx11\\nx21\\n⋯\\nxN1\\n1\\nx12\\nx22\\n⋯\\nxN2\\n⋮\\n1\\nx1n\\nx2n\\n⋯\\nxNn\\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦\\n,\\nY =\\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣\\ny1\\ny2\\n⋮\\nyn\\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦\\n,\\nB =\\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣\\nβ0\\nβ1\\n⋮\\nβN\\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦\\nThen it can be shown that the regression coefﬁcients are given by\\nB = (XT X)−1XT Y\\n7.5.1\\nExample\\nExample\\nFit a multiple linear regression model to the following data:\\nx1\\n1\\n1\\n2\\n0\\nx2\\n1\\n2\\n2\\n1\\ny\\n3.25\\n6.5\\n3.5\\n5.0\\nTable 7.4: Example data for multi-linear regression\\nSolution\\nIn this problem, there are two independent variables andfour sets of values of the variables. Thus,\\nin the notations used above, we have n = 2 and N = 4. The multiple linear regression model for this\\nproblem has the form\\ny = β0 + β1x1 + β2x2.\\nThe computations are shown below.\\nX =\\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣\\n1\\n1\\n1\\n1\\n1\\n2\\n1\\n2\\n2\\n1\\n0\\n1\\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦\\n,\\nY =\\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣\\n3.25\\n6.5\\n3.5\\n5.0\\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦\\n,\\nB =\\n⎡⎢⎢⎢⎢⎢⎣\\nβ0\\nβ1\\nβ2\\n⎤⎥⎥⎥⎥⎥⎦'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 94}, page_content='CHAPTER 7. REGRESSION\\n80\\nXT X =\\n⎡⎢⎢⎢⎢⎢⎣\\n4\\n4\\n6\\n4\\n6\\n7\\n6\\n7\\n10\\n⎤⎥⎥⎥⎥⎥⎦\\n(XT X)−1 =\\n⎡⎢⎢⎢⎢⎢⎣\\n11\\n4\\n1\\n2\\n−2\\n1\\n2\\n1\\n−1\\n−2\\n−1\\n2\\n⎤⎥⎥⎥⎥⎥⎦\\nB = (XT X)−1XT Y\\n=\\n⎡⎢⎢⎢⎢⎢⎣\\n2.0625\\n−2.3750\\n3.2500\\n⎤⎥⎥⎥⎥⎥⎦\\nThe required model is\\ny = 2.0625 −2.3750x1 + 3.2500x2.\\nx1\\nx2\\ny\\n(1,1,3.25)\\n(1,2,6.25)\\n(2,2,3.25)\\n(0,1,5.0)\\ny = 2.0625 −2.3750x1 + 3.2500x2\\nFigure 7.4: The regression plane for the data in Table 7.4\\n7.6\\nSample questions\\n(a) Short answer questions\\n1. What are the different types of regression.\\n2. Is regression a supervised learning? Why?\\n3. Explain the ordinary least squares method for regression.\\n4. What are linear, multinomial and polynomial regressions.\\n5. If model used for regression is\\ny = a + b(x −1)2,\\nis it a multinomial regression? If not, what type of regression is it?\\n6. What does the line of regression tell you?'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 95}, page_content='CHAPTER 7. REGRESSION\\n81\\n(b) Long answer questions\\n1. Discuss linear regression with an example.\\n2. In the table below, the xi row shows scores in an aptitude test. Similarly, the yi row shows\\nstatistics grades. If a student made an 80 on the aptitude test, what grade would we expect her\\nto make in statistics?\\nStudent i\\n1\\n2\\n3\\n4\\n5\\nxi\\n95\\n85\\n80\\n70\\n60\\nyi\\n85\\n95\\n70\\n65\\n70\\n3. Use the following data to construct a linear regression model for the auto insurance premium\\nas a function of driving experience.\\nDriving experience (in years)\\n5\\n2\\n12\\n9\\n15\\n6\\n25\\n16\\nMonthly auto insurance premium ($)\\n64\\n87\\n50\\n71\\n44\\n56\\n42\\n60\\n4. Determine the regression equation by ﬁnding the regression slope coefﬁcient and the intercept\\nvalue using the following data.\\nx\\n55\\n60\\n65\\n70\\n80\\ny\\n52\\n54\\n56\\n58\\n62\\n5. The following table contains measurements of yield from an experiment done at ﬁve different\\ntemperature levels. The variables are y = yield and x = temperature in degrees Fahrenheit.\\nCompute a second degree polynomial regression model to predict the yield given the temper-\\nature.\\nTemperature (x)\\nYield (y)\\n50\\n3.0\\n70\\n2.7\\n80\\n2.6\\n90\\n2.9\\n100\\n3.3\\n6. An experiment was done to assess how moisture content and sweetness of a pastry product\\naffect a tasterâ ˘A´Zs rating of the product. The following table summarises the ﬁndings.\\nRating\\nMoisture\\nSweetness\\n64\\n4\\n2\\n73\\n4\\n4\\n61\\n4\\n2\\n76\\n4\\n4\\n72\\n6\\n2\\n80\\n6\\n4\\n71\\n6\\n2\\n83\\n6\\n4\\n83\\n8\\n2\\n89\\n8\\n4\\n86\\n8\\n2\\n93\\n8\\n4\\n88\\n10\\n2\\n95\\n10\\n4\\n94\\n10\\n2\\n100\\n10\\n4'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 96}, page_content='CHAPTER 7. REGRESSION\\n82\\nCompute a linear regression model to predict the rating of the pastry product.\\n7. The following data contains the Performance IQ scores (PIQ) (in appropriate scales), brain\\nsizes (in standard units), heights (in inches) and weights (in pounds) of 15 American college\\nstudents. Obtain a linear regression model to predict the PIQ given the values of the other\\nfeatures.\\nPIQ\\nBrain\\nHeight\\nWeight\\n124\\n81.69\\n64.5\\n118\\n150\\n103.84\\n73.3\\n143\\n128\\n96.54\\n68.8\\n172\\n134\\n95.15\\n65.0\\n147\\n110\\n92.88\\n69.0\\n146\\n131\\n99.13\\n64.5\\n138\\n98\\n85.43\\n66.0\\n175\\n84\\n90.49\\n66.3\\n134\\n147\\n95.55\\n68.8\\n172\\n124\\n83.39\\n64.5\\n118\\n128\\n107.95\\n70.0\\n151\\n124\\n92.41\\n69.0\\n155\\n147\\n85.65\\n70.5\\n155\\n90\\n87.89\\n66.0\\n146\\n96\\n86.54\\n68.0\\n135\\n8. Use the following data to generate a linear regression model for annual salary as function of\\nGPA and number of months worked.\\nExample no.\\nAnnual salary ($)\\nGPA\\nMonths worked\\n1\\n20000\\n2.8\\n48\\n2\\n24500\\n3.4\\n24\\n3\\n23000\\n3.2\\n24\\n4\\n25000\\n3.8\\n24\\n5\\n20000\\n3.2\\n48\\n6\\n22500\\n3.4\\n36\\n7\\n27500\\n4.0\\n24\\n8\\n19000\\n2.6\\n48\\n9\\n24000\\n3.2\\n36\\n10\\n28500\\n3.8\\n12'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 97}, page_content='Chapter 8\\nDecision trees\\n“Decision tree learning is a method for approximating discrete valued target functions, in which the\\nlearned function is represented by a decision tree. Decision tree learning is one of the most widely\\nused and practical methods for inductive inference.” ([4] p.52)\\n8.1\\nDecision tree: Example\\nConsider the following situation. Somebody is hunting for a job. At the very beginning, he decides\\nthat he will consider only those jobs for which the monthly salary is at least Rs.50,000. Our job\\nhunter does not like spending much time traveling to place of work. He is comfortable only if the\\ncommuting time is less than one hour. Also, he expects the company to arrange for a free coffee\\nevery morning! The decisions to be made before deciding to accept or reject a job offer can be\\nschematically represented as in Figure 8.6. This ﬁgure represents a decision tree1.\\nRoot node\\nSalary ≥Rs.50000?\\nCommute one hour?\\nDecline offer\\nYes\\nOffers free coffee?\\nAccept offer\\nYes\\nDecline offer\\nNo\\nNo\\nYes\\nDecline offer\\nNo\\nFigure 8.1: Example for a decision tree\\nHere, the term “tree” refers to the concept of a tree in graph theory in mathematics2. In graph\\ntheory, a tree is deﬁned as an undirected graph in which any two vertices are connected by exactly\\none path. Using the conventions of graph theory, the decision tree shown in Figure 8.6 can be\\nrepresented as a graph-theoretical tree as in Figure 8.2. Since a decision tree is a graph-theoretical\\ntree, all terminology related to graph-theoretical trees can be applied to describe decision trees also.\\nFor example, in Figure 8.6, the nodes or vertices shown as ellipses are called the leaf nodes. All\\nother nodes, except the root node, are called the internal nodes.\\n1In such diagrams, the “tree” is shown upside down with the root node at the top and all the leaves at the bottom.\\n2The term “tree” was coined in 1857 by the British mathematician Arthur Cayley (see Wikipedia).\\n83'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 98}, page_content='CHAPTER 8. DECISION TREES\\n84\\nRoot node\\nYes\\nYes\\nNo\\nNo\\nYes\\nNo\\nFigure 8.2: The graph-theoretical representation of the decision tree in Figure 8.6\\n8.2\\nTwo types of decision trees\\nThere are two types of decision trees.\\n1. Classiﬁcation trees\\nTree models where the target variable can take a discrete set of values are called classiﬁcation\\ntrees. In these tree structures, leaves represent class labels and branches represent conjunc-\\ntions of features that lead to those class labels.\\n2. Regression trees\\nDecision trees where the target variable can take continuous values (real numbers) like the\\nprice of a house, or a patient’s length of stay in a hospital, are called regression trees.\\n8.3\\nClassiﬁcation trees\\nWe illustrate the concept with an example.\\n8.3.1\\nExample\\nData\\nNam\\nFeatures\\nClass label\\ngives birth\\naquatic\\nanimal\\naerial\\nanimal\\nhas legs\\nhuman\\nyes\\nno\\nno\\nyes\\nmammal\\npython\\nno\\nno\\nno\\nno\\nreptile\\nsalmon\\nno\\nyes\\nno\\nno\\nﬁsh\\nfrog\\nno\\nsemi\\nno\\nyes\\namphibian\\nbat\\nyes\\nno\\nyes\\nyes\\nbird\\npigeon\\nno\\nno\\nyes\\nyes\\nbird\\ncat\\nyes\\nno\\nno\\nyes\\nmammal\\nshark\\nyes\\nyes\\nno\\nno\\nﬁsh\\nturtle\\nno\\nsemi\\nno\\nyes\\namphibian\\nsalamander\\nno\\nsemi\\nno\\nyes\\namphibian\\nTable 8.1: The vertebrate data set'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 99}, page_content='CHAPTER 8. DECISION TREES\\n85\\nConsider the data given in Table 8.1 which specify the features of certain vertebrates and the class\\nto which they belong. For each species, four features have been identiﬁed: “gives birth”, ”aquatic\\nanimal”, “aerial animal” and “has legs”. There are ﬁve class labels, namely, “amphibian”, “bird”,\\n“ﬁsh”, “mammal” and “reptile”. The problem is how to use this data to identify the class of a newly\\ndiscovered vertebrate.\\nConstruction of the tree\\nStep 1\\nWe split the set of examples given in Table 8.1 into disjoint subsets according to the values of the\\nfeature “gives birth”. Since there are only two possible values for this feature, we have only two\\nsubsets: One subset consisting of those examples for which the value of “gives birth” is “yes” and\\none subset for which the value is “no”. The former is given in Table 8.2 and the latter in Table 8.3.\\nThis stage of the classiﬁcation can be represented as in Figure 8.3.\\nName\\nGives\\nbirth\\nAquatic\\nanimal\\nAerial\\nanimal\\nHas legs\\nClass la-\\nbel\\nhuman\\nyes\\nno\\nno\\nyes\\nmammal\\nbat\\nyes\\nno\\nyes\\nyes\\nbird\\ncat\\nyes\\nno\\nno\\nyes\\nmammal\\nshark\\nyes\\nyes\\nno\\nno\\nﬁsh\\nTable 8.2: The subset of Table 8.1 with “gives birth” = ”yes\"\\nName\\ngives birth\\naquatic\\nanimal\\naerial\\nanimal\\nhas legs\\nClass la-\\nbel\\npython\\nno\\nno\\nno\\nno\\nreptile\\nsalmon\\nno\\nyes\\nno\\nno\\nﬁsh\\nfrog\\nno\\nsemi\\nno\\nyes\\namphibian\\npigeon\\nno\\nno\\nyes\\nyes\\nbird\\nturtle\\nno\\nsemi\\nno\\nyes\\namphibian\\nsalamander\\nno\\nsemi\\nno\\nyes\\namphibian\\nTable 8.3: The subset of Table 8.1 with “gives birth” = ”no\"\\nRoot node\\nTable 8.1:\\ngives birth?\\nTable 8.2:\\naquatic?\\nYes\\nTable 8.3:\\naquatic?\\nNo\\nFigure 8.3: Classiﬁcation tree\\nStep 2\\nWe now consider the examples in Table 8.2. We split these examples based on the values of the\\nfeature “aquatic animal”. There are three possible values for this feature. However, only two of'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 100}, page_content='CHAPTER 8. DECISION TREES\\n86\\nName\\ngives birth\\naquatic\\nanimal\\naerial\\nanimal\\nhas legs\\nClass la-\\nbel\\nhuman\\nyes\\nno\\nno\\nyes\\nmammal\\nbat\\nyes\\nno\\nyes\\nyes\\nbird\\ncat\\nyes\\nno\\nno\\nyes\\nmammal\\nTable 8.5: The vertebrate data set\\nRoot node\\nTable 8.1:\\ngives birth?\\nTable 8.2:\\naquatic?\\nTable 8.4\\nﬁsh\\nyes\\nTable 8.5:\\naerial?\\nPart of\\nTable 8.5\\nbird\\nyes\\nPart of\\nTable 8.5\\nmammal\\nno\\nno\\nYes\\nTable 8.3:\\naquatic?\\nno\\nFigure 8.4: Classiﬁcation tree\\nthese appear in Table 8.2. Accordingly, we need consider only two subsets. These are shown in\\nTables 8.4 and 8.5.\\nName\\ngives birth\\naquatic\\nanimal\\naerial\\nanimal\\nhas legs\\nClass la-\\nbel\\nshark\\nyes\\nyes\\nno\\nno\\nﬁsh\\nTable 8.4: The vertebrate data set\\n• Table 8.4 contains only one example and hence no further splitting is required. It leads to the\\nassignment of the class label “ﬁsh”.\\n• The examples in Table 8.5 need to be split into subsets based on the values of “aerial animal”.\\nIt can be seen that these subsets immediately lead to unambiguous assignment of class labels:\\nThe value of “no” leads to “mammal” and the value “yes” leads to ”bird”.\\nAt this stage, the classiﬁcation tree is as shown in Figure 8.4'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 101}, page_content='CHAPTER 8. DECISION TREES\\n87\\nStep 3\\nNext we consider the examples in Table 8.3 and split them into disjoint subsets based on the values\\nof “aquatic animal”. We get the examples in Table 8.6 for “yes”, the examples in Table ?? for “no”\\nand the examples in Table ?? for “semi”. We now split the resulting subsets based on the values of\\nName\\ngives birth\\naquatic\\nanimal\\naerial\\nanimal\\nhas legs\\nClass la-\\nbel\\nsalmon\\nno\\nyes\\nno\\nno\\nﬁsh\\nTable 8.6: The vertebrate data set\\nName\\ngives birth\\naquatic\\nanimal\\naerial\\nanimal\\nhas legs\\nClass la-\\nbel\\nfrog\\nno\\nsemi\\nno\\nyes\\namphibian\\nturtle\\nno\\nsemi\\nno\\nyes\\namphibian\\nsalamander\\nno\\nsemi\\nno\\nyes\\namphibian\\nTable 8.7: The vertebrate data set\\nName\\ngives birth\\naquatic\\nanimal\\naerial\\nanimal\\nhas legs\\nClass la-\\nbel\\npython\\nno\\nno\\nno\\nno\\nreptile\\npigeon\\nno\\nno\\nyes\\nyes\\nbird\\nTable 8.8: The vertebrate data set\\n“has legs”, etc. Putting all these together, we get the the diagram in Figure 8.5 as the classiﬁcation\\ntree for the data in Table 8.1.\\n8.3.2\\nClassiﬁcation tree in rule format\\nThe classiﬁcation tree shown in Figure 8.5 can be presented as a set of rules in the form of an\\nalgorithm.\\nAlgorithm for classiﬁcation of vertebrates\\n1. if give birth = ”yes” then\\n2.\\nif aquatic = “yes” then\\n3.\\nreturn class = “ﬁsh”\\n4.\\nelse\\n5.\\nif aerial = “yes” then\\n6.\\nreturn class = “bird”\\n7.\\nelse\\n8.\\nreturn class = “mammal”\\n9.\\nend if\\n10.\\nend if\\n11. else\\n12.\\nif aquatic = “yes” then\\n13.\\nreturn class = “ﬁsh”'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 102}, page_content='CHAPTER 8. DECISION TREES\\n88\\nRoot node\\nTable 8.1:\\ngives birth?\\nTable 8.2:\\naquatic?\\nTable 8.4\\nﬁsh\\nyes\\nTable 8.5:\\naerial?\\nPart of\\nTable 8.5\\nbird\\nyes\\nPart of\\nTable 8.5\\nmammal\\nno\\nno\\nyes\\nTable 8.3:\\naquatic?\\nTable 8.6\\nﬁsh\\nyes\\nTable 8.7\\namph\\nsemi\\nTable 8.8\\naerial?\\nPart of\\nTable 8.8\\nbird\\nyes\\nPart of\\nTable 8.8\\nreptile\\nno\\nno\\nno\\nFigure 8.5: Classiﬁcation tree\\n14.\\nend if\\n15.\\nif aquatic = “semi” then\\n16.\\nreturn class = “amphibian”\\n17.\\nelse\\n18.\\nif aerial = “yes” then\\n19.\\nreturn class = “amphibian”\\n20.\\nelse\\n21.\\nreturn class = “reptile”\\n22.\\nend if\\n23.\\nend if\\n24. end if\\n8.3.3\\nSome remarks\\n1. On the elements of a classiﬁcation tree\\nThe various elements in a classiﬁcation tree are identiﬁed as follows.\\n• Nodes in the classiﬁcation tree are identiﬁed by the feature names of the given data.\\n• Branches in the tree are identiﬁed by the values of features.\\n• The leaf nodes identiﬁed by are the class labels.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 103}, page_content='CHAPTER 8. DECISION TREES\\n89\\n2. On the order in which the features are selected\\nIn the example discussed above, initially we chose the feature “gives birth” to split the data set\\ninto disjoint subsets and then the feature “aquatic animal”, and so on. There was no theoretical\\njustiﬁcation for this choice. We could as well have chosen the feature “aquatic animal”, or any other\\nfeature, as the initial feature for splitting the data. The classiﬁcation tree depends on the order in\\nwhich the features are selected for partitioning the data.\\n3. Stopping criteria\\nA real-world data will contain much more example record than the example we considered earlier.\\nIn general, there will be a large number of features each feature having several possible values. Thus,\\nthe corresponding classiﬁcation trees will naturally be more complex. In such cases, it may not be\\nadvisable to construct all branches and leaf nodes of the tree. The following are some of commonly\\nused criteria for stopping the construction of further nodes and branches.\\n• All (or nearly all) of the examples at the node have the same class.\\n• There are no remaining features to distinguish among the examples.\\n• The tree has grown to a predeﬁned size limit.\\n8.4\\nFeature selection measures\\nIf a dataset consists of n attributes then deciding which attribute is to be to placed at the root or at\\ndifferent levels of the tree as internal nodes is a complicated problem. It is not enough that we just\\nrandomly select any node to be the root. If we do this, it may give us bad results with low accuracy.\\nThe most important problem in implementing the decision tree algorithm is deciding which\\nfeatures are to be considered as the root node and at each level. Several methods have been developed\\nto assign numerical values to the various features such that the values reﬂect the relative importance\\nof the various features. These are called the feature selection measures. Two of the popular feature\\nselection measures are information gain and Gini index. These are explained in the next section.\\n8.5\\nEntropy\\nThe degree to which a subset of examples contains only a single class is known as purity, and any\\nsubset composed of only a single class is called a pure class. Informally, entropy3 is a measure of\\n“impurity” in a dataset. Sets with high entropy are very diverse and provide little information about\\nother items that may also belong in the set, as there is no apparent commonality.\\nEntropy is measured in bits. If there are only two possible classes, entropy values can range from\\n0 to 1. For n classes, entropy ranges from 0 to log2(n). In each case, the minimum value indicates\\nthat the sample is completely homogeneous, while the maximum value indicates that the data are as\\ndiverse as possible, and no group has even a small plurality.\\n8.5.1\\nDeﬁnition\\nConsider a segment S of a dataset having c number of class labels. Let pi be the proportion of\\nexamples in S having the i th class label. The entropy of S is deﬁned as\\nEntropy(S) =\\nc\\n∑\\ni=1\\n−pi log2(pi).\\n3From German Entropie “measure of the disorder of a system,” coined in 1865 (on analogy of Energie) by German\\nphysicist Rudolph Clausius (1822-1888), in his work on the laws of thermodynamics, from Greek entropia “a turning toward,”\\nfrom en “in” + trope “a turning, a transformation,”'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 104}, page_content='CHAPTER 8. DECISION TREES\\n90\\nFigure 8.6: Plot of p vs. Entropy\\nRemark\\nIn the expression for entropy, the value of 0 × log2(0) is taken as zero.\\nSpecial case\\nLet the data segment S has only two class labels, say, “yes” and “no”. If p is the proportion of\\nexamples having the label “yes” then the proportion of examples having label “no” will be 1 −p. In\\nthis case, the entropy of S is given by\\nEntropy(S) = −plog2(p) −(1 −p)log2(1 −p).\\nIf we plot the values of graph of Entropy(S) for all possible values of p, we get the diagram shown\\nin Figure 8.64.\\n8.5.2\\nExamples\\nLet “xxx” be some class label. We denote by pxxx the proportion of examples with class label “xxx”.\\n1. Entropy of data in Table 8.1\\nLet S be the data in Table 8.1. The class labels are ”amphi”, “bird”, ”ﬁsh”, ”mammal” and\\n”reptile”. In S we have the following numbers.\\nNumber of examples with class label “amphi”\\n= 3\\nNumber of examples with class label “bird”\\n= 2\\nNumber of examples with class label “ﬁsh”\\n= 2\\nNumber of examples with class label “mammal”\\n= 2\\nNumber of examples with class label “reptile”\\n= 1\\nTotal number of examples\\n= 10\\nTherefore, we have:\\nEntropy (S) =\\n∑\\nfor all classes “xxx”\\n−pxxx log2(pxxx)\\n4Plot created using R language.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 105}, page_content='CHAPTER 8. DECISION TREES\\n91\\n= −pamphi log2(pamphi) −pbird log2(pbird)\\n−pﬁsh log2(pﬁsh) −pmammal log2(pmammal)\\n−preptile log2(preptile)\\n= −(3/10)log2(3/10) −(2/10)log2(2/10)\\n−(2/10)log2(2/10) −(2/10)log2(2/10)\\n−(1/10)log2(1/10)\\n= 2.2464\\n2. Entropy of data in Table 8.2\\nConsider the segment S of the data in Table 8.1 given in Table 8.2. For quick reference, the\\ntable has been reproduced below:\\nName\\nGives\\nbirth\\nAquatic\\nanimal\\nAerial\\nanimal\\nHas legs\\nClass la-\\nbel\\nhuman\\nyes\\nno\\nno\\nyes\\nmammal\\nbat\\nyes\\nno\\nyes\\nyes\\nbird\\ncat\\nyes\\nno\\nno\\nyes\\nmammal\\nshark\\nyes\\nyes\\nno\\nno\\nﬁsh\\nThree class labels appear in this segment, namely, “bird”, “ﬁsh” and “mammal”. We have:\\nNumber of examples with class label “bird”\\n1\\nNumber of examples with class label “ﬁsh”\\n1\\nNumber of examples with class label “mammal”\\n2\\nTotal number of examples\\n4\\nTherefore we have\\nEntropy(S) =\\n∑\\nfor all classes “xxx”\\n−pxxx log2(pxxx)\\n= −pbird log2(pbird) −pﬁsh log2(pﬁsh)\\n−pmammal log2(pmammal)\\n= −(1/4)log2(1/4) −(1/4)log2(1/4) −(2/4)log2(2/4)\\n= −(1/4) × (−2) −(1/4) × (−2) −(2/4) × (−1)\\n= 1.5\\n(8.1)\\n3. Entropy of data in Table 8.3\\nConsider the segment S of the data in Table 8.1 given in Table 8.3. For quick reference, the\\ntable has been reproduced below:\\nName\\ngives birth\\naquatic\\nanimal\\naerial\\nanimal\\nhas legs\\nClass la-\\nbel\\npython\\nno\\nno\\nno\\nno\\nreptile\\nsalmon\\nno\\nyes\\nno\\nno\\nﬁsh\\nfrog\\nno\\nsemi\\nno\\nyes\\namphibian\\npigeon\\nno\\nno\\nyes\\nyes\\nbird\\nturtle\\nno\\nsemi\\nno\\nyes\\namphibian\\nsalamander\\nno\\nsemi\\nno\\nyes\\namphibian\\nFour class labels appear in this segment, namely, “amphi”, “bird”, “ﬁsh” and “reptile”. We\\nhave:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 106}, page_content='CHAPTER 8. DECISION TREES\\n92\\nNumber of examples with class label “amphi”\\n3\\nNumber of examples with class label “bird”\\n1\\nNumber of examples with class label “ﬁsh”\\n1\\nNumber of examples with class label “reptile”\\n1\\nTotal number of examples\\n6\\nTherefore, we have:\\nEntropy(S) =\\n∑\\nfor all classes “xxx”\\n−pxxx log2(pxxx)\\n= −pamphi log2(pamphi) −pbird log2(pbird) −pﬁsh log2(pﬁsh)\\n−preptile log2(preptile)\\n= −(3/6)log2(3/6) −(1/6)log2(1/6) −(1/6)log2(1/6)\\n−(1/6)log2(1/6)\\n= 1.7925\\n(8.2)\\n8.6\\nInformation gain\\n8.6.1\\nDeﬁnition\\nLet S be a set of examples, A be a feature (or, an attribute), Sv be the subset of S with A = v,\\nand Values(A) be the set of all possible values of A. Then the information gain of an attribute A\\nrelative to the set S, denoted by Gain(S,A), is deﬁned as\\nGain(S,A) = Entropy(S) −\\n∑\\nv∈Values (A)\\n∣Sv∣\\n∣S∣× Entropy(Sv).\\nwhere ∣S∣denotes the number of elements in S.\\n8.6.2\\nExample 1\\nConsider the data S given in Table 8.1. We have have already seen that\\n∣S∣= 10\\nEntropy (S) = 2.2464.\\nWe denote the information gain corresponding to the feature “xxx” by Gain(S,xxx).\\n1. Computation of Gain(S,gives birth)\\nA1 = gives birth\\nValues of A1 = {“yes”,“no”}\\nSA1=yes = Data in Table 8.2\\n∣SA1=yes∣= 4\\nEntropy(SA1=yes) = 1.5\\n(See Eq.(8.1))\\nSA1=no = Data in Table 8.3\\n∣SA1=no∣= 6\\nEntropy(SA1=no) = 1.7925\\n(See Eq.(8.2))\\nNow we have\\nGain(S,A1) = Entropy(S) −\\n∑\\nv∈Values(A1)\\n∣Sv∣\\n∣S∣× Entropy(Sv)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 107}, page_content='CHAPTER 8. DECISION TREES\\n93\\n= Entropy(S) −∣SA1=yes∣\\n∣S∣\\n× Entropy(SA1=yes)\\n−∣SA1=no∣\\n∣S∣\\n× Entropy(SA1=no)\\n= 2.2464 −(4/10) × 1.5 −(6/10) × 1.7925\\n= 0.5709\\n2. Computation of Gain(S,aquatic)\\nA2 = aquatic\\nValues of A2 = {“yes”,“no”,“semi”}\\nSA2=yes = See Table 8.1\\n∣SA2=yes∣= 2\\nEntropy(SA2=yes) = −pﬁsh log2(pﬁsh)\\n= −(2/2)log2(2/2)\\n= 0\\nSA2=no = See Table 8.1\\n∣SA2=no∣= 5\\nEntropy(SA2=no) = −pmammal log2(pmammal) −preptile log2(preptile)\\n−pbird log2(pbird)\\n= −(2/5) × log2(2/5) −(1/5) × log2(1/5)\\n−(2/5) × log2(2/5)\\n= 1.5219\\nSA2=semi = See Table 8.1\\n∣SA2=semi∣= 3\\nEntropy(SA2=semi) = −pamphi log2(pamphi)\\n= −(3/3) × log2(3/3)\\n= 0\\nGain(S,A2) = Entropy(S) −\\n∑\\nv∈Values(A2)\\n∣Sv∣\\n∣S∣× Entropy(Sv)\\n= Entropy(S) −∣SA1=yes∣\\n∣S∣\\n× Entropy(SA1=yes)\\n−∣SA1=no∣\\n∣S∣\\n× Entropy(SA1=no)\\n−∣SA1=semi∣\\n∣S∣\\n× Entropy(SA1=semi)\\n= 2.2464 −(2/10) × 0 −(5/10) × 1.5219 −(3/3) × 0\\n= 1.48545\\n3. Computations of Gain(S,aerial animal) and Gain(S,has legs)\\nThese are left as exercises.\\n8.7\\nGini indices\\nThe Gini split index of a data set is another feature selection measure in the construction of classiﬁ-\\ncation trees. This measure is used in the CART algorithm.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 108}, page_content='CHAPTER 8. DECISION TREES\\n94\\n8.7.1\\nGini index\\nConsider a data set S having r class labels c1,...,cr. Let pi be the proportion of examples having\\nthe class label ci. The Gini index of the data set S, denoted by Gini(S), is deﬁned by\\nGini(S) = 1 −\\nr\\n∑\\ni=1\\np2\\ni .\\nExample\\nLet S be the data in Table 8.1. There are four class labels ”amphi”, “bird”, ”ﬁsh”, ”mammal” and\\n”reptile”. The numbers of examples having these class labels are as follows:\\nNumber of examples with class label “amphi”\\n= 3\\nNumber of examples with class label “bird”\\n= 2\\nNumber of examples with class label “ﬁsh”\\n= 2\\nNumber of examples with class label “mammal”\\n= 2\\nNumber of examples with class label “reptile”\\n= 1\\nTotal number of examples\\n= 10\\nThe Gini index of S is given by\\nGini(S) = 1 −\\nr\\n∑\\ni=1\\np2\\ni\\n= 1 −(3/10)2 −(2/10)2 −(2/10)2 −(2/10)2 −(1/10)2\\n= 0.78\\n8.7.2\\nGini split index\\nLet S be a set of examples, A be a feature (or, an attribute), Sv be the subset of S with A = v,\\nand Values(A) be the set of all possible values of A. Then the Gini split index of A relative to S,\\ndenoted by Ginisplit(S,A), is deﬁned as\\nGinisplit(S,A) =\\n∑\\nv∈Values (A)\\n∣Sv∣\\n∣S∣× Gini(Sv).\\nwhere ∣S∣denotes the number of elements in S.\\n8.8\\nGain ratio\\nThe gain ratio is a third feature selection measure in the construction of classiﬁcation trees.\\nLet S be a set of examples, A a feature having c different values and let the set of values of A be\\ndenoted by Values(A). We deﬁned the information gain of A relative to S, denoted by Gain(S,A),\\nby\\nGain(S,A) = Entropy(S) −\\n∑\\nv∈Values(A)\\n∣Sv∣\\n∣S∣× Entropy(Sv).\\nWe now deﬁne thesplit information of A relative to S, dented by SplitInformation(S,A), by\\nSplitInformation(S,A) = −\\nc\\n∑\\ni=1\\n∣Si∣\\n∣S∣log2\\n∣Si∣\\n∣S∣\\nwhere S1,...Sc are the c subsets of examples resulting from partitioning S into the c values of the\\nattribute A. The gain ratio of A relative to S, denoted by GainRatio(S,A), by\\nGainRatio(S,A) =\\nGain(S,A)\\nSplitInformation(S,A).'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 109}, page_content='CHAPTER 8. DECISION TREES\\n95\\n8.8.1\\nExample\\nConsider the data S given in Table 8.1. Let A denote the attribute “gives birth”.We have have already\\nseen that\\n∣S∣= 10\\nEntropy (S) = 2.2464\\nGain(S,A) = 0.5709\\nNow we have\\nSplitInformation(S,A) = −∣Syes∣\\n∣S∣log2\\n∣Syes∣\\n∣S∣−∣Sno∣\\n∣S∣log2\\n∣Sno∣\\n∣S∣\\n= −4\\n10 × log2\\n4\\n10 −6\\n10 × log2\\n6\\n10\\n= 0.9710\\nGainRatio = 0.5709\\n0.9710\\n= 0.5880\\nIn a similar way we can compute the gain ratios Gain(S,“aquatic”), Gain(S,“aerial”) and Gain(S,“has legs”).\\n8.9\\nDecision tree algorithms\\n8.9.1\\nOutline\\nDecision tree algorithm: Outline\\n1. Place the “best” feature (or, attribute) of the dataset at the root of the tree.\\n2. Split the training set into subsets. Subsets should be made in such a way that each subset\\ncontains data with the same value for a feature.\\n3. Repeat Step 1 and Step 2 on each subset until we ﬁnd leaf nodes in all the branches of the tree.\\n8.9.2\\nSome well-known decision tree algorithms\\n1. ID3 (Iterative Dichotomiser 3) developed by Ross Quinlan\\n2. C4.5 developed by Ross Quinlan\\n3. C5.0 developed by Ross Quinlan\\n4. CART (Classiﬁcation And Regression Trees)\\n5. 1R (One Rule) developed by Robert Holte in 1993.\\n6. RIPPER (Repeated Incremental Pruning to Produce Error Reduction) Introduced in 1995 by\\nWilliam W. Cohen.\\nAs an example of decision tree algorithms, we discuss the details of the ID3 algorithm and illustrate\\nit with an example.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 110}, page_content='CHAPTER 8. DECISION TREES\\n96\\n8.10\\nThe ID3 algorithm\\nRoss Quinlan, while working at University of Sydney, developed the ID3 (Iterative Dichotomiser\\n3)5 algorithm and published it in 1975.\\nAssumptions\\n• The algorithm uses information gain to select the most useful attribute for classiﬁcation.\\n• We assume that there are only two class labels, namely, “+” and “−”. The examples with class\\nlabels “+” are called positive examples and others negative examples.\\n8.10.1\\nThe algorithm\\nNotations\\nThe following notations are used in the algorithm:\\nS\\nThe set of examples\\nC\\nThe set of class labels\\nF\\nThe set of features\\nA\\nAn arbitrary feature (attribute)\\nValues(A)\\nThe set of values of the feature A\\nv\\nAn arbitrary value of A\\nSv\\nThe set of examples with A = v\\nRoot\\nThe root node of a tree\\nAlgorithm ID3(S, F, C)\\n1. Create a root node for the tree.\\n2. if (all examples in S are positive) then\\n3.\\nreturn single node tree Root with label “+”\\n4. end if\\n5. if (all examples are negative) then\\n6.\\nreturn single node tree Root with label “–”\\n7. end if\\n8. if (number of features is 0) then\\n9.\\nreturn single node tree Root with label equal to the most common class label.\\n10. else\\n11.\\nLet A be the feature in F with the highest information gain.\\n12.\\nAssign A to the Root node in decision tree.\\n13.\\nfor all (values v of A) do\\n14.\\nAdd a new tree branch below Root corresponding to v.\\n15.\\nif (Sv is empty) then\\n16.\\nBelow this branch add a leaf node with label equal to the most common class\\nlabel in the set S.\\n17.\\nelse\\n18.\\nBelow this branch add the subtree formed by applying the same algorithm ID3\\nwith the values ID3(Sv,C,F −{A}).\\n19.\\nend if\\n20.\\nend for\\n21. end if\\n5dichotomy: A division into two parts or classiﬁcations especially when they are sharply distinguished or opposed'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 111}, page_content='CHAPTER 8. DECISION TREES\\n97\\n8.10.2\\nExample\\nProblem\\nUse ID3 algorithm to construct a decision tree for the data in Table 8.9.\\nDay\\noutlook\\ntemperature\\nhumidity\\nwind\\nplaytennis\\nD1\\nsunny\\nhot\\nhigh\\nweak\\nno\\nD2\\nsunny\\nhot\\nhigh\\nstrong\\nno\\nD3\\novercast\\nhot\\nhigh\\nweak\\nyes\\nD4\\nrain\\nmild\\nhigh\\nweak\\nyes\\nD5\\nrain\\ncool\\nnormal\\nweak\\nyes\\nD6\\nrain\\ncool\\nnormal\\nstrong\\nno\\nD7\\novercast\\ncool\\nnormal\\nstrong\\nyes\\nD8\\nsunny\\nmild\\nhigh\\nweak\\nno\\nD9\\nsunny\\ncool\\nnormal\\nweak\\nyes\\nD10\\nrain\\nmild\\nnormal\\nweak\\nyes\\nD11\\nsunny\\nmild\\nnormal\\nstrong\\nyes\\nD12\\novercast\\nmild\\nhigh\\nstrong\\nyes\\nD13\\novercast\\nhot\\nnormal\\nweak\\nyes\\nD14\\nrain\\nmild\\nhigh\\nstrong\\nno\\nTable 8.9: Training examples for the target concept “PlayTennis”\\nSolution\\nNote that, in the given data, there are four features but only two class labels (or, target variables),\\nnamely, “yes” and “no”.\\nStep 1\\nWe ﬁrst create a root node for the tree (see Figure 8.7).\\nRoot node\\nTable 8.9\\nFigure 8.7: Root node of the decision tree for data in Table 8.9\\nStep 2\\nNote that not all examples are positive (class label “yes”) and not all examples are negative (class\\nlabel “no”). Also the number of features is not zero.\\nStep 3\\nWe have to decide which feature is to be placed at the root node. For this, we have to calculate the\\ninformation gains corresponding to each of the four features. The computations are shown below.\\n(i) Calculation of Entropy(S)\\nEntropy(S) = −pyes log2(pyes) −pno log2(pno)\\n= −(9/14) × log2 (9/14) −(5/14) × log2 (5/14)\\n= 0.9405'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 112}, page_content='CHAPTER 8. DECISION TREES\\n98\\n(ii) Calculation of Gain(S,outlook)\\nThe values of the attribute “outlook” are “sunny”, “ overcast” and “rain”. We have to calculate\\nEntropy(Sv) for v = sunny, v = overcast and v = rain.\\nEntropy(Ssunny) = −(3/5) × log2 (3/5) −(2/5) × log2 (2/5)\\n= 0.9710\\nEntropy(Sovercast) = −(4/4) × log2 (4/4)\\n= 0\\nEntropy(Srain) = −(3/5) × log2 (3/5) −(2/5) × log2 (2/5)\\n= 0.9710\\nGain(S, outlook) = Entropy(S) −∣Ssunny∣\\n∣S∣\\n× Entropy(Ssunny)\\n−∣Sovercast∣\\n∣S∣\\n× Entropy(Sovercast)\\n−∣Srain∣\\n∣S∣\\n× Entropy(Srain)\\n= 0.9405 −(5/14) × 0.9710 −(4/14) × 0\\n−(5/14) × 0.9710\\n= 0.2469\\n(iii) Calculation of Gain(S,temperature)\\nThe values of the attribute “temperature” are “hot”, “mild” and “cool”. We have to calculate\\nEntropy(Sv) for v = hot, v = mild and v = cool.\\nEntropy(Shot) = −(2/4) × log2 (2/4) −(2/4) × log2 (2/4)\\n= 1.0000\\nEntropy(Smild) = −(4/6) × log2 (4/6) −(2/6) × log2 (2/6)\\n= 0.9186\\nEntropy(Scool) = −(3/4) × log2 (3/4) −(1/4) × log2 (1/4)\\n= 0.8113\\nGain(S, temperature) = Entropy(S) −∣Shot∣\\n∣S∣× Entropy(Shot)\\n−∣Smild∣\\n∣S∣\\n× Entropy(Smild)\\n−∣Scool∣\\n∣S∣\\n× Entropy(Scool)\\n= 0.9405 −(4/14) × 1.0000 −(6/14) × 0.9186\\n−(4/14) × 0.8113\\n= 0.0293\\n(iv) Calculation of Gain(S,humidity) and Gain(S,wind)\\nThe following information gains can be calculated in a similar way:\\nGain(S, humidity) = 0.151\\nGain(S, wind) = 0.048'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 113}, page_content='CHAPTER 8. DECISION TREES\\n99\\nStep 4\\nWe ﬁnd the highest information gain whic is the maximum among Gain(S,outlook), Gain(S,temperature),\\nGain(S,humidity) and Gain(S,wind). Therefore, we have:\\nhighest information gain = max{0.2469,0.0293,0.151,0.048}\\n= 0.2469\\nThis corresponds to the feature “outlook”. Therefore, we place “outlook” at the root node. We now\\nsplit the root node in Figure 8.7 into three branches according to the values of the feature “outlook”\\nas in Figure 8.8.\\nRoot node\\nTable 8.9\\noutlook?\\nNode 1\\nsunny\\nNode 2\\novercast\\nNode 3\\nrain\\nFigure 8.8: Decision tree for data in Table 8.9, after selecting the branching feature at root node\\nStep 5\\nLet S(1) = Soutlook=sunny. We have ∣S(1)∣= 5. The examples in S(1)are shown in Table 8.10.\\nDay\\noutlook\\ntemperature\\nhumidity\\nwind\\nplaytennis\\nD1\\nsunny\\nhot\\nhigh\\nweak\\nno\\nD2\\nsunny\\nhot\\nhigh\\nstrong\\nno\\nD8\\nsunny\\nmild\\nhigh\\nweak\\nno\\nD9\\nsunny\\ncool\\nnormal\\nweak\\nyes\\nD11\\nsunny\\nmild\\nnormal\\nstrong\\nyes\\nTable 8.10: Training examples with outlook = “sunny”\\nGain(S(1),temp) = Entropy(S(1)) −\\n∣S(1)\\ntemp = hot∣\\n∣S(1)∣\\n× Entropy(S(1)\\ntemp = hot)\\n−\\n∣S(1)\\ntemp = mild∣\\n∣S(1)∣\\n× Entropy(S(1)\\ntemp = mild)\\n−\\n∣S(1)\\ntemp = cool∣\\n∣S(1)∣\\n× Entropy(S(1)\\ntemp = cool)\\n= [−(2/5)log2(2/5) −(3/5)log2(3/5)]\\n−(2/5) × [−(2/2)log(2/2))]\\n−(2/5) × [−(1/2)log(1/2) −(1/2)log2(1/2)]\\n−(1/5) × [−(1/1)log(1/1)]\\n= 0.5709'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 114}, page_content='CHAPTER 8. DECISION TREES\\n100\\nGain(S(1),hum) = Entropy(S(1)) −\\n∣S(1)\\nhum = high∣\\n∣S(1)∣\\n× Entropy(S(1)\\nhum = high)\\n−\\n∣S(1)\\nhum = normal∣\\n∣S(1)∣\\n× Entropy(S(1)\\nhum = normal)\\n= [−(2/5)log2(2/5) −(3/5)log2(3/5)]\\n−(3/5) × [−(3/3)log(3/3))]\\n−(2/5) × [−(2/2)log(2/2)]\\n= 0.9709\\nGain(S(1),wind) = Entropy(S(1)) −\\n∣S(1)\\nwind = weak∣\\n∣S(1)∣\\n× Entropy(S(1)\\nwind = weak)\\n−\\n∣S(1)\\nwind = strong∣\\n∣S(1)∣\\n× Entropy(S(1)\\nwind = strong)\\n= [−(2/5)log2(2/5) −(3/5)log2(3/5)]\\n−(3/5) × [−(2/3)log(2/3) −(1/3)log2(1/3))]\\n−(2/5) × [−(1/2)log(1/2) −(1/2)log(1/2)]\\n= 0.0110\\nThe maximum of Gain(S(1),temp), Gain(S(1),hum) and Gain(S(1),wind) is Gain(S(1),hum).\\nHence we place “humidity” at Node 1 and split this node into two branches according to the values\\nof the feature “humidity” to get the tree in Figure 8.9.\\nRoot node\\nTable 8.9\\noutlook?\\nNode 1:\\nhumidity?\\nNode 4\\nhigh\\nNode 5\\nnormal\\nsunny\\nNode 2\\novercast\\nNode 3\\nrain\\nFigure 8.9: Decision tree for data in Table 8.9, after selecting the branching feature at Node 1\\nStep 6\\nIt can be seen that all the examples in the the data set corresponding to Node 4 in Figure 8.9 have\\nthe same class label “no” and all the examples corresponding to Node 5 have the same class label\\n“yes”. So we represent Node 4 as a leaf node with value “no” and Node 5 as a leaf node with value\\n“yes”. Similarly, all the examples corresponding to Node 2 have the same class label “yes”. So\\nwe convert Node 2 as a leaf node with value “ yes. Finally, let S(2) = Soutlook = rain. The highest\\ninformation gain for this data set is Gain(S(2),humidity). The branches resulting from splitting this\\nnode corresponding to the values “high” and “normal” of “humidity” lead to leaf nodes with class\\nlabels “no” and ”yes”. With these changes, we get the tree in Figure 8.10.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 115}, page_content='CHAPTER 8. DECISION TREES\\n101\\nRoot node\\nTable 8.9\\noutlook?\\nNode 1:\\nhumidity?\\nno\\nhigh\\nyes\\nnormal\\nsunny\\nyes\\novercast\\nNode 3:\\nhumidity?\\nno\\nhigh\\nyes\\nnormal\\nrain\\nFigure 8.10: Decision tree for data in Table 8.9\\n8.11\\nRegression trees\\nA regression problem is the problem of determining a relation between one or more independent\\nvariables and an output variable which is a real continuous variable and then using the relation\\nto predict the values of the dependent variables. Regression problems are in general related to\\nprediction of numerical values of variables. Trees can also be used to make such predictions. A tree\\nused for making predictions of numerical variables is called a prediction tree or a regression tree.\\n8.11.1\\nExample\\nUsing the data in Table 8.11, construct a tree to predict the values of y.\\nx1\\n1\\n3\\n4\\n6\\n10\\n15\\n2\\n7\\n16\\n0\\nx2\\n12\\n23\\n21\\n10\\n27\\n23\\n35\\n12\\n27\\n17\\ny\\n10.1\\n15.3\\n11.5\\n13.9\\n17.8\\n23.1\\n12.7\\n43.0\\n17.6\\n14.9\\nTable 8.11: Data for regression tree\\nSolution\\nWe shall construct a raw decision tree (a tree constructed without using any standard algorithm) to\\npredict the value of y corresponding to any untabulated values of x1 and x2.\\nStep 1.\\nWe arbitrarily split the values of x1 into two sets: One set deﬁned by x1 < 6 and the other\\nset deﬁned by x1 ≥6. This splits the data into two parts. This yields the tree in Figure ??.\\nx1\\n1\\n3\\n4\\n2\\n0\\nx2\\n12\\n23\\n21\\n35\\n17\\ny\\n10.1\\n15.3\\n11.5\\n12.7\\n14.9\\nTable 8.12: Data for regression tree\\nStep 2.\\nIn Figure 8.12, consider the node speciﬁed by Table 8.12. We arbitrarily split the values\\nof x2 into two sets: one speciﬁed by x2 < 21 and one speciﬁed by x2 ≥21. Similarly, the\\nnode speciﬁed by Table 8.13, we split the values of x2 into sets: one speciﬁed by x2 < 23'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 116}, page_content='CHAPTER 8. DECISION TREES\\n102\\nx1\\n6\\n10\\n15\\n7\\n16\\nx2\\n10\\n27\\n23\\n12\\n27\\ny\\n13.9\\n17.8\\n23.1\\n43.0\\n17.6\\nTable 8.13: Data for regression tree\\nTab 8.11\\nTab 8.12\\nTab 8.13\\nx1 < 6\\nx1 ≥6\\nFigure 8.11: Part of a regression tree for Table 8.11\\nand one speciﬁed by x2 ≥23. The split data are given in Table 8.14(a) - (d). This gives us\\nthe tree in Figure 8.12.\\nTab 8.11\\nTabe 8.12\\nTab 8.13\\nx1 < 6\\nx1 ≥6\\nTab 8.14(a)\\nTab 8.14(b)\\nx2 < 21\\nx2 ≥21\\nTab 8.14(c)\\nTab 8.14(d)\\nx2 < 23\\nx2 ≥23\\nFigure 8.12: Part of regression tree for Table 8.11\\nStep 3.\\nWe next make the nodes speciﬁed by Table 8.14(a), ..., Tab 8.14(d) into leaf nodes. In\\neach of these leaf nodes, we write the average of the values in the corresponding table (this\\nis a standard procedure). For, example, at Table 8.14(a), we write 1\\n2(10.1 + 14.9) = 12.5.\\nThen we get Figure 8.13.\\nx1\\n1\\n0\\nx2\\n12\\n17\\ny\\n10.1\\n14.9\\nx1\\n3\\n4\\n2\\nx2\\n23\\n21\\n35\\ny\\n15.3\\n11.5\\n12.7\\n(a)\\n(b)\\nx1\\n6\\n7\\nx2\\n10\\n12\\ny\\n13.9\\n43.0\\nx1\\n10\\n15\\n16\\nx2\\n27\\n23\\n27\\ny\\n17.8\\n23.1\\n17.6\\n(c)\\n(d)\\nTable 8.14: Data for regression tree'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 117}, page_content='CHAPTER 8. DECISION TREES\\n103\\nx1 < 6\\nx1 ≥6\\n12.5\\n13.17\\nx2 < 21\\nx2 ≥21\\n28.45\\n19.5\\nx2 < 23\\nx2 ≥23\\nFigure 8.13: A regression tree for Table 8.11\\nStep 4.\\nFigure 8.13 is the ﬁnal raw regression tree for predicting the values of y based on the data\\nin Table 8.11.\\n8.11.2\\nAn algorithm for constructing regression trees\\nStarting with a learning sample, three elements are necessary to determine a regression tree:\\n1. A way to select a split at every intermediate node\\n2. A rule for determining when a node is terminal\\n3. A rule for assigning a value for the output variable to every terminal node\\nNotations\\nx1,x2,...,xn\\n:\\nThe input variables\\nN\\n:\\nNumber of samples in the data set\\ny1,y2,...,yN\\n:\\nThe values of the output variables\\nT\\n:\\nA tree\\nc\\n:\\nA leaf of T\\nnc\\n:\\nNumber of data elements in the leaf c\\nC\\n:\\nThe set of indices of data elements which\\nare in the leaf c\\nmc\\n:\\nThe mean of the values of y which are in\\nthe leaf c\\nST\\n:\\nSum of squares of errors in T\\nWe have\\nmc = 1\\nnc ∑\\ni∈C\\nyi\\nST =\\n∑\\nc∈leaves(T )\\n∑\\ni∈C\\n(yi −mc)2\\nAlgorithm\\nStep 1.\\nStart with a single node containing all data points. Calculate mc and ST .\\nStep 1.\\nIf all the points in the node have the same value for all the independent variables, stop.\\nStep 1.\\nOtherwise, search over all binary splits of all variables for the one which will reduce ST as\\nmuch as possible.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 118}, page_content='CHAPTER 8. DECISION TREES\\n104\\n(a) If the largest decrease in ST would be less than some threshold δ, or one of the\\nresulting nodes would contain less than q points, stop and if c is a node where we\\nhave stopped, then assign the value mc to the node.\\n(b) Otherwise, take that split, creating two new nodes.\\nStep 1.\\nIn each new node, go back to Step 1.\\nRemarks\\n1. We have seen entropy and information deﬁned for discrete variables. We can deﬁne them for\\ncontinuous variables also. But in the case of regression trees, it is more common to use the\\nsum of squares. The above algorithm is based on sum of squares of errors.\\n2. The CART algorithm mentioned below searches every distinct value of every predictor vari-\\nable to ﬁnd the predictor variable and split value which will reduce ST as much as possible.\\n3. In the above algorithm, we have given the simplest criteria for stopping growing of trees.\\nMore sophisticated criteria which produce much less error have been developed.\\n8.11.3\\nExample\\nConsider the data given in Table 8.11.\\n1. Computation of ST for the entire data set. Initially, there is only one node. So, we have:\\nmc = 1\\nnc ∑\\nc∈C\\nyi\\n= 1\\n10(10.1 + 15.3 + ⋯+ 14.9)\\n= 17.99\\nST =\\n∑\\nc∈leaves(T )\\n∑\\ni∈C\\n(yi −mc)2\\n= (10.1 −17.99)2 + (15.3 −17.99)2 + ⋯+ (14.9 −17.99)2\\n= 817.669\\n2. As suggested in the remarks above, we have to search every distinct value of x1 and x2 to ﬁnd\\nthe predictor variable and split value which will reduce ST as much as possible.\\n3. Let us consider the value 6 of x1. This splits the data set into two parts c1 and c2. Let c1 be\\nthe part deﬁned by x1 < 6 and c2 the part deﬁned by x1 ≥6. S1 is given in Table 8.12 and S2\\nby Table 8.13.Now\\nleaves(T) = {c1,c2}.\\nLet T1 be the tree corresponding to this partition. Then\\nST1 =\\n∑\\nc∈leaves(T1)\\n∑\\ni∈C\\n(yi −mc)2\\n= ∑\\ni∈C1\\n(yi −mc1)2 + ∑\\ni∈C2\\n(yi −mc2)2\\nmc1 =\\n1\\nnc1\\n∑\\ni∈C1\\nyi\\n= 1\\n5(10.1 + 15.3 + 11.5 + 12.7 + 14.9)\\n= 12.9'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 119}, page_content='CHAPTER 8. DECISION TREES\\n105\\nmc2 =\\n1\\nnc2\\n∑\\ni∈C2\\nyi\\n= 1\\n5(13.9 + 17.8 + 23.1 + 43.0 + 17.6)\\n= 23.08\\nST1 = [(10.1 −12.9)2 + ⋯+ (14.9 −12.9)2]+\\n[(13.9 −23.08)2 + ⋯+ (17.6 −23.08)2]\\n= 558.588\\nThe reduction in sum of squares of errors is\\nST −ST1 = 817.669 −558.588 = 259.081.\\n4. In this way, we have compute the reduction in the sum of squares of errors corresponding to\\nall other values of x1 and each of the values of x2 and choose the one for which the reduction\\nis maximum.\\n5. The process has be continued. (Software package may be required to complete the problem.)\\n8.12\\nCART algorithm\\nWe have seen how decision trees can be used to create a model that predicts the value of a target (or\\ndependent variable) based on the values of several input or independent variables.\\nThe CART, or Classiﬁcation And Regression Trees methodology, was introduced in 1984 by Leo\\nBreiman, Jerome Friedman, Richard Olshen and Charles Stone as an umbrella term to refer to the\\nfollowing types of decision trees:\\n• Classiﬁcation trees where the target variable is categorical and the tree is used to identify the\\n“class” within which a target variable would likely fall into.\\n• Regression trees where the target variable is continuous and tree is used to predict it’s value.\\nThe main elements of CART are:\\n• Rules for splitting data at a node based on the value of one variable\\n• Stopping rules for deciding when a branch is terminal and can be split no more\\n• A prediction for the target variable in each terminal node\\n8.13\\nOther decision tree algorithms\\n8.13.1\\nThe C4.5 algorithm\\nThe C4.5 algorithm is an algorithm developed by Ross Quinlan as an improvement of the ID3\\nalgorithm. The following are some of the improvements incorporated in C4.5.\\n• Handling both continuous and discrete attributes\\n• Handling training data with missing attribute values\\n• Handling attributes with differing costs\\n• Pruning trees after creation'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 120}, page_content='CHAPTER 8. DECISION TREES\\n106\\n8.13.2\\nThe C5.0 algorithm\\nThe C5.0 algorithm represents a further improvement on the C4.5 algorithm. This was also devel-\\noped by Ross Quinlan.\\n• Speed - C5.0 is signiﬁcantly faster than C4.5.\\n• Memory usage - C5.0 is more memory efﬁcient than C4.5.\\n• C5.0 gets similar results to C4.5 with considerably smaller decision trees.\\nThe C5.0 algorithm is one of the most well-known implementations of the the decision tree\\nalgorithm. The source code for a single-threaded version of the algorithm is publicly available,\\nand it has been incorporated into programs such as R. The C5.0 algorithm has become the industry\\nstandard to produce decision trees.\\n8.14\\nIssues in decision tree learning\\nIn thie next feww sections, we discuss some of the practical issues in learning decision trees.\\n8.15\\nAvoiding overﬁtting of data\\nWhen we construct a decision tree, the various branches are grown (that is, sub-branches are con-\\nstructed) just deeply enough to perfectly classify the training examples. This leads to difﬁculties\\nwhen there is noise in the data or when the number of training examples are too small. In these\\ncases the algorithm can produce trees that overﬁt the training examples.\\nDeﬁnition\\nWe say that a hypothesis overﬁts the training examples if some other hypothesis that ﬁts the train-\\ning examples less well actually performs better over the entire distribution of instances, including\\ninstances beyond the training set.\\nImpact of overﬁtting\\nFigure 8.14 illustrates the impact of overﬁtting in a typical decision tree learning. From the ﬁgure,\\nwe can see that the accuracy of the tree over training examples increases monotonically whereas the\\naccuracy measured over independent test samples ﬁrst increases then decreases.\\n8.15.1\\nApproaches to avoiding overﬁtting\\nThe main approach to avoid overﬁtting is pruning. Pruning is a technique that reduces the size\\nof decision trees by removing sections of the tree that provide little power to classify instances.\\nPruning reduces the complexity of the ﬁnal classiﬁer, and hence improves predictive accuracy by\\nthe reduction of overﬁtting.\\n• We may apply pruning earlier, that is, before it reaches the point where it perfectly classiﬁes\\nthe training data.\\n• We may allow the tree to overﬁt the data, and then post-prune the true.\\nNow there is the problem of what criterion is to be used to determine the correct ﬁnal tree\\nsize. One commonly used criterion is to use a separate set of examples, distinct from the training\\nexamples, to evaluate the utility of post-pruning nodes from the tree.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 121}, page_content='CHAPTER 8. DECISION TREES\\n107\\nFigure 8.14: Impact of overﬁtting in decision tree learning\\nCase\\nTemperature\\nHeadache\\nNausea\\nDecision (Flue)\\n1\\nhigh\\n?\\nno\\nyes\\n2\\nvery high\\nyes\\nno\\nyes\\n3\\n?\\nno\\nno\\nno\\n4\\nhigh\\nyes\\nyes\\nyes\\n5\\nhigh\\n?\\nyes\\nno\\n6\\nnormal\\nyes\\nno\\nno\\n7\\nnormal\\nno\\nyes\\nno\\n8\\n?\\nyes\\n?\\nyes\\nTable 8.15: A dataset with missing attribute values\\n8.15.2\\nReduced error pruning\\nIn reduced-error pruning, we consider each of the decision tress to be a candidate for pruning. Prun-\\ning a decision node consists of removing the subtree rooted at that node, making it a leaf node, and\\nassigning it the most common classiﬁcation of the training examples afﬁliated to that node. Nodes\\nare removed only if the resulting pruned tree performs no worse than the original over validation set.\\nNodes are pruned iteratively, always choosing the node whose removal most increases the accuracy\\nover the validation set. Pruning of nodes is continued until further pruning decreases the accuracy\\nover the validation set.\\n8.16\\nProblem of missing attributes\\nTable 8.15 shows a dataset with missing attribute values. the missing values are indicated by “?”s.\\nThe following are some of the methods used to handle the problem of missing attributes.\\n• Deleting cases with missing attribute values\\n• Replacing a missing attribute value by the most common value of that attribute'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 122}, page_content='CHAPTER 8. DECISION TREES\\n108\\n• Assigning all possible values to the missing attribute value\\n• Replacing a missing attribute value by the mean for numerical attributes\\n• Assigning to a missing attribute value the corresponding value taken from the closest t cases,\\nor replacing a missing attribute value by a new value\\n8.17\\nSample questions\\n(a) Short answer questions\\n1. Explain the concept of a decision tree with an example.\\n2. What are the different types of decision trees?\\n3. Deﬁne the entropy of a dataset.\\n4. Write a formula to compute the entropy of a two-class dataset.\\n5. Deﬁne information gain and Gini index.\\n6. Give the names of ﬁve different decision-tree algorithms.\\n7. Can decision tree be used for regression? If yes, explain how. If no, explain why.\\n8. What is the difference between classiﬁcation and regression trees?\\n(b) Long answer questions\\n1. Explain classiﬁcation tree using an example.\\n2. Consider the following set of training examples:\\nInstance\\nClassiﬁcation\\na1\\na2\\n1\\n+\\nT\\nT\\n2\\n+\\nT\\nT\\n3\\n−\\nT\\nF\\n4\\n+\\nF\\nF\\n5\\n−\\nF\\nT\\n6\\n−\\nF\\nT\\n(a) What is the entropy of this collection of training examples with respect to the target\\nfunction “classiﬁcation”?\\n(b) What is the information gain of a2 relative to these training examples?\\n3. Explain the ID3 algorithm for learning decision trees.\\n4. Explain CART algorithm.\\n5. What are issues in decision tree learning? How are they overcome?\\n6. Describe an algorithm to construct regression trees.\\n7. What do you mean by information gain and entropy? How is it used to build the decision\\ntrees? Illustrate using an example.\\n8. Use ID3 algorithm to construct a decision tree for the data in the following table.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 123}, page_content='CHAPTER 8. DECISION TREES\\n109\\nInstance no.\\nClass label\\nx1\\nx2\\n1\\n1\\nT\\nT\\n2\\n1\\nT\\nT\\n3\\n0\\nT\\nF\\n4\\n1\\nF\\nF\\n5\\n0\\nF\\nT\\n6\\n0\\nF\\nT\\n9. Use ID3 algorithm to construct a decision tree for the data in the following table.\\nGender\\nCar ownership\\nTravel cost\\nIncome level\\nClass\\n(mode of transportation)\\nMale\\n0\\nCheap\\nLow\\nBus\\nMale\\n1\\nCheap\\nMedium\\nBus\\nFemale\\n1\\nCheap\\nMedium\\nTrain\\nFemale\\n0\\nCheap\\nLow\\nBus\\nMale\\n1\\nCheap\\nMedium\\nBus\\nMale\\n0\\nStandard\\nMedium\\nTrain\\nFemale\\n1\\nStandard\\nMedium\\nTrain\\nFemale\\n1\\nExpensive\\nHigh\\nCar\\nMale\\n2\\nExpensive\\nMedium\\nCar\\nFemale\\n2\\nExpensive\\nHigh\\nCar\\n10. Use ID3 algorithm to construct a decision tree for the data in the following table.\\nAge\\nCompetition\\nType\\nClass (proﬁt)\\nOld\\nYes\\nSoftware\\nDown\\nOld\\nNo\\nSoftware\\nDown\\nOld\\nNo\\nHardware\\nDown\\nMid\\nYes\\nSoftware\\nDown\\nMid\\nYes\\nHardware\\nDown\\nMid\\nNo\\nHardware\\nUp\\nMid\\nNo\\nSoftware\\nUp\\nNew\\nYes\\nSoftware\\nUp\\nNew\\nNo\\nHardware\\nUp\\nNew\\nNo\\nSoftware\\nUp\\n11. Construct a decision tree for the following data.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 124}, page_content='CHAPTER 8. DECISION TREES\\n110\\nClass label (risk)\\nCollateral\\nIncome\\nDebt\\nCredit history\\nhigh\\nnone\\nlow\\nhigh\\nbad\\nhigh\\nnone\\nmiddle\\nhigh\\nunknown\\nmoderate\\nnone\\nmiddle\\nlow\\nunknown\\nhigh\\nnone\\nlow\\nlow\\nunknown\\nlow\\nnone\\nupper\\nlow\\nunknown\\nlow\\nadequate\\nupper\\nlow\\nunknown\\nhigh\\nnone\\nlow\\nlow\\nbad\\nmoderate\\nadequate\\nupper\\nlow\\nbad\\nlow\\nnone\\nupper\\nlow\\ngood\\nlow\\nadequate\\nupper\\nhigh\\ngood\\nhigh\\nnone\\nlow\\nhigh\\ngood\\nmoderate\\nnone\\nmiddle\\nhigh\\ngood\\nlow\\nnone\\nupper\\nhigh\\ngood\\nhigh\\nnone\\nmiddle\\nhigh\\nbad'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 125}, page_content='Chapter 9\\nNeural networks\\n9.1\\nIntroduction\\nAn Artiﬁcial Neural Network (ANN) models the relationship between a set of input signals and an\\noutput signal using a model derived from our understanding of how a biological brain responds to\\nstimuli from sensory inputs. Just as a brain uses a network of interconnected cells called neurons\\nto create a massive parallel processor, ANN uses a network of artiﬁcial neurons or nodes to solve\\nlearning problems.\\n9.2\\nBiological motivation\\nLet us examine how a biological neuron functions. Figure 9.2 gives a schematic representation of\\nthe functioning of a biological neuron.\\nIn the cell, the incoming signals are received by the cell’s dendrites through a biochemical pro-\\ncess. The process allows the impulse to be weighted according to its relative importance or fre-\\nquency. As the cell body begins accumulating the incoming signals, a threshold is reached at which\\nthe cell ﬁres and the output signal is transmitted via an electrochemical process down the axon. At\\nthe axon’s terminals, the electric signal is again processed as a chemical signal to be passed to the\\nneighboring neurons across a tiny gap known as a synapse.1\\nBiological learning systems are built of very complex webs of interconnected neurons. The hu-\\nman brain has an interconnected network of approximately 1011 neurons, each connected, on an\\naverage, to 104 other neurons. Even though the neuron switching speeds are much slower than than\\n1Neuron. (2018, February 15). In Wikipedia, The Free Encyclopedia. Retrieved 01:44, February 23, 2018.\\nFigure 9.1: Anatomy of a neuron\\n111'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 126}, page_content='CHAPTER 9. NEURAL NETWORKS\\n112\\nFigure 9.2: Flow of signals in a biological neuron\\ncomputer switching speeds, we are able to take complex decisions relatively quickly. Because of\\nthis, it is believed that the information processing capabilities of biological neural systems is a con-\\nsequence of the ability of such systems to carry out a huge number of parallel processes distributed\\nover many neurons. The developments in ANN systems are motivated by the desire to implement\\nthis kind of highly parallel computation using distributed representations.\\n9.3\\nArtiﬁcial neurons\\nDeﬁnition\\nAn artiﬁcial neuron is a mathematical function conceived as a model of biological neurons. Artiﬁcial\\nneurons are elementary units in an artiﬁcial neural network. The artiﬁcial neuron receives one or\\nmore inputs (representing excitatory postsynaptic potentials and inhibitory postsynaptic potentials\\nat neural dendrites) and sums them to produce an output. Each input is separately weighted, and the\\nsum is passed through a function known as an activation function or transfer function.\\nSchematic representation of an artiﬁcial neuron\\nThe diagram shown in Figure ?? gives a schematic representation of a model of an artiﬁcial neuron.\\nThe notations in the diagram have the following meanings:\\n∑\\nf\\n...\\nx0 = 1\\nx1\\nw0\\nw1\\nx2\\nw2\\nxn\\nwn\\nn\\n∑\\ni=0\\nwixi\\nf ⎛\\n⎝\\nn\\n∑\\ni=0\\nwixi\\n⎞\\n⎠\\nOutput (y)\\ny = f ⎛\\n⎝\\nn\\n∑\\ni=0\\nwixi\\n⎞\\n⎠\\nFigure 9.3: Schematic representation of an artiﬁcial neuron\\nx1,x2,...xn ∶\\ninput signals\\nw1,w2,...wn ∶\\nweights associated with input signals'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 127}, page_content='CHAPTER 9. NEURAL NETWORKS\\n113\\nx0 ∶\\ninput signal taking the constant value 1\\nw0 ∶\\nweight associated with x0 (called bias)\\n∑∶\\nindicates summation of input signals\\nf ∶\\nfunction which produces the output\\ny ∶\\noutput signal\\nThe function f can be expressed in the following form:\\ny = f(\\nn\\n∑\\ni=0\\nwixi)\\n(9.1)\\nRemarks\\nThe small circles in the schematic representation of the artiﬁcial neuron shown in Figure 9.3 are\\ncalled the nodes of the neuron. The circles on the left side which receives the values of x0,x1,...,xn\\nare called the input nodes and the circle on the right side which outputs the value of y is called\\noutput node. The squares represent the processes that are taking place before the result is outputted.\\nThey need not be explicitly shown in the schematic representation. Figure 9.4 shows a simpliﬁed\\nrepresentation of an artiﬁcial neuron.\\n...\\nx0 = 1\\nx1\\nw0\\nw1\\nx2\\nw2\\nxn\\nwn\\nOutput (y)\\ny = f ⎛\\n⎝\\nn\\n∑\\ni=0\\nwixi\\n⎞\\n⎠\\nFigure 9.4: Simpliﬁed representation of an artiﬁcial neuron\\n9.4\\nActivation function\\n9.4.1\\nDeﬁnition\\nIn an artiﬁcial neural network, the function which takes the incoming signals as input and produces\\nthe output signal is known as the activation function.\\nRemark\\nEq.(9.1) represents the activation function of the ANN model shown in Figure ??.\\n9.4.2\\nSome simple activation functions\\nThe following are some of the simple activation functions.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 128}, page_content='CHAPTER 9. NEURAL NETWORKS\\n114\\n1. Threshold activation function\\nThe threshold activation function is deﬁned by\\nf(x) =\\n⎧⎪⎪⎨⎪⎪⎩\\n1\\nif x > 0\\n−1\\nif x ≤0\\nThe graph of this function is shown in Figure 9.5.\\nx\\n1\\n−1\\n0\\nFigure 9.5: Threshold activation function\\n2. Unit step functions\\nSometimes, the threshold activation function is also deﬁned as a unit step function in which case it\\nis called a unit-step activation function. This is deﬁned as follows:\\nf(x) =\\n⎧⎪⎪⎨⎪⎪⎩\\n1\\nif x ≥0\\n0\\nif x < 0\\nThe graph of this function is shown in Figure 9.6.\\nx\\n1\\n−1\\n0\\nFigure 9.6: Unit step activation function\\n3. Sigmoid activation function (logistic function)\\nOne of the must commonly used activation functions is the sigmoid activation function. It is deﬁned\\nas follows:\\nf(x) =\\n1\\n1 + e−x\\nThe graph of the function is shown in Figure 9.7.\\nx\\nf(x)\\n1\\n0\\nFigure 9.7: The sigmoid activation function'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 129}, page_content='CHAPTER 9. NEURAL NETWORKS\\n115\\n4. Linear activation function\\nThe linear activation function is deﬁned by\\nF(x) = mx + c.\\nThis deﬁnes a straight line in the xy-plane.\\nx\\n1\\n−1\\n0\\nFigure 9.8: Linear activation function\\n5. Piecewise (or, saturated) linear activation function\\nThis is deﬁned by\\nf(x) =\\n⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩\\n0\\nif x < xmin\\nmx + c\\nif xmin ≤x ≤xmax\\n0\\nif x > xmax\\nx\\n1\\n−1\\n0\\nFigure 9.9: Piecewise linear activation function\\n6. Gaussian activation function\\nThis is deﬁned by\\nf(x) =\\n1\\nσ\\n√\\n2π\\ne−(x−µ)2\\n2σ2 .\\nx\\n1\\n−1\\n0\\nFigure 9.10: Gaussian activation function'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 130}, page_content='CHAPTER 9. NEURAL NETWORKS\\n116\\n7. Hyperbolic tangential activation function\\nThis is deﬁned by\\nf(x) = ex −e−x\\nex + e−x .\\nx\\n1\\n−1\\n0\\nFigure 9.11: Hyperbolic tangent activation function\\n9.5\\nPerceptron\\nThe perceptron is a special type of artiﬁcial neuron in which thee activation function has a special\\nform.\\n9.5.1\\nDeﬁnition\\nA perceptron is an artiﬁcial neuron in which the activation function is the threshold function.\\nConsider an artiﬁcial neuron having x1, x2, ⋯, xn as the input signals and w1, w2, ⋯, wn as the\\nassociated weights. Let w0 be some constant. The neuron is called a perceptron if the output of the\\nneuron is given by the following function:\\no(x1,x2,...,xn) =\\n⎧⎪⎪⎨⎪⎪⎩\\n1\\nif w0 + w1x1 + ⋯+ wnxn > 0\\n−1\\nif w0 + w1x1 + ⋯+ wnxn ≤0\\nFigure 9.12 shows the schematic representation of a perceptron.\\n∑\\n \\n...\\nx0 = 1\\nx1\\nw0\\nw1\\nx2\\nw2\\nxn\\nwn\\nn\\n∑\\ni=0\\nwixi\\ny =\\n⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩\\n1\\nif\\nn\\n∑\\ni=0\\nwixi > 0\\n−1\\notherwise\\nOutput (y)\\nFigure 9.12: Schematic representation of a perceptrn'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 131}, page_content='CHAPTER 9. NEURAL NETWORKS\\n117\\nRemarks\\n1. The quantity −w0 can be looked upon as a “threshold” that should be crossed by the weighted\\nsum w1x1 + ⋯+ wnxn in order for the neuron to output a “1”.\\n9.5.2\\nRepresentations of boolean functions by perceptrons\\nIn this section we examine whether simple boolean functions like x1 ANDx2 can be represented by\\nperceptrons. To be consistent with the conventions in the deﬁnition of a perceptron we assume that\\nthe values −1 and 1 represent the boolean constants “false” and “true” respectively.\\n9.5.3\\nRepresentation of x1 ANDx2\\nLet x1 and x2 be two boolean variables. Then the boolean function x1 ANDx2 is represented by\\nTable 9.1. It can be easily veriﬁed that the perceptron shown in Figure 9.13 represents the function\\nx1\\nx2\\nx1 AND x2\\n−1\\n−1\\n−1\\n−1\\n1\\n−1\\n1\\n−1\\n−1\\n1\\n1\\n1\\nTable 9.1: The boolean function x1 ANDx2\\nx1 ANDx2.\\n∑\\n \\nx0 = 1\\nx1\\nx2\\nw0 = −0.8\\nw1 = 0.5\\nw3 = 0.5\\n3\\n∑\\ni=0\\nwixi\\ny =\\n⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩\\n1\\nif\\n3\\n∑\\ni=0\\nwixi > 0\\n−1\\notherwise\\nOutput (y)\\nFigure 9.13: Representation of x1 ANDx2 by a perceptron\\nIn the perceptron shown in Figure 9.13, the output is given by\\ny =\\n⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩\\n1\\nif\\n3\\n∑\\ni=0\\nwixi > 0\\n−1\\notherwise\\n=\\n⎧⎪⎪⎨⎪⎪⎩\\n1\\nif −0.8 + 0.5x1 + 0.5x2 > 0\\n−1\\notherwise\\nRepresentations of OR, NAND and NOR\\nThe functions x1 ORx2, x1 NANDx2 and x1 NORx2 can also be represented by perceptrons. Table\\n9.2 shows the values to be assigned to the weights w0,w1,w2 for getting these boolean functions.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 132}, page_content='CHAPTER 9. NEURAL NETWORKS\\n118\\nBoolean function\\nw0\\nw1\\nw2\\nx1 ANDx2\\n−0.8\\n0.5\\n0.5\\nx1 ORx2\\n−0.3\\n0.5\\n0.5\\nx1 NANDx2\\n0.8\\n−0.5\\n−0.5\\nx1 NORx2\\n0.3\\n−0.5\\n−0.5\\nTable 9.2: Representations of boolean functions by perceptrons\\nRemarks\\nNot all boolean functions can be represented by perceptrons. For example, the boolean function\\nx1 XORx2 cannot be represented by a perceptron. This means that we cannot assign values to\\nw0,w1,w2 such that the expression w0 + w1x1 + w2x2 takes the values of x1 XORx2, and that this\\nis the case can be easily veriﬁed also.\\n9.5.4\\nLearning a perceptron\\nBy “learning a perceptron” we mean the process of assigning values to the weights and the thresh-\\nold such that the perceptron produces correct output for each of the given training examples. The\\nfollowing are two algorithms to solve this learning problem:\\n9.5.5\\nPerceptron learning algorithm\\nDeﬁnitions\\nIn the algorithm, we use the following notations:\\nn\\n:\\nNumber of input variables\\ny = f(z)\\n:\\nOutput from the perceptron for an input\\nvector z\\nD = {(x1,d1),...,(xs,ds)}\\n:\\nTraining set of s samples\\nxj = (xj0,xj1,...,xjn)\\n:\\nThe n-dimensional input vector\\ndj\\n:\\nDesired output value of the perceptron for\\nthe input xj\\nxji\\n:\\nValue of the i-th feature of the j-th training\\ninput vector\\nxj0\\n:\\n1\\nwi\\n:\\nWeight of the i-th input variable\\nwi(t)\\n:\\nWeight i at the t-th iteration\\nAlgorithm\\nStep 1.\\nInitialize the weights and the threshold. Weights may be initialized to 0 or to a small\\nrandom value.\\nStep 2.\\nFor each example j in the training set D, perform the following steps over the input xj\\nand desired output dj:\\na) Calculate the actual output:\\nyj(t) = f[w0(t)xj0 + w1(t)xj1 + w2(t)xj2 + ⋯+ wn(t)xjn]'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 133}, page_content='CHAPTER 9. NEURAL NETWORKS\\n119\\nb) Update the weights:\\nwi(t + 1) = wi(t) + (dj −yj(t))xji\\nfor all features 0 ≤i ≤n.\\nStep 3.\\nStep 2 is repeated until the iteration error 1\\ns ∑s\\nj=1 ∣dj −yj(t)∣is less than a user-speciﬁed\\nerror threshold γ, or a predetermined number of iterations have been completed, where s\\nis again the size of the sample set.\\nRemarks\\nThe above algorithm can be applied only if the training examples are linearly separable.\\n9.6\\nArtiﬁcial neural networks\\nAn artiﬁcial neural network (ANN) is a computing system inspired by the biological neural networks\\nthat constitute animal brains. An ANN is based on a collection of connected units called artiﬁcial\\nneurons. Each connection between artiﬁcial neurons can transmit a signal from one to another. The\\nartiﬁcial neuron that receives the signal can process it and then signal artiﬁcial neurons connected to\\nit.\\neach connection between artiﬁcial neurons has a weight attached to it that get adjusted as learning\\nproceeds. Artiﬁcial neurons may have a threshold such that only if the aggregate signal crosses that\\nthreshold the signal is sent. Artiﬁcial neurons are organized in layers. Different layers may perform\\ndifferent kinds of transformations on their inputs. Signals travel from the input layer to the output\\nlayer, possibly after traversing the layers multiple times.\\n9.7\\nCharacteristics of an ANN\\nAn ANN can be deﬁned and implemented in several different ways. The way the following charac-\\nteristics are deﬁned determines a particular variant of an ANN.\\n• The activation function\\nThis function deﬁnes how a neuron’s combined input signals are transformed into a single\\noutput signal to be broadcasted further in the network.\\n• The network topology (or architecture)\\nThis describes the number of neurons in the model as well as the number of layers and manner\\nin which they are connected.\\n• The training algorithm\\nThis algorithm speciﬁes how connection weights are set in order to inhibit or excite neurons\\nin proportion to the input signal.\\n9.7.1\\nActivation functions\\nThe activation function is the mechanism by which the artiﬁcial neuron processes incoming informa-\\ntion and passes it throughout the network. Just as the artiﬁcial neuron is modeled after the biological\\nversion, so is the activation function modeled after nature’s design.\\nLet x1, x2, ..., xn be the input signals, w1, w2, ..., wn be the associated weights and −w0 the\\nthreshold. Let\\nx = w0 + w1x1 + ⋯+ wnxn.\\nThe activation function is some function of x. Some of the simplest and commonly used activations\\nare given in Section 9.4.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 134}, page_content='CHAPTER 9. NEURAL NETWORKS\\n120\\n9.7.2\\nNetwork topology\\nBy “network topology” we mean the patterns and structures in the collection of interconnected\\nnodes. The topology determines the complexity of tasks that can be learned by the network. Gener-\\nally, larger and more complex networks are capable of identifying more subtle patterns and complex\\ndecision boundaries. However, the power of a network is not only a function of the network size,\\nbut also the way units are arranged.\\nDifferent forms of forms of network architecture can be differentiated by the following charac-\\nteristics:\\n• The number of layers\\n• Whether information in the network is allowed to travel backward\\n• The number of nodes within each layer of the network\\n1. The number of layers\\nIn an ANN, the input nodes are those nodes which receive unprocessed signals directly from the\\ninput data. The output nodes (there may be more than one) are those nodes which generate the ﬁnal\\npredicted values. A hidden node is a node that processes the signals from the input nodes (or other\\nsuch nodes) prior to reaching the output nodes.\\nThe nodes are arranged in layers. The set of nodes which receive the unprocessed signals from\\nthe input data constitute the ﬁrst layer of nodes. The set of hidden nodes which receive the outputs\\nfrom the nodes in the ﬁrst layer of nodes constitute the second layer of nodes. In a similar way we\\ncan deﬁne the third, fourth, etc. layers. Figure 9.14 shows an ANN with only one layer of nodes.\\nFigure 9.15 shows an ANN with two layers.\\n...\\nx0\\nx1\\nw0\\nw1\\nx2\\nw2\\nxn\\nwn\\nOutput (y)\\nInput layer\\nOutput layer\\nFigure 9.14: An ANN with only one layer\\n2. The direction of information travel\\nNetworks in which the input signal is fed continuously in one direction from connection to connec-\\ntion until it reaches the output layer are called feedforward networks. The network shown in Figure\\n9.15 is a feedforward network.\\nNetworks which allows signals to travel in both directions using loops are called recurrent net-\\nworks (or, feedback networks).\\nIn spite of their potential, recurrent networks are still largely theoretical and are rarely used\\nin practice. On the other hand, feedforward networks have been extensively applied to real-world\\nproblems. In fact, the multilayer feedforward network, sometimes called the Multilayer Perceptron\\n(MLP), is the de facto standard ANN topology. If someone mentions that they are ﬁtting a neural\\nnetwork, they are most likely referring to a MLP.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 135}, page_content='CHAPTER 9. NEURAL NETWORKS\\n121\\nInput\\nlayer\\nHidden\\nlayer\\nOutput\\nlayer\\nx0\\nx1\\nx2\\n⋯\\nxn\\nOutput\\nFigure 9.15: An ANN with two layers\\n3. The number of nodes in each layer\\nThe number of input nodes is predetermined by the number of features in the input data. Similarly,\\nthe number of output nodes is predetermined by the number of outcomes to be modeled or the\\nnumber of class levels in the outcome. However, the number of hidden nodes is left to the user to\\ndecide prior to training the model. Unfortunately, there is no reliable rule to determine the number\\nof neurons in the hidden layer. The appropriate number depends on the number of input nodes, the\\namount of training data, the amount of noisy data, and the complexity of the learning task, among\\nmany other factors.\\n9.7.3\\nThe training algorithm\\nThere are two commonly used algorithms for learning a single perceptron, namely, the perceptron\\nrule and the delta rule. The former is used when the training data set is linearly separable and the\\nlatter when the training data set is not linearly separable.\\nThe algorithm which is now commonly used to train an ANN is known simply as backpropaga-\\ntion.\\n9.7.4\\nThe cost function\\nDeﬁnition\\nIn a machine learning algorithm, the cost function is a function that measures how well the algorithm\\nmaps the target function that it is trying to guess or a function that determines how well the algorithm\\nperforms in an optimization problem.\\nRemaarks\\nThe cost function is also called the loss function, the objective function, the scoring function, or the\\nerror function.\\nExample\\nLet y be the the output variable. Let y1,...,yn be the actual values of y in n examples and ˆy1,..., ˆyn\\nbe the values predicted by an algorithm.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 136}, page_content='CHAPTER 9. NEURAL NETWORKS\\n122\\nInput\\nlayer\\nHidden\\nlayer\\nOutput\\nlayer\\nx0\\nx1\\nx2\\n⋯\\nxn\\nOutput 1\\nOutput 2\\n(a) Network with one hidden layer and two output nodes\\nInput\\nlayer\\nHidden\\nlayer 1\\nHidden\\nlayer 2\\nOutput\\nlayer\\nx0\\nx1\\nx2\\n⋯\\nxn\\nOutput\\n(b) Network with two hidden layers\\nFigure 9.16: Examples of different topologies of networks\\n1. The sum of squares of the differences between the predicted and actual values of y, denoted\\nby SSE and deﬁned below, can be taken as a cost function for the algorithm.\\nSSE =\\nn\\n∑\\ni=1\\n(yi −ˆyi)2.\\n2. The mean of the sum of squares of the differences between the predicted and actual values of\\ny, denoted by MSE and deﬁned below, can be taken as a cost function for the algorithm.\\nMSE = 1\\nn\\nn\\n∑\\ni=1\\n(yi −ˆyi)2.\\n9.8\\nBackpropagation\\nThe backpropagation algorithm was discovered in 1985-86. Here is an outline of the algorithm.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 137}, page_content='CHAPTER 9. NEURAL NETWORKS\\n123\\nFigure 9.17: A simpliﬁed model of the error surface showing the direction of gradient\\n9.8.1\\nOutline of the algorithm\\n1. Initially the weights are assigned at random.\\n2. Then the algorithm iterates through many cycles of two processes until a stopping criterion is\\nreached. Each cycle is known as an epoch. Each epoch includes:\\n(a) A forward phase in which the neurons are activated in sequence from the input layer to\\nthe output layer, applying each neuron’s weights and activation function along the way.\\nUpon reaching the ﬁnal layer, an output signal is produced.\\n(b) A backward phase in which the network’s output signal resulting from the forward phase\\nis compared to the true target value in the training data. The difference between the\\nnetwork’s output signal and the true value results in an error that is propagated backwards\\nin the network to modify the connection weights between neurons and reduce future\\nerrors.\\n3. The technique used to determine how much a weight should be changed is known as gradient\\ndescent method. At every stage of the computation, the error is a function of the weights. If\\nwe plot the error against the wights, we get a higher dimensional analog of something like a\\ncurve or surface. At any point on this surface, the gradient suggests how steeply the error will\\nbe reduced or increased for a change in the weight. The algorithm will attempt to change the\\nweights that result in the greatest reduction in error (see Figure 9.17).\\n9.8.2\\nIllustrative example\\nTo illustrate the various steps in the backpropagation algorithm, we consider a small network with\\ntwo inputs, two outputs and one hidden layer as shown in Figure 9.18.2\\nWe assume that there are two observations:\\nSample\\nInput 1\\nInput 2\\nOutput target 1\\nOutput target 2\\ni1\\ni2\\nT1\\nT2\\n1\\n0.05\\n0.10\\n0.01\\n0.99\\n2\\n0.25\\n0.18\\n0.23\\n0.79\\nWe are required to estimate the optimal values of the weights w1,...,w8,b1,b2. Here b1 and b2 are\\nthe biases. For simplicity, we have assigned the same biases to both nodes in the same layer.\\nStep 1.\\nWe initialise the connection weights to small random values. These initial weights are\\nshown in Figure 9.19.\\n2Thanks\\nto\\nhttps://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-\\nexample/ for this example.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 138}, page_content='CHAPTER 9. NEURAL NETWORKS\\n124\\nInput 1\\nInput 2\\n1\\nOutput 1\\nOutput 2\\n1\\nw1\\nh1\\nw2\\nw5\\nw6\\no1\\nw3\\nw4\\nh2\\nw7\\no2\\nw8\\nb1\\nb2\\nb3\\nb4\\nFigure 9.18: ANN for illustrating backpropagation algorithm\\ni1 = .05\\ni2 = .10\\n1\\nT1 = .01\\nT2 = .99\\n1\\nw1 = .15\\nh1\\nw2 = .20\\nw5 = .40\\no1\\nw6 = .45\\nw3 = .25\\nh2\\nw4 = .30\\nw7 = .50\\nw8 = .55\\no2\\nb1 = .35\\nb2 = .35\\nb3 = .60\\nb4 = .60\\nFigure 9.19: ANN for illustrating backpropagation algorithm with initial values for weights\\nStep 2.\\nPresent the ﬁrst sample inputs and the corresponding output targets to the network. This is\\nshown in Figure 9.19.\\nStep 3.\\nPass the input values to the ﬁrst layer (the layer with nodes h1 and h2).\\nStep 4.\\nWe calculate the outputs from h1 and h2. We use the logistic activation function\\nf(x) =\\n1\\n1 + e−x .\\nouth1 = f(w1 × i1 + w2 × i2 + b1 × 1)\\n= f(0.15 × 0.05 + 0.20 × 0.10 + 0.35 × 1)\\n= f(0.3775)\\n=\\n1\\n1 + e−0.3775\\n= 0.59327\\nouth2 = f(w3 × i1 + w4 × i2 + b2 × 1)\\n= f(0.25 × 0.05 + 0.30 × 0.10 + 0.35 × 1)\\n= f(0.3925)\\n=\\n1\\n1 + e−0.3925\\n= 0.59689'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 139}, page_content='CHAPTER 9. NEURAL NETWORKS\\n125\\nStep 5.\\nWe repeat this process for every layer. We get the outputs from the nodes in the output\\nlayer as follows:\\nouto1 = f(w5 × outh1 + w6 × outh2 + b3 × 1)\\n= f(0.40 × 0.59327 + 0.45 × 0.59689 + 0.60 × 1)\\n= f(1.10591)\\n=\\n1\\n1 + e−1.10591\\n= 0.75137\\nouto2 = f(w7 × outh1 + w8 × outh2 + b4 × 1)\\n= f(0.50 × 0.59327 + 0.55 × 0.59689 + 0.60 × 1)\\n= f(1.22492)\\n=\\n1\\n1 + e−1.22492\\n= 0.77293\\nThe sum of the squares of the output errors is given by\\nE = 1\\n2(T1 −outo1)2 + 1\\n2(T2 −outo2)2\\n= (0.01 −0.75137)2 + (0.99 −0.77293)2\\n= 0.298371\\nStep 6.\\nWe begin backward phase. We adjust the weights. We ﬁrst adjust the weights leading to\\nthe nodes o1 and o2 in the output layer and then the weights leading to the nodes h1 and h2\\nin the hidden layer. The adjusted values of the weights w1,...,w8,b1,...,b4 are denoted\\nby w+\\n1,...,w+\\n8,b+\\n1,...,b+\\n4. The computations use a certain constant η called the learning\\nrate. In the following we have taken η = 0.5.\\n(a) Computation of adjusted weights leading to o1 and o2:\\nδo1 = (T1 −outo1) × outo1 × (1 −outo1)\\n= (0.01 −0.75137) × 0.75137 × (1 −0.75137)\\n= −0.13850\\nw+\\n5 = w5 + η × δo1 × outh1\\n= 0.40 + 0.5 × (−0.13850) × 0.59327\\n= 0.35892\\nw+\\n6 = w6 + η × δo1 × outh2\\n= 0.45 + 0.5 × (−0.13850) × 0.59689\\n= 0.40867\\nb+\\n3 = b3 + η × δo1 × 1\\n= 0.60 + 0.5 × (−0.13850) × 1\\n= 0.53075\\nδo2 = (T2 −outo2) × outo2 × (1 −outo2)\\n= (0.99 −0.77293) × 0.77293 × (1 −0.77293)\\n= 0.03810\\nw+\\n7 = w7 + η × δo2 × outh1\\n= 0.50 + 0.5 × 0.03810 × 0.59327'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 140}, page_content='CHAPTER 9. NEURAL NETWORKS\\n126\\n= 0.51130\\nw+\\n8 = w8 + η × δo2 × outh2\\n= 0.55 + 0.5 × 0.03810 × 0.59689\\n= 0.56137\\nb+\\n4 = b4 + η × δo2 × 1\\n= 0.60 + 0.5 × 0.03810 × 1\\n= 0.61905\\n(b) Computation of adjusted weights leading to h1 and h2:\\nδh1 = (δo1 × w5 + δo2 × w7) × outh1 × (1 −outh1)\\n= (−0.13850 × 0.40 + 0.03810 × 0.50) × 0.59327 × (1 −0.59327)\\n= −0.00877\\nw+\\n1 = w1 + η × δh1 × i1\\n= 0.15 + 0.5 × (−0.00877) × 0.05\\n= 0.14978\\nw+\\n2 = w2 + η × δh1 × i2\\n= 0.20 + 0.5 × (−0.00877) × 0.10\\n= 0.19956\\nb+\\n1 = b1 + η × δh1 × 1\\n= 0.35 + 0.5 × (−0.00877) × 1\\n= 0.34562\\nδh2 = (δo1 × w6 + δo2 × w8) × outh2 × (1 −outh2)\\n= ((−0.13850) × 0.45 + 0.03810 × 0.55) × 0.59689 × (1 −0.59689)\\n= −0.00995\\nw+\\n3 = w3 + η × δh2 × i1\\n= 0.25 + 0.5 × (−0.00995) × 0.05\\n= 0.24975\\nw+\\n4 = w4 + η × δh2 × i2\\n= 0.30 + 0.5 × (−0.00995) × 0.10\\n= 0.29950\\nb+\\n2 = b2 + η × δh2 × 1\\n= 0.35 + 0.5 × (−0.00995) × 1\\n= 0.34503\\nStep 7.\\nNow we set:\\nw1 = w+\\n1,\\nw2 = w+\\n2,\\nw3 = w+\\n3,\\nw4 = w+\\n4\\nw5 = w+\\n5,\\nw6 = w+\\n6,\\nw7 = w+\\n7,\\nw8 = w+\\n8\\nb1 = b+\\n1,\\nb2 = b+\\n2,\\nb3 = b+\\n3,\\nb4 = b+\\n4\\nWe choose the next sample input and the corresponding output targets to the network and\\nrepeat Steps 2 to 6.\\nStep 8.\\nThe process in Step 7 is repeated until the root mean square of output errors is minimised.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 141}, page_content='CHAPTER 9. NEURAL NETWORKS\\n127\\nRemarks\\n1. The constant 1\\n2 is included in the expression for E so that the exponent is cancelled when we\\ndifferentiate it. The result has been multiplied by a learning rate η = 0.5 and so it doesnâ ˘A´Zt\\nmatter that we introduce the constant 1\\n2 in E.\\n2. In the above computations, the method used to calculate the adjusted weights is known as the\\ndelta rule.\\n3. The rule for computing the adjusted weights can be succinctly stated as follows. Let w be a\\nweight and w+ its adjusted weight. Let E be the the total sum of squares of errors. Then w+\\nis computed by\\nw+ = w −η ∂E\\n∂w .\\nHere ∂E\\n∂w is the gradient of E with respect to w; that is, the rate at which E is changing with\\nrespect to w. (The set of all such gradients speciﬁes the direction in which E is decreasing\\nthe most rapidly, that is, the direction of quickest descent.) For example, it can be shown that\\n∂E\\n∂w5\\n= −(T1 −outo1) × outo1 × (1 −outo1) × outh1\\n= −δo1 × outh1\\nand so\\nw+\\n5 = w5 −η ∂E\\n∂w5\\n= w5 + η × δo1 × outh1\\n9.8.3\\nThe algorithm\\nThe backpropagation algorithm trains a given feed-forward multilayer neural network for a given set\\nof input patterns with known classiﬁcations. When each entry of the sample set is presented to the\\nnetwork, the network examines its output response to the sample input pattern. The output response\\nis then compared to the known and desired output and the error value is calculated. Based on the\\nerror, the connection weights are adjusted. The adjustments are based on the mean square error of\\nthe output response to the sample input and it is known as the delta learning rule. The set of these\\nsample patterns are repeatedly presented to the network until the error value is minimized.\\nNotations\\nFigures 9.20 and 9.21 show the various notations used in the algorithm.\\nM\\n: Number of layers (excluding the input layer\\nwhich is assigned the layer number 0)\\nNj\\n: Number of neurons (nodes) in j-th layer\\nXp = (Xp1,Xp2,...,XpN0)\\n: p-th training sample\\nTp = (Tp1,Tp2,...,TpNM )\\n: Known output corresponding to\\nthe p-th training sample\\nOp = (Op1,Op2,...,OpNM )\\n: Actual output by the network corresponding to\\nthe p-th training sample\\nYji\\n: Output from the i-th neuron in layer j\\nWjik\\n: Connection weight from k-th neuron in\\nlayer (j −1) to i-th neuron in layer j\\nδji\\n: Error value associated with the i-th neuron in layer j'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 142}, page_content='CHAPTER 9. NEURAL NETWORKS\\n128\\n⋯\\nTp1\\nOp1\\n⋯\\nTp2\\nOp2\\n⋯\\n⋯\\n⋯\\n⋯\\nTpN0\\nOpN0\\nj(layer #)\\nj = 0\\nj = 1\\nj = M\\nNj(# neurons) N0\\nN1\\nNM\\nXp1\\nXp2\\nXpN0\\nFigure 9.20: Notations of backpropagation algorithm\\nji\\nYij\\n...\\nY(j−1)1\\nY(j−1)2\\nWji1\\nWji2\\nY(j−1)3\\nWji3\\nY(j−1)Nj−1\\nWjiNj−1\\nYij = f (∑\\nNj−1\\nk=1 Y(j−1)kWjik)\\nδij\\nFigure 9.21: Notations of backpropagation algorithm: The i-th node in layer j\\nThe algorithm\\nStep 1.\\nInitialize connection weights into small random values.\\nStep 2.\\nPresent the pth sample input vector of pattern\\nXp = (Xp1,Xp2,...,XpN0)\\nand the corresponding output target\\nTp = (Tp1,Tp2,...,TpNM )\\nto the network.\\nStep 3.\\nPass the input values to the ﬁrst layer, layer 1. For every input node i in layer 0, perform:\\nY0i = Xpi.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 143}, page_content='CHAPTER 9. NEURAL NETWORKS\\n129\\nStep 4.\\nFor every neuron i in every layer j = 1,2,...,M, ﬁnd the output from the neuron:\\nYji = f (∑\\nNj−1\\nk=1 Y(j−1)kWjik),\\nwhere\\nf(x) =\\n1\\n1 + exp(−x).\\nStep 5.\\nObtain output values. For every output node i in layer M, perform:\\nOpi = YMi.\\nStep 6.\\nCalculate error value δji for every neuron i in every layer in backward order j = M,M −\\n1,...,2,1, from output to input layer, followed by weight adjustments. For the output\\nlayer, the error value is:\\nδMi = YMi(1 −YMi)(Tpi −YMi),\\nand for hidden layers:\\nδji = Yji(1 −Yji)∑\\nNj+1\\nk=1 δ(j+1)kW(j+1)ki.\\nThe weight adjustment can be done for every connection from neuron k in layer (j −1) to\\nevery neuron j in every layer i:\\nW +\\njik = Wjik + ηδjiYji,\\nwhere η represents weight adjustment factor (called the learning rate) normalized between\\n0 and 1.\\nStep 7.\\nThe actions in steps 2 through 6 will be repeated for every training sample pattern p, and\\nrepeated for these sets until the sum of the squares of output errors is minimized.\\n9.9\\nIntroduction to deep learning\\n9.9.1\\nDeﬁnition\\nA neural network with multiple hidden layers is called a Deep Neural Network (DNN) and the\\npractice of training such network is referred to as deep learning.\\nRemarks\\nIn the terminology “deep learning”, the term “deep” is a technical term. It refers to the number of\\nlayers in a neural network. A shallow network has one so-called hidden layer, and a deep network\\nhas more than one. Multiple hidden layers allow deep neural networks to learn features of the data\\nin a so-called feature hierarchy, because simple features recombine from one layer to the next, to\\nform more complex features. Networks with many layers pass input data (features) through more\\nmathematical operations than networks with few layers, and are therefore more computationally\\nintensive to train. Computational intensivity is one of the hallmarks of deep learning.\\nFigure 9.22 shows a shallow neural network and Figure 9.23 shows a deep neural network with\\nthree hidden layers.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 144}, page_content='CHAPTER 9. NEURAL NETWORKS\\n130\\nFigure 9.22: A shallow neural network\\nFigure 9.23: A deep neural network with three hidden layers\\n9.9.2\\nSome applications\\nDeep learning applications are used in industries from automated driving to medical devices.\\n1. Automated driving:\\nAutomotive researchers are using deep learning to automatically detect objects such as stop\\nsigns and trafﬁc lights. In addition, deep learning is used to detect pedestrians, which helps\\ndecrease accidents.\\n2. Aerospace and defense:\\nDeep learning is used to identify objects from satellites that locate areas of interest, and iden-\\ntify safe or unsafe zones for troops.\\n3. Medical research:\\nCancer researchers are using deep learning to automatically detect cancer cells. Teams at\\nUCLA built an advanced microscope that yields a high-dimensional data set used to train a\\ndeep learning application to accurately identify cancer cells.\\n4. Industrial automation:\\nDeep learning is helping to improve worker safety around heavy machinery by automatically\\ndetecting when people or objects are within an unsafe distance of machines.\\n5. Electronics:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 145}, page_content='CHAPTER 9. NEURAL NETWORKS\\n131\\nDeep learning is being used in automated hearing and speech translation. For example, home\\nassistance devices that respond to your voice and know your preferences are powered by deep\\nlearning applications.\\n9.10\\nSample questions\\n(a) Short answer questions\\n1. Explain the biological motivation for the formulation of the concept of artiﬁcial neural net-\\nworks.\\n2. With the aid of a diagram, explain the concept of an artiﬁcial neuron.\\n3. What is an activation function in an artiﬁcial neuron? Give some examples.\\n4. Deﬁne a perceptron.\\n5. Is neural network supervised or unsupervised learning? Why?\\n6. Is deep learning supervised or unsupervised? Why?\\n7. What is the basic idea of the backpropagation algorithm?\\n8. In the context of ANNs, what is meant by network topology?\\n9. Explain the different types of layers in an ANN.\\n10. What is the gradient descent method? How is used in the backpropagation algorithm?\\n11. A neuron with 4 inputs has the weights 1,2,3,4 and bias 0. The activation function is linear,\\nsay the function f(x) = 2x. If the inputs are 4,8,5,6, compute the output. Draw a diagram\\nrepresenting the neuron.\\n(b) Long answer questions\\n1. Design a two layer network of perceptrons to implement A XOR B.\\n2. Explain the backpropagation algorithm.\\n3. Describe the perceptron learning algorithm.\\n4. What are the characteristics of an artiﬁcial neural networks.\\n5. Explain the concept of deep learning. Give some real life problems where this concept has\\nbeen successfully applied.\\n6. Compute the output of the following neuron if the activation function is (i) the threshold\\nfunction (ii) the sigmoid function (iii) the hyperbolic tangent function (assume the same bias\\n0.5 for each node).\\nx0 = 3.5\\nx1 = 2.9\\nw0 = 0.89\\nw1 = −2.07\\nx2 = 1.2\\nw2 = 0.08\\nOutput (y)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 146}, page_content='CHAPTER 9. NEURAL NETWORKS\\n132\\n7. Which of the boolean functions AND, OR, XOR (or none of these) is represented by the\\nfollowing network of perceptrons (with unit step function as the activation function)?\\nx1\\nw1 = 1\\nx2\\nw2 = 1\\nw3 = −1\\nw4 = −1\\nw5 = 3\\nb1 = −0.5\\nb2 = −0.5\\nb4 = 0.5\\nb3 == −1.5\\nOutput (y)\\n8. Given the following network, compute the outputs from o1 and o2 (assume that the activation\\nfunction is the sigmoid function).\\nInput i1 = .25\\nInput i2 = .30\\n1\\nOutput 1\\nOutput 2\\n1\\nw1 = .17\\nh1\\nw2 = .21\\nw5 = .52\\no1\\nw6 = .61\\nw3 = .18\\nh2\\nw4 = .27\\nw7 = .55\\nw8 = .72\\no2\\nb1 = .12\\nb2 = .24\\nb3 = .48\\nb4 = .36\\n9. (Assignment question) Given the following data, use ANN with one hidden layer, appropriate\\ninitial weights and biases to compute the optimal values of the weights. Perform one iteration\\nof the forward and phases of the backpropagation algorithm for each samples.\\nSample\\nInput 1\\nInput 2\\nOutput target 1\\nOutput target 2\\n1\\n1.20\\n2.30\\n0.53\\n0.76\\n2\\n0.23\\n0.37\\n1.17\\n2.09'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 147}, page_content='Chapter 10\\nSupport vector machines\\nWe begin this chapter by illustrating the basic concepts and terminology of the theory of support\\nvector machines by a simple example. We then introduce the necessary mathematical background,\\nwhich is essentially an introduction to ﬁnite dimensional vector spaces, for describing the general\\nconcepts in the theory of support vector machines. The related algorithms without proofs are then\\npresented.\\n10.1\\nAn example\\n10.1.1\\nProblem statement\\nSuppose we want to develop some criteria for determining the weather conditions under which tennis\\ncan be played. To simplify the matters it has been decided to use the measures of temperature and\\nhumidity as the critical parameters for the investigation. We have some data as given in Table 10.1\\nregarding the values of the parameters and the decisions taken as to whether to play tennis or not.\\nWe are required to develop a criteria to know whether one would be playing tennis on a future date\\nif we know the values of the temperature and humidity of that date in advance.\\n10.1.2\\nDiscussion and solution\\nWe shall now see the various steps that lead to a solution of the problem using the ideas of support\\nvector machines.\\ntemperature\\nhumidity\\nplay\\n85\\n85\\nno\\n60\\n70\\nyes\\n80\\n90\\nno\\n72\\n95\\nno\\n68\\n80\\nyes\\n74\\n73\\nyes\\n69\\n70\\nyes\\n75\\n85\\nno\\n83\\n78\\nno\\nTable 10.1: Example data with two class labels\\n133'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 148}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n134\\n1. Two-class data set\\nThis is our ﬁrst observation regarding the data in Table 10.1. In Table 10.1, the data are classiﬁed\\nbased on the values of the variable “play”. This variable has only two values or labels, namely “yes”\\nand ”no”. When there are only two class labels the data is said to be a “two-class data set”. So the\\ndata in Table 10.1 is a two-class data set.\\n2. Scatter plot of the data\\nSince there are only two features or parameters, we may plot the values of one of the parameters, say\\n“temperature”, along the horizontal axis (that is, the x-axis) and the values of the other parameter\\n“humidity”, along the vertical axis (that is, the y-axis). The data can be plotted in a coordinate plane\\nto get a scatter plot of the data. Figure 10.1 shows the scatter plot. In the ﬁgure the points which\\ncorrespond to the decision “yes” on playing tennis has been plotted as ﬁlled squares (◾) and which\\ncorrespond to the decision “no” has been marked as hollow circles (○).\\nFigure 10.1: Scatter plot of data in Table 10.1 (ﬁlled circles represent “yes” and unﬁlled circles\\n“no”)\\n3. A separating line\\nIf we examine the plot in Figure 10.1, we can see that we can draw a straight line in the plane\\nseparating the two types of points in the sense that all points plotted as ﬁlled squares are on one side\\nof the line and all points marked as hollow circles are on the other side of the line. Such a line is\\ncalled a “separating line” for the data. Figure 10.2 shows a separating line for the data in Table 10.1.\\nThe equation of the separating line shown in Figure 10.2 is\\n5x + 2y −535 = 0.\\n(10.1)\\nIt has the following property:\\n• If the data point with values (x′,y′) has the value “yes” for “play” (ﬁlled square), then\\n5x′ + 2y′ −535 < 0.\\n(10.2)\\n• If the data point with values (x,y) has the value “no” for “play” (hollow circle), then\\n5x′ + 2y′ −535 > 0.\\n(10.3)\\nIf such a separating line exists for a given data then the data is said to be “linearly separable”.\\nThus the data in table 10.1 is linearly separable. However note that not all data are linearly separable.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 149}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n135\\nFigure 10.2: Scatter plot of data in Table 10.1 with a separating line\\n4. Several separating lines\\nApparently, the conditions given in Eqs. (10.2) and (10.3) may be used as the criteria to know\\nwhether one would be playing tennis on a future date if we know the values of the temperature\\nand humidity of that date in advance. But there are several separating lines and the problem of\\ndetermining which one to choose arises. Figure 10.3 shows two separating lines for the given data.\\nFigure 10.3: Two separating lines for the data in Table 10.1\\n4. Margin of a separating line\\nTo choose the “best” separating line, we introduce the concept of the margin of a separating line.\\nGiven a separating line for the data, we consider the perpendicular distances of the data points\\nfrom the separating line. Th double of the shortest perpendicular distance is called the “margin of the\\nseparating line”. Figure ?? shows some of the perpendicular distances and the shortest perpendicular\\ndistance for the data in Table 10.1 and for the separating line given by Eq. (10.1).\\n5. Maximal margin separating line\\nThe “best” separating line is the one with the maximum margin.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 150}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n136\\nFigure 10.4: Shortest perpendicular distance of a separating line from data points\\nThe separating line with the maximum margin is called the “maximum margin line” or the “op-\\ntimal separating line”. This line is also called the “support vector machine” for the data in Table\\n10.1.\\nUnfortunately, ﬁnding the equation of the maximum margin line is not a trivial problem. Figure\\n10.5 shows the maximum margin line for the data in Table 10.1. The equation of the maximum\\nmargin line can be shown to be\\n7x + 6y −995.5 = 0.\\n(10.4)\\nFigure 10.5: Maximum margin line for data in Table 10.1\\n6. Support vectors\\nThe data points which are closest to the maximum margin line are called the “support vectors”. The\\nsupport vectors are shown in Figure 10.6.\\n7. The required criterion\\nAs per theory of support vector machines, the equation of the maximum margin line is used to\\ndevise a criterion for taking a decision on whether to play tennis or not. Let x′ and y′ be the values'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 151}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n137\\nFigure 10.6: Support vectors for data in Table 10.1\\nof temperature and humidity on a given day. Then the decision as to whether play tennis on that day\\nis “yes” if\\n7x + 6y −995.5 < 0\\nand “no” if\\n7x + 6y −995.5 > 0.\\n8. “Street” of maximum width separating “yes” points and “no” points\\nConsidering Figure 10.6, we may draw a line through the support vectors 1 and 2 parallel to the\\nmaximum margin line, and a line through support vector 3 parallel to the maximum margin line.\\nThe two lines are shown as dashed lines in Figure 10.7. The region between these two dashed lines\\ncan be thought of as a “road” or a “street” of maximum width that separates the “yes” data points\\nand the “no” data points.\\nFigure 10.7: Boundaries of “street” of maximum width separating “yes” points and “no” points in\\nTable 10.1'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 152}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n138\\n9. Final comments\\ni) Any line given an equation of the form\\nax + by + c = 0\\nseparates the coordinate plane into two halves. One half consists of all points for which\\nax + by + c > 0 and the other half consists of all points for which ax + by + c < 0. Which half\\nis which depends the signs of the coefﬁcients a,b,c.\\nii) Figure 10.8 shows the plot of the maximum margin line produced using the R programming\\nlanguage.\\nFigure 10.8: Plot of the maximum margin line of data in Table 10.1 produced by the R programming\\nlanguage\\niii) In the sections below, we generalise the concepts introduced above to data sets having more\\nthan two features.\\n10.2\\nFinite dimensional vector spaces\\nIn Section 10.1 we have geometrically examined in detail the concepts of the theory of support\\nvector machines with an example having only two features. But, obviously, such a geometrical\\napproach is infeasible if there are more than two features. In such cases we have to resort to formal\\nalgebraic/mathematical formalism to investigate the problem. The theory of what are known as\\n“ﬁnite dimensional vector spaces” provides such a formalism. We present below the absolutely\\nessential parts of this theory. Those who are interested in learning about the abstract concept of a\\nvector space may refer to any well written book on linear algebra.\\n10.2.1\\nDeﬁnition\\nWe give the deﬁnition of a ﬁnite dimensional vector space here. We once again warn the reader\\nthat we are introducing the terms with reference to a very special case of a ﬁnite dimensional vector'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 153}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n139\\nspace and that all the terms given below have more general meanings.\\nDeﬁnition\\nLet n be a positive integer. By a n-dimensional vector we mean an ordered n-tuple of real numbers\\nof the form (x1,x2,...,xn). We denote vectors by ⃗x, ⃗y, etc. In the vector ⃗x = (x1,x2,...,xn), the\\nnumbers x1,x2,...xn are called the coordinates or the components of ⃗x. In the following, we call\\nreal numbers as scalars.\\nThe set of all n-dimensional vectors with the operations of addition of vectors and multiplication\\nof a vector by a scalar and with the deﬁnitions of the zero vector and the negative of a vector as\\ndeﬁned below is a n-dimensional vector space. It is denoted by Rn.\\n1. Addition of vectors\\nLet ⃗x = (x1,x2,...,xn) and ⃗y = (y1,y2,...,yn) be two n-dimensional vectors. The sum of\\n⃗x and ⃗y, denoted by ⃗x + ⃗y, is deﬁned by\\n⃗x + ⃗y = (x1 + y1,x2 + y2,...,xn + yn).\\n2. Multiplication by scalar\\nLet α be a scalar and ⃗x = (x1,x2,...,xn) be a n-dimensional vector. The product of ⃗x by α,\\ndenoted by α⃗x, is deﬁned by\\nα⃗x = (αx1,αx2,...,αxn).\\nWhen we write the product of ⃗x by α, we always write the scalar α on the left side of the\\nvector ⃗x as we have done above.\\n3. The zero vector\\nThe n-dimensional vector (0,0,...,0), which has all components equal to 0, is called the\\nzero vector. It is also denoted by 0. From the context of the usage we can understand whether\\n0 denotes the scalar 0 or the zero vector.\\n4. Negative of a vector\\nLet ⃗x = (x1,x2,...,xn) be any n-dimensional vector. The negative of ⃗x is a vector denoted\\nby −⃗x and is deﬁned by\\n−⃗x = (−x1,−x2,...,−xn).\\nWe write ⃗x + (−⃗y) as ⃗x −⃗y.\\n10.2.2\\nProperties\\nLet n be a positive integer. Let ⃗x, ⃗y, ⃗z be arbitrary vectors in Rn and let α,β,γ be arbitrary scalars.\\n1. Closure under addition: ⃗x + ⃗y is also a n-dimensional vector.\\n2. Commutativity: ⃗x + ⃗y = ⃗y + ⃗x\\n3. Associativity: ⃗x + (⃗y + ⃗z) = (⃗x + ⃗y) + ⃗z\\n(Because of this property, we can write the sums ⃗x + (⃗y + ⃗z) and (⃗x + ⃗y) + ⃗z in the form\\n⃗x + ⃗y + ⃗z.)\\n4. Existence of identity for addition: ⃗x + 0 = ⃗x\\n5. Existence of inverse for addition: ⃗x + (−⃗x) = 0\\n6. Closure under scalar multiplication: α⃗x is also a n-dimensional vector.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 154}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n140\\n7. Compatibility of multiplication of a vector by a scalar with multiplication of scalars: α(β⃗x) =\\n(αβ)⃗x\\n8. Distributivity of scalar multiplication over vector addition: α(⃗x + ⃗y) = α⃗x + α⃗y\\n9. Distributivity of scalar multiplication over addition of scalars: (α + β)⃗x = α⃗x + β⃗x\\n10. Existence of identity element for scalar multiplication: 1⃗x = ⃗x\\nExample of computation\\nLet n = 3. Let ⃗x = (−1,2,3), ⃗y = (2,0,−1), ⃗z = (1,1,0), α = 2,β = −3,γ = 4 and λ = 5. The\\nexpression λ(α⃗x+β⃗y+γ⃗z) can be computed in several different ways. One of the methods is shown\\nbelow.\\nλ(α⃗x + β⃗y + γ⃗z) = 5(2(−1,2,3) + (−3)(2,0,−1) + 4(1,1,0))\\n= 5((−2,4,6) + (−6,0,3) + (4,4,0))\\n= 5((−8,4,9) + (4,4,0))\\n= 5(−4,8,9)\\n= (−20,40,45)\\n10.2.3\\nNorm and inner product\\n1. Norm\\nThe norm of the n-dimensional vector ⃗x = (x1,x2,...,xn), denoted by ∣∣⃗x∣∣, is deﬁned by\\n∥⃗x∥=\\n√\\nx2\\n1 + x2\\n2 + ⋯+ x2n.\\n2. Inner product\\nThe inner product of ⃗x = (x1,x2,...,xn) and ⃗y = (y1,y2,...,yn), denoted by ⃗x⋅⃗y, is deﬁned\\nby\\n⃗x ⋅⃗y = x1y1 + x2y2 + ⋯+ xnyn.\\nNote that we have\\n∥⃗x∥=\\n√\\n⃗x ⋅⃗x.\\n3. Angle between two vectors\\nThe angle θ between two vectors ⃗x and ⃗y is deﬁned by\\ncosθ =\\n⃗x ⋅⃗y\\n∥⃗x∥∥⃗y∥.\\n4. Perpendicularity\\nTwo vectors ⃗x = (x1,x2,...,xn) and ⃗y = (y1,y2,...,yn) are said to be perpendicular (or,\\northogonal) if\\n⃗x ⋅⃗y = 0.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 155}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n141\\nExample\\nLet n = 4 and let ⃗x = (−1,2,0,3) and ⃗y = (2,3,1,−4).\\n∥⃗x∥=\\n√\\n(−1)2 + 22 + 02 + 32\\n=\\n√\\n14\\n∥⃗y∥=\\n√\\n22 + 32 + 12 + (−4)2\\n=\\n√\\n30\\n⃗x ⋅⃗y = (−1) × 2 + 2 × 3 + 0 × 1 + 3 × (−4)\\n= −8\\ncosθ =\\n−8\\n√\\n14\\n√\\n30\\n= −0.39036\\nθ = 112.98 degrees\\nSince ⃗x ⋅⃗y ≠0 the vectors ⃗x and ⃗y are not orthogonal.\\n10.3\\nHyperplanes\\nHyperplanes are certain subsets of ﬁnite dimensional vector spaces which are similar to straight lines\\nin planes and planes in three-dimensional spaces.\\n10.3.1\\nDeﬁnition\\nConsider the n-dimensional vector space Rn. The set of all vectors\\n⃗x = (x1,x2,...,xn)\\nin Rn whose components satisfy an equation of the form\\nα0 + α1x1 + α2x2 + ⋯+ αnxn = 0,\\n(10.5)\\nwhere α0,α1,α2,...,αn are scalars, is called a hyperplane in the vector space Rn.\\nRemarks 1\\nLet ⃗x = (x1,x2,...,xn) and ⃗α = (α1,α2,...,αn), then using the notation of inner product,\\nEq.(10.5) can be written in the following form:\\nα0 + ⃗α ⋅⃗x = 0.\\nRemarks 2\\nThe hyperplane in Rn deﬁned by Eq.(10.5) divides the space Rn into two disjoint halves. One of\\nthe two halves consists of all vectors ⃗x for which\\nα0 + α1x1 + α2x2 + ⋯+ αnxn > 0\\nand the other half consists of all vectors ⃗x for which\\nα0 + α1x1 + α2x2 + ⋯+ αnxn < 0.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 156}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n142\\n10.3.2\\nSpecial cases\\nHyperplanes in 2-dimensional vector spaces: Straight lines\\nConsider the 2-dimensional vector space R2. Vectors in this space are ordered pairs of the form\\n(x1,x2). Choosing appropriate coordinate axes, such a vector can be represented by a point with\\ncoordinates ⃗x = (x1,x2) in the plane. So, the vector space R2 can be identiﬁed with the set of points\\nin a plane. In this special case, the norm∥x∥is the distance of the point (x1,x2) in the plane from\\nthe origin. The angle between the vectors ⃗x = (x1,x2) and ⃗y = (y1,y2) is the angle between the\\nlines joining the origin to the points (x1,x2) and (y1,y2).\\nConsider the set of all vectors ⃗x = (x1,x2) in R2 which satisfy the following equation:\\nα0 + α1x1 + α2x2 = 0\\nwhere α0 +α1,α2 are scalars. From elementary analytical geometry we can see that the correspond-\\ning set of points in the plane form a straight line in the plane. This straight line divides the plane\\ninto two disjoint halves (see Figure 10.9). It can be proved that one of the two halves consists of all\\npoints for which\\nα0 + α1x1 + α2x2 > 0\\nand the other half consists of all points for which\\nα0 + α1x1 + α2x2 < 0.\\nx1\\nx2\\nO\\nEquation of line:\\nα0 + α1x1 + α2x2 = 0\\n(assume α0 < 0)\\nHalf plane where\\nα0 + α1x1 + α2x2 < 0\\nHalf plane where\\nα0 + α1x1 + α2x2 > 0\\nFigure 10.9: Half planes deﬁned by a line\\nHyperplanes in 3-dimensional vector spaces: Planes\\nConsider the 3-dimensional vector space R3. Vectors in this space are ordered triples of the form\\n(x1,x2,x3). Choosing appropriate coordinate axes, such a vector can be represented by a point\\nwith coordinates ⃗x = (x1,x2,x3) in the ordinary three-dimensional space. So, the vector space R3\\ncan be identiﬁed with the set of points in the three-dimensional space. As in the case of R2, the\\nnorm ∥x∥is the distance of the point (x1,x2,x3) from the origin. The angle between the vectors\\n⃗x = (x1,x2,x3) and ⃗y = (y1,y2,y3) is the angle between the lines joining the origin to the points\\n(x1,x2,x3) and (y1,y2,y3).\\nConsider the set of all vectors ⃗x = (x1,x2,x3) in R3 which satisfy the following equation:\\nα0 + α1x1 + α2x2 + α3x3 = 0'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 157}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n143\\nwhere α0,α1,α2,α3 are scalars. From elementary analytical geometry we can see that the corre-\\nsponding set of points in space form a plane. This plane divides the space into two disjoint halves.\\nIt can be proved that one of the two halves consists of all points for which\\nα0 + α1x1 + α2x2 + α3x3 > 0\\nand the other half consists of all points for which\\nα0 + α1x1 + α2x2 + α3x3 < 0.\\nGeometry of hyperplanes in n-dimensional vector spaces\\nBy analogy with a plane (which is a geometrical object having two dimensions) and the space of\\nour experience (which is a geometrical world having three dimensions) we imagine that there is a\\ngeometrical world or object having n dimensions for any value of n. We also imagine that the points\\nin this world can be represented by ordered n tuples of the form ⃗x = (x1,x2,...,xn). We now\\nidentify the set of n-dimensional vectors with the points in this geometrical world of n-dimensions.\\nBecause of this identiﬁcation, vectors in the n-dimensional vector space Rn are also referred as\\npoints in a n-dimensional space. The hyperplanes in Rn are deﬁned by analogy with the geometrical\\nstraight lines and planes.\\n10.3.3\\nDistance of a hyperplane from a point\\nIn two-dimensional space, that is, in a plane, using elementary analytical geometry, it can be shown\\nthat the perpendicular distance PN of a point P(x′\\n1,y′\\n1) from a line\\nα0 + α1x1 + α2x2 = 0\\nis given by\\nPN = ∣α0 + α1x′\\n1 + α2x′\\n2∣\\n√\\nα2\\n1 + α2\\n2\\n.\\nSimilarly, in three-dimensional space, using elementary analytical geometry, it can be shown that\\nthe perpendicular distance PN of a point P(x′\\n1,x′\\n2,x′\\n3) from a plane\\nα0 + α1x1 + α2x2 + α3x3 = 0\\nis given by (see Figure 10.10)\\nPN = ∣α0 + α1x′\\n1 + α2x′\\n2 + α3x′\\n3∣\\n√\\nα2\\n1 + α2\\n2 + α2\\n3\\n.\\nα0 + α1x1 + α2x2 + α3x3 = 0\\nN\\nP(x′\\n1,x′\\n2,x′\\n3)\\nFigure 10.10: Perpendicular distance of a point from a plane\\nMotivated by these special cases, we introduce the following deﬁnition.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 158}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n144\\nDeﬁnition\\nIn Rn, the perpendicular distance PN of a point P(x′\\n1,x′\\n2,...,x′\\nn) from a hyperplane\\nα0 + α1x1 + α2x2 + ... + αnxn = 0\\nis given by\\nPN = ∣α0 + α1x′\\n1 + α2x′\\n2 + ... + αnx′\\nn∣\\n√\\nα2\\n1 + α2\\n2 + ... + α2n\\n.\\n(10.6)\\nRemarks\\nLet ⃗x′ = (x′\\n1,x′\\n2,...,x′\\nn) and ⃗α = (α1,α2,...,αn), then using the notations of inner product and\\nnorm, Eq.(10.6) can be written in the following form:\\nPN = ∣α0 + ⃗α ⋅⃗x′∣\\n∥⃗x′∥\\n.\\n10.4\\nTwo-class data sets\\nIn a machine learning problem, the variable being predicted is called the output variable, the target\\nvariable, the dependent variable or the response. A two-class data set is a data set in which the\\ntarget variable takes only one of two possible values only. If the target variable takes more than two\\npossible values, the data set is called a multi-class dataset.\\nIn a two-class data set, the set of values of the target variable may be {“yes”, “no”}, or {“TRUE”, ”FALSE”},\\nor {0,1}, or {−1,+1} or any such similar set.\\nThe methods of support vector machines were originally developed for classiﬁcation problems\\ninvolving two-class data sets. So in this chapter we consider mainly two-class data sets.\\n10.5\\nLinearly separable data\\n10.5.1\\nDeﬁnitions\\nConsider a two-class data set having n numeric features and two possible class labels −1 and +1.\\nLet the vector ⃗x = (x1,...,xn) represent the values of the features in one instance of the data set.\\nWe say that the data set is linearly separable if we can ﬁnd a hyperplane in the n-dimensional vector\\nspace Rn, say\\nα0 + α1x1 + α2x2 + ⋯+ αnxn = 0\\n(10.7)\\nhaving the following two properties:\\n1. For each instance ⃗x with class label −1 we have\\nα0 + α1x1 + α2x2 + ⋯+ αnxn < 0.\\n2. For each instance ⃗x with class label +1 we have\\nα0 + α1x1 + α2x2 + ⋯+ αnxn > 0.\\nA hyperplane given by Eq.(10.7) having the two properties given above is called a separating hy-\\nperplane for the data set.\\nRemarks 1\\nIf a data set with two class labels is linearly separable, then, in general, there will be several sepa-\\nrating hyperplanes for the data set. This is illustrated in the example below.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 159}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n145\\nRemarks 2\\nGiven a two-class data set, there is no simple method for determining whether the data set is linearly\\nseparable. One of the efﬁcient ways for doing this is to apply the methods of linear programming.\\nWe omit the details.\\n10.5.2\\nExample\\nExample 1\\nWe have seen in Section 10.1 that the data in Table 10.1 is linearly separable.\\nExample 2\\nShow that the data set given in Table 10.2 is not separable.\\nx\\ny\\nClass label\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n1\\n1\\n1\\n0\\nTable 10.2: Example of a two-class data that is not linearly separable\\nSolution\\nThe scatterplot of data in TableTableVXOR shown in Figure 10.11 shows that the data is not linearly\\nseparable.\\nFigure 10.11: Scatterplot of data in Table 10.2\\n10.6\\nMaximal margin hyperplanes\\n10.6.1\\nDeﬁnitions\\nConsider a linearly separable data set having two class labels “−1” and “+1”. Consider a separating\\nhyperplane H for the data set.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 160}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n146\\n1. Consider the perpendicular distances from the training instances to the separating hyperplane\\nH and consider the smallest such perpendicular distance. The double of this smallest distance\\nis called the margin of the separating hyperplane H.\\n2. The hyperplane for which the margin is the largest is called the maximal margin hyperplane\\n(also called maximum margin hyperplane) or the optimal separating hyperplane.\\n3. The maximal margin hyperplane is also called the support vector machine for the data set.\\n4. The data points that lie closest to the maximal margin hyperplane are called the support vec-\\ntors.\\nFigure 10.12: Maximal separating hyperplane, margin and support vectors\\n10.6.2\\nSpecial cases\\nTo ﬁx ideas, let us consider two special datasets in 2-dimensional space, namely, datasets having 2\\nand 3 examples.\\nDataset with two examples\\nConsider the dataset in Table 10.3.\\nExample no.\\nx1\\nx2\\nClass\\n1\\n2\\n1\\n+1\\n2\\n4\\n3\\n−1\\nTable 10.3: 2-dimensional dataset with 2 examples\\nGeometrically it can be easily seen that the maximum margin hyperplane for this data is the\\nperpendicular bisector of the line segment joining the points (2,1) and (4,3) (see Figure 10.13).\\nThis is true for any two-sample dataset in two-dimensional space.\\nDataset with three examples\\nConsider a dataset with three examples from a two-dimensional space. Let these examples corre-\\nspond to the points A,B,C in the coordinate plane. Two of these examples, say B and C, must have\\nthe same class label say +1 and the other point A must have a different class label, say −1.\\nThe maximal margin hyperplane of the dataset can be obtained as follows. Draw the line joining\\nB and C and draw the line through A parallel to BC. The line midway between these two lines in\\nthe maximal margin hyperplane of the three-sample dataset'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 161}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n147\\nx1\\nx2\\nA(2,1)\\nB (4,3)\\n(3,2) Midpoint of AB\\n(0,0)\\nMaximum margin hyperplane:\\nx1 + x2 −5 = 0\\nFigure 10.13: Maximal margin hyperplane of a 2-sample set in 2-dimensional space\\nx1\\nx2\\nA(2,2)\\nB (4,5)\\nC (7,4)\\n(0,0)\\nMaximal margin hyperplane\\nx1 + 3x2 −27\\n2 = 0\\nFigure 10.14: Maximal margin hyperplane of a 3-sample set in 2-dimensional space\\n10.7\\nMathematical formulation of the SVM problem\\nThe SVM problem is the problem of ﬁnding the equation of the SVM, that is, the maximal margin\\nhyperplane, given a linearly separable two-class data set. By the very deﬁnition of SVM, this is an\\noptimisation problem. The give below the mathematical formulation of this optimisation problem.\\n10.7.1\\nNotations and preliminaries\\n• Assume that we are given a two-class training dataset of N points of the form\\n(⃗x1,y1),(⃗x2,y2),...,(⃗xN,yN).\\nwhere the yi’s are either +1 or 1 (the class labels). Each ⃗xi is a n-dimensional real vector.\\n• We assume that the dataset is linearly separable.\\n• Any hyperplane can be written as the set of points ⃗x = (x1,...,xn) satisfying an equation of\\nthe form\\n⃗w ⋅⃗x −b = 0.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 162}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n148\\n• Since the training data is linearly separable, we can select two parallel hyperplanes that sep-\\narate the two classes of data, so that the distance between them is as large as possible. The\\nmaximum margin hyperplane is the hyperplane that lies halfway between them. It can be\\nshown that these hyperplanes can be described by equations of the following forms:\\n⃗w ⋅⃗x −b = +1\\n(10.8)\\n⃗w ⋅⃗x −b = −1\\n(10.9)\\n• For any point on or “above” the hyperplane Rq.(10.8), the class label is +1. This implies that\\n⃗w ⋅⃗xi −b ≥+1, if yi = +1\\n(10.10)\\nSimilarly, for any point on or “below” the hyperplane Eq.(10.9), the class label is −1. This\\nimplies that\\n⃗w ⋅⃗xi −b ≤−1, if yi = −1.\\n(10.11)\\n• The two conditions in Eq.10.10 and Eq.10.11 can be written as a single condition as follows:\\nyi( ⃗w ⋅⃗xi −b) ≥1,\\nfor all 1 ≤i ≤N.\\n• Now, the distance between the two hyperplanes in Eq.(10.8) and Eq.(10.9) is\\n2\\n∥⃗w∥.\\nSo, to maximize the distance between the planes we have to minimize ∥⃗w∥. Further we also\\nnote that ∥⃗w∥is minimum when 1\\n2∥⃗w∥2 is minimum. (The square of the norm is used to avoid\\nsquare-roots and the factor “ 1\\n2” is introduced to simplify certain expressions.)\\n10.7.2\\nFormulation of the problem\\nBased on the above discussion, we now formulate the SVM problem as the following optimization\\nproblem.\\nProblem\\nGiven a two-class linearly separable dataset of N points of the form\\n(⃗x1,y1),(⃗x2,y2),...,(⃗xN,yN).\\nwhere the yi’s are either +1 or 1, ﬁnd a vector ⃗w and a number b which\\nminimize\\n1\\n2∥⃗w∥2\\nsubject to\\nyi( ⃗w ⋅⃗xi −b) ≥1, for i = 1,...N\\n10.7.3\\nThe SVM classiﬁer\\nThe solution of the SYM problem gives us a claasiﬁer for classifying unclassiﬁed data instances.\\nThis is known as the SVM classiﬁer for a given dataset.\\nThe classiﬁer\\nLet ⃗w = ⃗w∗and b = b∗be a solution of the SVM problem. Let ⃗x be an unclassiﬁed data instance.\\n• Assign the class label +1 to ⃗x if ⃗w∗⋅⃗x −b∗> 0.\\n• Assign the class label −1 to ⃗x if ⃗w∗⋅⃗x −b∗< 0.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 163}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n149\\n10.8\\nSolution of the SVM problem\\nThe SVM optimization problem as formulated above is an example of a constrained optimization\\nproblem. The general method for solving it is to convert it into a quadratic programming problem\\nand then apply the algorithms for solving quadratic programming problems. These methods yield\\nthe following solution to the SVM problem. The details of these processes are beyond the scope of\\nthese notes.\\n10.8.1\\nSolution\\nThe vector ⃗w and the scalar b are given by\\n⃗w =\\nN\\n∑\\ni=1\\nαiyi⃗xi\\n(10.12)\\nb = 1\\n2 ( min\\ni∶yi=+1( ⃗w ⋅⃗xi) + max\\ni∶yi=−1( ⃗w ⋅⃗xi))\\n(10.13)\\nwhere ⃗α = (α1,α2,...,αN) is a vector which maximizes\\nN\\n∑\\ni=1\\nαi −1\\n2\\nN\\n∑\\ni=1,j=1\\nαiαjyiyj(⃗xi ⋅⃗xj)\\nsubject to\\nN\\n∑\\ni=1\\nαiyi = 0\\nαi > 0 for i = 1,2,...,N.\\nRemarks\\nIt can be proved that an αi is nonzero only if ⃗xi lies on the two margin boundaries, that is, only if ⃗xi\\nis a support vector. So, to specify a solution to the SVM problem, we need only specify the support\\nvectors ⃗xi and the corresponding coefﬁcients αiyi.\\n10.8.2\\nAn algorithm to ﬁnd the SVM classiﬁer\\nThe solution of the SVM problem given in Section ?? can be used to develop an algorithm to ﬁnd a\\nSVM classiﬁer for linearly separable two-class dataset. Here is an outline of such an algorithm.\\nAlgorithm to ﬁnd SVM classiﬁer\\nGiven a two-class linearly separable dataset of N points of the form\\n(⃗x1,y1),(⃗x2,y2),...,(⃗xN,yN),\\nwhere the yi’s are either +1 or 1:\\nStep 1.\\nFind ⃗α = (α1,α2,...,αN) which maximizes\\nφ(⃗α) =\\nN\\n∑\\ni=1\\nαi −1\\n2\\nN\\n∑\\ni=1,j=1\\nαiαjyiyj(⃗xi ⋅⃗xj)\\nsubject to\\nN\\n∑\\ni=1\\nαiyi = 0\\nαi > 0 for i = 1,2,...,N.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 164}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n150\\nStep 2.\\nCompute ⃗w = ∑N\\ni=1 αiyi⃗xi.\\nStep 3.\\nCompute b = 1\\n2 (mini∶yi=+1( ⃗w ⋅⃗xi) + maxi∶yi=−1( ⃗w ⋅⃗xi)).\\nStep 4.\\nThe SVM classiﬁer function is given by\\nf(⃗x) = ⃗w ⋅⃗x −b\\n(10.14)\\nwhere αi is nonzero only if ⃗xi is a support vector.\\nRemarks\\nThere are specialised software packages for solving the SVM optimization problem. For example,\\nthere is a special package called svm in the R programming language to solve such problems.\\n10.8.3\\nIllustrative example\\nProblem 1\\nUsing the SVM algorithm, ﬁnd the SVM classiﬁer for the follwoing data.\\nExample no.\\nx1\\nx2\\nClass\\n1\\n2\\n1\\n+1\\n2\\n4\\n3\\n−1\\nSolution\\nFor this data we have:\\nN = 2\\n⃗x1 = (2,1),\\ny1 = +1\\n⃗x2 = (4,3),\\ny2 = −1\\n⃗α = (α1,α2)\\nStep 1.\\nWe have:\\nφ(⃗α) =\\nN\\n∑\\ni=1\\nαi −1\\n2\\nN\\n∑\\ni=1,j=1\\nαiαjyiyj(⃗xi ⋅⃗xj)\\n= (α1 + α2) −1\\n2[α1α1y1y1(⃗x1 ⋅⃗x1) + α1α2y1y2(⃗x1 ⋅⃗x2)+\\nα2α1y2y1(⃗x2 ⋅⃗x1) + α2α2y2y2(⃗x2 ⋅⃗x2)]\\n= (α1 + α2)−\\n1\\n2[α2\\n1(+1)(+1)(2 × 2 + 1 × 1) + α1α2(+1)(−1)(2 × 4 + 1 × 3)+\\nα2α1(−1)(+1)(4 × 2 + 3 × 1) + α2\\n2(−1)(−1)(4 × 4 + 3 × 3)]\\n= (α1 + α2) −1\\n2 [5α2\\n1 −22α1α2 + 25α2\\n2]\\nN\\n∑\\ni=1\\nαiyi = α1y1 + α2y2\\n= α1 −α2\\nWe have to solve the following problem.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 165}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n151\\nProblem\\nFind values of α1 and α2 which maximizes\\nφ(⃗α) = (α1 + α2) −1\\n2 [5α2\\n1 −22α1α2 + 25α2\\n2]\\nsubject to the conditions\\nα1 −α2 = 0,\\nα1 > 0,α2 > 0.\\nSolution\\nTo ﬁnd the required values of α1 and α2, we note that from the constraints we have α2 = α1.\\nUsing this in the expression for φ we get\\nφ(⃗α) = 2α1 −4α2\\n1.\\nFor φ to be maximum we must have\\ndφ\\ndα1\\n= 2 −8α1 = 0\\nthat is\\nα1 = 1\\n4\\nand so we also have\\nα2 = 1\\n4.\\n(For this value of α1, clearly d2f\\ndα2\\n1 < 0 and f is indeed maximum. Also we have α1 > 0 and\\nα2 > 0.)\\nStep 2.\\nNow we have\\n⃗w =\\nN\\n∑\\ni=1\\nαiyi⃗xi\\n= α1y1⃗x1 + α2y2⃗x2\\n= 1\\n4(+1)(2,1) + 1\\n4(−1)(4,3)\\n= 1\\n4(−2,−2)\\n= (−1\\n2,−1\\n2)\\nStep 3.\\nNext we ﬁnd\\nb = 1\\n2 ( min\\ni∶yi=+1( ⃗w ⋅⃗xi) + max\\ni∶yi=−1( ⃗w ⋅⃗xi))\\n= 1\\n2 (( ⃗w ⋅⃗x1) + ( ⃗w ⋅⃗x2))\\n= 1\\n2 ((−1\\n4 × 2 −1\\n2 × 1) + (−1\\n2 × 4 −1\\n2 × 3))\\n= 1\\n2 (−10\\n2 )\\n= −5\\n2'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 166}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n152\\nStep 4.\\nLet ⃗x = (x1,x2). The SVM classiﬁer function is given by\\nf(⃗x) = ⃗w ⋅⃗x −b\\n= (−1\\n2,−1\\n2) ⋅(x1,x2) −(−5\\n2)\\n= −1\\n2x1 −1\\n2x2 + 5\\n2\\n= −1\\n2(x1 + x2 −5)\\nStep 5.\\nThe equation of the maximal margin hyperplane is\\nf(⃗x) = 0\\nthat is\\n−1\\n2(x1 + x2 −5) = 0\\nthat is\\nx1 + x2 −5 = 0.\\nNote that this the equation of the perpendicular bisector of the line segment joining the\\npoints (2,1) and (4,3) (see Figure 10.13).\\nProblem 2\\nUsing the SVM algorithm, ﬁnd the SVM classiﬁer for the follwoing data.\\nExample no.\\nx1\\nx2\\nClass\\n1\\n2\\n2\\n−1\\n2\\n4\\n5\\n+1\\n3\\n7\\n4\\n+1\\nSolution\\nFor this data we have:\\nN = 3\\n⃗x1 = (2,2),\\ny1 = −1\\n⃗x2 = (4,5),\\ny2 = +1\\n⃗x3 = (7,4),\\ny3 = +1\\n⃗α = (α1,α2,α3)\\n⃗x = (x1,x2)\\nSrep 1.\\nWe have\\nφ(⃗α) =\\nN\\n∑\\ni=1\\nα1 −1\\n2\\nN\\n∑\\ni=1,j=1\\nαiαjyiyj(⃗xi ⋅⃗xj)\\n=\\n3\\n∑\\ni=1\\nα1 −1\\n2\\n3\\n∑\\ni=1,j=1\\nαiαjyiyj(⃗xi ⋅⃗xj)\\nWe have\\n(⃗x1 ⋅⃗x1) = 08,\\n(⃗x1 ⋅⃗x2) = 18,\\n(⃗x1 ⋅⃗x3) = 22\\n(⃗x2 ⋅⃗x1) = 18,\\n(⃗x2 ⋅⃗x2) = 41,\\n(⃗x2 ⋅⃗x3) = 48,\\n(⃗x3 ⋅⃗x1) = 22,\\n(⃗x3 ⋅⃗x2) = 48,\\n(⃗x3 ⋅⃗x3) = 65'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 167}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n153\\nSubstituting these and simplifying we get\\nφ(⃗α) = (α1 + α2 + α3) −1\\n2[8α2\\n1 + 41α2\\n2 + 65α2\\n3 −36α1α2 −44α1α3 + 96α2α3]\\nWe also have\\nN\\n∑\\ni=1\\nαiyi = −α1 + α2 + α3\\nNow we have to solve the following problem.\\nProblem\\nFind ⃗α = (α1,α2,α3) which maximizes\\nφ(⃗α) = (α1 + α2 + α3) −1\\n2[8α2\\n1 + 41α2\\n2 + 65α2\\n3 −36α1α2 −44α1α3 + 96α2α3]\\nsubject to the conditions\\n−α1 + α2 + α3 = 0,\\nα1 > 0,α2 > 0,α3 > 0.\\nSolution\\nFrom the constraints we have\\nα1 = α2 + α3.\\nUsing this in the expression for φ(⃗α) and simplifying we get\\nφ(⃗α) = 2(α2 + α3) −1\\n2(13α2\\n2 + 32α2α3 + 29α2\\n3)\\nWhen φ(⃗α) is maximum we have\\n∂φ\\n∂α2\\n= 0,\\n∂φ\\n∂α3\\n= 0\\n(10.15)\\nthat is\\n2 −13α2 −16α3 = 0,\\n2 −16α2 −29α3 = 0.\\nSolving these equations we get\\nα2 = 26\\n121,\\nα3 = −6\\n121\\nHence\\nα1 = 26\\n121 −\\n6\\n121 = 20\\n121.\\n(The conditions given in Eq.(??) are only necessary conditions for getting a maximum\\nvalue for φ(⃗α). It can be shown that the values for α2 and α3 obtained above do indeed\\nsatisfy the sufﬁcient conditions for yielding a maximum value of φ(⃗α).)\\nSrep 2.\\nNow we have\\n⃗w =\\nN\\n∑\\ni=1\\nαiyi⃗xi\\n= 20\\n121(−1)(2,2) + 26\\n121(+1)(4,5) −\\n6\\n121(+1)(7,4)\\n= ( 2\\n11, 6\\n11)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 168}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n154\\nSrep 3.\\nWe have\\nb = 1\\n2 ( min\\ni∶yi=+1( ⃗w ⋅⃗xi) + max\\ni∶yi=−1( ⃗w ⋅⃗xi))\\n= 1\\n2 (min{( ⃗w ⋅⃗x2),( ⃗w ⋅⃗x3)} + max{( ⃗w ⋅⃗x1)})\\n= 1\\n2 (min{ 38\\n11, 38\\n11} + max{ 16\\n11})\\n= 1\\n2 (38\\n11 + 16\\n11)\\n= 27\\n11\\nSrep 4.\\nThe SVM classiﬁer function is\\nf(⃗x) = ⃗w ⋅⃗x −b\\n= 2\\n11x1 + 6\\n11x2 −27\\n11.\\nSrep 5.\\nThe equation of the maximal hyperplane is\\nf(⃗x) = 0\\nthat is\\n2\\n11x1 + 6\\n11x2 −27\\n11 = 0\\nthat is\\nx1 + 3x2 −27\\n2 = 0.\\n(See Figure 10.14.)\\n10.9\\nSoft margin hyperlanes\\nThe algorithm for ﬁnding the SVM classiﬁer will give give a solution only if the the given two-class\\ndataset is linearly separable. But, in real life problems, two-class datasets are only rarely linearly\\nseparable. In such a case we introduce additional variables, ξi, called slack variables which store\\ndeviations from the margin. There are two types of deviation: An instance may lie on the wrong\\nside of the hyperplane and be misclassiﬁed. Or, it may be on the right side but may lie in the margin,\\nnamely, not sufﬁciently away from the hyperplane (see Figure 10.15).\\nIf ξi = 0, then ⃗xi is correctly classiﬁed and there is no problem with ⃗xi. If 0 < ξi < 1 then ⃗xi is\\ncorrectly classiﬁed but it is in the margin. If ξi > 1, ⃗xi is misclassiﬁed. Th sum ∑N\\ni=1 ξi is deﬁned\\nas the soft error and this is added as a penalty to the function to be minimized. We also introduce a\\nfactor C to the soft error.\\nWith these modiﬁcations, we now reformulate the SVM problem as follows (see Section 10.7.2\\nfor the original formulation of the problem):\\nReformulated problem\\nGiven a two-class linearly separable dataset of N points of the form\\n(⃗x1,y1),(⃗x2,y2),...,(⃗xN,yN).\\nwhere the yi’s are either +1 or 1, ﬁnd vectors ⃗w and ⃗ξ and a number b which\\nminimize\\n1\\n2∥⃗w∥2 + C\\nN\\n∑\\ni=1\\nξi'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 169}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n155\\nFigure 10.15: Soft margin hyperplanes\\nsubject to\\nyi( ⃗w ⋅⃗xi −b) ≥1 −ξi, for i = 1,...N\\nξi ≥0, for i = 1,...,N\\nRemarks\\n1. There are algorithms for solving the reformulated SVM problem given above. The details of\\nthese algorithms are beyond the scope of these notes.\\n2. The hyperplanes given by the equations\\n⃗w ⋅⃗xi −b = +1\\nand\\n⃗w ⋅⃗xi −b = −1\\nwith the values of ⃗w and b obtained as solutions of the reformulated problem, are called the\\nsoft margin hyperplanes for the SVM problem.\\n10.10\\nKernel functions\\nIn the context of SVM’s, a kernel function is a function of the form K(⃗x, ⃗y), where ⃗x and ⃗y are\\nn-dimensional vectors, having a special property. These functions are used to obtain SVM-like\\nclassiﬁers for two-class datasets which are not linearly separable.\\n10.10.1\\nDeﬁnition\\nLet ⃗x and ⃗y be arbitrary vectors in the n-dimensional vector space Rn. Let φ be a mapping from Rn\\nto some vector space. A function K(⃗x, ⃗y) is called a kernel function if there is a function φ such\\nthat K(⃗x, ⃗y) = φ(⃗x) ⋅φ(⃗y).\\n10.10.2\\nExamples\\nExample 1\\nLet\\n⃗x = (x1,x2) ∈R2\\n⃗y = (y1,y2) ∈R2\\nWe deﬁne\\nK(⃗x, ⃗y) = (⃗x ⋅⃗y)2.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 170}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n156\\nWe show that this is a kernel function. To do this, we note that\\nK(⃗x, ⃗y) = (⃗x ⋅⃗y)2\\n= (x1y1 + x2y2)2\\n= x2\\n1y2\\n1 + 2x1y1x2y2 + x2\\n2y2\\n2\\nNow we deﬁne\\nφ(⃗x) = (x2\\n1,\\n√\\n2x1x2,x2\\n2) ∈R3\\nφ(⃗y) = (y2\\n1,\\n√\\n2y1y2,y2\\n2) ∈R3\\nThen we have\\nφ(⃗x) ⋅φ(⃗y) = x2\\n1y2\\n1 + (\\n√\\n2x1x2)(\\n√\\n2y1y2) + x2\\n2y2\\n2\\n= x2\\n1y2\\n1 + 2x1x2y1y2 + x2\\n2y2\\n2\\n= K(⃗x, ⃗y)\\nThis shows that K(⃗x, ⃗y) is indeed a kernel function.\\nExample 2\\nLet\\n⃗x = (x1,x2) ∈R2\\n⃗y = (y1,y2) ∈R2\\nWe deﬁne\\nK(⃗x, ⃗y) = (⃗x ⋅⃗y + θ)2.\\nWe show that this is a kernel function. To do this, we note that\\nK(⃗x, ⃗y) = (⃗x ⋅⃗y + θ)2\\n= (x1y1 + x2y2 + θ)2\\n= φ(⃗x) ⋅φ(⃗y)\\nwhere\\nφ(⃗x) = (x2\\n1,x2\\n2,\\n√\\n2x1x2,\\n√\\n2θx1,\\n√\\n2θx2,\\n√\\nθ) ∈R6.\\nThis shows that K(⃗x, ⃗y) is indeed a kernel function.\\n10.10.3\\nSome important kernel functions\\nIn the following we assume that ⃗x = (x1,x2,...,xn) and ⃗y = (y1,y2,...,yn).\\n1. Homogeneous polynomial kernel\\nK(⃗x, ⃗y) = (⃗x ⋅⃗y)d\\nwhere d is some positive integer.\\n2. Non-homogeneous polynomial kernel\\nK(⃗x, ⃗y) = (⃗x ⋅⃗y + θ)d\\nwhere d is some positive integer and θ is a real constant.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 171}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n157\\n3. Radial basis function (RBF) kernel\\nK(⃗x, ⃗y) = e−∥⃗x−⃗y∥2/2σ2\\nThis is also called the Gaussian radial function kernel.1\\n4. Laplacian kernel function\\nK(⃗x, ⃗y) = e−∥⃗x−⃗y∥/σ\\n5. Hyperbolic tangent kernel function (Sigmoid kernel function)\\nK(⃗x, ⃗y) = tanh(α(⃗x ⋅⃗y) + c)\\n10.11\\nThe kernel method (kernel trick)\\n10.11.1\\nOutline\\n1. Choose an appropriate kernel function K(⃗x, ⃗y).\\n2. Formulate and solve the optimization problem obtained by replacing each inner product ⃗x ⋅⃗y\\nby K(⃗x, ⃗y) in the SVM optimization problem.\\n3. In the formulation of the classiﬁer function for the SVM problem using the inner products of\\nunclassiﬁed data ⃗z and input vectors ⃗xi, replace each inner product ⃗z ⋅⃗xi with K(⃗z, ⃗xi) to\\nobtain the new classiﬁer function.\\n10.11.2\\nAlgorithm\\nAlgorithm of the kernel method\\nGiven a two-class linearly separable dataset of N points of the form\\n(⃗x1,y1),(⃗x2,y2),...,(⃗xN,yN),\\nwhere the yi’s are either +1 or 1 and appropriate kernel function K(⃗x, ⃗y):\\nStep 1.\\nFind ⃗α = (α1,α2,...,αN) which maximizes\\nN\\n∑\\ni=1\\nαi −1\\n2\\nN\\n∑\\ni=1,j=1\\nαiαjyiyjK(⃗xi, ⃗xj)\\nsubject to\\nN\\n∑\\ni=1\\nαiyi = 0\\nαi > 0 for i = 1,2,...,N.\\nStep 2.\\nCompute ⃗w = ∑N\\ni=1 αiyi⃗xi.\\nStep 3.\\nCompute b = 1\\n2 (mini∶yi=+1 K( ⃗w, ⃗xi) + maxi∶yi=−1 K( ⃗w, ⃗xi)).\\nStep 4.\\nThe SVM classiﬁer function is given by f(⃗z) = ∑N\\ni=1 αiyiK(⃗xi, ⃗z) + b.\\n1To represent this kernel as an inner product, we need map φ from Rn into an inﬁnite-dimensional vector space. A\\ndiscussion of these ideas is beyond the scope of these notes.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 172}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n158\\n10.12\\nMulticlass SVM’s\\nIn machine learning, the multiclass classiﬁcation is the problem of classifying instances into one of\\nthree or more classes. Classifying instances into one of the two classes is called binary classiﬁcation.\\nSupport vector machines can be constructed only when the dataset has only two class-labels and\\nis linearly separable. We have already discussed a method to extend the concept of SVM’s to the\\ncase where the dataset is not linearly separable. In this section we consider how the SVM’s can be\\nused to obtain classiﬁers when there are more than two class labels. Two methods are generally used\\nto handle such cases known by the names ”One-against-all\" and “one-against-one”.\\n10.12.1\\n“One-against-all” method\\nThe One-Against-All (OAA) SVMs were ﬁrst introduced by Vladimir Vapnik in 1995.\\nFigure 10.16: One-against all\\nLet there be p class labels, say, c1,c2,...,cp. We construct the following p two-class datasets\\nand obtain the corresponding SVM classiﬁers. First, we assign the class labels +1 to all instances\\nhaving class label c1 and the class label −1 to all the remaining instances in the data set. Let f1(⃗x)\\nbe the SVM classiﬁer function for the resulting two-class dataset. Next, we assign the class labels\\n+1 to all instances having class label c2 and the class label −1 to all the remaining instances in the\\ndata set. Let f2(⃗x) be the SVM classiﬁer function for the resulting two-class dataset. We continue\\nlike this and generate SVM classiﬁer functions f3(⃗x), ..., fp(⃗x)\\nTwo criteria have been developed to assign a class label to a test instance ⃗z.\\n1. A data point ⃗z would be classiﬁed under a certain class if and only if that class’s SVM accepted\\nit and all other classes’ SVMs rejected it. Thus ⃗z will be assigned ci if fi(⃗z) > 0 and fj(⃗z) < 0\\nfor all j ≠i.\\n2. ⃗z is the assigned the class label ci if fi(⃗z) has the highest value among f1(⃗z),...,fp(⃗z),\\nregardless of sign.\\nFigure 10.16 illustrates the one-against-all method with three classes.\\n10.12.2\\n“One-against-one” method\\nIn the one-against-one (OAO) (also called one-vs-one (OVO)) strategy, a SVM classiﬁer is con-\\nstructed for each pair of classes. If there are p different class labels, a total of p(p −1)/2 classiﬁers\\nare constructed. An unknown instance is classiﬁed with the class getting the most votes. Ties are\\nbroken arbitrarily.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 173}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n159\\nFigure 10.17: One-against-one\\nFor example, let there be three classes, A, B and C. In the OVO method we construct 3(3 −\\n1)/2 = 3 SVM binary classiﬁers. Now, if ⃗z is to be classiﬁed, we apply each of the three classiﬁers\\nto ⃗z. Let the three classiﬁers assign the classes A, B, B respectively to ⃗z. Since a label to ⃗z is\\nassigned by the majority voting, in this example, we assign the class label of B to ⃗z.\\nOne-vs-one (OVO) strategy is not a particular feature of SVM. Indeed, OVO can be applied to\\nany binary classiﬁer to solve multi-class classiﬁcation problem.\\n10.13\\nSample questions\\n(a) Short answer questions\\n1. Deﬁne an hyperplane in an n-dimensional space. What are the hyperplanes in 2-dimensional\\nand 3-dimensional spaces?\\n2. Find the distance of the point (1,−2,3) from the hyperplane\\n3x1 −4x2 + 12x3 −1 = 0.\\n3. What is a linearly separable dataset? Give an example. Give an example for a dataset which\\nis not linearly separable.\\n4. What is meant by maximum margin hyperplane?\\n5. Deﬁne the support vector machine of a two-class dataset.\\n6. Deﬁne the support vectors of a two-class dataset.\\n7. What is a kernel function? Give an example.\\n(b) Long answer questions\\n1. State the mathematical formulation of the SVM problem. Give an outline of the method for\\nsolving the problem.\\n2. Explain the signiﬁcance of soft margin hyperplanes and explain how they are computed.\\n3. Show that the function\\nK(⃗x, ⃗y) = (⃗x ⋅⃗y)3\\nis a kernel function.\\n4. What is meant by kernel trick in context of support vector machines? How is it used to ﬁnd a\\nSVM classiﬁer.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 174}, page_content='CHAPTER 10. SUPPORT VECTOR MACHINES\\n160\\n5. Given the following dataset, using elementary geometry ﬁnd the maximum margin hyperplane\\nfor the data. Verify the result by ﬁnding the same using the SVM algorithm.\\nExample\\nx1\\nx2\\nClass label\\n1\\n2\\n1\\n−1\\n2\\n4\\n5\\n+1\\n3\\n3\\n6\\n+1'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 175}, page_content='Chapter 11\\nHidden Markov models\\nThis chapter contains a brief introduction to hidden Markov models (HMM’s). The HMM is one\\nof the most important machine learning models in speech and language processing. To deﬁne it\\nproperly, we need to ﬁrst understand the concept of discrete Markov processes. So, we begin the\\nchapter with a description of Markov processes and then discuss HMM’s. The three basic problems\\nassociated with a HMM are stated, but algorithms for their solutions are not given as they are beyond\\nthe scope of these notes.\\n11.1\\nDiscrete Markov processes: Examples\\n11.1.1\\nExample 1\\nThrough this example we introduce the various elements that constitute a discrete homogeneous\\nMarkov process.\\n1. System and states\\nLet us consider a highly simpliﬁed model of the different states a stock-market is in, in a given\\nweek. We assume that there are only three possible states:\\nS1\\n:\\nBull market trend\\nS2\\n:\\nBear market trend\\nS3\\n:\\nStagnant market trend\\n2. Transition probabilities\\nWeek after week, the stock-market moves from one state to another state. From previous data,\\nit has been estimated that there are certain probabilities associated with these movements.\\nThese probabilities are called transition probabilities.\\n3. Markov assumption\\nWe assume that the following statement (called Markov assumption or Markov property) re-\\ngarding transition probabilities is true:\\n• Let the weeks be counted as 1,2,... and let an arbitrary week be the t-th week. Then,\\nthe state in week t + 1 depends only on the state in week t, regardless of the states in\\nthe previous weeks. This corresponds to saying that, given the present state, the future\\nis independent of the past.\\n4. Homogeneity assumption\\nTo simplify the computations, we assume that the following property, called the homogeneity\\nassumption, is also true.\\n161'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 176}, page_content='CHAPTER 11. HIDDEN MARKOV MODELS\\n162\\n• The probability that the stock market is in a particular state in a particular week t + 1\\ngiven that it is in a particular state in week t, is independent of t.\\n5. Representation of transition probabilities Let the probability that a bull week is followed\\nby another bull week be 90%, a bear week be 7.5%, and a stagnant week be 2.5%. Similarly,\\nlet the probability that a bear week is followed by another bull week be 15%, bear week be\\n80% and a stagnant week be 5%. Finally, let the probability that a stagnant week be followed\\nby a bull week is 25%, a bear week be 25% and a stagnant week be 50%. The transition\\nprobabilities can be represented in two ways:\\n(a) The states and the state transition probabilities can be represented diagrammatically as\\nin Figure 11.1.\\nFigure 11.1: A state diagram showing state transition probabilities\\n(b) The state transition probabilities can also be represented by a matrix called the state\\ntransition matrix. Let us label the states as “1 = bull”, “2 = bear” and “3 = stagnant” and\\nconsider the matrix\\nP =\\n⎡⎢⎢⎢⎢⎢⎣\\n0.90\\n0.075\\n0.025\\n0.15\\n0.80\\n0.05\\n0.25\\n0.50\\n0.25\\n⎤⎥⎥⎥⎥⎥⎦\\nIn this matrix, the element in the i-th row, j-th column represents the probability that the\\nmarket in state i is followed by market in state j.\\nNote that in the state transition matrix P, the sum of the elements in every row is 1.\\n6. Initial probabilities\\nThe initial probabilities are the probabilities that the stock-market is in a particular state ini-\\ntially. These are denoted by π1,π2,π3: π1 is the probability that the stock-market is in bull\\nstate initially; similarly, π2 and π3. the values of these probabilities can be presented as a\\nvector:\\nΠ =\\n⎡⎢⎢⎢⎢⎢⎣\\nπ1\\nπ2\\nπ3\\n⎤⎥⎥⎥⎥⎥⎦\\n=\\n⎡⎢⎢⎢⎢⎢⎣\\n0.5\\n0.3\\n0.2\\n⎤⎥⎥⎥⎥⎥⎦\\n7. The discrete Markov process\\nThe functioning of the stock-markets with the three states S1,S2,S3 with the assumption that\\nthe Markov property is true, the transition probabilities given by the matrix P and the initial'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 177}, page_content='CHAPTER 11. HIDDEN MARKOV MODELS\\n163\\nprobabilities given by the vector Π constitute a discrete Markov process. Since we also assume\\nthe homogeneity property for the transition probabilities is true, it is a homogeneous discrete\\nMarkov process.\\nProbabilities for future states\\nConsider the matrix:\\nΠT P = [0.5\\n0.3\\n0.2]\\n⎡⎢⎢⎢⎢⎢⎣\\n0.90\\n0.075\\n0.025\\n0.15\\n0.80\\n0.05\\n0.25\\n0.50\\n0.25\\n⎤⎥⎥⎥⎥⎥⎦\\n= [0.5450\\n0.3775\\n0.0775]\\nThe elements in this row vector represent the probabilities that the stock-market is in the bull state,\\nthe bear state and the stagnant state respectively in the second week.\\nIn general, the elements of the row vector ΠT P n represent the probabilities that the stock-market\\nis in the bull state, the bear state and the stagnant state respectively in the (n + 1)-th week.\\n11.1.2\\nExample 2\\nConsider a simpliﬁed model of weather. We assume that the weather conditions are observed once\\na day at noon and it is recorded as in one of the following states:\\nS1\\n:\\nRainy\\nS2\\n:\\nCloudy\\nS3\\n:\\nSunny\\nAssuming that the Markov property and the homogeneity property are true, we can write the state\\ntransition probability matrix P. Let the matrix be\\nP =\\n⎡⎢⎢⎢⎢⎢⎣\\n0.4\\n0.3\\n0.3\\n0.2\\n0.6\\n0.2\\n0.1\\n0.1\\n0.8\\n⎤⎥⎥⎥⎥⎥⎦\\nLet the initial probabilities be\\nΠ = [0.25\\n0.25\\n0.50]\\nThe changes in weather with the three sates S1,S2,S3 satisfying the Markov property and the ho-\\nmogeneity property, the transition probability matrix P and the initial probabilities given by Π con-\\nstitute a discrete homogeneous Markov process.\\n11.2\\nDiscrete Markov processes: General case\\nA Markov process is a random process indexed by time, and with the property that the future is\\nindependent of the past, given the present. The time space may be discrete taking the values 1,2,...\\nor continuous taking any nonnegative real number as a value. In these notes, we consider only\\ndiscrete time Markov processes.\\n1. System and states\\nConsider a system that at any time is in one of N distinct states:\\nS1,S2,...,SN\\nWe denote the state at time t by qt for t = 1,2,.... So, qt = Si means that the system is in\\nstate Si at time t.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 178}, page_content='CHAPTER 11. HIDDEN MARKOV MODELS\\n164\\n2. Transition probabilities\\nAt regularly spaced discrete times, the system moves to a new state with a given probability,\\ndepending on the values of the previous states. These probabilities are called the transition\\nprobabilities.\\n3. Markov assumptions (Markov property)\\nWe assume the following called the Markov assumption or the Markov property:\\n• The state at time t + 1 depends only on state at time t, regardless of the states in the\\nprevious times. This corresponds to saying that, given the present state, the future is\\nindependent of the past.\\n4. Homogeneity property\\nWe assume that the following property, called the homogeneity property, is true.\\n• We also assume that these transition probabilities are independent of time, that is, the\\nprobabilities P(qt+1 = Sj ∣qt = Si) are constants and do not depend on t. We denote this\\nprobablity by aij:\\naij = P(qt+1 = Sj ∣qt = Si).\\nWe immediately note that\\naij ≥0 and\\nN\\n∑\\nj=1\\naij = 1 for all i.\\n5. Representation of transition probabilities\\nThe transition probabilities can be represented in two ways:\\n(a) If the number of states is small, the state transition probabilities can be represented\\ndiagrammatically as in Figure 11.1.\\n(b) The state transition probabilities can also be represented by a matrix called the state\\ntransition matrix.\\nA =\\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣\\na11\\na12\\n...\\na1N\\na21\\na22\\n...\\na2N\\n⋯\\naN1\\naN2\\n...\\naNN\\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦\\nIn this matrix, the element in the i-th row, j-th column represents the probability that the\\nsystem in state Si moves to state Sj. Note that in the state transition matrix A, the sum\\nof the elements in every row is 1.\\n6. Initial probabilities\\nWe deﬁne the initial probabilities πi which is the probability that the ﬁrst state in the sequence\\nis Si:\\nπ = P(q1 = Si).\\nWe also write\\nΠ =\\n⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣\\nπ1\\nπ2\\n⋯\\nπN\\n⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦\\nWe must have\\nN\\n∑\\ni=1\\nπi = 1.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 179}, page_content='CHAPTER 11. HIDDEN MARKOV MODELS\\n165\\n7. Discrete Markov process\\nA system with the states S1,S2,...,SN satisfying the Markov property is called a discrete\\nMarkov process. If it satisﬁes the homogeneity property, then it is called a homogeneous\\ndiscrete Markov process.\\n11.2.1\\nProbability for an observation sequence\\nObservable Markov model\\nThe discrete Markov process described in Section 11.2 is also called an observable Markov model or\\nobservable discrete Markov process. It is so called because the state of the system at any time t can\\nbe directly observed. This is in contrast to models where the state of the system cannot be directly\\nobserved. If the state of the system cannot be directly observed the system is called a hidden Markov\\nmodel. Such systems are considered in Section ??.\\nProbability for an observation sequence\\nIn an observable Markov model, the states are observable. At any time t we know qt, and as the\\nsystem moves from one state to another, we get an observation sequence that is a sequence of states.\\nThe output of the process is the set of states at each instant of time where each state corresponds to\\na physical observable event.\\nLet O be an arbitrary observation sequence of length T. Let us consider a particular observation\\nsequence\\nQ = (q1,q2,...,qT ).\\nNow, given the transition matrix A and the initial probabilities Π we can calculate the probability\\nP(O = Q) as follows.\\nP(O = Q) = P(q1)P(q2∣q1)P(q3∣q2)...P(qT ∣qT −1)\\n= πq1aq1q2aq2q3 ...aqT −1qT\\nHere, πq1 is the probability that the ﬁrst state is q1, aq1q2 is the probability of going from q1 to q2,\\nand so on. We multiply these probabilities to get the probability of the whole sequence.\\nExample\\nConsider the discrete Markov process described in Section 11.1.1. Let us compute the probability\\nof having a bull week followed by a stagnant week followed by two bear weeks. In this case the\\nobservation sequence is\\nQ = (bull,stagnant,bear,bear)\\n= (S1,S2,S3,S3)\\nThe required probability is\\nP(O = Q) = P(S1)P(S2∣S1)P(S3∣S2)P(S3∣S3)\\n= π1a12a23a33\\n= 0.5 × 0.075 × 0.05 × 0.25\\n= 0.00046875\\n11.2.2\\nLearning the parameters\\nConsider a homogeneous discrete Markov process with transition matrix A and initial probability\\nvector Π. A and Π are the parameters of the process. The following procedure may be applied to\\nlearn these parameters.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 180}, page_content='CHAPTER 11. HIDDEN MARKOV MODELS\\n166\\nStep 1.\\nObtain K observation sequences each of length T. Let qtk be the observed state at time t\\nin the k-th observation sequence.\\nStep 2.\\nLet ˆπi be the estimate of the initial probability πi. Then\\nˆπi = number of sequences starting with Si\\ntotal number of sequences\\n.\\nStep 3.\\nLet ˆaij be the estimate of aij. Then\\nˆaij = number of transitions from Si to Sj\\nnumber of transitions from Si\\nExample\\nLet there be a discrete Markov process with three states S1, S2 and S3. Suppose we have the\\nfollowing 10 observation sequences each of length 5:\\nO1 ∶\\nS1 S2 S1 S1 S1\\nO2 ∶\\nS2 S1 S1 S3 S1\\nO3 ∶\\nS3 S1 S3 S2 S2\\nO4 ∶\\nS1 S3 S3 S1 S1\\nO5 ∶\\nS3 S2 S1 S1 S3\\nO6 ∶\\nS3 S1 S1 S2 S1\\nO7 ∶\\nS1 S1 S2 S3 S2\\nO8 ∶\\nS2 S3 S1 S2 S2\\nO9 ∶\\nS3 S2 S1 S1 S2\\nO10 ∶\\nS1 S2 S2 S1 S1\\nWe have:\\nˆπ1 = number of sequences starting with S1\\ntotal number of sequences\\n= 4\\n10\\nˆπ2 = number of sequences starting with S2\\ntotal number of sequences\\n= 2\\n10\\nˆπ3 = number of sequences starting with S3\\ntotal number of sequences\\n= 4\\n10\\nTherefor\\nΠ =\\n⎡⎢⎢⎢⎢⎢⎣\\n4/10\\n2/10\\n4/10\\n⎤⎥⎥⎥⎥⎥⎦\\nWe illustrate the computation of aij’s with an example.\\nˆa21 = number of transitions from S2 to S1\\nnumber of transitions from S2\\n= 6\\n11\\nˆa22 = number of transitions from S2 to S2\\nnumber of transitions from S2\\n= 3\\n11\\nˆa23 = number of transitions from S2 to S3\\nnumber of transitions from S2\\n= 2\\n11\\nThe remaining transition probabilities can be estimated in a similar way.\\nˆA =\\n⎡⎢⎢⎢⎢⎢⎣\\n9/19\\n6/19\\n4/19\\n6/11\\n3/11\\n2/11\\n5/10\\n4/10\\n1/10\\n⎤⎥⎥⎥⎥⎥⎦\\n.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 181}, page_content='CHAPTER 11. HIDDEN MARKOV MODELS\\n167\\nFigure 11.2: A two-coin model of an HMM\\n11.3\\nHidden Markov models\\n11.3.1\\nCoin tossing example\\nLet us consider the following scenario:\\nConsider a room which is divided into two parts by a curtain through which we cannot see what\\nis happening on the other half of the room. Person A is sitting in one half and person B is sitting\\nin the other half. Person B is doing some coin tossing experiment, but she will not tell person A\\nanything about what she is doing. Person B will only announce the result of each coin ﬂip. Let a\\ntypical sequence of announcements be\\nO = O1 O2 ... OT\\n= H H T H H T T T ... H\\n(say)\\nwhere as usual H stands for heads and T stands for tails. Person A wants to create a mathematical\\nmodel which explains this sequence of observation. Person A suspects that person B is announcing\\nthe results based on the outcomes of some discrete Markov process. If that is true, then the Markov\\nprocess that is happening behind the curtain is hidden from the rest of the world and we are left with\\na hidden Markov process. To verify whether actually a Markov process is happening is a daunting\\ntask. Based on the observations like O alone, we have to decide on the following:\\n• A Markov process has different states. What should the states in the process correspond to\\nwhat is happening behind the curtain?\\n• How many states should be there?\\n• What should be the initial probabilities?\\n• What should be the transition probabilities?\\nLet us assume that person B is doing something like the following before announcing the outcomes.\\n1. Let person B be in possession of two biased coins (or, three coins, or any number of coins)\\nand she is ﬂipping these coins in some order. When ﬂipping a particular coin, the system is\\nin the state of that coin. So, each of these coins may be identiﬁed as a state and there are two\\nstates, say S1 and S2.\\n2. The outcomes of the ﬂips of the coins are the observations. These observations are represented\\nby the observation symbols “H” (for “head”) and “T” (for “tail”).'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 182}, page_content='CHAPTER 11. HIDDEN MARKOV MODELS\\n168\\n3. After ﬂipping coin, one of the two coins should be ﬂipped next. There must be some deﬁnite\\nprocedure for doing this. The procedure is some random process with deﬁnite probabilities\\nfor selecting the coins. These are the transition probabilities and they deﬁne the transition\\nprobability matrix A.\\n4. Since the coins are biased, there would be deﬁnite probabilities for getting “H” or “T” each\\ntime the coin is ﬂipped. These probabilities are called the observation probabilities.\\n5. There must be some procedure for selecting the ﬁrst coin. This is speciﬁed by the initial\\nprobabilities vector Π.\\n11.3.2\\nThe urn and ball model\\nAgain, consider a room which is divided into two parts by a curtain through which we cannot see\\nwhat is happening on the other half of the room. Person A is sitting in one half and person B is\\nsitting in the other half. Person B is doing some experiment, but she will not tell person A anything\\nabout what she is doing. Person B will only announce the result of each experiment. Let a typical\\nsequence of announcements be\\nO = O1 O2 ... OT\\n= “red”, “green”, “red”, . . . , “blue”\\nPerson A wants to create a mathematical model which explains this sequence of observations.\\nFigure 11.3: An N-state urn and ball model which illustrates the general case of a discrete symbol\\nHMM\\nPerson A suspects that person B is announcing the results based on the outcomes of some discrete\\nMarkov process. If that is true, then the Markov process that is happening behind the curtain is\\nhidden from the rest of the world and we are left with a hidden Markov process.\\nIn this example, let us assume that person A suspects that something like the following is hap-\\npening behind the curtain.\\nThere are N large urns behind the curtain. Within each urn there are large number of coloured\\nballs. There are M distinct colours of balls. Person B, according to some random process, chooses\\nan initial urn. From this urn a ball is chosen at random and the colour of the ball is announced.\\nThe ball is then replaced in the urn. A new urn is then selected according to some random selection\\nprocess associated with the current urn and the ball selection process is repeated.\\nThis process is a typical example of a hidden Markov process. Note the following:\\n1. Selection of an urn may be made to correspond to a state of the process. Then, there are N\\nstates in the process.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 183}, page_content='CHAPTER 11. HIDDEN MARKOV MODELS\\n169\\n2. The colours of the balls selected are the observations. The name of the colour may be referred\\nto as the “observation symbol”. Hence, there are M observation symbols in the process.\\n3. The random selection process associated with the current urn speciﬁes the transition probabil-\\nities.\\n4. Each urn contains a mixture of balls of different colours. So, corresponding to each urn, there\\nare deﬁnite probabilities for getting balls of different colours. These probabilities are called\\nthe observation probabilities.\\n5. The procedure for selecting the ﬁrst urn provides the initial probabilities.\\n11.3.3\\nHidden Markov model (HMM): The general case\\nA hidden Markov model (HMM) is characterized by the following:\\n1. The number of states in the model, say N. Let the states be S1,S2,...,SN.\\n2. The number of distinct observation symbols, say M. Let the observation symbols be v1,v2,...,vM.\\n(The observation symbols correspond to the physical outputs of the system.)\\n3. The state transition probabilities speciﬁed by an N × N matrix A = [aj]:\\naij = P(qt+1 = Sj∣qt = Si), for i,j = 1,2,...,N.\\nwhere qt is the state at time t.\\n4. The observation symbol probability distributions bj(k) for j = 1,...,N and k = 1,...,M.\\nbj(k) is the probability that, at time t, the outcome is the symbol vk given that the system is\\nin state Sj:\\nbj(k) = P(vk at t∣qt = Sj).\\nWe denote by B the N × M matrix whose element in the j-th row k-column is bj(k).\\n5. The initial probabilities Π = [πi]:\\nπ = P(q1 = Si), for i = 1,2,...,N.\\nThe values of N and M are implicitly deﬁned in A, B and Π. So, a HMM is completely deﬁned by\\nthe parameter set\\nλ = (A,B,Π).\\n11.4\\nThree basic problems of HMMs\\nGiven the general model of HMM, there are three basic problems that must be solved for the model\\nto be useful for real-world applications. These problems are the following:\\nProblem 1.\\nEvaluation problem\\nGiven the observation sequence\\nO = O1 O2 ... OT ,\\nand a HMM model\\nλ = (A,B,Π)\\nhow do we efﬁciently compute\\nP(O∣λ),\\nthe probability of the observation sequence O given the model λ?'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 184}, page_content='CHAPTER 11. HIDDEN MARKOV MODELS\\n170\\nProblem 2.\\nFinding state sequence problem\\nGiven the observation sequence\\nO = O1 O2 ... OT ,\\nand a HMM model\\nλ = (A,B,Π)\\nhow do we ﬁnd the the state sequence\\nQ = q1 q2 ...,qT\\nwhich has the highest probability of generating O; that is, how do we ﬁnd Q⋆that\\nmaximizes the probability P(Q∣O,λ)?\\nProblem 3.\\nLearning model parameters problem\\nGiven a training set X observation sequences, how do we learn the model\\nλ = (A,B,Π)\\nthat maximizes the probability of generating X; that is, how do we ﬁnd λ⋆that maxi-\\nmizes the probability\\nP(X∣λ).\\n11.4.1\\nSolutions of the basic problems\\nThe details of the algorithms for solving these problems are beyond the scope of these notes. Prob-\\nlem 1 is solved using the Forwards-Backwards algorithms. Problem 2 is solved by the Viterbi\\nalgorithm and posterior decoding. Finally, Problem 3 is solved by the Baum-Welch algorithm.1\\n11.5\\nHMM application: Isolated word recognition\\nMost speech-recognition systems are classiﬁed as isolated or continuous. Isolated word recognition\\nrequires a brief pause between each spoken word, whereas continuous speech recognition does not.\\nSpeech-recognition systems can be further classiﬁed as speaker-dependent or speaker-independent.\\nA speaker-dependent system only recognizes speech from one particular speaker’s voice, whereas a\\nspeaker-independent system can recognize speech from anybody.\\nIn this section, we consider in an outline form how HMMs are used in building an isolated word\\nrecogniser.\\n1. Assume that we have a vocabulary V of words to be recognised.\\n2. For each word in the vocabulary, there is a training set of K occurrences of each spoken word\\n(spoken by 1 or more talkers) where each occurrence of the word constitute an observation\\nsequence.\\n3. The observations are some appropriate representation of the characteristics of the word. These\\nrepresentations are obtained via some preprocessing of the speech signal like linear predictive\\ncoding (LPC).\\n4. For each word v ∈V , we build an HMM, say\\nλv = (Av,Bv,Πv).\\nFor this, we have to apply the algorithms for learning an HMM to estimate the parameters\\n(Av,Bv,Πv) that maximise the probability of generating the observations in the training set\\nof K occurrences of the word v.\\n1For\\na\\nconcise\\npresentation\\nof\\nthe\\nalgorithms,\\nvisit\\nhttp://www.shokhirev.com/\\nnikolai/abc/alg/hmm/hmm.html.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 185}, page_content='CHAPTER 11. HIDDEN MARKOV MODELS\\n171\\nFigure 11.4: Block diagram of an isolated word HMM recogniser\\n5. Now consider an unknown word v which needs to be recognised. The following procedure is\\nused to recognise the word.\\n(a) The speech signal corresponding to the word w is subjected to preprocessing like LPC\\nand converted to the representation used in building the HMMs and the measurement of\\nthe observation sequence O = O1 O2 ... OT is obtained.\\n(b) The probabilities P(O∣λv), for each word v ∈V are calculated.\\n(c) Choose the word v for which P(O∣λv) is highest:\\nv⋆= arg max\\nv∈V P(O∣λv).\\n(d) The word w is recognised as the word v⋆.\\n11.6\\nSample questions\\n(a) Short answer questions\\n1. What is the state transition matrix of a discrete Markov process?\\n2. What is the Markov property of a discrete Markov process?\\n3. Consider a Markov process with two states “Rainy” and “Dry” and the transition probabilities\\nas shown in the following diagram.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 186}, page_content='CHAPTER 11. HIDDEN MARKOV MODELS\\n172\\nRainy\\nDry\\n0.3\\n0.8\\n0.7\\n0.2\\nIf P(Rain) = 0.4 and P(Dry) = 0.6 compute the probability for the sequence “Rain, Rain,\\nDry, Dry”.\\n(b) Long answer questions\\n1. Describe a discrete Markov process with an example.\\n2. Describe a hidden Markov model.\\n3. Explain how hidden Markov models are used in speech recognition.\\n4. What are the basic problems associated with a hidden Markov model.\\n5. Describe the urn and ball model of a hidden Markov model.\\n6. Describe the coin tossing model of a hidden Markov model.\\n7. Let there be a discrete Markov process with two states S1 and S2. Given the following se-\\nquences of observations of these states, estimate the initial probabilities and the transition\\nprobabilities of the process.\\nS1S2,\\nS2S2,\\nS1S2,\\nS2S1,\\nS1S1,\\nS2S1,\\nS1S2,\\nS1S1.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 187}, page_content='Chapter 12\\nCombining multiple learners\\nIn general there are several algorithms for learning the same task. Though these are generally suc-\\ncessful, no one single algorithm is always the most accurate. Now, we shall discuss models com-\\nposed of multiple learners that complement each other so that by combining them, we attain higher\\naccuracy.\\n12.1\\nWhy combine many learners\\nThere are several reasons why a single learner may not produce accurate results.\\n• Each learning algorithm carries with it a set of assumptions. This leads to error if the assump-\\ntions do not hold. We cannot be fully sure whether the assumptions are true in a particular\\nsituation.\\n• Learning is an ill-posed problem. With ﬁnite data, each algorithm may converge to a different\\nsolution and may fail in certain circumstances.\\n• The performance of a learner may be ﬁne-tuned to get the highest possible accuracy on a\\nvalidation set. But this ﬁne-tuning is a complex task and still there are instances on which\\neven the best learner is not accurate enough.\\n• It has been proved that there is no single learning algorithm that always produces the most\\naccurate output.\\n12.2\\nWays to achieve diversity\\nWhen many learning algorithms are combined, the individual algorithms in the collection are called\\nthe base learners of the collection.\\nWhen we generate multiple base-learners, we want them to be reasonably accurate but do not\\nrequire them to be very accurate individually. The base-learners are not chosen for their accuracy,\\nbut for their simplicity. What we care for is the ﬁnal accuracy when the base- learners are combined,\\nrather than the accuracies of the bas-learners we started from.\\nThere are several different ways for selecting the base learners.\\n1. Use different learning algorithms\\nThere may be several learning algorithms for performing a given task. For example, for\\nclassiﬁcation, one may choose the naive Bayes’ algorithm, or the decision tree algorithm or\\neven the SVM algorithm.\\nDifferent algorithms make different assumptions about the data and lead to different results.\\nWhen we decide on a single algorithm, we give emphasis to a single method and ignore all\\nothers. Combining multiple learners based on multiple algorithms, we get better results.\\n173'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 188}, page_content='CHAPTER 12. COMBINING MULTIPLE LEARNERS\\n174\\n2. Use the same algorithm with different hyperparameters\\nIn machine learning, a hyperparameter is a parameter whose value is set before the learning\\nprocess begins. By contrast, the values of other parameters are derived via training.\\nThe number of layers, the number of nodes in each layer and the initial weights are all hyper-\\nparameters in an artiﬁcial neural network. When we train multiple base-learners with different\\nhyperparameter values, we average over it and reduce variance, and therefore error.\\n3. Use different representations of the input object\\nFor example, in speech recognition, to recognize the uttered words, words may be represented\\nby the acoustic input. Words can also be represented by video images of the speaker’s lips as\\nthe words are spoken.\\nDifferent representations make different characteristics explicit allowing better identiﬁcation.\\nIn many applications, there are multiple sources of information, and it is desirable to use all\\nof these data to extract more information and achieve higher accuracy in prediction. We make\\nseparate predictions based on different sources using separate base-learners, then combine\\ntheir predictions.\\n4. Use different training sets to train different base-learners\\n• This can be done by drawing random training sets from the given sample; this is calledbagging.\\n• The learners can be trained serially so that instances on which the preceding base-\\nlearners are not accurate are given more emphasis in training later base-learners; ex-\\namples are boosting and cascading.\\n• The partitioning of the training sample can also be done based on locality in the input\\nspace so that each base-learner is trained on instances in a certain local part of the input\\nspace.\\n5. Multiexpert combination methods\\nThese base learners work in parallel. All of them are trained and then given an instance,\\nthey all give their decisions, and a separate combiner computes the ﬁnal decision using their\\npredictions. Examples include voting and its variants.\\n6. Multistage combination methods\\nThese methods use a serial approach where the next base-learner is trained with or tested on\\nonly the instances where the previous base-learners are not accurate enough.\\n12.3\\nModel combination schemes\\n12.3.1\\nVoting\\nThis is the simplest procedure for combining the outcomes of several learning algorithms. Let us\\nexamine some special cases of this scheme\\n1. Binary classiﬁcation problem\\nConsider a binary classiﬁcation problem with class labels −1 and +1. Let there be L base\\nlearners and let x be a test instance. Each of the base learners will assign a class label to x. If\\nthe class label assigned is +1, we say that the learner votes for +1 and that the label +1 gets\\na vote. The number of votes obtained by the class labels when the different base learners are\\napplied is counted. In the voting scheme for combining the learners, the label which gets the\\nmajority votes is assigned to x.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 189}, page_content='CHAPTER 12. COMBINING MULTIPLE LEARNERS\\n175\\n2. Multi-class classiﬁcation problem\\nLet there be n class labels C1,C2,...,Cn. Let x be a test instance and let there be L base\\nlearners. Here also, each of the base learners will assign a class label to x and when a class\\nlabel is assigned a label, the label gets a vote. In the voting scheme, the class label which gets\\nthe maximum number of votes is assigned to x.\\n3. Regression\\nConsider L base learners for predicting the value of a variable y. Let ˆyi be the output predicted\\nby the i-th base learner. The ﬁnal output is computed as\\ny = wiˆy1 + w2ˆy2 + ⋯+ wLˆyL\\nwhere w1,w2,...,wL are called the weights attached to the outputs of the various base learn-\\ners and they must satisfy the following conditions:\\nwj ≥0 for j = 1,2,...,L\\nw1 + w2 + ⋯+ wL = 1.\\nThis is the weighted voting scheme. In simple voting, we take\\nwi = 1\\nL for j = 1,2,...,L.\\n12.3.2\\nBagging\\nBagging is a voting method whereby base-learners are made different by training them over slightly\\ndifferent training sets.\\nGenerating L slightly different samples from a given sample is done by bootstrap, where given a\\ntraining set X of size N, we draw N instances randomly from X with replacement (see Section ??).\\nBecause sampling is done with replacement, it is possible that some instances are drawn more than\\nonce and that certain instances are not drawn at all. When this is done to generate L samples Xj,\\nj = 1,...,L, these samples are similar because they are all drawn from the same original sample,\\nbut they are also slightly different due to chance.\\nThe base-learners are trained with these L samples Xj. A learning algorithm is an unstable\\nalgorithm if small changes in the training set causes a large difference in the generated learner.\\nBagging, short for bootstrap aggregating, uses bootstrap to generate L training sets, trains L base-\\nlearners using an unstable learning procedure and then during testing, takes an average. Bagging\\ncan be used both for classiﬁcation and regression. In the case of regression, to be more robust, one\\ncan take the median instead of the average when combining predictions.\\nAlgorithms such as decision trees and multilayer perceptrons are unstable.\\n12.3.3\\nBoosting\\nIn bagging, generating complementary base-learners is left to chance and to the unstability of the\\nlearning method. In boosting, we actively try to generate complementary base-learners by training\\nthe next learner on the mistakes of the previous learners. The original boosting algorithm combines\\nthree weak learners to generate a strong learner. A weak learner has error probability less than\\n1/2, which makes it better than random guessing on a two-class problem, and a strong learner has\\narbitrarily small error probability.\\nThe boosting method\\n1. Let d1,d2,d3 be three learning algorithms for a particular task. Let a large training set X be\\ngiven.\\n2. We randomly divide X into three sets, say X1,X2,X3.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 190}, page_content='CHAPTER 12. COMBINING MULTIPLE LEARNERS\\n176\\n3. We use X1 and train d1.\\n4. We then take X2 and feed it to d1.\\n5. We take all instances misclassiﬁed by d1 and also as many instances on which d1 is correct\\nfrom X2, and these together form the training set of d2.\\n6. We then take X3 and feed it to d1 and d2.\\n7. The instances on which d1 and d2 disagree form the training set of d3.\\n8. During testing, given an instance, we give it to d1 and d2 if they agree, that is the response;\\notherwise the response of d3 is taken as the output.\\nIt has been shown that this overall system has reduced error rate, and the error rate can arbitrar-\\nily be reduced by using such systems recursively. One disadvantage of the system is thaaaaaat it\\nrequires a very large training sample. An improved algorithm known as AdaBoost (short for “adap-\\ntive boosting”), uses the same training set over and over and thus need not be large. AdaBoost can\\nalso combine an arbitrary number of base-learners, not three.\\n12.4\\nEnsemble learning⋆\\nThe word “ensemble” literally means “a group of things or people acting or taken together as a\\nwhole, especially a group of musicians who regularly play together.”\\nIn machine learning, an ensemble learning method consists of the following two steps:\\n1. Create different models for solving a particular problem using a given data.\\n2. Combine the models created to produce improved results.\\nThe different models may be chosen in many different ways:\\n• The models may be created using appropriate different algorithms like k-NN algorithm, Naive-\\nBayes algorithm, decision tree algorithm, etc.\\n• The models may be created by using the same algorithm but using different splits of the same\\ndataset into training data and test data.\\n• The models may be created by assigning different initial values to the parameters in the algo-\\nrithm as in ANN algorithms.\\nThe models created in the ensemble learning methods are combined in several ways.\\n• Simple majority voting in classiﬁcation problems: Every model makes a prediction (votes)\\nfor each test instance and the ﬁnal output prediction is the one that receives more than half of\\nthe votes.\\n• Weighted majority voting in classiﬁcation problem: In weighted voting we count the predic-\\ntion of the better models multiple times. Finding a reasonable set of weights is up to us.\\n• Simple averaging in prediction problems: In simple averaging method, for every instance of\\ntest dataset, the average predictions are calculated.\\n• Weighted averaging in prediction problems: In this method, the prediction of each model is\\nmultiplied by the weight and then their average is calculated.\\n12.5\\nRandom forest⋆\\nA random forest is an ensemble learning method where multiple decision trees are constructed and\\nthen they are merged to get a more accurate prediction.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 191}, page_content='CHAPTER 12. COMBINING MULTIPLE LEARNERS\\n177\\nFigure 12.1: Example of random forest with majority voting\\n12.5.1\\nAlgorithm\\nHere is an outline of the random forest algorithm.\\n1. The random forests algorithm generates many classiﬁcation trees. Each tree is generated as\\nfollows:\\n(a) If the number of examples in the training set is N, take a sample of N examples at\\nrandom - but with replacement, from the original data. This sample will be the training\\nset for generating the tree.\\n(b) If there are M input variables, a number m is speciﬁed such that at each node, m vari-\\nables are selected at random out of the M and the best split on these m is used to split\\nthe node. The value of m is held constant during the generation of the various trees in\\nthe forest.\\n(c) Each tree is grown to the largest extent possible.\\n2. To classify a new object from an input vector, put the input vector down each of the trees in\\nthe forest. Each tree gives a classiﬁcation, and we say the tree “votes” for that class. The\\nforest chooses the classiﬁcation\\n12.5.2\\nStrengths and weaknesses\\nStrengths\\nThe following are some of the important strengths of random forests.\\n• It runs efﬁciently on large data bases.\\n• It can handle thousands of input variables without variable deletion.\\n• It gives estimates of what variables are important in the classiﬁcation.\\n• It has an effective method for estimating missing data and maintains accuracy when a large\\nproportion of the data are missing.\\n• Generated forests can be saved for future use on other data.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 192}, page_content='CHAPTER 12. COMBINING MULTIPLE LEARNERS\\n178\\n• Prototypes are computed that give information about the relation between the variables and\\nthe classiﬁcation.\\n• The capabilities of the above can be extended to unlabeled data, leading to unsupervised\\nclustering, data views and outlier detection.\\n• It offers an experimental method for detecting variable interactions.\\n• Random forest run times are quite fast, and they are able to deal with unbalanced and missing\\ndata.\\n• They can handle binary features, categorical features, numerical features without any need for\\nscaling.\\n• There are lots of excellent, free, and open-source implementations of the random forest algo-\\nrithm. We can ﬁnd a good implementation in almost all major ML libraries and toolkits.\\nWeaknesses\\n• A weakness of random forest algorithms is that when used for regression they cannot predict\\nbeyond the range in the training data, and that they may over-ﬁt data sets that are particularly\\nnoisy.\\n• The sizes of the models created by random forests may be very large. It may take hundreds of\\nmegabytes of memory and may be slow to evaluate.\\n• Random forest models are black boxes that are very hard to interpret.\\n12.6\\nSample questions\\n(a) Short answer questions\\n1. Explain the necessity of combining several algorithms for accomplishing a particular task.\\n2. What is a base learner? How do we select base learners?\\n(b) Long answer questions\\n1. Explain the following: (i) voting (ii) bagging (iii) boosting.\\n2. Explain what is meant by random forests.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 193}, page_content='Chapter 13\\nClustering methods\\n13.1\\nClustering\\nClustering or cluster analysis is the task of grouping a set of objects in such a way that objects in the\\nsame group (called a cluster) are more similar (in some sense) to each other than to those in other\\ngroups (clusters).\\nClustering is a main task of exploratory data mining and used in many ﬁelds, including machine\\nlearning, pattern recognition, image analysis, information retrieval, bioinformatics, data compres-\\nsion, and computer graphics. It can be achieved by various algorithms that differ signiﬁcantly in\\ntheir notion of what constitutes a cluster and how to efﬁciently ﬁnd them. Popular notions of clus-\\nters include groups with small distances between cluster members, dense areas of the data space,\\netc.\\n13.1.1\\nExamples of data with natural clusters\\nIn many applications, there will naturally be several groups or clusters in samples.\\n1. Consider the case of optical character recognition: There are two ways of writing the digit 7;\\nthe American writing is ‘7’, whereas the European writing style has a horizontal bar in the\\nmiddle (something like 7−). In such a case, when the sample contains examples from both\\ncontinents, the sample will contain two clusters or groups one corresponding to the American\\n7 and the other corresponding to the European 7−.\\n2. In speech recognition, where the same word can be uttered in different ways, due to different\\npronunciation, accent, gender, age, and so forth, there is not a single, universal prototype. In\\na large sample of utterances of a speciﬁc word, All the different ways should be represented\\nin the sample.\\n13.2\\nk-means clustering\\n13.2.1\\nOutline\\nThe k-means clustering algorithm is one of the simplest unsupervised learning algorithms for solving\\nthe clustering problem.\\nLet it be required to classify a given data set into a certain number of clusters, say, k clusters.\\nWe start by choosing k points arbitrarily as the “centres” of the clusters, one for each cluster. We\\nthen associate each of the given data points with the nearest centre. We now take the averages of\\nthe data points associated with a centre and replace the centre with the average, and this is done for\\neach of the centres. We repeat the process until the centres converge to some ﬁxed points. The data\\npoints nearest to the centres form the various clusters in the dataset. Each cluster is represented by\\nthe associated centre.\\n179'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 194}, page_content='CHAPTER 13. CLUSTERING METHODS\\n180\\n13.2.2\\nExample\\nWe illustrate the algorithm in the case where there are only two variables so that the data points\\nand cluster centres can be geometrically represented by points in a coordinate plane. The distance\\nbetween the points (x1,x2) and (y1,y2) will be calculated using the familiar distance formula of\\nelementary analytical geometry:\\n√\\n(x1 −y1)2 + (x2 −y2)2.\\nProblem\\nUse k-means clustering algorithm to divide the following data into two clusters and also compute\\nthe the representative data points for the clusters.\\nx1\\n1\\n2\\n2\\n3\\n4\\n5\\nx2\\n1\\n1\\n3\\n2\\n3\\n5\\nTable 13.1: Data for k-means algorithm example\\nSolution\\nx1\\nx2\\n0\\n1\\n2\\n3\\n4\\n5\\n1\\n2\\n3\\n4\\n5\\nFigure 13.1: Scatter diagram of data in Table 13.1\\n1. In the problem, the required number of clusters is 2 and we take k = 2.\\n2. We choose two points arbitrarily as the initial cluster centres. Let us choose arbitrarily (see\\nFigure 13.2)\\n⃗v1 = (2,1),\\n⃗v2 = (2,3).\\n3. We compute the distances of the given data points from the cluster centers.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 195}, page_content='CHAPTER 13. CLUSTERING METHODS\\n181\\nx1\\nx2\\n0\\n1\\n2\\n3\\n4\\n5\\n1\\n2\\n3\\n4\\n5\\n⃗v1\\n⃗v2\\nFigure 13.2: Initial choice of cluster centres and the resulting clusters\\n⃗xi\\nData point\\nDistance\\nDistance\\nMinimum\\nAssigned\\nfrom ⃗v1 = (2,1)\\nfrom ⃗v2 = (2,3)\\ndistance\\ncenter\\n⃗x1\\n(1,1)\\n1\\n2.24\\n1\\n⃗v1\\n⃗x2\\n(2,1)\\n0\\n2\\n0\\n⃗v1\\n⃗x3\\n(2,3)\\n2\\n0\\n0\\n⃗v2\\n⃗x4\\n(3,2)\\n1.41\\n1.41\\n0\\n⃗v1\\n⃗x5\\n(4,3)\\n2.82\\n2\\n2\\n⃗v2\\n⃗x6\\n(5,5)\\n5\\n3.61\\n3.61\\n⃗v2\\n(The distances of ⃗x4 from ⃗v1 and ⃗v2 are equal. We have assigned ⃗v1 to ⃗x4 arbitrarily.)\\nThis divides the data into two clusters as follows (see Figure 13.2):\\nCluster 1: {⃗x1, ⃗x2, ⃗x4} represented by ⃗v1\\nNumber of data points in Cluster 1: c1 = 3.\\nCluster 2 : {⃗x3, ⃗x5, ⃗x6} represented by ⃗v2\\nNumber of data points in Cluster 2: c2 = 3.\\n4. The cluster centres are recalculated as follows:\\n⃗v1 = 1\\nc1\\n(⃗x1 + ⃗x2 + ⃗x4)\\n= 1\\n3(⃗x1 + ⃗x2 + ⃗x4)\\n= (2.00,1.33)\\n⃗v2 = 1\\nc2\\n(⃗x3 + ⃗x5 + ⃗x6)\\n= 1\\n3(⃗x3 + ⃗x5 + ⃗x6)\\n= (3.67,3.67)\\n5. We compute the distances of the given data points from the new cluster centers.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 196}, page_content='CHAPTER 13. CLUSTERING METHODS\\n182\\n⃗xi\\nData point\\nDistance\\nDistance\\nMinimum\\nAssigned\\nfrom ⃗v1 = (2,1)\\nfrom ⃗v2 = (2,3)\\ndistance\\ncenter\\n⃗x1\\n(1,1)\\n1.05\\n3.77\\n1.05\\n⃗v1\\n⃗x2\\n(2,1)\\n0.33\\n3.14\\n0.33\\n⃗v1\\n⃗x3\\n(2,3)\\n1.67\\n1.80\\n1.67\\n⃗v1\\n⃗x4\\n(3,2)\\n1.20\\n1.80\\n1.20\\n⃗v1\\n⃗x5\\n(4,3)\\n2.60\\n0.75\\n0.75\\n⃗v2\\n⃗x6\\n(5,5)\\n4.74\\n1.89\\n1.89\\n⃗v2\\nThis divides the data into two clusters as follows (see Figure 13.4):\\nCluster 1 : {⃗x1, ⃗x2, ⃗x3, ⃗x4} represented by ⃗v1\\nNumber of data points in Cluster 1: c1 = 4.\\nCluster 2 : {⃗x5, ⃗x6} represented by ⃗v2\\nNumber of data points in Cluster 1: c2 = 2.\\n6. The cluster centres are recalculated as follows:\\n⃗v1 == 1\\nc1\\n(⃗x1 + ⃗x2 + ⃗x3 + ⃗x4)\\n= 1\\n4(⃗x1 + ⃗x2 + ⃗x3 + ⃗x4)\\n= (2.00,1.33)\\n⃗v2 = 1\\n2(⃗x5 + ⃗x6) = (3.67,3.67)\\nx1\\nx2\\n0\\n1\\n2\\n3\\n4\\n5\\n1\\n2\\n3\\n4\\n5\\n⃗v1\\n⃗v2\\nFigure 13.3: Cluster centres after ﬁrst iteration and the corresponding clusters\\n7. We compute the distances of the given data points from the new cluster centers.\\n4.609772 3.905125 2.692582 2.500000 1.118034 1.118034'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 197}, page_content='CHAPTER 13. CLUSTERING METHODS\\n183\\n⃗xi\\nData point\\nDistance\\nDistance\\nMinimum\\nAssigned\\nfrom ⃗v1 = (2,1)\\nfrom ⃗v2 = (2,3)\\ndistance\\ncenter\\n⃗x1\\n(1,1)\\n1.25\\n4.61\\n1.25\\n⃗v1\\n⃗x2\\n(2,1)\\n0.75\\n3.91\\n0.75\\n⃗v1\\n⃗x3\\n(2,3)\\n1.25\\n2.69\\n1.25\\n⃗v1\\n⃗x4\\n(3,2)\\n1.03\\n2.50\\n1.03\\n⃗v1\\n⃗x5\\n(4,3)\\n2.36\\n1.12\\n1.12\\n⃗v2\\n⃗x6\\n(5,5)\\n4.42\\n1.12\\n1.12\\n⃗v2\\nThis divides the data into two clusters as follows (see Figure ??):\\nCluster 1 : {⃗x1, ⃗x2, ⃗x3, ⃗x4} represented by ⃗v1\\nNumber of data points in Cluster 1: c1 = 4.\\nCluster 2 : {⃗x5, ⃗x6} represented by ⃗v2\\nNumber of data points in Cluster 1: c1 = 2.\\n8. The cluster centres are recalculated as follows:\\n⃗v1 = 1\\nc1\\n(⃗x1 + ⃗x2 + ⃗x3 + ⃗x4)\\n= 1\\n4(⃗x1 + ⃗x2 + ⃗x3 + ⃗x4)\\n= (2.00,1.75)\\n⃗v2 = 1\\nc2\\n(⃗x5 + ⃗x6)\\n= 1\\n2(⃗x5 + ⃗x6)\\n= (4.00,4.50)\\nx1\\nx2\\n0\\n1\\n2\\n3\\n4\\n5\\n1\\n2\\n3\\n4\\n5\\n⃗v1\\n⃗v2\\nFigure 13.4: New cluster centres and the corresponding clusters\\n9. This divides the data into two clusters as follows (see Figure ??):\\nCluster 1 : {⃗x1, ⃗x2, ⃗x3, ⃗x4} represented by ⃗v1\\nCluster 2 : {⃗x5, ⃗x6} represented by ⃗v2'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 198}, page_content='CHAPTER 13. CLUSTERING METHODS\\n184\\n10. The cluster centres are recalculated as follows:\\n⃗v1 = 1\\n4(⃗x1 + ⃗x2 + ⃗x3 + ⃗x4) = (2.00,1.75)\\n⃗v2 = 1\\n2(⃗x5 + ⃗x6) = (4.00,4.50)\\nWe note that these are identical to the cluster centres calculated in Step 8. So there will be no\\nreassignment of data points to different clusters and hence the computations are stopped here.\\n11. Conclusion: The k means clustering algorithm with k = 2 applied to the dataset in Table 13.1\\nyields the following clusters and the associated cluster centres:\\nCluster 1 : {⃗x1, ⃗x2, ⃗x3, ⃗x4} represented by ⃗v1 = (2.00,1.75)\\nCluster 2 : {⃗x5, ⃗x6} represented by ⃗v2 = (2.00,4.75)\\n13.2.3\\nThe algorithm\\nNotations\\nWe assume that each data point is a n-dimensional vector:\\n⃗x = (x1,x2,...,xn).\\nThe distance between two data points\\n⃗x = (x1,x2,...,xn)\\nand\\n⃗y = (y1,y2,...,xn)\\nis deﬁned as\\n∣∣⃗x −⃗y∣∣=\\n√\\n(x1 −y1)2 + ⋯(xn −yn)2.\\nLet X = {⃗x1,..., ⃗xN} be the set of data points, V = {⃗v1,..., ⃗vk} be the set of centres and ci for\\ni = 1,...,k be the number of data points in the i-th cluster\\nBasic idea\\nWhat the algorithm aims to achieve is to ﬁnd a partition the set X into k mutually disjoint subsets\\nS = {S1,S2,...,Sk} and a set of data points V which minimizes the following within-cluster sum\\nof errors:\\nk\\n∑\\ni=1\\n∑\\n⃗x∈Si\\n∣∣⃗x −⃗vi∣∣2\\nAlgorithm\\nStep 1.\\nRandomly select k cluster centers ⃗v1,..., ⃗vk.\\nStep 2.\\nCalculate the distance between each data point ⃗xi and each cluster center ⃗vj.\\nStep 3.\\nFor each j = 1,2,...,N, assign the data point ⃗xj to the cluster center ⃗vi for which the\\ndistance ∣∣⃗xj −⃗vi∣∣is minimum. Let ⃗xi1, ⃗xi2, ..., ⃗xici be the data points assigned to ⃗vi.\\nStep 4.\\nRecalculate the cluster centres using\\n⃗vi = 1\\nci\\n(⃗xi1 + ⋯+ ⃗xici),\\ni = 1,2,...,k.\\nStep 5.\\nRecalculate the distance between each data point and newly obtained cluster centers.\\nStep 6.\\nIf no data point was reassigned then stop. Otherwise repeat from Step 3.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 199}, page_content='CHAPTER 13. CLUSTERING METHODS\\n185\\nSome methods for initialisation\\nThe following are some of the methods for choosing the initial vi’s.\\n• Randomly take some k data points as the initial vi’s.\\n• Calculate the mean of all data and add small random vectors to the mean to get the k initial\\nvi’s.\\n• Calculate the principal component, divide its range into k equal intervals, partition the data\\ninto k groups, and then take the means of these groups as the initial centres.\\n13.2.4\\nDisadvantages\\nEven though the k-means algorithm is fast, robust and easy to understand, there are several disad-\\nvantages to the algorithm.\\n• The learning algorithm requires apriori speciﬁcation of the number of cluster centers.\\n• The ﬁnal cluster centres depend on the initial vi’s.\\n• With different representation of data we get different results (data represented in form of\\ncartesian co-ordinates and polar co-ordinates will give different results).\\n• Euclidean distance measures can unequally weight underlying factors.\\n• The learning algorithm provides the local optima of the squared error function.\\n• Randomly choosing of the initial cluster centres may not lead to a fruitful result.\\n• The algorithm cannot be applied to categorical data.\\n13.2.5\\nApplication: Image segmentation and compression\\nImage segmentation\\nThe goal of segmentation is to partition an image into regions each of which has a reasonably\\nhomogeneous visual appearance or which corresponds to objects or parts of objects. Each pixel in\\nan image is a point in a 3-dimensional space comprising the intensities of the red, blue, and green\\nchannels. A segmentation algorithm simply treats each pixel in the image as a separate data point.\\nFor any value of k, each pixel is replaced by the pixel vector with the (R,G,B) intensity triplet\\ngiven by the centre µk to which that pixel has been assigned. For a given value of k, the algorithm\\nis representing the image using a palette of only k colours. It should be emphasized that this use of\\nk-means is a very crude approach to image segmentation. The image segmentation problem is in\\ngeneral extremely difﬁcult.\\nData compression\\nWe can also the clustering algorithm to perform data compression. There are two types of data\\ncompression: lossless data compression, in which the goal is to be able to reconstruct the original\\ndata exactly from the compressed representation, and lossy data compression, in which we accept\\nsome errors in the reconstruction in return for higher levels of compression than can be achieved in\\nthe lossless case.\\nWe can apply the k-means algorithm to the problem of lossy data compression as follows. For\\neach of the N data points, we store only the identity of the cluster to which it is assigned. We also\\nstore the values of the k cluster centres µk, which requires much less data, provided we choose\\nk much smaller than N. Each data point is then approximated by its nearest centre µk. New data\\npoints can similarly be compressed by ﬁrst ﬁnding the nearest µk and then storing the label k instead\\nof the original data vector. This framework is often called vector quantization, and the vectors Îijµk\\nare called code-book vectors.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 200}, page_content='CHAPTER 13. CLUSTERING METHODS\\n186\\n13.3\\nMulti-modal distributions\\n13.3.1\\nDeﬁnitions\\n1. In statistics, a unimodal distribution is a continuous probability distribution with only one\\nmode (or “peak”).\\nA random variable having the normal distribution is a unimodal distribution. Similarly, the\\nt-distribution and the chi-squared distribution are also unimodal distributions.\\nUnimodal\\nBimodal\\nMultimodal\\nFigure 13.5: Probability distributions\\n2. A bimodal distribution is a continuous probability distribution with two different modes. The\\nmodes appear as distinct peaks in the graph of the probability density function.\\n3. A multimodal distribution is a continuous probability distribution with two or more modes.\\n13.4\\nMixture of normal distributions\\n13.4.1\\nBimodal mixture\\nConsider the following functions which are probability density functions of normally distributed\\nrandom variables.\\nf1(x) =\\n1\\nσ1\\n√\\n2π\\ne\\n−(x−µ1)2\\n2σ2\\n1\\n(13.1)\\nf2(x) =\\n1\\nσ2\\n√\\n2π\\ne\\n−(x−µ2)2\\n2σ2\\n2\\n(13.2)\\nNow consider the following function:\\nf(x) = π1f1(x) + π2f2(x)\\n(13.3)\\nwhere π1 and π2 are some constants satisfying the relation\\nπ1 + π2 = 1.\\n(13.4)\\nIt can be shown that the function given in Eq.(13.3) together with Eq.(13.4) deﬁnes a probability\\ndensity function. It can also be shown that the graph of this function has two peaks. Hence this\\nfunction deﬁnes a bimodal distribution. This distribution is called a mixture of the normal distribu-\\ntions deﬁned by Eqs.(13.1) and (13.2). We may mix more than two normal distributions.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 201}, page_content='CHAPTER 13. CLUSTERING METHODS\\n187\\n13.4.2\\nDeﬁnition\\nConsider the following k probability density functions:\\nfi(x) =\\n1\\nσi\\n√\\n2π\\ne\\n−(x−µi)2\\n2σ2\\ni\\n,\\ni = 1,2,...,k.\\n(13.5)\\nLet π1,π2,...,πk be constants such that\\nπi ≥0,\\ni = 1,2,...,k\\n(13.6)\\nπ1 + π2 + ⋯+ πk = 1.\\n(13.7)\\nThen the random variable X whose probability density function is\\nf(x) = f1(x) + f2(x) + ⋯+ fk(x),\\n(13.8)\\nis said to be a mixture of the k normal distributions having the probability density functions deﬁned\\nin Eq.(13.5).\\nA natural example\\nAs a natural example for such mixtures of normal populations, we consider the probability distribu-\\ntion of heights of people in a region. This is a mixture of two normal distributions: the distribution\\nof heights of males and the distribution of heights of females. Given only the height data and not\\nthe gender assignments for each data point, the distribution of all heights would follow the weighted\\nsum of two normal distributions.\\n13.4.3\\nExample for mixture of two normal distributions\\nData and histogram\\nConsider the 100 observations of some attribute X given in Table 13.2.\\n[1] 5.39 1.30 2.95 2.16 2.37 2.33 4.76 2.99 1.71 2.41\\n[11] 2.71 2.79 0.54 1.37 5.16 1.22 1.58 4.34 3.83 3.44\\n[21] 3.68 5.03 0.92 2.57 1.97 2.17 5.02 2.73 1.63 3.09\\n[31] 4.05 3.76 3.13 6.50 5.10 3.62 3.14 2.36 2.73 4.08\\n[41] 3.28 2.28 1.52 3.86 2.10 0.86 2.94 2.18 3.39 2.55\\n[51] 3.23 3.30 2.16 3.86 1.92 2.55 4.33 0.86 2.68 2.24\\n[61] 2.82 3.63 2.84 3.82 2.49 3.25 2.39 3.18 6.35 4.16\\n[71] 6.68 5.26 8.00 6.27 7.98 6.50 6.56 8.50 7.48 6.42\\n[81] 5.99 7.44 6.96 7.10 8.48 6.99 7.29 6.87 6.71 7.99\\n[91] 8.19 8.28 6.98 7.43 8.33 5.65 8.96 7.36 5.24 7.30\\nTable 13.2: A set of 100 observations of a numeric attribute X\\nTo make some sense of this set of observations, let us construct the frequency table for the data\\nas in Table 13.3.\\nRange\\n0-1\\n1-2\\n2-3\\n3-4\\n4-5\\n5-6\\n6-7\\n7 -8\\n8-9\\n9-10\\nFrequency\\n4\\n9\\n26\\n18\\n6\\n9\\n12\\n9\\n7\\n0\\nRelative\\nfrequency\\n0.04\\n0.09\\n0.26\\n0.18\\n0.06\\n0.09\\n0.12\\n0.09\\n0.07\\n0.00\\nTable 13.3: Frequency table of data in Table 13.2'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 202}, page_content='CHAPTER 13. CLUSTERING METHODS\\n188\\nFigure 13.6 shows the histogram of the relative frequencies. Notice that the histogram has two\\n“peaks”, one near x = 2.5 and one near x = 6.5. So, the graph of the probability density function of\\nthe attribute X must have two peaks. Recall that the graph of the probability density function of a\\nrandom variable having the normal distribution has only one peak.\\nProbability distribution\\nThe data in Table 13.2 was generated using the R programming language. It is a true “mixture” of\\nthe values two normally distributed random variables. 70% of the observations are random values\\nof a normally distributed random variable with µ1 = 3 and σ1 = 1.20 and 30% of the observations\\nare values of a normally distributed random variable with µ2 = 7 and σ2 = 0.87. The weight for the\\nﬁrst normal distribution is π1 = 70% = 0.7 and that for the second distribution is π2 = 30% = 0.3.\\nThe probability density function for the mixed distribution is\\nf(x) = 0.7 ×\\n1\\n1.20\\n√\\n2π\\ne−(x−3)2/(2×1.202) + 0.3 ×\\n1\\n0.87\\n√\\n2π\\ne−(x−7)2/(2×0.872).\\n(13.9)\\nFigure 13.6 also shows the curve deﬁned by Eq.(13.9) superimposed on the histogram of the relative\\nfrequency distribution.\\nFigure 13.6: Graph of pdf deﬁned by Eq.(13.9) superimposed on the histogram of the data in Table\\n13.3\\n13.5\\nMixtures in terms of latent variables\\nConsider the mixture of k normal distributions deﬁned by Eqs.(13.5) – (13.8).\\nLet us deﬁne a k-dimensional random variable\\n⃗Z = (z1,z2,...,zk)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 203}, page_content='CHAPTER 13. CLUSTERING METHODS\\n189\\nwhere each z1 is either 0 or 1 and a 1 appears only at one place; that is,\\nzi ∈{0,1} and z1 + z2 + ⋯+ zk = 0.\\nWe also assume that\\nP(zk = 1) = πk.\\nThe probability function of ⃗Z can be written in the form\\nP( ⃗Z) = πz1\\n1 πz2\\n2 ...πzk\\nk .\\nNow, suppose we have a set of observations {x1,x2,...,xN}. Suppose that, in some way, we\\ncan associate a value of the random variable ⃗Z, say ⃗Zi, with each value xi and think of the given set\\nof observations as a set of ordered pairs\\n{(x1, ⃗Z1),(x2, ⃗Z2),...,(xN, ⃗ZN)}.\\nHere, only the xi-s are known; the ⃗Zi-s are unknown. Let us further assume that the conditional\\nprobability distribution p(x∣⃗Z) be given by\\np(x∣⃗Z) = [f1(x)]z1 × ⋯× [fk(x)]zk.\\nThen the marginal distribution of x is given by\\np(x) = ∑\\n⃗Z\\np( ⃗Z)P(x∣⃗Z)\\n= π1f1(x) + ⋯+ πkfk(x).\\n(13.10)\\nThe right hand side of Eq.(13.10) is the probability density function of a mixture of k normal distri-\\nbutions with weights π1,...,πk.\\nThus, a mixture of normal distributions is the marginal distribution of a bivariate distribution\\n(x, ⃗Z) where ⃗Z is an unobserved or latent variable.\\n13.6\\nExpectation-maximisation algorithm\\nThe maximum likelihood estimation method (MLE) is a method for estimating the parameters of a\\nstatistical model, given observations (see Section 6.5 for details). The method attempts to ﬁnd the\\nparameter values that maximize the likelihood function, or equivalently the log-likelihood function,\\ngiven the observations.\\nThe expectation-maximisation algorithm (sometimes abbreviated as the EM algorithm) is used\\nto ﬁnd maximum likelihood estimates of the parameters of a statistical model in cases where the\\nequations cannot be solved directly. These models generally involve latent or unobserved variables\\nin addition to unknown parameters and known data observations. For example, a Gaussian mixture\\nmodel can be described by assuming that each observed data point has a corresponding unobserved\\ndata point, or latent variable, specifying the mixture component to which each data point belongs.\\nThe EM Algorithm is not really an algorithm. Rather it is a general procedure to create algo-\\nrithms for speciﬁc MLE problems. The complete details of this general procedure are beyond the\\nscope of this book. However, we present below a minimal outline of the algorithm\\nOutline of EM algorithm\\nStep 1.\\nInitialise the parameters θ to be estimated.\\nStep 2.\\nExpectation step (E-step)\\nTake the expected value of the complete data given the observation and the current param-\\neter estimate, say, ˆθj. This is a function of θ and ˆθj, say, Q(θ, ˆθj).\\nStep 3.\\nMaximization step (M-step)\\nFind the values θ that maximizes the function Q(θ, ˆθj).\\nStep 4.\\nRepeat Steps 1 and 2 until the parameter values or the likelihood function converge.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 204}, page_content='CHAPTER 13. CLUSTERING METHODS\\n190\\n13.7\\nThe EM algorithm for Gaussian mixtures\\nIn the case of Gaussian mixture problems, because of the nature of the function, ﬁnding a maximum\\nlikelihood estimate by taking the derivatives of the log-likelihood function with respect to all the\\nparameters and simultaneously solving the resulting equations is nearly impossible. So we apply the\\nEM algorithm to solve the problem.\\nAs already indicated, the EM algorithm is a general procedure for estimating the parameters\\nin a statistical model. This algorithm can be adapted to develop an algorithm for estimating the\\nparameters in a Gaussian mixture model. The adapted EM algorithm has been explained below.\\n(The details of how the EM algorithm can be adapted to estimate the parameters in a Gaussian\\nmixture model are also beyond the scope of this book. For details on these matters, one may refer to\\n[1]).\\nProblem\\nSuppose we are given a set of N observations\\n{x1,x2,...,xN}\\nof a numeric variable X. Let X be a mix of k normal distributions and let the probability density\\nfunction of X be\\nf(x) = π1f1(x) + ⋯+ πkfk(x)\\nwhere\\nπi ≥0,\\ni = 1,2,...,k\\nπi + ⋯+ πk = 1\\nfi(x) =\\n1\\nσi\\n√\\n2π\\ne\\n−(x−µi)2\\n2σ2\\ni\\n,\\ni = 1,2,...,k.\\nEstimate the parameters µ1,...,µk, σ1,...,σk and π1 ...,πk.\\nLog-likelihood function\\nLet θ denote the set of parameters µi,σi,πi(i = 1,...,k). The log-likelihood function for the above\\nproblem is given below:\\nlog L(θ) = log f(x1) + ⋯+ log f(xN)\\n=\\nN\\n∑\\ni=1\\nlog ⎛\\n⎝\\nπ1\\nσ1\\n√\\n2π\\ne\\n−(xi−µ1)2\\n2σ2\\n1\\n+ ⋯+\\nπk\\nσk\\n√\\n2π\\ne\\n−\\n(xi−µk)2\\n2σ2\\nk\\n⎞\\n⎠\\n(13.11)\\nThe algorithm\\nStep 1.\\nInitialise the means µi’s, the variances σ2\\ni ’s and the mixing coefﬁcients πi’s.\\nStep 2.\\nCalculate the following for n = 1,...,N and i = 1,...,k:\\nγin =\\nπifi(xn)\\nπ1f1(xn) + ⋯+ πkfk(xn)\\nNi = γi1 + ⋯+ γiN\\nStep 3.\\nRecalculate the parameters using the following:\\nµ(new)\\ni\\n= 1\\nNi\\n(γi1x1 + ⋯γiNxN)'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 205}, page_content='CHAPTER 13. CLUSTERING METHODS\\n191\\nσ2(new)\\ni\\n= 1\\nNi\\n(γi1(x1 −µ(new)\\ni\\n)2 + ⋯+ γiN(x1 −µ(new)\\ni\\n)2)\\nπ(new)\\ni\\n= Ni\\nN\\nStep 4.\\nEvaluate the log-likelihood function given in Eq.(13.11) and check for convergence of ei-\\nther the parameters or the log-likelihood function. If the convergence criterion is not satis-\\nﬁed, return to Step 2.\\n13.8\\nHierarchical clustering\\nHierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster\\nanalysis which seeks to build a hierarchy of clusters (or groups) in a given dataset. The hierarchical\\nclustering produces clusters in which the clusters at each level of the hierarchy are created by merg-\\ning clusters at the next lower level. At the lowest level, each cluster contains a single observation.\\nAt the highest level there is only one cluster containing all of the data.\\nThe decision regarding whether two clusters are to be merged or not is taken based on the mea-\\nsure of dissimilarity between the clusters. The distance between two clusters is usually taken as the\\nmeasure of dissimilarity between the clusters.\\nIn Section ??, we shall see various methods for measuring the distance between two clusters.\\n13.8.1\\nDendrograms\\nHierarchical clustering can be represented by a rooted binary tree. The nodes of the trees represent\\ngroups or clusters. The root node represents the entire data set. The terminal nodes each represent\\none of the individual observations (singleton clusters). Each nonterminal node has two daughter\\nnodes.\\nThe distance between merged clusters is monotone increasing with the level of the merger. The\\nheight of each node above the level of the terminal nodes in the tree is proportional to the value of\\nthe distance between its two daughters (see Figure 13.9).\\nA dendrogram is a tree diagram used to illustrate the arrangement of the clusters produced by\\nhierarchical clustering.\\nThe dendrogram may be drawn with the root node at the top and the branches growing vertically\\ndownwards (see Figure 13.8(a)). It may also be drawn with the root node at the left and the branches\\ngrowing horizontally rightwards (see Figure 13.8(b)). In some contexts, the opposite directions may\\nalso be more appropriate.\\nDendrograms are commonly used in computational biology to illustrate the clustering of genes\\nor samples.\\nExample\\nFigure 13.7 is a dendrogram of the dataset {a,b,c,d,e}. Note that the root node represents the en-\\ntire dataset and the terminal nodes represent the individual observations. However, the dendrograms\\nare presented in a simpliﬁed format in which only the terminal nodes (that is, the nodes represent-\\ning the singleton clusters) are explicitly displayed. Figure 13.8 shows the simpliﬁed format of the\\ndendrogram in Figure 13.7.\\nFigure 13.9 shows the distances of the clusters at the various levels. Note that the clusters are at\\n4 levels. The distance between the clusters {a} and {b} is 15, between {c} and {d} is 7.5, between\\n{c,d} and {e} is 15 and between {a,b} and {c,d,e} is 25.\\n13.8.2\\nMethods for hierarchical clustering\\nThere are two methods for the hierarchical clustering of a dataset. These are known as the agglom-\\nerative method (or the bottom-up method) and the divisive method (or, the top-down method).'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 206}, page_content='CHAPTER 13. CLUSTERING METHODS\\n192\\na\\nb\\nc\\nd\\ne\\na,b\\nc,d\\nc,d,e\\na,b,c,d,e\\nFigure 13.7: A dendrogram of the dataset {a,b,c,d,e}\\na\\nb\\nc\\nd\\ne\\na\\nb\\nc\\nd\\ne\\n(a)\\n(b)\\nFigure 13.8: Different ways of drawing dendrogram\\nDistance\\n0\\n5\\n10\\n15\\n20\\n25\\na\\nb\\nc\\nd\\ne\\nLevel 1\\nLevel 2\\nLevel 3\\nLevel 4\\nFigure 13.9: A dendrogram of the dataset {a,b,c,d,e} showing the distances (heights) of the clus-\\nters at different levels\\nAgglomerative method\\nIn the agglomerative we start at the bottom and at each level recursively merge a selected pair of\\nclusters into a single cluster. This produces a grouping at the next higher level with one less cluster.\\nIf there are N observations in the dataset, there will be N −1 levels in the hierarchy. The pair chosen\\nfor merging consist of the two groups with the smallest “intergroup dissimilarity”.\\nFor example, the hierarchical clustering shown in Figure 13.7 can be constructed by the agglom-\\nerative method as shown in Figure 13.10. Each nonterminal node has two daughter nodes. The\\ndaughters represent the two groups that were merged to form the parent.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 207}, page_content='CHAPTER 13. CLUSTERING METHODS\\n193\\na\\nb\\nc\\nd\\ne\\nStep 1\\na\\nb\\nc\\nd\\ne\\na,b\\nStep 2\\na\\nb\\nc\\nd\\ne\\na,b\\nc,d\\nStep 3\\na\\nb\\nc\\nd\\ne\\na,b\\nc,d\\nc,d,e\\nStep 4\\na\\nb\\nc\\nd\\ne\\na,b\\nc,d\\nc,d,e\\na,b,c,d,e\\nStep 5\\nFigure 13.10: Hierarchical clustering using agglomerative method'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 208}, page_content='CHAPTER 13. CLUSTERING METHODS\\n194\\nDivisive method\\nThe divisive method starts at the top and at each level recursively split one of the existing clusters at\\nthat level into two new clusters. If there are N observations in the dataset, there the divisive method\\nalso will produce N −1 levels in the hierarchy. The split is chosen to produce two new groups with\\nthe largest “between-group dissimilarity”.\\nFor example, the hierarchical clustering shown in Figure 13.7 can be constructed by the divi-\\nsive method as shown in Figure 13.11. Each nonterminal node has two daughter nodes. The two\\ndaughters represent the two groups resulting from the split of the parent.\\n13.9\\nMeasures of dissimilarity\\nIn order to decide which clusters should be combined (for agglomerative), or where a cluster should\\nbe split (for divisive), a measure of dissimilarity between sets of observations is required. In most\\nmethods of hierarchical clustering, the dissimilarity between two groups of observations is measured\\nby using an appropriate measure of distance between the groups of observations. The distance\\nbetween two groups of observations is deﬁned in terms of the distance between two observations.\\nThere are several ways in which the distance between two observations can be deﬁned and also there\\nare also several ways in which the distance between two groups of observations can be deﬁned.\\n13.9.1\\nMeasures of distance between data points\\nNumeric data\\nWe assume that each observation or data point is a n-dimensional vector. Let ⃗x = (x1,...,xn)\\nand ⃗y = (y1,...,yn) be two observations. Then the following are the commonly used measures of\\ndistances in the hierarchical clustering of numeric data.\\nName\\nFormula\\nEuclidean distance\\n∣∣⃗x −⃗y∣∣2 =\\n√\\n(x1 −y1)2 + ⋯+ (xn −yn)2\\nSquared Euclidean distance\\n∣∣⃗x −⃗y∣∣2\\n2 = (x1 −y1)2 + ⋯+ (xn −yn)2\\nManhattan distance\\n∣∣⃗x −⃗y∣∣1 = ∣x1 −y1∣+ ⋯+ ∣xn −yn∣\\nMaximum distance\\n∣∣⃗x −⃗y∣∣∞= max{∣x1 −y1∣,...,∣xn −yn∣}\\nNon-numeric data\\nFor text or other non-numeric data, metrics such as the Levenshtein distance are often used.\\nThe Levenshtein distance is a measure of the ”distance” between two words. The Levenshtein\\ndistance between two words is the minimum number of single-character edits (insertions, deletions\\nor substitutions) required to change one word into the other.\\nFor example, the Levenshtein distance between “kitten” and “sitting” is 3, since the following\\nthree edits change one into the other, and there is no way to do it with fewer than three edits:\\nkitten →sitten (substitution of “s” for “k”)\\nsitten →sittin (substitution of “i” for “e”)\\nsittin →sitting (insertion of‘g” at the end)\\n13.9.2\\nMeasures of distance between groups of data points\\nLet A and B be two groups of observations and let x and y be arbitrary data points in A and B\\nrespectively. Suppose we have chosen some formula, say Euclidean distance formula, to measure\\nthe distance between data points. Let d(x,y) denote the distance between x and y. We denote by'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 209}, page_content='CHAPTER 13. CLUSTERING METHODS\\n195\\na,b,c,d,e\\nStep 1\\na,b,c,d,e\\na,b\\nc,d,e\\nStep 2\\na\\nb\\na,b,c,d,e\\na,b\\nc,d,e\\nStep 3\\na\\nb\\ne\\na,b,c,d,e\\na,b\\nc,d,e\\nc,d\\nStep 4\\na\\nb\\nc\\nd\\ne\\na,b\\nc,d\\nc,d,e\\na,b,c,d,e\\nStep 5\\nFigure 13.11: Hierarchical clustering using divisive method'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 210}, page_content='CHAPTER 13. CLUSTERING METHODS\\n196\\nd(A,B) the distance between the groups A and B. The following are some of the different methods\\nin which d(A,B) is deﬁned.\\n1. d(A,B) = max{d(x,y) ∶x ∈A,y ∈B}.\\nAgglomerative hierarchical clustering using this measure of dissimilarity is known as complete-\\nlinkage clustering. The method is also known as farthest neighbour clustering.\\na\\nb\\nc\\nd\\ne\\nA\\nB\\nFigure 13.12: Length of the solid line “ae” is max{d(x,y) ∶x ∈A,y ∈B}\\n2. d(A,B) = min{d(x,y) ∶x ∈A,y ∈B}.\\nAgglomerative hierarchical clustering using this measure of dissimilarity is known as single-\\nlinkage clustering. The method is also known as nearest neighbour clustering.\\na\\nb\\nc\\nd\\ne\\nA\\nB\\nFigure 13.13: Length of the solid line “bc” is min{d(x,y) ∶x ∈A,y ∈B}\\n3. d(A,B) =\\n1\\n∣A∣∣B∣\\n∑\\nx∈A,y∈B\\nd(x,y) where ∣A∣, ∣B∣are respectively the number of elements in\\nA and B.\\nAgglomerative hierarchical clustering using this measure of dissimilarity is known as mean\\nor average linkage clustering. It is also known as UPGMA (Unweighted Pair Group Method\\nwith Arithmetic Mean).\\n13.10\\nAlgorithm for agglomerative hierarchical clustering\\nGiven a set of N items to be clustered and an N × N distance matrix, required to construct a\\nhierarchical clustering of the data using the agglomerative method.\\nStep 1.\\nStart by assigning each item to its own cluster, so that we have N clusters, each containing\\njust one item. Let the distances between the clusters equal the distances between the items\\nthey contain.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 211}, page_content='CHAPTER 13. CLUSTERING METHODS\\n197\\nStep 2.\\nFind the closest pair of clusters and merge them into a single cluster, so that now we have\\none less cluster.\\nStep 3.\\nCompute distances between the new cluster and each of the old clusters.\\nStep 4.\\nRepeat Steps 2 and 3 until all items are clustered into a single cluster of size N.\\n13.10.1\\nExample\\nProblem 1\\nGiven the dataset {a,b,c,d,e} and the following distance matrix, construct a dendrogram by complete-\\nlinkage hierarchical clustering using the agglomerative method.\\na\\nb\\nc\\nd\\ne\\na\\n0\\n9\\n3\\n6\\n11\\nb\\n9\\n0\\n7\\n5\\n10\\nc\\n3\\n7\\n0\\n9\\n2\\nd\\n6\\n5\\n9\\n0\\n8\\ne\\n11\\n10\\n2\\n8\\n0\\nTable 13.4: Example for distance matrix\\nSolution\\nThe complete-linkage clustering uses the “maximum formula”, that is, the following formula to\\ncompute the distance between two clusters A and B:\\nd(A,B) = max{d(x,y) ∶x ∈A,y ∈B}\\n1. Dataset : {a,b,c,d,e}.\\nInitial clustering (singleton sets) C1: {a}, {b}, {c}, {d}, {e}.\\n2. The following table gives the distances between the various clusters in C1:\\n{a}\\n{b}\\n{c}\\n{d}\\n{e}\\n{a}\\n0\\n9\\n3\\n6\\n11\\n{b}\\n9\\n0\\n7\\n5\\n10\\n{c}\\n3\\n7\\n0\\n9\\n2\\n{d}\\n6\\n5\\n9\\n0\\n8\\n{e}\\n11\\n10\\n2\\n8\\n0\\nIn the above table, the minimum distance is the distance between the clusters {c} and {e}.\\nAlso\\nd({c},{e}) = 2.\\nWe merge {c} and {e} to form the cluster {c,e}.\\nThe new set of clusters C2: {a}, {b}, {d}, {c,e}.\\n3. Let us compute the distance of {c,e} from other clusters.\\nd({c,e},{a}) = max{d(c,a),d(e,a)} = max{3,11} = 11.\\nd({c,e},{b}) = max{d(c,b),d(e,b)} = max{7,10} = 10.\\nd({c,e},{d}) = max{d(c,d),d(e,d)} = max{9,8} = 9.\\nThe following table gives the distances between the various clusters in C2.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 212}, page_content='CHAPTER 13. CLUSTERING METHODS\\n198\\n{a}\\n{b}\\n{d}\\n{c,e}\\n{a}\\n0\\n9\\n6\\n11\\n{b}\\n9\\n0\\n5\\n10\\n{d}\\n6\\n5\\n0\\n9\\n{c,e}\\n11\\n10\\n9\\n0\\nIn the above table, the minimum distance is the distance between the clusters {b} and {d}.\\nAlso\\nd({b},{d}) = 5.\\nWe merge {b} and {d} to form the cluster {b,d}.\\nThe new set of clusters C3: {a}, {b,d}, {c,e}.\\n4. Let us compute the distance of {b,d} from other clusters.\\nd({b,d},{a}) = max{d(b,a),d(d,a)} = max{9,6} = 9.\\nd({b,d},{c,e}) = max{d(b,c),d(b,e),d(d,c),d(d,e)} = max{7,10,9,8} = 10.\\nThe following table gives the distances between the various clusters in C3.\\n{a}\\n{b,d}\\n{c,e}\\n{a}\\n0\\n9\\n11\\n{b,d}\\n9\\n0\\n10\\n{c,e}\\n11\\n10\\n0\\nIn the above table, the minimum distance is the distance between the clusters {a} and {b,d}.\\nAlso\\nd({a},{b,d}) = 9.\\nWe merge {a} and {b,d} to form the cluster {a,b,d}.\\nThe new set of clusters C4: {a,b,d}, {c,e}\\n5. Only two clusters are left. We merge them form a single cluster containing all data points. We\\nhave\\nd({a,b,d},{c,e}) = max{d(a,c),d(a,e),d(b,c),d(b,e),d(d,c),d(d,e)}\\n= max{3,11,7,10,9,8}\\n= 11\\n6. Figure 13.14 shows the dendrogram of the hierarchical clustering.\\nProblem 2\\nGiven the dataset {a,b,c,d,e} and the distance matrix given in Table 13.4, construct a dendrogram\\nby single-linkage hierarchical clustering using the agglomerative method.\\nSolution\\nThe complete-linkage clustering uses the “maximum formula”, that is, the following formula to\\ncompute the distance between two clusters A and B:\\nd(A,B) = min{d(x,y) ∶x ∈A,y ∈B}\\n1. Dataset : {a,b,c,d,e}.\\nInitial clustering (singleton sets) C1: {a}, {b}, {c}, {d}, {e}.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 213}, page_content='CHAPTER 13. CLUSTERING METHODS\\n199\\nDistance\\n0\\n2\\n4\\n6\\n8\\n10\\na\\nb\\nd\\nc\\ne\\nFigure 13.14: Dendrogram for the data given in Table 13.4 (complete linkage clustering)\\n2. The following table gives the distances between the various clusters in C1:\\n{a}\\n{b}\\n{c}\\n{d}\\n{e}\\n{a}\\n0\\n9\\n3\\n6\\n11\\n{b}\\n9\\n0\\n7\\n5\\n10\\n{c}\\n3\\n7\\n0\\n9\\n2\\n{d}\\n6\\n5\\n9\\n0\\n8\\n{e}\\n11\\n10\\n2\\n8\\n0\\nIn the above table, the minimum distance is the distance between the clusters {c} and {e}.\\nAlso\\nd({c},{e}) = 2.\\nWe merge {c} and {e} to form the cluster {c,e}.\\nThe new set of clusters C2: {a}, {b}, {d}, {c,e}.\\n3. Let us compute the distance of {c,e} from other clusters.\\nd({c,e},{a}) = min{d(c,a),d(e,a)} = max{3,11} = 3.\\nd({c,e},{b}) = min{d(c,b),d(e,b)} = max{7,10} = 7.\\nd({c,e},{d}) = min{d(c,d),d(e,d)} = max{9,8} = 8.\\nThe following table gives the distances between the various clusters in C2.\\n{a}\\n{b}\\n{d}\\n{c,e}\\n{a}\\n0\\n9\\n6\\n3\\n{b}\\n9\\n0\\n5\\n7\\n{d}\\n6\\n5\\n0\\n8\\n{c,e}\\n3\\n7\\n8\\n0\\nIn the above table, the minimum distance is the distance between the clusters {a} and {c,e}.\\nAlso\\nd({a},{c,e}) = 3.\\nWe merge {a} and {c,e} to form the cluster {a,c,e}.\\nThe new set of clusters C3: {a,c,e}, {b}, {d}.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 214}, page_content='CHAPTER 13. CLUSTERING METHODS\\n200\\n4. Let us compute the distance of {a,c,e} from other clusters.\\nd({a,c,e},{b}) = min{d(a,b),d(c,b),d(e,b)} = {9,7,10} = 7\\nd({a,c,e},{d}) = min{d(a,d),d(c,d),d(e,d)} = {6,9,8} = 6\\nThe following table gives the distances between the various clusters in C3.\\n{a,c,e}\\n{b}\\n{d}\\n{a,c,e}\\n0\\n7\\n6\\n{b}\\n7\\n0\\n5\\n{d}\\n6\\n5\\n0\\nIn the above table, the minimum distance is between {b} and {d}. Also\\nd({b},{d}) = 5.\\nWe merge {b} and {d} to form the cluster {b,d}.\\nThe new set of clusters C4: {a,c,e}, {b,d}\\n5. Only two clusters are left. We merge them form a single cluster containing all data points. We\\nhave\\nd({a,c,e},{b,d}) = min{d(a,b),d(a,d),d(c,b),d(c,d),d(e,b),d(e,d)}\\n= min{9,6,7,9,10,8}\\n= 6\\n6. Figure 13.15 shows the dendrogram of the hierarchical clustering.\\nDistance\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\na\\nc\\ne\\nb\\nd\\nFigure 13.15: Dendrogram for the data given in Table 13.4 (single linkage clustering)\\n13.11\\nAlgorithm for divisive hierarchical clustering\\nDivisive clustering algorithms begin with the entire data set as a single cluster, and recursively divide\\none of the existing clusters into two daughter clusters at each iteration in a top-down fashion. To\\napply this procedure, we need a separate algorithm to divide a given dataset into two clusters.\\n• The divisive algorithm may be implemented by using the k-means algorithm with k = 2 to\\nperform the splits at each iteration. However, it would not necessarily produce a splitting\\nsequence that possesses the monotonicity property required for dendrogram representation.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 215}, page_content='CHAPTER 13. CLUSTERING METHODS\\n201\\n13.11.1\\nDIANA (DIvisive ANAlysis)\\nDIANA is a divisive hierarchical clustering technique. Here is an outline of the algorithm.\\nStep 1. Suppose that cluster Cl is going to be split into clusters Ci and Cj.\\nStep 2. Let Ci = Cl and Cj = ∅.\\nStep 3. For each object x ∈Ci:\\n(a) For the ﬁrst iteration, compute the average distance of x to all other objects.\\n(b) For the remaining iterations, compute\\nDx = average{d(x,y) ∶y ∈Ci} −average{d(x,y) ∶y ∈Cj}.\\nx\\nCi\\nCj\\nFigure 13.16: Dx= (average of dashed lines) −(average of solid lines)\\nStep 4.\\n(a) For the ﬁrst iteration, move the object with the maximum average distance to Cj.\\n(b) For the remaining iterations, ﬁnd an object x in Ci for which Dx is the largest. If\\nDx > 0 then move x to Cj.\\nStep 5. Repeat Steps 3(b) and 4(b) until all differences Dx are negative. Then Cl is split into Ci and\\nCj.\\nStep 6. Select the smaller cluster with the largest diameter. (The diameter of a cluster is the largest\\ndissimilarity between any two of its objects.) Then divide this cluster, following Steps 1-5.\\nStep 7. Repeat Step 6 until all clusters contain only a single object.\\n13.11.2\\nExample\\nProblem\\nGiven the dataset {a,b,c,d,e} and the distance matrix in Table 13.4, construct a dendrogram by the\\ndivisive analysis algorithm.\\nSolution\\n1. We have, initially\\nCl = {a,b,c,d,e}\\n2. We write\\nCi = Cl,\\nCj = ∅.\\n3. Division into clusters'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 216}, page_content='CHAPTER 13. CLUSTERING METHODS\\n202\\n(a) Initial iteration\\nLet us calculate the average dissimilarities of the objects in Ci with the other objects in\\nCi.\\nAverage dissimilarity of a\\n= 1\\n4(d(a,b) + d(a,c) + d(a,e)) = 1\\n4(9 + 3 + 6 + 11) = 7.25\\nSimilarly we have :\\nAverage dissimilarity of b = 7.75\\nAverage dissimilarity of c = 5.25\\nAverage dissimilarity of d = 7.00\\nAverage dissimilarity of e = 7.75\\nThe highest average distance is 7.75 and there are two corresponding objects. We choose\\none of them, b, arbitrarily. We move b to Cj.\\nWe now have\\nCi = {a,c,d,e},\\nCj = ∅∪{b} = {b}.\\n(b) Remaining iterations\\n(i) 2-nd iteration.\\nDa = 1\\n3(d(a,c) + d(a,d) + d(a,e)) −1\\n1(d(a,b)) = 20\\n3 −9 = −2.33\\nDc = 1\\n3(d(c,a) + d(c,d) + d(c,e)) −1\\n1(d(c,b)) = 14\\n3 −7 = −2.33\\nDd = 1\\n3(d(d,a) + d(d,c) + d(d,e)) −1\\n1(d(c,b)) = 23\\n3 −7 = 0.67\\nDe = 1\\n3(d(e,a) + d(e,c) + d(e,d)) −1\\n1(d(e,b)) = 21\\n3 −7 = 0\\nDd is the largest and Dd > 0. So we move, d to Cj.\\nWe now have\\nCi = {a,c,e},\\nCj = {b} ∪{d} = {b,d}.\\n(ii) 3-rd iteration\\nDa = 1\\n2(d(a,c) + d(a,e)) −1\\n2(d(a,b) + d(a,d)) = 14\\n2 −15\\n2 = −0.5\\nDc = 1\\n2(d(c,a) + d(c,e)) −1\\n2(d(c,b) + d(c,d)) = 5\\n2 −16\\n2 = −13.5\\nDe = 1\\n2(d(e,a) + d(e,c)) −1\\n2(d(e,b) + d(e,d)) = 13\\n2 −18\\n2 = −2.5\\nAll are negative. So we stop and form the clusters Ci and Cj.\\n4. To divide, Ci and Cj, we compute their diameters.\\ndiameter(Ci) = max{d(a,c),d(a,e),d(c,e)}\\n= max{3,11,2}\\n= 11\\ndiameter(Cj) = max{d(b,d)}\\n= 5\\nThe cluster with the largest diameter is Ci. So we now split Ci.\\nWe repeat the process by taking Cl = {a,c,e}. The remaining computations are left as an\\nexercise to the reader.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 217}, page_content='CHAPTER 13. CLUSTERING METHODS\\n203\\n13.12\\nDensity-based clustering\\nIn density-based clustering, clusters are deﬁned as areas of higher density than the remainder of the\\ndata set. Objects in these sparse areas - that are required to separate clusters - are usually considered\\nto be noise and border points. The most popular density based clustering method is DBSCAN\\n(Density-Based Spatial Clustering of Applications with Noise).\\nFigure 13.17: Clusters of points and noise points not belonging to any of those clusters\\n13.12.1\\nDensity\\nWe introduce some terminology and notations.\\n• Let ϵ (epsilon) be some constant distance. Let p be an arbitrary data point. The ϵ-neighbourhood\\nof p is the set\\nNϵ(p) = {q ∶d(p,q) < ϵ}\\n• We choose some number m0 to deﬁne points of “high density”: We say that a point p is point\\nof high density if Nϵ(p) contains at least m0 points.\\n• We deﬁne a point p as a core point if Nϵ(p) has more than m0 points.\\n• We deﬁne a point p as a border point if Nϵ(p) has fewer than m0 points, but is in the ϵ-\\nneighbourhood of a core point.\\n• A point which is neither a core point nor a border point is called a noise point.\\np\\np\\np\\nq\\nq\\nr\\n(a)\\n(b)\\n(c)\\n(d)\\nFigure 13.18: With m0 = 4: (a) p a point of high density (b) p a core point (c) p a border point\\n(d) r a noise point\\n• An object q is directly density-reachable from object p if p is a core object and q is in Nϵ(p).\\n• An object q is indirectly density-reachable from an object p if there is a ﬁnite set of objects\\np1,...,pr such that p1 is directly density-reachable form p, p2 is directly density reachable\\nfrom p1, etc., q is directly density-reachable form pr.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 218}, page_content='CHAPTER 13. CLUSTERING METHODS\\n204\\nq\\np\\np\\np1\\np2\\np3\\nq\\n(a)\\n(b)\\nFigure 13.19: With m0 = 4: (a) q is directly density-reachable from p (b) q is indirectly\\ndensity-reachable from p\\n13.12.2\\nDBSCAN algorithm\\nLet X = {x1,x2,...,xn} be the set of data points. DBSCAN requires two parameters: ϵ (eps) and\\nthe minimum number of points required to form a cluster (m0).\\nStep 1.\\nStart with an arbitrary starting point p that has not been visited.\\nStep 2.\\nExtract the ϵ-neighborhood Nϵ(p) of p.\\nStep 3.\\nIf the number of points in Nϵ(p) is not greater than m0 then the point p is labeled as noise\\n(later this point can become the part of the cluster).\\nStep 4.\\nIf the number of points in Nϵ(p) is greater than m0 then the point p is a core point and is\\nmarked as visited. Select a new cluster-id and mark all objects in Nϵ(p) with this cluster-id.\\nStep 5.\\nIf a point is found to be a part of the cluster then its ϵ-neighborhood is also the part of the\\ncluster and the above procedure from step 2 is repeated for all ϵ-neighborhood points. This\\nis repeated until all points in the cluster are determined.\\nStep 6.\\nA new unvisited point is retrieved and processed, leading to the discovery of a further\\ncluster or noise.\\nStep 7.\\nThis process continues until all points are marked as visited.\\n13.13\\nSample questions\\n(a) Short answer questions\\n1. What is clustering?\\n2. Is clustering supervised learning? Why?\\n3. Explain some applications of the k-means algorithm.\\n4. Explain how clustering technique is used in image segmentation problem.\\n5. Explain how clustering technique used in data compression.\\n6. What is meant by the mixture of two normal distributions?\\n7. Explain hierarchical clustering.\\n8. What is a dendrogram? Give an example.\\n9. Is hierarchical clustering unsupervised learning? Why?\\n10. Describe the two methods for hierarchical clustering.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 219}, page_content='CHAPTER 13. CLUSTERING METHODS\\n205\\n11. In a clustering problem, what does the measure of dissimilarity measure? Give some examples\\nof measures of dissimilarity.\\n12. Explain the different types of linkages in clustering.\\n13. In the context of density-based clustering, deﬁne high density point, core point, border point\\nand noise point.\\n14. What is agglomerative hierarchical clustering?\\n(b) Long answer questions\\n1. Apply k-means algorithm for given data with k = 3. Use C1(2), C2(16) and C3(38) as initial\\ncenters. Data:\\n2,4,6,3,31,12,15,16,38,35,14,21,3,25,30\\n2. Explain K-means algorithm and group the points (1, 0, 1), (1, 1, 0), (0, 0, 1) and (1, 1, 1) using\\nK-means algorithm.\\n3. Applying the k-means algorithm, ﬁnd two clusters in the following data.\\nx\\n185\\n170\\n168\\n179\\n182\\n188\\n180\\n180\\n183\\n180\\n180\\n177\\ny\\n72\\n56\\n60\\n68\\n72\\n77\\n71\\n70\\n84\\n88\\n67\\n76\\n4. Use k-means algorithm to ﬁnd 2 clusters in the following data:\\nNo.\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nx1\\n1.0\\n1.5\\n3.0\\n5.0\\n3.5\\n4.5\\n3.5\\nx2\\n1.0\\n2.0\\n4.0\\n7.0\\n5.0\\n5.0\\n4.5\\n5. Give a general outline of the expectation-maximization algorithm.\\n6. Describe EM algorithm for Gaussian mixtures.\\n7. Describe an algorithm for agglomerative hierarchical clustering.\\n8. Given the following distance matrix, construct the dendrogram using agglomerative clustering\\nwith single linkage, complete linkage and average linkage.\\nA\\nB\\nC\\nD\\nE\\nA\\n0\\n1\\n2\\n2\\n3\\nB\\n1\\n0\\n2\\n4\\n3\\nC\\n2\\n2\\n0\\n1\\n5\\nD\\n2\\n4\\n1\\n0\\n3\\nE\\n3\\n3\\n5\\n3\\n0\\n9. Describe an algorithm for divisive hierarchical clustering.\\n10. For the data in Question 8, construct a dendrogram using DIANA algorithm.\\n11. Describe the DBSCAN algorithm for clustering.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 220}, page_content='Bibliography\\n[1] Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006.\\n[2] Ethem Alpaydin, Introduction to Machine Learning, The MIT Press, Cambridge, Mas-\\nsachusetts, 2004.\\n[3] Margaret H. Dunham, Data Mining: Introductory and Advanced Topics, Pearson, 2006.\\n[4] Mitchell T., Machine Learning, McGraw Hill.\\n[5] Ryszard S. Michalski, Jaime G. Carbonell, and Tom M. Mitchell, Machine Learning : An\\nArtiﬁcial Intelligence Approach, Tioga Publishing Company.\\n[6] Michael J. Kearns and Umesh V. Vazirani, An Introduction to Computational Learning Theory,\\nThe MIT Press, Cambridge, Massachusetts, 1994.\\n[7] D. H. Wolpert, W. G. Macready (1997), “No Free Lunch Theorems for Optimization”, IEEE\\nTransactions on Evolutionary Computation 1, 67.\\n206'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 221}, page_content='Index\\n5-by-2 cross-validation, 50\\nabstraction, 3\\naccuracy, 54\\nactivation function, 113\\nGaussian -, 115\\nhyperbolic -, 116\\nlinear -, 115\\nthreshold -, 114\\nunit step -, 114\\nagglomerative method, 192\\nalgorithm\\nbackpropagation -, 123\\nbackward selection -, 37\\nBaum-Welch, 170\\nC4.5 -, 105\\nDBSCAN -, 204\\ndecision tree -, 95\\nDIANA -, 201\\nforward selection -, 36\\nForwards-Backwards, 170\\nID3 -, 96\\nkernel method -, 157\\nnaive Bayes -, 65\\nPCA -, 40\\nperceptron learning -, 118\\nrandom forest -, 177\\nSVM -, 149\\nViterbi -, 170\\nANN, 119\\nArthur Samuel, 1\\nartiﬁcial neural networks, 119\\nassociation rule, 6\\nattribute, 4\\naxis-aligned rectangle, 18\\naxon, 111\\nbackpropagation algorithm, 123\\nbackward phase, 123\\nbackward selection, 37\\nBasic problems of HMM’s, 169\\nBaum-Welch algorithm, 170\\nBayes’ theorem, 62\\nbias, 23\\nbimodal mixture, 186\\nbinary classiﬁcation, 15\\nbootstrap, 51\\nbootstrap sampling, 51\\nbootstrapping, 51\\nborder point, 203\\nC4.5 algorithm, 105\\nCART algorithm, 105\\nclassiﬁcation, 7\\nclassiﬁcation tree, 84\\ncluster analysis, 179\\nclustering, 179\\ncomplete-linkage -, 196\\ndensity-based -, 203\\nfarthest neighbour -, 196\\nhierarchical -, 191\\nk-means -, 179\\nnearest neighbour -, 196\\nsingle-linkage -, 196\\ncomplete-linkage clustering, 196\\ncompression, 8\\ncomputational learning theory, 31\\nconcept class, 31\\nconditional probability, 61\\nconfusion matrix, 52\\nconsistent, 16\\nconstruction of tree, 85\\ncore point, 203\\ncost function, 121\\ncovariance matrix, 40\\ncross-validation, 25, 49\\n5-by-2 -, 50\\nhold-out -, 49\\nK-fold -, 49\\nleave-one-out -, 50\\ndata\\ncategorical -, 5\\nnominal -, 5\\nnumeric - , 5\\nordinal -, 5\\ndata compression, 8, 185\\ndata storage, 2\\nDBSCAN algorithm, 204\\ndecision tree, 83\\n207'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 222}, page_content='INDEX\\n208\\ndecision tree algorithm, 95\\ndeep learning, 129\\ndeep neural network, 129\\ndelta learning rule, 127\\ndendrogram, 191\\ndenrite, 111\\ndensity-based clustering, 203\\nDIANA, 201\\ndichotomy, 27\\ndimensionality reduction, 35\\ndirectly-density reachable, 203\\ndiscrete Markov process, 165\\ndiscriminant, 9\\ndissimilarity, 192\\nDIvisive ANAlysis, 201\\ndivisive method, 194\\nE-step, 189\\neigenvalue, 40\\neigenvector, 41\\nEM algorithm, 189\\nensemble learning, 176\\nentropy, 89\\nepoch, 123\\nerror rate, 54\\nevaluation, 3\\nevent\\nindependent -, 61\\nexample, 4\\nexpectation step, 189\\nexpectation-maximization algorithm, 189\\nexperience\\nlearning from -, 1\\nface recognition, 8\\nfalse negative, 51\\nfalse positive, 51\\nfalse positive rate, 55\\nfarthest neighbour clustering, 196\\nfeature, 4\\nfeature extraction, 35\\nfeature selection, 35\\nfeedforward network, 120\\nﬁrst layer, 120\\nﬁrst principal component, 41\\nforward phase, 123\\nforward selection, 36\\nForwards-Backwards algorithms, 170\\nFPR, 55\\nGaussian activation function, 115\\nGaussian mixture, 190\\ngenralisation, 3\\nGini index, 94\\nGini split index, 94\\ngradient descent method, 123\\nhidden Markov model, 169\\nhidden node, 120\\nhierarchical clustering, 191\\nhigh density point, 203\\nHMM, 169\\nbasic problems, 169\\ncoin tossing example, 167\\nEvaluation problem, 169\\nlearning parameter problem, 170\\nstate sequence problem, 170\\nurn and ball model, 168\\nholdout method, 49\\nhomogeneity property, 164\\nhyperplane, 141\\nhypothesis, 15\\nhypothesis space, 16\\nID3 algorithm, 96\\nimage segmentation, 185\\nindependent\\nmutually -, 61\\npairwise -, 61\\nindependent event, 61\\nindirectly density-reachable, 203\\ninductive bias, 23\\ninformation gain, 92\\ninitial probability, 164\\ninner product, 140\\ninput feature, 15\\ninput node, 120\\ninput representation, 15\\ninstance, 4\\ninstance space, 29\\ninternal node, 83\\nisolated word recognition, 170\\nK-fold cross-validation, 49\\nk-means clustering, 179\\nkernel\\nGaussian -, 157\\nhomogeneous polynomial -, 156\\nLaplacian -, 157\\nnon-homogeneous polynomial -, 156\\nradial basis function -, 157\\nkernel function, 155\\nkernel method, 157\\nkernel method algorithm, 157\\nknowledge extraction, 8\\nLaplacian kernel, 157\\nlatent variable, 188\\nlayer in networks, 120\\nleaf node, 83'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 223}, page_content='INDEX\\n209\\nlearner, 2\\nlearning, 1\\nreinforcement -, 13\\nsupervised -, 11\\nunsupervised - , 12\\nlearning associations, 6\\nlearning program, 2\\nlearning theory, 31\\nleave-one-out, 50\\nlength of an instance, 32\\nLevenshtein distance, 194\\nlikelihood, 63\\nlinear activation function, 115\\nlinear regression, 73\\nlinearly separable data, 144\\nlogistic function, 114\\nlogistic regression, 73\\nM-step, 189\\nmachine learning, 1\\ndeﬁnition of -, 1\\nmachine learning program, 2\\nMarkov property, 164\\nmaximal margin hyperplane, 145\\nmaximisation step, 189\\nmaximum margin hyperplane, 145\\nmean squared error, 35\\nmeasure of dissimilarity, 194\\nmisclassiﬁcation rate, 36\\nmixture of distributions, 186\\nmodel, 1\\nmodel selection, 23\\nmore general than, 18\\nmore speciﬁc than, 18\\nmulticlass SVM, 158\\nmultimodal distribution, 186\\nmultiple class, 22\\nmultiple linear regression, 78\\nmultiple regression, 73\\nnaive Bayes algorithm, 65\\nnearest neighbour clustering, 196\\nnegative example, 15\\nneighbourhood, 203\\nnetwork topology, 119\\nneural networks, 119\\nneuron\\nartiﬁcial -, 112\\nbiological -, 111\\nno-free lunch theorem, 48\\nnoise, 22\\nnoise point, 203\\nnorm, 140\\nobservable Markov model, 165\\nOccam’s razor, 24\\nOLS method, 74\\none-against-all, 22\\none-against-all method, 158\\none-against-one, 23\\none-against-one method, 158\\noptical character recognition, 8\\noptimal separating hyperplane, 146\\nordinary least square, 74\\northogonality, 140\\noutput node, 120\\noverﬁtting, 24\\nPAC learnability, 31\\nPAC learning, 31\\nPCA, 38\\nPCA algorithm, 40\\nperceptron, 116\\nperceptron learning algorithm, 118\\nperformance measure, 1\\nperpendicular distance, 144\\nperpendicularity, 140\\npolynomial kernel, 156\\npolynomial regression, 73\\npositive example, 15\\nprecision, 53\\nprincipal component, 41\\nprincipal component analysis, 38\\nprobability\\nconditional -, 61\\nposterior -, 63\\nprior -, 62\\nprobably approximately correct learning, 31\\nradial basis function kernel, 157\\nrandom forest, 176\\nrandom forest algorithm, 177\\nrandom performance, 55\\nRDF kernel, 157\\nrecall, 53\\nReceiver Operating Characteristic, 54\\nrecord, 4\\nrecurrent network, 120\\nregression, 10\\nlogistic -, 73\\nmultiple , 73\\npolynomial -, 73\\nsimple linear -, 73\\nregression function, 10\\nregression problem, 72\\nregression tree, 84, 101\\nreinforcement learning, 13\\nROC, 54\\nROC curve, 56'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 224}, page_content='INDEX\\n210\\nROC space, 55\\nsaturated linear function, 115\\nscalar, 139\\nsensitivity, 54\\nseparating line, 134\\nshallow network, 129\\nshattering, 28\\nsigmoid function, 114\\nsimple linear regression, 73\\nsingle-linkage clustering, 196\\nsize of a concept, 32\\nslack variable, 154\\nsoft margin hyperplane, 154\\nspeciﬁcity, 54\\nspeech recognition, 8\\nstorage, 2\\nstrictly more general than, 18\\nstrictly more speciﬁc than, 18\\nsubset selection, 36\\nsupervised learning, 11\\nsupport vector, 146\\nsupport vector machine, 146\\nSVM, 146\\nSVM algorithm, 149\\nSVM classiﬁer, 148\\nsynapse, 111\\nthreshold function, 114\\nTPR, 55\\ntraining, 3\\ntransition probability, 164\\ntree, 83\\nclassiﬁcation -, 84\\nregression -, 84\\ntrue negative, 51\\ntrue positive, 51\\ntrue positive rate, 55\\ntwo-class data set, 144\\nunderﬁtting, 24\\nunimodal distribution, 186\\nunit of observation, 4\\nunit step function, 114\\nunsupervised learning, 12\\nvalidation set, 25\\nVapnik-Chervonenkis dimension, 29\\nvariable, 4\\nVC dimension, 29\\nvector space, 138\\nﬁnite dimensional -, 138\\nversion space, 19\\nViterbi algorithm, 170\\nweighted least squares, 75\\nword recognition, 170\\nzero vector, 139'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'TeX', 'creationdate': '2018-07-26T14:06:57+05:30', 'source': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'file_path': 'rag-dataset-main/machine-learning/NotesOnMachineLearningForBTech-1.pdf', 'total_pages': 226, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2018-09-29T11:07:05+00:00', 'trapped': '', 'modDate': 'D:20180929110705Z', 'creationDate': \"D:20180726140657+05'30'\", 'page': 225}, page_content=''),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 0}, page_content='CS229 Lecture Notes\\nAndrew Ng and Tengyu Ma\\nJune 11, 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 1}, page_content='Contents\\nI\\nSupervised learning\\n5\\n1\\nLinear regression\\n8\\n1.1\\nLMS algorithm\\n. . . . . . . . . . . . . . . . . . . . . . . . . .\\n9\\n1.2\\nThe normal equations . . . . . . . . . . . . . . . . . . . . . . .\\n13\\n1.2.1\\nMatrix derivatives . . . . . . . . . . . . . . . . . . . . .\\n13\\n1.2.2\\nLeast squares revisited . . . . . . . . . . . . . . . . . .\\n14\\n1.3\\nProbabilistic interpretation . . . . . . . . . . . . . . . . . . . .\\n15\\n1.4\\nLocally weighted linear regression (optional reading) . . . . . .\\n17\\n2\\nClassiﬁcation and logistic regression\\n20\\n2.1\\nLogistic regression\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n20\\n2.2\\nDigression: the perceptron learning algorithm\\n. . . . . . . . .\\n23\\n2.3\\nMulti-class classiﬁcation\\n. . . . . . . . . . . . . . . . . . . . .\\n24\\n2.4\\nAnother algorithm for maximizing ℓ(θ) . . . . . . . . . . . . .\\n27\\n3\\nGeneralized linear models\\n29\\n3.1\\nThe exponential family . . . . . . . . . . . . . . . . . . . . . .\\n29\\n3.2\\nConstructing GLMs . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n3.2.1\\nOrdinary least squares . . . . . . . . . . . . . . . . . .\\n32\\n3.2.2\\nLogistic regression\\n. . . . . . . . . . . . . . . . . . . .\\n33\\n4\\nGenerative learning algorithms\\n34\\n4.1\\nGaussian discriminant analysis . . . . . . . . . . . . . . . . . .\\n35\\n4.1.1\\nThe multivariate normal distribution . . . . . . . . . .\\n35\\n4.1.2\\nThe Gaussian discriminant analysis model . . . . . . .\\n38\\n4.1.3\\nDiscussion: GDA and logistic regression\\n. . . . . . . .\\n40\\n4.2\\nNaive bayes (Option Reading) . . . . . . . . . . . . . . . . . .\\n41\\n4.2.1\\nLaplace smoothing . . . . . . . . . . . . . . . . . . . .\\n44\\n4.2.2\\nEvent models for text classiﬁcation . . . . . . . . . . .\\n46\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 2}, page_content='CS229 Spring 20223\\n2\\n5\\nKernel methods\\n48\\n5.1\\nFeature maps . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n48\\n5.2\\nLMS (least mean squares) with features . . . . . . . . . . . . .\\n49\\n5.3\\nLMS with the kernel trick\\n. . . . . . . . . . . . . . . . . . . .\\n49\\n5.4\\nProperties of kernels\\n. . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6\\nSupport vector machines\\n59\\n6.1\\nMargins: intuition . . . . . . . . . . . . . . . . . . . . . . . . .\\n59\\n6.2\\nNotation (option reading)\\n. . . . . . . . . . . . . . . . . . . .\\n61\\n6.3\\nFunctional and geometric margins (option reading)\\n. . . . . .\\n61\\n6.4\\nThe optimal margin classiﬁer (option reading) . . . . . . . . .\\n63\\n6.5\\nLagrange duality (optional reading) . . . . . . . . . . . . . . .\\n65\\n6.6\\nOptimal margin classiﬁers: the dual form (option reading)\\n. .\\n68\\n6.7\\nRegularization and the non-separable case (optional reading) .\\n72\\n6.8\\nThe SMO algorithm (optional reading) . . . . . . . . . . . . .\\n73\\n6.8.1\\nCoordinate ascent . . . . . . . . . . . . . . . . . . . . .\\n74\\n6.8.2\\nSMO . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n75\\nII\\nDeep learning\\n79\\n7\\nDeep learning\\n80\\n7.1\\nSupervised learning with non-linear models . . . . . . . . . . .\\n80\\n7.2\\nNeural networks . . . . . . . . . . . . . . . . . . . . . . . . . .\\n84\\n7.3\\nModules in Modern Neural Networks . . . . . . . . . . . . . .\\n92\\n7.4\\nBackpropagation\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n98\\n7.4.1\\nPreliminaries on partial derivatives . . . . . . . . . . .\\n99\\n7.4.2\\nGeneral strategy of backpropagation\\n. . . . . . . . . . 102\\n7.4.3\\nBackward functions for basic modules . . . . . . . . . . 105\\n7.4.4\\nBack-propagation for MLPs . . . . . . . . . . . . . . . 107\\n7.5\\nVectorization over training examples\\n. . . . . . . . . . . . . . 109\\nIII\\nGeneralization and regularization\\n112\\n8\\nGeneralization\\n113\\n8.1\\nBias-variance tradeoﬀ. . . . . . . . . . . . . . . . . . . . . . . 115\\n8.1.1\\nA mathematical decomposition (for regression) . . . . . 120\\n8.2\\nThe double descent phenomenon . . . . . . . . . . . . . . . . . 121\\n8.3\\nSample complexity bounds (optional readings) . . . . . . . . . 126'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 3}, page_content='CS229 Spring 20223\\n3\\n8.3.1\\nPreliminaries\\n. . . . . . . . . . . . . . . . . . . . . . . 126\\n8.3.2\\nThe case of ﬁnite H . . . . . . . . . . . . . . . . . . . . 128\\n8.3.3\\nThe case of inﬁnite H\\n. . . . . . . . . . . . . . . . . . 131\\n9\\nRegularization and model selection\\n135\\n9.1\\nRegularization . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\\n9.2\\nImplicit regularization eﬀect (optional reading) . . . . . . . . . 137\\n9.3\\nModel selection via cross validation . . . . . . . . . . . . . . . 139\\n9.4\\nBayesian statistics and regularization . . . . . . . . . . . . . . 142\\nIV\\nUnsupervised learning\\n144\\n10 Clustering and the k-means algorithm\\n145\\n11 EM algorithms\\n148\\n11.1 EM for mixture of Gaussians . . . . . . . . . . . . . . . . . . . 148\\n11.2 Jensen’s inequality\\n. . . . . . . . . . . . . . . . . . . . . . . . 151\\n11.3 General EM algorithms . . . . . . . . . . . . . . . . . . . . . . 152\\n11.3.1 Other interpretation of ELBO . . . . . . . . . . . . . . 158\\n11.4 Mixture of Gaussians revisited . . . . . . . . . . . . . . . . . . 158\\n11.5 Variational inference and variational auto-encoder (optional\\nreading) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\\n12 Principal components analysis\\n165\\n13 Independent components analysis\\n171\\n13.1 ICA ambiguities . . . . . . . . . . . . . . . . . . . . . . . . . . 172\\n13.2 Densities and linear transformations . . . . . . . . . . . . . . . 173\\n13.3 ICA algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\\n14 Self-supervised learning and foundation models\\n177\\n14.1 Pretraining and adaptation . . . . . . . . . . . . . . . . . . . . 177\\n14.2 Pretraining methods in computer vision . . . . . . . . . . . . . 179\\n14.3 Pretrained large language models . . . . . . . . . . . . . . . . 181\\n14.3.1 Open up the blackbox of Transformers . . . . . . . . . 183\\n14.3.2 Zero-shot learning and in-context learning\\n. . . . . . . 186'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 4}, page_content='CS229 Spring 20223\\n4\\nV\\nReinforcement Learning and Control\\n188\\n15 Reinforcement learning\\n189\\n15.1 Markov decision processes . . . . . . . . . . . . . . . . . . . . 190\\n15.2 Value iteration and policy iteration . . . . . . . . . . . . . . . 192\\n15.3 Learning a model for an MDP . . . . . . . . . . . . . . . . . . 194\\n15.4 Continuous state MDPs\\n. . . . . . . . . . . . . . . . . . . . . 196\\n15.4.1 Discretization . . . . . . . . . . . . . . . . . . . . . . . 196\\n15.4.2 Value function approximation . . . . . . . . . . . . . . 199\\n15.5 Connections between Policy and Value Iteration (Optional) . . 203\\n16 LQR, DDP and LQG\\n206\\n16.1 Finite-horizon MDPs . . . . . . . . . . . . . . . . . . . . . . . 206\\n16.2 Linear Quadratic Regulation (LQR) . . . . . . . . . . . . . . . 210\\n16.3 From non-linear dynamics to LQR\\n. . . . . . . . . . . . . . . 213\\n16.3.1 Linearization of dynamics\\n. . . . . . . . . . . . . . . . 214\\n16.3.2 Diﬀerential Dynamic Programming (DDP) . . . . . . . 214\\n16.4 Linear Quadratic Gaussian (LQG) . . . . . . . . . . . . . . . . 216\\n17 Policy Gradient (REINFORCE)\\n220'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 5}, page_content='Part I\\nSupervised learning\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 6}, page_content='6\\nLet’s start by talking about a few examples of supervised learning prob-\\nlems. Suppose we have a dataset giving the living areas and prices of 47\\nhouses from Portland, Oregon:\\nLiving area (feet2)\\nPrice (1000$s)\\n2104\\n400\\n1600\\n330\\n2400\\n369\\n1416\\n232\\n3000\\n540\\n...\\n...\\nWe can plot this data:\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\n3500\\n4000\\n4500\\n5000\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n800\\n900\\n1000\\nhousing prices\\nsquare feet\\nprice (in $1000)\\nGiven data like this, how can we learn to predict the prices of other houses\\nin Portland, as a function of the size of their living areas?\\nTo establish notation for future use, we’ll use x(i) to denote the “input”\\nvariables (living area in this example), also called input features, and y(i)\\nto denote the “output” or target variable that we are trying to predict\\n(price). A pair (x(i), y(i)) is called a training example, and the dataset\\nthat we’ll be using to learn—a list of n training examples {(x(i), y(i)); i =\\n1, . . . , n}—is called a training set. Note that the superscript “(i)” in the\\nnotation is simply an index into the training set, and has nothing to do with\\nexponentiation. We will also use X denote the space of input values, and Y\\nthe space of output values. In this example, X = Y = R.\\nTo describe the supervised learning problem slightly more formally, our\\ngoal is, given a training set, to learn a function h : X 7→Y so that h(x) is a\\n“good” predictor for the corresponding value of y. For historical reasons, this'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 7}, page_content='7\\nfunction h is called a hypothesis. Seen pictorially, the process is therefore\\nlike this:\\nTraining \\n    set\\nhouse.)\\n(living area of\\nLearning \\nalgorithm\\nh\\npredicted y\\nx\\n(predicted price)\\nof house)\\nWhen the target variable that we’re trying to predict is continuous, such\\nas in our housing example, we call the learning problem a regression prob-\\nlem. When y can take on only a small number of discrete values (such as\\nif, given the living area, we wanted to predict if a dwelling is a house or an\\napartment, say), we call it a classiﬁcation problem.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 8}, page_content='Chapter 1\\nLinear regression\\nTo make our housing example more interesting, let’s consider a slightly richer\\ndataset in which we also know the number of bedrooms in each house:\\nLiving area (feet2)\\n#bedrooms\\nPrice (1000$s)\\n2104\\n3\\n400\\n1600\\n3\\n330\\n2400\\n3\\n369\\n1416\\n2\\n232\\n3000\\n4\\n540\\n...\\n...\\n...\\nHere, the x’s are two-dimensional vectors in R2. For instance, x(i)\\n1\\nis the\\nliving area of the i-th house in the training set, and x(i)\\n2\\nis its number of\\nbedrooms. (In general, when designing a learning problem, it will be up to\\nyou to decide what features to choose, so if you are out in Portland gathering\\nhousing data, you might also decide to include other features such as whether\\neach house has a ﬁreplace, the number of bathrooms, and so on. We’ll say\\nmore about feature selection later, but for now let’s take the features as\\ngiven.)\\nTo perform supervised learning, we must decide how we’re going to rep-\\nresent functions/hypotheses h in a computer. As an initial choice, let’s say\\nwe decide to approximate y as a linear function of x:\\nhθ(x) = θ0 + θ1x1 + θ2x2\\nHere, the θi’s are the parameters (also called weights) parameterizing the\\nspace of linear functions mapping from X to Y. When there is no risk of\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 9}, page_content='9\\nconfusion, we will drop the θ subscript in hθ(x), and write it more simply as\\nh(x). To simplify our notation, we also introduce the convention of letting\\nx0 = 1 (this is the intercept term), so that\\nh(x) =\\nd\\nX\\ni=0\\nθixi = θTx,\\nwhere on the right-hand side above we are viewing θ and x both as vectors,\\nand here d is the number of input variables (not counting x0).\\nNow, given a training set, how do we pick, or learn, the parameters θ?\\nOne reasonable method seems to be to make h(x) close to y, at least for\\nthe training examples we have. To formalize this, we will deﬁne a function\\nthat measures, for each value of the θ’s, how close the h(x(i))’s are to the\\ncorresponding y(i)’s. We deﬁne the cost function:\\nJ(θ) = 1\\n2\\nn\\nX\\ni=1\\n(hθ(x(i)) −y(i))2.\\nIf you’ve seen linear regression before, you may recognize this as the familiar\\nleast-squares cost function that gives rise to the ordinary least squares\\nregression model.\\nWhether or not you have seen it previously, let’s keep\\ngoing, and we’ll eventually show this to be a special case of a much broader\\nfamily of algorithms.\\n1.1\\nLMS algorithm\\nWe want to choose θ so as to minimize J(θ). To do so, let’s use a search\\nalgorithm that starts with some “initial guess” for θ, and that repeatedly\\nchanges θ to make J(θ) smaller, until hopefully we converge to a value of\\nθ that minimizes J(θ).\\nSpeciﬁcally, let’s consider the gradient descent\\nalgorithm, which starts with some initial θ, and repeatedly performs the\\nupdate:\\nθj := θj −α ∂\\n∂θj\\nJ(θ).\\n(This update is simultaneously performed for all values of j = 0, . . . , d.)\\nHere, α is called the learning rate. This is a very natural algorithm that\\nrepeatedly takes a step in the direction of steepest decrease of J.\\nIn order to implement this algorithm, we have to work out what is the\\npartial derivative term on the right hand side. Let’s ﬁrst work it out for the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 10}, page_content='10\\ncase of if we have only one training example (x, y), so that we can neglect\\nthe sum in the deﬁnition of J. We have:\\n∂\\n∂θj\\nJ(θ)\\n=\\n∂\\n∂θj\\n1\\n2 (hθ(x) −y)2\\n=\\n2 · 1\\n2 (hθ(x) −y) · ∂\\n∂θj\\n(hθ(x) −y)\\n=\\n(hθ(x) −y) · ∂\\n∂θj\\n \\nd\\nX\\ni=0\\nθixi −y\\n!\\n=\\n(hθ(x) −y) xj\\nFor a single training example, this gives the update rule:1\\nθj := θj + α\\n\\x00y(i) −hθ(x(i))\\n\\x01\\nx(i)\\nj .\\nThe rule is called the LMS update rule (LMS stands for “least mean squares”),\\nand is also known as the Widrow-Hoﬀlearning rule. This rule has several\\nproperties that seem natural and intuitive. For instance, the magnitude of\\nthe update is proportional to the error term (y(i) −hθ(x(i))); thus, for in-\\nstance, if we are encountering a training example on which our prediction\\nnearly matches the actual value of y(i), then we ﬁnd that there is little need\\nto change the parameters; in contrast, a larger change to the parameters will\\nbe made if our prediction hθ(x(i)) has a large error (i.e., if it is very far from\\ny(i)).\\nWe’d derived the LMS rule for when there was only a single training\\nexample. There are two ways to modify this method for a training set of\\nmore than one example. The ﬁrst is replace it with the following algorithm:\\nRepeat until convergence {\\nθj := θj + α\\nn\\nX\\ni=1\\n\\x00y(i) −hθ(x(i))\\n\\x01\\nx(i)\\nj , (for every j)\\n(1.1)\\n}\\n1We use the notation “a := b” to denote an operation (in a computer program) in\\nwhich we set the value of a variable a to be equal to the value of b. In other words, this\\noperation overwrites a with the value of b. In contrast, we will write “a = b” when we are\\nasserting a statement of fact, that the value of a is equal to the value of b.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 11}, page_content='11\\nBy grouping the updates of the coordinates into an update of the vector\\nθ, we can rewrite update (1.1) in a slightly more succinct way:\\nθ := θ + α\\nn\\nX\\ni=1\\n\\x00y(i) −hθ(x(i))\\n\\x01\\nx(i)\\nThe reader can easily verify that the quantity in the summation in the\\nupdate rule above is just ∂J(θ)/∂θj (for the original deﬁnition of J). So, this\\nis simply gradient descent on the original cost function J. This method looks\\nat every example in the entire training set on every step, and is called batch\\ngradient descent. Note that, while gradient descent can be susceptible\\nto local minima in general, the optimization problem we have posed here\\nfor linear regression has only one global, and no other local, optima; thus\\ngradient descent always converges (assuming the learning rate α is not too\\nlarge) to the global minimum.\\nIndeed, J is a convex quadratic function.\\nHere is an example of gradient descent as it is run to minimize a quadratic\\nfunction.\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\nThe ellipses shown above are the contours of a quadratic function.\\nAlso\\nshown is the trajectory taken by gradient descent, which was initialized at\\n(48,30). The x’s in the ﬁgure (joined by straight lines) mark the successive\\nvalues of θ that gradient descent went through.\\nWhen we run batch gradient descent to ﬁt θ on our previous dataset,\\nto learn to predict housing price as a function of living area, we obtain\\nθ0 = 71.27, θ1 = 0.1345. If we plot hθ(x) as a function of x (area), along\\nwith the training data, we obtain the following ﬁgure:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 12}, page_content='12\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\n3500\\n4000\\n4500\\n5000\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n800\\n900\\n1000\\nhousing prices\\nsquare feet\\nprice (in $1000)\\nIf the number of bedrooms were included as one of the input features as well,\\nwe get θ0 = 89.60, θ1 = 0.1392, θ2 = −8.738.\\nThe above results were obtained with batch gradient descent. There is\\nan alternative to batch gradient descent that also works very well. Consider\\nthe following algorithm:\\nLoop {\\nfor i = 1 to n, {\\nθj := θj + α\\n\\x00y(i) −hθ(x(i))\\n\\x01\\nx(i)\\nj ,\\n(for every j)\\n(1.2)\\n}\\n}\\nBy grouping the updates of the coordinates into an update of the vector\\nθ, we can rewrite update (1.2) in a slightly more succinct way:\\nθ := θ + α\\n\\x00y(i) −hθ(x(i))\\n\\x01\\nx(i)\\nIn this algorithm, we repeatedly run through the training set, and each\\ntime we encounter a training example, we update the parameters according\\nto the gradient of the error with respect to that single training example only.\\nThis algorithm is called stochastic gradient descent (also incremental\\ngradient descent). Whereas batch gradient descent has to scan through\\nthe entire training set before taking a single step—a costly operation if n is\\nlarge—stochastic gradient descent can start making progress right away, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 13}, page_content='13\\ncontinues to make progress with each example it looks at. Often, stochastic\\ngradient descent gets θ “close” to the minimum much faster than batch gra-\\ndient descent. (Note however that it may never “converge” to the minimum,\\nand the parameters θ will keep oscillating around the minimum of J(θ); but\\nin practice most of the values near the minimum will be reasonably good\\napproximations to the true minimum.2) For these reasons, particularly when\\nthe training set is large, stochastic gradient descent is often preferred over\\nbatch gradient descent.\\n1.2\\nThe normal equations\\nGradient descent gives one way of minimizing J. Let’s discuss a second way\\nof doing so, this time performing the minimization explicitly and without\\nresorting to an iterative algorithm. In this method, we will minimize J by\\nexplicitly taking its derivatives with respect to the θj’s, and setting them to\\nzero. To enable us to do this without having to write reams of algebra and\\npages full of matrices of derivatives, let’s introduce some notation for doing\\ncalculus with matrices.\\n1.2.1\\nMatrix derivatives\\nFor a function f : Rn×d 7→R mapping from n-by-d matrices to the real\\nnumbers, we deﬁne the derivative of f with respect to A to be:\\n∇Af(A) =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n∂f\\n∂A11\\n· · ·\\n∂f\\n∂A1d\\n...\\n...\\n...\\n∂f\\n∂An1\\n· · ·\\n∂f\\n∂And\\n\\uf8f9\\n\\uf8fa\\uf8fb\\nThus, the gradient ∇Af(A) is itself an n-by-d matrix, whose (i, j)-element is\\n∂f/∂Aij. For example, suppose A =\\n\\x14\\nA11\\nA12\\nA21\\nA22\\n\\x15\\nis a 2-by-2 matrix, and\\nthe function f : R2×2 7→R is given by\\nf(A) = 3\\n2A11 + 5A2\\n12 + A21A22.\\n2By slowly letting the learning rate α decrease to zero as the algorithm runs, it is also\\npossible to ensure that the parameters will converge to the global minimum rather than\\nmerely oscillate around the minimum.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 14}, page_content='14\\nHere, Aij denotes the (i, j) entry of the matrix A. We then have\\n∇Af(A) =\\n\\x14\\n3\\n2\\n10A12\\nA22\\nA21\\n\\x15\\n.\\n1.2.2\\nLeast squares revisited\\nArmed with the tools of matrix derivatives, let us now proceed to ﬁnd in\\nclosed-form the value of θ that minimizes J(θ). We begin by re-writing J in\\nmatrix-vectorial notation.\\nGiven a training set, deﬁne the design matrix X to be the n-by-d matrix\\n(actually n-by-d + 1, if we include the intercept term) that contains the\\ntraining examples’ input values in its rows:\\nX =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n— (x(1))T —\\n— (x(2))T —\\n...\\n— (x(n))T —\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb.\\nAlso, let ⃗y be the n-dimensional vector containing all the target values from\\nthe training set:\\n⃗y =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\ny(1)\\ny(2)\\n...\\ny(n)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb.\\nNow, since hθ(x(i)) = (x(i))Tθ, we can easily verify that\\nXθ −⃗y\\n=\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n(x(1))Tθ\\n...\\n(x(n))Tθ\\n\\uf8f9\\n\\uf8fa\\uf8fb−\\n\\uf8ee\\n\\uf8ef\\uf8f0\\ny(1)\\n...\\ny(n)\\n\\uf8f9\\n\\uf8fa\\uf8fb\\n=\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nhθ(x(1)) −y(1)\\n...\\nhθ(x(n)) −y(n)\\n\\uf8f9\\n\\uf8fa\\uf8fb.\\nThus, using the fact that for a vector z, we have that zTz = P\\ni z2\\ni :\\n1\\n2(Xθ −⃗y)T(Xθ −⃗y)\\n=\\n1\\n2\\nn\\nX\\ni=1\\n(hθ(x(i)) −y(i))2\\n=\\nJ(θ)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 15}, page_content='15\\nFinally, to minimize J, let’s ﬁnd its derivatives with respect to θ. Hence,\\n∇θJ(θ)\\n=\\n∇θ\\n1\\n2(Xθ −⃗y)T(Xθ −⃗y)\\n=\\n1\\n2∇θ\\n\\x00(Xθ)TXθ −(Xθ)T⃗y −⃗yT(Xθ) + ⃗yT⃗y\\n\\x01\\n=\\n1\\n2∇θ\\n\\x00θT(XTX)θ −⃗yT(Xθ) −⃗yT(Xθ)\\n\\x01\\n=\\n1\\n2∇θ\\n\\x00θT(XTX)θ −2(XT⃗y)Tθ\\n\\x01\\n=\\n1\\n2\\n\\x002XTXθ −2XT⃗y\\n\\x01\\n=\\nXTXθ −XT⃗y\\nIn the third step, we used the fact that aTb = bTa, and in the ﬁfth step\\nused the facts ∇xbTx = b and ∇xxTAx = 2Ax for symmetric matrix A (for\\nmore details, see Section 4.3 of “Linear Algebra Review and Reference”). To\\nminimize J, we set its derivatives to zero, and obtain the normal equations:\\nXTXθ = XT⃗y\\nThus, the value of θ that minimizes J(θ) is given in closed form by the\\nequation\\nθ = (XTX)−1XT⃗y.3\\n1.3\\nProbabilistic interpretation\\nWhen faced with a regression problem, why might linear regression, and\\nspeciﬁcally why might the least-squares cost function J, be a reasonable\\nchoice? In this section, we will give a set of probabilistic assumptions, under\\nwhich least-squares regression is derived as a very natural algorithm.\\nLet us assume that the target variables and the inputs are related via the\\nequation\\ny(i) = θTx(i) + ϵ(i),\\n3Note that in the above step, we are implicitly assuming that XT X is an invertible\\nmatrix.\\nThis can be checked before calculating the inverse.\\nIf either the number of\\nlinearly independent examples is fewer than the number of features, or if the features\\nare not linearly independent, then XT X will not be invertible. Even in such cases, it is\\npossible to “ﬁx” the situation with additional techniques, which we skip here for the sake\\nof simplicty.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 16}, page_content='16\\nwhere ϵ(i) is an error term that captures either unmodeled eﬀects (such as\\nif there are some features very pertinent to predicting housing price, but\\nthat we’d left out of the regression), or random noise. Let us further assume\\nthat the ϵ(i) are distributed IID (independently and identically distributed)\\naccording to a Gaussian distribution (also called a Normal distribution) with\\nmean zero and some variance σ2. We can write this assumption as “ϵ(i) ∼\\nN(0, σ2).” I.e., the density of ϵ(i) is given by\\np(ϵ(i)) =\\n1\\n√\\n2πσ exp\\n\\x12\\n−(ϵ(i))2\\n2σ2\\n\\x13\\n.\\nThis implies that\\np(y(i)|x(i); θ) =\\n1\\n√\\n2πσ exp\\n\\x12\\n−(y(i) −θTx(i))2\\n2σ2\\n\\x13\\n.\\nThe notation “p(y(i)|x(i); θ)” indicates that this is the distribution of y(i)\\ngiven x(i) and parameterized by θ. Note that we should not condition on θ\\n(“p(y(i)|x(i), θ)”), since θ is not a random variable. We can also write the\\ndistribution of y(i) as y(i) | x(i); θ ∼N(θTx(i), σ2).\\nGiven X (the design matrix, which contains all the x(i)’s) and θ, what\\nis the distribution of the y(i)’s?\\nThe probability of the data is given by\\np(⃗y|X; θ). This quantity is typically viewed a function of ⃗y (and perhaps X),\\nfor a ﬁxed value of θ. When we wish to explicitly view this as a function of\\nθ, we will instead call it the likelihood function:\\nL(θ) = L(θ; X, ⃗y) = p(⃗y|X; θ).\\nNote that by the independence assumption on the ϵ(i)’s (and hence also the\\ny(i)’s given the x(i)’s), this can also be written\\nL(θ)\\n=\\nn\\nY\\ni=1\\np(y(i) | x(i); θ)\\n=\\nn\\nY\\ni=1\\n1\\n√\\n2πσ exp\\n\\x12\\n−(y(i) −θTx(i))2\\n2σ2\\n\\x13\\n.\\nNow, given this probabilistic model relating the y(i)’s and the x(i)’s, what\\nis a reasonable way of choosing our best guess of the parameters θ? The\\nprincipal of maximum likelihood says that we should choose θ so as to\\nmake the data as high probability as possible. I.e., we should choose θ to\\nmaximize L(θ).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 17}, page_content='17\\nInstead of maximizing L(θ), we can also maximize any strictly increasing\\nfunction of L(θ). In particular, the derivations will be a bit simpler if we\\ninstead maximize the log likelihood ℓ(θ):\\nℓ(θ)\\n=\\nlog L(θ)\\n=\\nlog\\nn\\nY\\ni=1\\n1\\n√\\n2πσ exp\\n\\x12\\n−(y(i) −θTx(i))2\\n2σ2\\n\\x13\\n=\\nn\\nX\\ni=1\\nlog\\n1\\n√\\n2πσ exp\\n\\x12\\n−(y(i) −θTx(i))2\\n2σ2\\n\\x13\\n=\\nn log\\n1\\n√\\n2πσ −1\\nσ2 · 1\\n2\\nn\\nX\\ni=1\\n(y(i) −θTx(i))2.\\nHence, maximizing ℓ(θ) gives the same answer as minimizing\\n1\\n2\\nn\\nX\\ni=1\\n(y(i) −θTx(i))2,\\nwhich we recognize to be J(θ), our original least-squares cost function.\\nTo summarize: Under the previous probabilistic assumptions on the data,\\nleast-squares regression corresponds to ﬁnding the maximum likelihood esti-\\nmate of θ. This is thus one set of assumptions under which least-squares re-\\ngression can be justiﬁed as a very natural method that’s just doing maximum\\nlikelihood estimation. (Note however that the probabilistic assumptions are\\nby no means necessary for least-squares to be a perfectly good and rational\\nprocedure, and there may—and indeed there are—other natural assumptions\\nthat can also be used to justify it.)\\nNote also that, in our previous discussion, our ﬁnal choice of θ did not\\ndepend on what was σ2, and indeed we’d have arrived at the same result\\neven if σ2 were unknown. We will use this fact again later, when we talk\\nabout the exponential family and generalized linear models.\\n1.4\\nLocally weighted linear regression (optional\\nreading)\\nConsider the problem of predicting y from x ∈R. The leftmost ﬁgure below\\nshows the result of ﬁtting a y = θ0 + θ1x to a dataset. We see that the data\\ndoesn’t really lie on straight line, and so the ﬁt is not very good.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 18}, page_content='18\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n0.5\\n1\\n1.5\\n2\\n2.5\\n3\\n3.5\\n4\\n4.5\\nx\\ny\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n0.5\\n1\\n1.5\\n2\\n2.5\\n3\\n3.5\\n4\\n4.5\\nx\\ny\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n0.5\\n1\\n1.5\\n2\\n2.5\\n3\\n3.5\\n4\\n4.5\\nx\\ny\\nInstead, if we had added an extra feature x2, and ﬁt y = θ0 + θ1x + θ2x2,\\nthen we obtain a slightly better ﬁt to the data. (See middle ﬁgure) Naively, it\\nmight seem that the more features we add, the better. However, there is also\\na danger in adding too many features: The rightmost ﬁgure is the result of\\nﬁtting a 5-th order polynomial y = P5\\nj=0 θjxj. We see that even though the\\nﬁtted curve passes through the data perfectly, we would not expect this to\\nbe a very good predictor of, say, housing prices (y) for diﬀerent living areas\\n(x). Without formally deﬁning what these terms mean, we’ll say the ﬁgure\\non the left shows an instance of underﬁtting—in which the data clearly\\nshows structure not captured by the model—and the ﬁgure on the right is\\nan example of overﬁtting. (Later in this class, when we talk about learning\\ntheory we’ll formalize some of these notions, and also deﬁne more carefully\\njust what it means for a hypothesis to be good or bad.)\\nAs discussed previously, and as shown in the example above, the choice of\\nfeatures is important to ensuring good performance of a learning algorithm.\\n(When we talk about model selection, we’ll also see algorithms for automat-\\nically choosing a good set of features.) In this section, let us brieﬂy talk\\nabout the locally weighted linear regression (LWR) algorithm which, assum-\\ning there is suﬃcient training data, makes the choice of features less critical.\\nThis treatment will be brief, since you’ll get a chance to explore some of the\\nproperties of the LWR algorithm yourself in the homework.\\nIn the original linear regression algorithm, to make a prediction at a query\\npoint x (i.e., to evaluate h(x)), we would:\\n1. Fit θ to minimize P\\ni(y(i) −θTx(i))2.\\n2. Output θTx.\\nIn contrast, the locally weighted linear regression algorithm does the fol-\\nlowing:\\n1. Fit θ to minimize P\\ni w(i)(y(i) −θTx(i))2.\\n2. Output θTx.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 19}, page_content='19\\nHere, the w(i)’s are non-negative valued weights. Intuitively, if w(i) is large\\nfor a particular value of i, then in picking θ, we’ll try hard to make (y(i) −\\nθTx(i))2 small. If w(i) is small, then the (y(i) −θTx(i))2 error term will be\\npretty much ignored in the ﬁt.\\nA fairly standard choice for the weights is4\\nw(i) = exp\\n\\x12\\n−(x(i) −x)2\\n2τ 2\\n\\x13\\nNote that the weights depend on the particular point x at which we’re trying\\nto evaluate x. Moreover, if |x(i) −x| is small, then w(i) is close to 1; and\\nif |x(i) −x| is large, then w(i) is small. Hence, θ is chosen giving a much\\nhigher “weight” to the (errors on) training examples close to the query point\\nx. (Note also that while the formula for the weights takes a form that is\\ncosmetically similar to the density of a Gaussian distribution, the w(i)’s do\\nnot directly have anything to do with Gaussians, and in particular the w(i)\\nare not random variables, normally distributed or otherwise.) The parameter\\nτ controls how quickly the weight of a training example falls oﬀwith distance\\nof its x(i) from the query point x; τ is called the bandwidth parameter, and\\nis also something that you’ll get to experiment with in your homework.\\nLocally weighted linear regression is the ﬁrst example we’re seeing of a\\nnon-parametric algorithm. The (unweighted) linear regression algorithm\\nthat we saw earlier is known as a parametric learning algorithm, because\\nit has a ﬁxed, ﬁnite number of parameters (the θi’s), which are ﬁt to the\\ndata. Once we’ve ﬁt the θi’s and stored them away, we no longer need to\\nkeep the training data around to make future predictions. In contrast, to\\nmake predictions using locally weighted linear regression, we need to keep\\nthe entire training set around. The term “non-parametric” (roughly) refers\\nto the fact that the amount of stuﬀwe need to keep in order to represent the\\nhypothesis h grows linearly with the size of the training set.\\n4If x is vector-valued, this is generalized to be w(i) = exp(−(x(i) −x)T (x(i) −x)/(2τ 2)),\\nor w(i) = exp(−(x(i) −x)T Σ−1(x(i) −x)/(2τ 2)), for an appropriate choice of τ or Σ.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 20}, page_content='Chapter 2\\nClassiﬁcation and logistic\\nregression\\nLet’s now talk about the classiﬁcation problem. This is just like the regression\\nproblem, except that the values y we now want to predict take on only\\na small number of discrete values. For now, we will focus on the binary\\nclassiﬁcation problem in which y can take on only two values, 0 and 1.\\n(Most of what we say here will also generalize to the multiple-class case.)\\nFor instance, if we are trying to build a spam classiﬁer for email, then x(i)\\nmay be some features of a piece of email, and y may be 1 if it is a piece\\nof spam mail, and 0 otherwise. 0 is also called the negative class, and 1\\nthe positive class, and they are sometimes also denoted by the symbols “-”\\nand “+.” Given x(i), the corresponding y(i) is also called the label for the\\ntraining example.\\n2.1\\nLogistic regression\\nWe could approach the classiﬁcation problem ignoring the fact that y is\\ndiscrete-valued, and use our old linear regression algorithm to try to predict\\ny given x.\\nHowever, it is easy to construct examples where this method\\nperforms very poorly. Intuitively, it also doesn’t make sense for hθ(x) to take\\nvalues larger than 1 or smaller than 0 when we know that y ∈{0, 1}.\\nTo ﬁx this, let’s change the form for our hypotheses hθ(x). We will choose\\nhθ(x) = g(θTx) =\\n1\\n1 + e−θT x,\\nwhere\\ng(z) =\\n1\\n1 + e−z\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 21}, page_content='21\\nis called the logistic function or the sigmoid function. Here is a plot\\nshowing g(z):\\n−5\\n−4\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n4\\n5\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\nz\\ng(z)\\nNotice that g(z) tends towards 1 as z →∞, and g(z) tends towards 0 as\\nz →−∞. Moreover, g(z), and hence also h(x), is always bounded between\\n0 and 1. As before, we are keeping the convention of letting x0 = 1, so that\\nθTx = θ0 + Pd\\nj=1 θjxj.\\nFor now, let’s take the choice of g as given. Other functions that smoothly\\nincrease from 0 to 1 can also be used, but for a couple of reasons that we’ll see\\nlater (when we talk about GLMs, and when we talk about generative learning\\nalgorithms), the choice of the logistic function is a fairly natural one. Before\\nmoving on, here’s a useful property of the derivative of the sigmoid function,\\nwhich we write as g′:\\ng′(z)\\n=\\nd\\ndz\\n1\\n1 + e−z\\n=\\n1\\n(1 + e−z)2\\n\\x00e−z\\x01\\n=\\n1\\n(1 + e−z) ·\\n\\x12\\n1 −\\n1\\n(1 + e−z)\\n\\x13\\n=\\ng(z)(1 −g(z)).\\nSo, given the logistic regression model, how do we ﬁt θ for it? Following\\nhow we saw least squares regression could be derived as the maximum like-\\nlihood estimator under a set of assumptions, let’s endow our classiﬁcation\\nmodel with a set of probabilistic assumptions, and then ﬁt the parameters\\nvia maximum likelihood.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 22}, page_content='22\\nLet us assume that\\nP(y = 1 | x; θ)\\n=\\nhθ(x)\\nP(y = 0 | x; θ)\\n=\\n1 −hθ(x)\\nNote that this can be written more compactly as\\np(y | x; θ) = (hθ(x))y (1 −hθ(x))1−y\\nAssuming that the n training examples were generated independently, we\\ncan then write down the likelihood of the parameters as\\nL(θ)\\n=\\np(⃗y | X; θ)\\n=\\nn\\nY\\ni=1\\np(y(i) | x(i); θ)\\n=\\nn\\nY\\ni=1\\n\\x00hθ(x(i))\\n\\x01y(i) \\x001 −hθ(x(i))\\n\\x011−y(i)\\nAs before, it will be easier to maximize the log likelihood:\\nℓ(θ) = log L(θ) =\\nn\\nX\\ni=1\\ny(i) log h(x(i)) + (1 −y(i)) log(1 −h(x(i)))\\n(2.1)\\nHow do we maximize the likelihood? Similar to our derivation in the case\\nof linear regression, we can use gradient ascent. Written in vectorial notation,\\nour updates will therefore be given by θ := θ + α∇θℓ(θ). (Note the positive\\nrather than negative sign in the update formula, since we’re maximizing,\\nrather than minimizing, a function now.) Let’s start by working with just\\none training example (x, y), and take derivatives to derive the stochastic\\ngradient ascent rule:\\n∂\\n∂θj\\nℓ(θ) =\\n\\x12\\ny\\n1\\ng(θTx) −(1 −y)\\n1\\n1 −g(θTx)\\n\\x13 ∂\\n∂θj\\ng(θTx)\\n=\\n\\x12\\ny\\n1\\ng(θTx) −(1 −y)\\n1\\n1 −g(θTx)\\n\\x13\\ng(θTx)(1 −g(θTx)) ∂\\n∂θj\\nθTx\\n=\\n\\x00y(1 −g(θTx)) −(1 −y)g(θTx)\\n\\x01\\nxj\\n= (y −hθ(x)) xj\\n(2.2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 23}, page_content='23\\nAbove, we used the fact that g′(z) = g(z)(1 −g(z)). This therefore gives us\\nthe stochastic gradient ascent rule\\nθj := θj + α\\n\\x00y(i) −hθ(x(i))\\n\\x01\\nx(i)\\nj\\nIf we compare this to the LMS update rule, we see that it looks identical; but\\nthis is not the same algorithm, because hθ(x(i)) is now deﬁned as a non-linear\\nfunction of θTx(i). Nonetheless, it’s a little surprising that we end up with\\nthe same update rule for a rather diﬀerent algorithm and learning problem.\\nIs this coincidence, or is there a deeper reason behind this? We’ll answer this\\nwhen we get to GLM models.\\nRemark 2.1.1: An alternative notational viewpoint of the same loss func-\\ntion is also useful, especially for Section 7.1 where we study nonlinear models.\\nLet ℓlogistic : R × {0, 1} →R≥0 be the logistic loss deﬁned as\\nℓlogistic(t, y) ≜y log(1 + exp(−t)) + (1 −y) log(1 + exp(t)) .\\n(2.3)\\nOne can verify by plugging in hθ(x) = 1/(1 + e−θ⊤x) that the negative log-\\nlikelihood (the negation of ℓ(θ) in equation (2.1)) can be re-written as\\n−ℓ(θ) = ℓlogistic(θ⊤x, y).\\n(2.4)\\nOftentimes θ⊤x or t is called the logit. Basic calculus gives us that\\n∂ℓlogistic(t, y)\\n∂t\\n= y −exp(−t)\\n1 + exp(−t) + (1 −y)\\n1\\n1 + exp(−t)\\n(2.5)\\n= 1/(1 + exp(−t)) −y.\\n(2.6)\\nThen, using the chain rule, we have that\\n∂\\n∂θj\\nℓ(θ) = −∂ℓlogistic(t, y)\\n∂t\\n· ∂t\\n∂θj\\n(2.7)\\n= (y −1/(1 + exp(−t))) · xj = (y −hθ(x))xj ,\\n(2.8)\\nwhich is consistent with the derivation in equation (2.2). We will see this\\nviewpoint can be extended nonlinear models in Section 7.1.\\n2.2\\nDigression: the perceptron learning algo-\\nrithm\\nWe now digress to talk brieﬂy about an algorithm that’s of some historical\\ninterest, and that we will also return to later when we talk about learning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 24}, page_content='24\\ntheory. Consider modifying the logistic regression method to “force” it to\\noutput values that are either 0 or 1 or exactly. To do so, it seems natural to\\nchange the deﬁnition of g to be the threshold function:\\ng(z) =\\n\\x1a 1\\nif z ≥0\\n0\\nif z < 0\\nIf we then let hθ(x) = g(θTx) as before but using this modiﬁed deﬁnition of\\ng, and if we use the update rule\\nθj := θj + α\\n\\x00y(i) −hθ(x(i))\\n\\x01\\nx(i)\\nj .\\nthen we have the perceptron learning algorithn.\\nIn the 1960s, this “perceptron” was argued to be a rough model for how\\nindividual neurons in the brain work. Given how simple the algorithm is, it\\nwill also provide a starting point for our analysis when we talk about learning\\ntheory later in this class. Note however that even though the perceptron may\\nbe cosmetically similar to the other algorithms we talked about, it is actually\\na very diﬀerent type of algorithm than logistic regression and least squares\\nlinear regression; in particular, it is diﬃcult to endow the perceptron’s predic-\\ntions with meaningful probabilistic interpretations, or derive the perceptron\\nas a maximum likelihood estimation algorithm.\\n2.3\\nMulti-class classiﬁcation\\nConsider a classiﬁcation problem in which the response variable y can take on\\nany one of k values, so y ∈{1, 2, . . . , k}. For example, rather than classifying\\nemails into the two classes spam or not-spam—which would have been a\\nbinary classiﬁcation problem—we might want to classify them into three\\nclasses, such as spam, personal mails, and work-related mails. The label /\\nresponse variable is still discrete, but can now take on more than two values.\\nWe will thus model it as distributed according to a multinomial distribution.\\nIn this case, p(y | x; θ) is a distribution over k possible discrete outcomes\\nand is thus a multinomial distribution. Recall that a multinomial distribu-\\ntion involves k numbers φ1, . . . , φk specifying the probability of each of the\\noutcomes. Note that these numbers must satisfy Pk\\ni=1 φi = 1. We will de-\\nsign a parameterized model that outputs φ1, . . . , φk satisfying this constraint\\ngiven the input x.\\nWe introduce k groups of parameters θ1, . . . , θk, each of them being a\\nvector in Rd.\\nIntuitively, we would like to use θ⊤\\n1 x, . . . , θ⊤\\nk x to represent'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 25}, page_content='25\\nφ1, . . . , φk, the probabilities P(y = 1 | x; θ), . . . , P(y = k | x; θ). However,\\nthere are two issues with such a direct approach. First, θ⊤\\nj x is not neces-\\nsarily within [0, 1].\\nSecond, the summation of θ⊤\\nj x’s is not necessarily 1.\\nThus, instead, we will use the softmax function to turn (θ⊤\\n1 x, · · · , θ⊤\\nk x) into\\na probability vector with nonnegative entries that sum up to 1.\\nDeﬁne the softmax function softmax : Rk →Rk as\\nsoftmax(t1, . . . , tk) =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nexp(t1)\\nPk\\nj=1 exp(tj)\\n...\\nexp(tk)\\nPk\\nj=1 exp(tj)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb.\\n(2.9)\\nThe inputs to the softmax function, the vector t here, are often called log-\\nits. Note that by deﬁnition, the output of the softmax function is always a\\nprobability vector whose entries are nonnegative and sum up to 1.\\nLet (t1, . . . , tk) = (θ⊤\\n1 x, · · · , θ⊤\\nk x).\\nWe apply the softmax function to\\n(t1, . . . , tk), and use the output as the probabilities P(y = 1 | x; θ), . . . , P(y =\\nk | x; θ). We obtain the following probabilistic model:\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nP(y = 1 | x; θ)\\n...\\nP(y = k | x; θ)\\n\\uf8f9\\n\\uf8fa\\uf8fb= softmax(t1, · · · , tk) =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nexp(θ⊤\\n1 x)\\nPk\\nj=1 exp(θ⊤\\nj x)\\n...\\nexp(θ⊤\\nk x)\\nPk\\nj=1 exp(θ⊤\\nj x)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb.\\n(2.10)\\nFor notational convenience, we will let φi =\\nexp(ti)\\nPk\\nj=1 exp(tj). More succinctly, the\\nequation above can be written as:\\nP(y = i | x; θ) = φi =\\nexp(ti)\\nPk\\nj=1 exp(tj)\\n=\\nexp(θ⊤\\ni x)\\nPk\\nj=1 exp(θ⊤\\nj x)\\n.\\n(2.11)\\nNext, we compute the negative log-likelihood of a single example (x, y).\\n−log p(y | x, θ) = −log\\n \\nexp(ty)\\nPk\\nj=1 exp(tj)\\n!\\n= −log\\n \\nexp(θ⊤\\ny x)\\nPk\\nj=1 exp(θ⊤\\nj x)\\n!\\n(2.12)\\nThus, the loss function, the negative log-likelihood of the training data, is\\ngiven as\\nℓ(θ) =\\nn\\nX\\ni=1\\n−log\\n \\nexp(θ⊤\\ny(i)x(i))\\nPk\\nj=1 exp(θ⊤\\nj x(i))\\n!\\n.\\n(2.13)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 26}, page_content='26\\nIt’s convenient to deﬁne the cross-entropy loss ℓce : Rk × {1, . . . , k} →R≥0,\\nwhich modularizes in the complex equation above:1\\nℓce((t1, . . . , tk), y) = −log\\n \\nexp(ty)\\nPk\\nj=1 exp(tj)\\n!\\n.\\n(2.14)\\nWith this notation, we can simply rewrite equation (2.13) as\\nℓ(θ) =\\nn\\nX\\ni=1\\nℓce((θ⊤\\n1 x(i), . . . , θ⊤\\nk x(i)), y(i)) .\\n(2.15)\\nMoreover, conveniently, the cross-entropy loss also has a simple gradient. Let\\nt = (t1, . . . , tk), and recall φi =\\nexp(ti)\\nPk\\nj=1 exp(tj). By basic calculus, we can derive\\n∂ℓce(t, y)\\n∂ti\\n= φi −1{y = i} ,\\n(2.16)\\nwhere 1{·} is the indicator function, that is, 1{y = i} = 1 if y = i, and\\n1{y = i} = 0 if y ̸= i. Alternatively, in vectorized notations, we have the\\nfollowing form which will be useful for Chapter 7:\\n∂ℓce(t, y)\\n∂t\\n= φ −ey ,\\n(2.17)\\nwhere es ∈Rk is the s-th natural basis vector (where the s-th entry is 1 and\\nall other entries are zeros.) Using Chain rule, we have that\\n∂ℓce((θ⊤\\n1 x, . . . , θ⊤\\nk x), y)\\n∂θi\\n= ∂ℓ(t, y)\\n∂ti\\n· ∂ti\\n∂θi\\n= (φi −1{y = i}) · x .\\n(2.18)\\nTherefore, the gradient of the loss with respect to the part of parameter θi is\\n∂ℓ(θ)\\n∂θi\\n=\\nn\\nX\\nj=1\\n(φ(j)\\ni\\n−1{y(j) = i}) · x(j) ,\\n(2.19)\\nwhere φ(j)\\ni\\n=\\nexp(θ⊤\\ni x(j))\\nPk\\ns=1 exp(θ⊤\\ns x(j)) is the probability that the model predicts item i\\nfor example x(j). With the gradients above, one can implement (stochastic)\\ngradient descent to minimize the loss function ℓ(θ).\\n1There are some ambiguity in the naming here. Some people call the cross-entropy loss\\nthe function that maps the probability vector (the φ in our language) and label y to the\\nﬁnal real number, and call our version of cross-entropy loss softmax-cross-entropy loss.\\nWe choose our current naming convention because it’s consistent with the naming of most\\nmodern deep learning library such as PyTorch and Jax.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 27}, page_content='27\\n1\\n1.5\\n2\\n2.5\\n3\\n3.5\\n4\\n4.5\\n5\\n−10\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\nx\\nf(x)\\n1\\n1.5\\n2\\n2.5\\n3\\n3.5\\n4\\n4.5\\n5\\n−10\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\nx\\nf(x)\\n1\\n1.5\\n2\\n2.5\\n3\\n3.5\\n4\\n4.5\\n5\\n−10\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\nx\\nf(x)\\n2.4\\nAnother algorithm for maximizing ℓ(θ)\\nReturning to logistic regression with g(z) being the sigmoid function, let’s\\nnow talk about a diﬀerent algorithm for maximizing ℓ(θ).\\nTo get us started, let’s consider Newton’s method for ﬁnding a zero of a\\nfunction. Speciﬁcally, suppose we have some function f : R 7→R, and we\\nwish to ﬁnd a value of θ so that f(θ) = 0. Here, θ ∈R is a real number.\\nNewton’s method performs the following update:\\nθ := θ −f(θ)\\nf ′(θ).\\nThis method has a natural interpretation in which we can think of it as\\napproximating the function f via a linear function that is tangent to f at\\nthe current guess θ, solving for where that linear function equals to zero, and\\nletting the next guess for θ be where that linear function is zero.\\nHere’s a picture of the Newton’s method in action:\\nIn the leftmost ﬁgure, we see the function f plotted along with the line\\ny = 0. We’re trying to ﬁnd θ so that f(θ) = 0; the value of θ that achieves this\\nis about 1.3. Suppose we initialized the algorithm with θ = 4.5. Newton’s\\nmethod then ﬁts a straight line tangent to f at θ = 4.5, and solves for the\\nwhere that line evaluates to 0. (Middle ﬁgure.) This give us the next guess\\nfor θ, which is about 2.8. The rightmost ﬁgure shows the result of running\\none more iteration, which the updates θ to about 1.8. After a few more\\niterations, we rapidly approach θ = 1.3.\\nNewton’s method gives a way of getting to f(θ) = 0. What if we want to\\nuse it to maximize some function ℓ? The maxima of ℓcorrespond to points\\nwhere its ﬁrst derivative ℓ′(θ) is zero. So, by letting f(θ) = ℓ′(θ), we can use\\nthe same algorithm to maximize ℓ, and we obtain update rule:\\nθ := θ −ℓ′(θ)\\nℓ′′(θ).\\n(Something to think about: How would this change if we wanted to use\\nNewton’s method to minimize rather than maximize a function?)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 28}, page_content='28\\nLastly, in our logistic regression setting, θ is vector-valued, so we need to\\ngeneralize Newton’s method to this setting. The generalization of Newton’s\\nmethod to this multidimensional setting (also called the Newton-Raphson\\nmethod) is given by\\nθ := θ −H−1∇θℓ(θ).\\nHere, ∇θℓ(θ) is, as usual, the vector of partial derivatives of ℓ(θ) with respect\\nto the θi’s; and H is an d-by-d matrix (actually, d+1−by−d+1, assuming that\\nwe include the intercept term) called the Hessian, whose entries are given\\nby\\nHij = ∂2ℓ(θ)\\n∂θi∂θj\\n.\\nNewton’s method typically enjoys faster convergence than (batch) gra-\\ndient descent, and requires many fewer iterations to get very close to the\\nminimum. One iteration of Newton’s can, however, be more expensive than\\none iteration of gradient descent, since it requires ﬁnding and inverting an\\nd-by-d Hessian; but so long as d is not too large, it is usually much faster\\noverall. When Newton’s method is applied to maximize the logistic regres-\\nsion log likelihood function ℓ(θ), the resulting method is also called Fisher\\nscoring.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 29}, page_content='Chapter 3\\nGeneralized linear models\\nSo far, we’ve seen a regression example, and a classiﬁcation example. In the\\nregression example, we had y|x; θ ∼N(µ, σ2), and in the classiﬁcation one,\\ny|x; θ ∼Bernoulli(φ), for some appropriate deﬁnitions of µ and φ as functions\\nof x and θ. In this section, we will show that both of these methods are\\nspecial cases of a broader family of models, called Generalized Linear Models\\n(GLMs).1 We will also show how other models in the GLM family can be\\nderived and applied to other classiﬁcation and regression problems.\\n3.1\\nThe exponential family\\nTo work our way up to GLMs, we will begin by deﬁning exponential family\\ndistributions. We say that a class of distributions is in the exponential family\\nif it can be written in the form\\np(y; η) = b(y) exp(ηTT(y) −a(η))\\n(3.1)\\nHere, η is called the natural parameter (also called the canonical param-\\neter) of the distribution; T(y) is the suﬃcient statistic (for the distribu-\\ntions we consider, it will often be the case that T(y) = y); and a(η) is the log\\npartition function. The quantity e−a(η) essentially plays the role of a nor-\\nmalization constant, that makes sure the distribution p(y; η) sums/integrates\\nover y to 1.\\nA ﬁxed choice of T, a and b deﬁnes a family (or set) of distributions that\\nis parameterized by η; as we vary η, we then get diﬀerent distributions within\\nthis family.\\n1The presentation of the material in this section takes inspiration from Michael I.\\nJordan, Learning in graphical models (unpublished book draft), and also McCullagh and\\nNelder, Generalized Linear Models (2nd ed.).\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 30}, page_content='30\\nWe now show that the Bernoulli and the Gaussian distributions are ex-\\namples of exponential family distributions. The Bernoulli distribution with\\nmean φ, written Bernoulli(φ), speciﬁes a distribution over y ∈{0, 1}, so that\\np(y = 1; φ) = φ; p(y = 0; φ) = 1 −φ. As we vary φ, we obtain Bernoulli\\ndistributions with diﬀerent means. We now show that this class of Bernoulli\\ndistributions, ones obtained by varying φ, is in the exponential family; i.e.,\\nthat there is a choice of T, a and b so that Equation (3.1) becomes exactly\\nthe class of Bernoulli distributions.\\nWe write the Bernoulli distribution as:\\np(y; φ)\\n=\\nφy(1 −φ)1−y\\n=\\nexp(y log φ + (1 −y) log(1 −φ))\\n=\\nexp\\n\\x12\\x12\\nlog\\n\\x12\\nφ\\n1 −φ\\n\\x13\\x13\\ny + log(1 −φ)\\n\\x13\\n.\\nThus, the natural parameter is given by η = log(φ/(1 −φ)). Interestingly, if\\nwe invert this deﬁnition for η by solving for φ in terms of η, we obtain φ =\\n1/(1 + e−η). This is the familiar sigmoid function! This will come up again\\nwhen we derive logistic regression as a GLM. To complete the formulation\\nof the Bernoulli distribution as an exponential family distribution, we also\\nhave\\nT(y)\\n=\\ny\\na(η)\\n=\\n−log(1 −φ)\\n=\\nlog(1 + eη)\\nb(y)\\n=\\n1\\nThis shows that the Bernoulli distribution can be written in the form of\\nEquation (3.1), using an appropriate choice of T, a and b.\\nLet’s now move on to consider the Gaussian distribution. Recall that,\\nwhen deriving linear regression, the value of σ2 had no eﬀect on our ﬁnal\\nchoice of θ and hθ(x). Thus, we can choose an arbitrary value for σ2 without\\nchanging anything. To simplify the derivation below, let’s set σ2 = 1.2 We\\n2If we leave σ2 as a variable, the Gaussian distribution can also be shown to be in the\\nexponential family, where η ∈R2 is now a 2-dimension vector that depends on both µ and\\nσ. For the purposes of GLMs, however, the σ2 parameter can also be treated by considering\\na more general deﬁnition of the exponential family: p(y; η, τ) = b(a, τ) exp((ηT T(y) −\\na(η))/c(τ)). Here, τ is called the dispersion parameter, and for the Gaussian, c(τ) = σ2;\\nbut given our simpliﬁcation above, we won’t need the more general deﬁnition for the\\nexamples we will consider here.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 31}, page_content='31\\nthen have:\\np(y; µ)\\n=\\n1\\n√\\n2π exp\\n\\x12\\n−1\\n2(y −µ)2\\n\\x13\\n=\\n1\\n√\\n2π exp\\n\\x12\\n−1\\n2y2\\n\\x13\\n· exp\\n\\x12\\nµy −1\\n2µ2\\n\\x13\\nThus, we see that the Gaussian is in the exponential family, with\\nη\\n=\\nµ\\nT(y)\\n=\\ny\\na(η)\\n=\\nµ2/2\\n=\\nη2/2\\nb(y)\\n=\\n(1/\\n√\\n2π) exp(−y2/2).\\nThere’re many other distributions that are members of the exponen-\\ntial family: The multinomial (which we’ll see later), the Poisson (for mod-\\nelling count-data; also see the problem set); the gamma and the exponen-\\ntial (for modelling continuous, non-negative random variables, such as time-\\nintervals); the beta and the Dirichlet (for distributions over probabilities);\\nand many more. In the next section, we will describe a general “recipe”\\nfor constructing models in which y (given x and θ) comes from any of these\\ndistributions.\\n3.2\\nConstructing GLMs\\nSuppose you would like to build a model to estimate the number y of cus-\\ntomers arriving in your store (or number of page-views on your website) in\\nany given hour, based on certain features x such as store promotions, recent\\nadvertising, weather, day-of-week, etc. We know that the Poisson distribu-\\ntion usually gives a good model for numbers of visitors. Knowing this, how\\ncan we come up with a model for our problem? Fortunately, the Poisson is an\\nexponential family distribution, so we can apply a Generalized Linear Model\\n(GLM). In this section, we will we will describe a method for constructing\\nGLM models for problems such as these.\\nMore generally, consider a classiﬁcation or regression problem where we\\nwould like to predict the value of some random variable y as a function of\\nx.\\nTo derive a GLM for this problem, we will make the following three\\nassumptions about the conditional distribution of y given x and about our\\nmodel:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 32}, page_content='32\\n1. y | x; θ ∼ExponentialFamily(η). I.e., given x and θ, the distribution of\\ny follows some exponential family distribution, with parameter η.\\n2. Given x, our goal is to predict the expected value of T(y) given x.\\nIn most of our examples, we will have T(y) = y, so this means we\\nwould like the prediction h(x) output by our learned hypothesis h to\\nsatisfy h(x) = E[y|x]. (Note that this assumption is satisﬁed in the\\nchoices for hθ(x) for both logistic regression and linear regression. For\\ninstance, in logistic regression, we had hθ(x) = p(y = 1|x; θ) = 0 · p(y =\\n0|x; θ) + 1 · p(y = 1|x; θ) = E[y|x; θ].)\\n3. The natural parameter η and the inputs x are related linearly: η = θTx.\\n(Or, if η is vector-valued, then ηi = θT\\ni x.)\\nThe third of these assumptions might seem the least well justiﬁed of\\nthe above, and it might be better thought of as a “design choice” in our\\nrecipe for designing GLMs, rather than as an assumption per se.\\nThese\\nthree assumptions/design choices will allow us to derive a very elegant class\\nof learning algorithms, namely GLMs, that have many desirable properties\\nsuch as ease of learning. Furthermore, the resulting models are often very\\neﬀective for modelling diﬀerent types of distributions over y; for example, we\\nwill shortly show that both logistic regression and ordinary least squares can\\nboth be derived as GLMs.\\n3.2.1\\nOrdinary least squares\\nTo show that ordinary least squares is a special case of the GLM family\\nof models, consider the setting where the target variable y (also called the\\nresponse variable in GLM terminology) is continuous, and we model the\\nconditional distribution of y given x as a Gaussian N(µ, σ2). (Here, µ may\\ndepend x.)\\nSo, we let the ExponentialFamily(η) distribution above be\\nthe Gaussian distribution. As we saw previously, in the formulation of the\\nGaussian as an exponential family distribution, we had µ = η. So, we have\\nhθ(x)\\n=\\nE[y|x; θ]\\n=\\nµ\\n=\\nη\\n=\\nθTx.\\nThe ﬁrst equality follows from Assumption 2, above; the second equality\\nfollows from the fact that y|x; θ ∼N(µ, σ2), and so its expected value is given'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 33}, page_content='33\\nby µ; the third equality follows from Assumption 1 (and our earlier derivation\\nshowing that µ = η in the formulation of the Gaussian as an exponential\\nfamily distribution); and the last equality follows from Assumption 3.\\n3.2.2\\nLogistic regression\\nWe now consider logistic regression. Here we are interested in binary classiﬁ-\\ncation, so y ∈{0, 1}. Given that y is binary-valued, it therefore seems natural\\nto choose the Bernoulli family of distributions to model the conditional dis-\\ntribution of y given x. In our formulation of the Bernoulli distribution as\\nan exponential family distribution, we had φ = 1/(1 + e−η). Furthermore,\\nnote that if y|x; θ ∼Bernoulli(φ), then E[y|x; θ] = φ. So, following a similar\\nderivation as the one for ordinary least squares, we get:\\nhθ(x)\\n=\\nE[y|x; θ]\\n=\\nφ\\n=\\n1/(1 + e−η)\\n=\\n1/(1 + e−θT x)\\nSo, this gives us hypothesis functions of the form hθ(x) = 1/(1 + e−θT x). If\\nyou are previously wondering how we came up with the form of the logistic\\nfunction 1/(1 + e−z), this gives one answer: Once we assume that y condi-\\ntioned on x is Bernoulli, it arises as a consequence of the deﬁnition of GLMs\\nand exponential family distributions.\\nTo introduce a little more terminology, the function g giving the distri-\\nbution’s mean as a function of the natural parameter (g(η) = E[T(y); η])\\nis called the canonical response function. Its inverse, g−1, is called the\\ncanonical link function. Thus, the canonical response function for the\\nGaussian family is just the identify function; and the canonical response\\nfunction for the Bernoulli is the logistic function.3\\n3Many texts use g to denote the link function, and g−1 to denote the response function;\\nbut the notation we’re using here, inherited from the early machine learning literature,\\nwill be more consistent with the notation used in the rest of the class.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 34}, page_content='Chapter 4\\nGenerative learning algorithms\\nSo far, we’ve mainly been talking about learning algorithms that model\\np(y|x; θ), the conditional distribution of y given x.\\nFor instance, logistic\\nregression modeled p(y|x; θ) as hθ(x) = g(θTx) where g is the sigmoid func-\\ntion. In these notes, we’ll talk about a diﬀerent type of learning algorithm.\\nConsider a classiﬁcation problem in which we want to learn to distinguish\\nbetween elephants (y = 1) and dogs (y = 0), based on some features of\\nan animal.\\nGiven a training set, an algorithm like logistic regression or\\nthe perceptron algorithm (basically) tries to ﬁnd a straight line—that is, a\\ndecision boundary—that separates the elephants and dogs. Then, to classify\\na new animal as either an elephant or a dog, it checks on which side of the\\ndecision boundary it falls, and makes its prediction accordingly.\\nHere’s a diﬀerent approach. First, looking at elephants, we can build a\\nmodel of what elephants look like. Then, looking at dogs, we can build a\\nseparate model of what dogs look like. Finally, to classify a new animal, we\\ncan match the new animal against the elephant model, and match it against\\nthe dog model, to see whether the new animal looks more like the elephants\\nor more like the dogs we had seen in the training set.\\nAlgorithms that try to learn p(y|x) directly (such as logistic regression),\\nor algorithms that try to learn mappings directly from the space of inputs X\\nto the labels {0, 1}, (such as the perceptron algorithm) are called discrim-\\ninative learning algorithms. Here, we’ll talk about algorithms that instead\\ntry to model p(x|y) (and p(y)).\\nThese algorithms are called generative\\nlearning algorithms. For instance, if y indicates whether an example is a\\ndog (0) or an elephant (1), then p(x|y = 0) models the distribution of dogs’\\nfeatures, and p(x|y = 1) models the distribution of elephants’ features.\\nAfter modeling p(y) (called the class priors) and p(x|y), our algorithm\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 35}, page_content='35\\ncan then use Bayes rule to derive the posterior distribution on y given x:\\np(y|x) = p(x|y)p(y)\\np(x)\\n.\\nHere, the denominator is given by p(x) = p(x|y = 1)p(y = 1) + p(x|y =\\n0)p(y = 0) (you should be able to verify that this is true from the standard\\nproperties of probabilities), and thus can also be expressed in terms of the\\nquantities p(x|y) and p(y) that we’ve learned. Actually, if were calculating\\np(y|x) in order to make a prediction, then we don’t actually need to calculate\\nthe denominator, since\\narg max\\ny\\np(y|x)\\n=\\narg max\\ny\\np(x|y)p(y)\\np(x)\\n=\\narg max\\ny\\np(x|y)p(y).\\n4.1\\nGaussian discriminant analysis\\nThe ﬁrst generative learning algorithm that we’ll look at is Gaussian discrim-\\ninant analysis (GDA). In this model, we’ll assume that p(x|y) is distributed\\naccording to a multivariate normal distribution. Let’s talk brieﬂy about the\\nproperties of multivariate normal distributions before moving on to the GDA\\nmodel itself.\\n4.1.1\\nThe multivariate normal distribution\\nThe multivariate normal distribution in d-dimensions, also called the multi-\\nvariate Gaussian distribution, is parameterized by a mean vector µ ∈Rd\\nand a covariance matrix Σ ∈Rd×d, where Σ ≥0 is symmetric and positive\\nsemi-deﬁnite. Also written “N(µ, Σ)”, its density is given by:\\np(x; µ, Σ) =\\n1\\n(2π)d/2|Σ|1/2 exp\\n\\x12\\n−1\\n2(x −µ)TΣ−1(x −µ)\\n\\x13\\n.\\nIn the equation above, “|Σ|” denotes the determinant of the matrix Σ.\\nFor a random variable X distributed N(µ, Σ), the mean is (unsurpris-\\ningly) given by µ:\\nE[X] =\\nZ\\nx\\nx p(x; µ, Σ)dx = µ\\nThe covariance of a vector-valued random variable Z is deﬁned as Cov(Z) =\\nE[(Z −E[Z])(Z −E[Z])T]. This generalizes the notion of the variance of a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 36}, page_content='36\\nreal-valued random variable. The covariance can also be deﬁned as Cov(Z) =\\nE[ZZT]−(E[Z])(E[Z])T. (You should be able to prove to yourself that these\\ntwo deﬁnitions are equivalent.) If X ∼N(µ, Σ), then\\nCov(X) = Σ.\\nHere are some examples of what the density of a Gaussian distribution\\nlooks like:\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\nThe left-most ﬁgure shows a Gaussian with mean zero (that is, the 2x1\\nzero-vector) and covariance matrix Σ = I (the 2x2 identity matrix). A Gaus-\\nsian with zero mean and identity covariance is also called the standard nor-\\nmal distribution. The middle ﬁgure shows the density of a Gaussian with\\nzero mean and Σ = 0.6I; and in the rightmost ﬁgure shows one with , Σ = 2I.\\nWe see that as Σ becomes larger, the Gaussian becomes more “spread-out,”\\nand as it becomes smaller, the distribution becomes more “compressed.”\\nLet’s look at some more examples.\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\nThe ﬁgures above show Gaussians with mean 0, and with covariance\\nmatrices respectively\\nΣ =\\n\\x14 1\\n0\\n0\\n1\\n\\x15\\n; Σ =\\n\\x14\\n1\\n0.5\\n0.5\\n1\\n\\x15\\n; Σ =\\n\\x14\\n1\\n0.8\\n0.8\\n1\\n\\x15\\n.\\nThe leftmost ﬁgure shows the familiar standard normal distribution, and we\\nsee that as we increase the oﬀ-diagonal entry in Σ, the density becomes more'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 37}, page_content='37\\n“compressed” towards the 45◦line (given by x1 = x2). We can see this more\\nclearly when we look at the contours of the same three densities:\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\nHere’s one last set of examples generated by varying Σ:\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\nThe plots above used, respectively,\\nΣ =\\n\\x14\\n1\\n-0.5\\n-0.5\\n1\\n\\x15\\n; Σ =\\n\\x14\\n1\\n-0.8\\n-0.8\\n1\\n\\x15\\n; Σ =\\n\\x14\\n3\\n0.8\\n0.8\\n1\\n\\x15\\n.\\nFrom the leftmost and middle ﬁgures, we see that by decreasing the oﬀ-\\ndiagonal elements of the covariance matrix, the density now becomes “com-\\npressed” again, but in the opposite direction. Lastly, as we vary the pa-\\nrameters, more generally the contours will form ellipses (the rightmost ﬁgure\\nshowing an example).\\nAs our last set of examples, ﬁxing Σ = I, by varying µ, we can also move\\nthe mean of the density around.\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 38}, page_content='38\\nThe ﬁgures above were generated using Σ = I, and respectively\\nµ =\\n\\x14 1\\n0\\n\\x15\\n; µ =\\n\\x14 -0.5\\n0\\n\\x15\\n; µ =\\n\\x14\\n-1\\n-1.5\\n\\x15\\n.\\n4.1.2\\nThe Gaussian discriminant analysis model\\nWhen we have a classiﬁcation problem in which the input features x are\\ncontinuous-valued random variables, we can then use the Gaussian Discrim-\\ninant Analysis (GDA) model, which models p(x|y) using a multivariate nor-\\nmal distribution. The model is:\\ny\\n∼\\nBernoulli(φ)\\nx|y = 0\\n∼\\nN(µ0, Σ)\\nx|y = 1\\n∼\\nN(µ1, Σ)\\nWriting out the distributions, this is:\\np(y)\\n=\\nφy(1 −φ)1−y\\np(x|y = 0)\\n=\\n1\\n(2π)d/2|Σ|1/2 exp\\n\\x12\\n−1\\n2(x −µ0)TΣ−1(x −µ0)\\n\\x13\\np(x|y = 1)\\n=\\n1\\n(2π)d/2|Σ|1/2 exp\\n\\x12\\n−1\\n2(x −µ1)TΣ−1(x −µ1)\\n\\x13\\nHere, the parameters of our model are φ, Σ, µ0 and µ1. (Note that while\\nthere’re two diﬀerent mean vectors µ0 and µ1, this model is usually applied\\nusing only one covariance matrix Σ.) The log-likelihood of the data is given\\nby\\nℓ(φ, µ0, µ1, Σ)\\n=\\nlog\\nn\\nY\\ni=1\\np(x(i), y(i); φ, µ0, µ1, Σ)\\n=\\nlog\\nn\\nY\\ni=1\\np(x(i)|y(i); µ0, µ1, Σ)p(y(i); φ).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 39}, page_content='39\\nBy maximizing ℓwith respect to the parameters, we ﬁnd the maximum like-\\nlihood estimate of the parameters (see problem set 1) to be:\\nφ\\n=\\n1\\nn\\nn\\nX\\ni=1\\n1{y(i) = 1}\\nµ0\\n=\\nPn\\ni=1 1{y(i) = 0}x(i)\\nPn\\ni=1 1{y(i) = 0}\\nµ1\\n=\\nPn\\ni=1 1{y(i) = 1}x(i)\\nPn\\ni=1 1{y(i) = 1}\\nΣ\\n=\\n1\\nn\\nn\\nX\\ni=1\\n(x(i) −µy(i))(x(i) −µy(i))T.\\nPictorially, what the algorithm is doing can be seen in as follows:\\n−2\\n−1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n−7\\n−6\\n−5\\n−4\\n−3\\n−2\\n−1\\n0\\n1\\nShown in the ﬁgure are the training set, as well as the contours of the\\ntwo Gaussian distributions that have been ﬁt to the data in each of the\\ntwo classes. Note that the two Gaussians have contours that are the same\\nshape and orientation, since they share a covariance matrix Σ, but they have\\ndiﬀerent means µ0 and µ1.\\nAlso shown in the ﬁgure is the straight line\\ngiving the decision boundary at which p(y = 1|x) = 0.5. On one side of\\nthe boundary, we’ll predict y = 1 to be the most likely outcome, and on the\\nother side, we’ll predict y = 0.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 40}, page_content='40\\n4.1.3\\nDiscussion: GDA and logistic regression\\nThe GDA model has an interesting relationship to logistic regression. If we\\nview the quantity p(y = 1|x; φ, µ0, µ1, Σ) as a function of x, we’ll ﬁnd that it\\ncan be expressed in the form\\np(y = 1|x; φ, Σ, µ0, µ1) =\\n1\\n1 + exp(−θTx),\\nwhere θ is some appropriate function of φ, Σ, µ0, µ1.1 This is exactly the form\\nthat logistic regression—a discriminative algorithm—used to model p(y =\\n1|x).\\nWhen would we prefer one model over another? GDA and logistic regres-\\nsion will, in general, give diﬀerent decision boundaries when trained on the\\nsame dataset. Which is better?\\nWe just argued that if p(x|y) is multivariate gaussian (with shared Σ),\\nthen p(y|x) necessarily follows a logistic function. The converse, however,\\nis not true; i.e., p(y|x) being a logistic function does not imply p(x|y) is\\nmultivariate gaussian. This shows that GDA makes stronger modeling as-\\nsumptions about the data than does logistic regression. It turns out that\\nwhen these modeling assumptions are correct, then GDA will ﬁnd better ﬁts\\nto the data, and is a better model. Speciﬁcally, when p(x|y) is indeed gaus-\\nsian (with shared Σ), then GDA is asymptotically eﬃcient. Informally,\\nthis means that in the limit of very large training sets (large n), there is no\\nalgorithm that is strictly better than GDA (in terms of, say, how accurately\\nthey estimate p(y|x)). In particular, it can be shown that in this setting,\\nGDA will be a better algorithm than logistic regression; and more generally,\\neven for small training set sizes, we would generally expect GDA to better.\\nIn contrast, by making signiﬁcantly weaker assumptions, logistic regres-\\nsion is also more robust and less sensitive to incorrect modeling assumptions.\\nThere are many diﬀerent sets of assumptions that would lead to p(y|x) taking\\nthe form of a logistic function. For example, if x|y = 0 ∼Poisson(λ0), and\\nx|y = 1 ∼Poisson(λ1), then p(y|x) will be logistic. Logistic regression will\\nalso work well on Poisson data like this. But if we were to use GDA on such\\ndata—and ﬁt Gaussian distributions to such non-Gaussian data—then the\\nresults will be less predictable, and GDA may (or may not) do well.\\nTo summarize: GDA makes stronger modeling assumptions, and is more\\ndata eﬃcient (i.e., requires less training data to learn “well”) when the mod-\\neling assumptions are correct or at least approximately correct.\\nLogistic\\n1This uses the convention of redeﬁning the x(i)’s on the right-hand-side to be (d + 1)-\\ndimensional vectors by adding the extra coordinate x(i)\\n0\\n= 1; see problem set 1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 41}, page_content='41\\nregression makes weaker assumptions, and is signiﬁcantly more robust to\\ndeviations from modeling assumptions.\\nSpeciﬁcally, when the data is in-\\ndeed non-Gaussian, then in the limit of large datasets, logistic regression will\\nalmost always do better than GDA. For this reason, in practice logistic re-\\ngression is used more often than GDA. (Some related considerations about\\ndiscriminative vs. generative models also apply for the Naive Bayes algo-\\nrithm that we discuss next, but the Naive Bayes algorithm is still considered\\na very good, and is certainly also a very popular, classiﬁcation algorithm.)\\n4.2\\nNaive bayes (Option Reading)\\nIn GDA, the feature vectors x were continuous, real-valued vectors. Let’s\\nnow talk about a diﬀerent learning algorithm in which the xj’s are discrete-\\nvalued.\\nFor our motivating example, consider building an email spam ﬁlter using\\nmachine learning. Here, we wish to classify messages according to whether\\nthey are unsolicited commercial (spam) email, or non-spam email.\\nAfter\\nlearning to do this, we can then have our mail reader automatically ﬁlter\\nout the spam messages and perhaps place them in a separate mail folder.\\nClassifying emails is one example of a broader set of problems called text\\nclassiﬁcation.\\nLet’s say we have a training set (a set of emails labeled as spam or non-\\nspam).\\nWe’ll begin our construction of our spam ﬁlter by specifying the\\nfeatures xj used to represent an email.\\nWe will represent an email via a feature vector whose length is equal to\\nthe number of words in the dictionary. Speciﬁcally, if an email contains the\\nj-th word of the dictionary, then we will set xj = 1; otherwise, we let xj = 0.\\nFor instance, the vector\\nx =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n0\\n0\\n...\\n1\\n...\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\na\\naardvark\\naardwolf\\n...\\nbuy\\n...\\nzygmurgy\\nis used to represent an email that contains the words “a” and “buy,” but not'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 42}, page_content='42\\n“aardvark,” “aardwolf” or “zygmurgy.”2 The set of words encoded into the\\nfeature vector is called the vocabulary, so the dimension of x is equal to\\nthe size of the vocabulary.\\nHaving chosen our feature vector, we now want to build a generative\\nmodel. So, we have to model p(x|y). But if we have, say, a vocabulary of\\n50000 words, then x ∈{0, 1}50000 (x is a 50000-dimensional vector of 0’s and\\n1’s), and if we were to model x explicitly with a multinomial distribution over\\nthe 250000 possible outcomes, then we’d end up with a (250000−1)-dimensional\\nparameter vector. This is clearly too many parameters.\\nTo model p(x|y), we will therefore make a very strong assumption. We will\\nassume that the xi’s are conditionally independent given y. This assumption\\nis called the Naive Bayes (NB) assumption, and the resulting algorithm is\\ncalled the Naive Bayes classiﬁer. For instance, if y = 1 means spam email;\\n“buy” is word 2087 and “price” is word 39831; then we are assuming that if\\nI tell you y = 1 (that a particular piece of email is spam), then knowledge\\nof x2087 (knowledge of whether “buy” appears in the message) will have no\\neﬀect on your beliefs about the value of x39831 (whether “price” appears).\\nMore formally, this can be written p(x2087|y) = p(x2087|y, x39831). (Note that\\nthis is not the same as saying that x2087 and x39831 are independent, which\\nwould have been written “p(x2087) = p(x2087|x39831)”; rather, we are only\\nassuming that x2087 and x39831 are conditionally independent given y.)\\nWe now have:\\np(x1, . . . , x50000|y)\\n=\\np(x1|y)p(x2|y, x1)p(x3|y, x1, x2) · · · p(x50000|y, x1, . . . , x49999)\\n=\\np(x1|y)p(x2|y)p(x3|y) · · · p(x50000|y)\\n=\\nd\\nY\\nj=1\\np(xj|y)\\nThe ﬁrst equality simply follows from the usual properties of probabilities,\\nand the second equality used the NB assumption. We note that even though\\n2Actually, rather than looking through an English dictionary for the list of all English\\nwords, in practice it is more common to look through our training set and encode in our\\nfeature vector only the words that occur at least once there. Apart from reducing the\\nnumber of words modeled and hence reducing our computational and space requirements,\\nthis also has the advantage of allowing us to model/include as a feature many words\\nthat may appear in your email (such as “cs229”) but that you won’t ﬁnd in a dictionary.\\nSometimes (as in the homework), we also exclude the very high frequency words (which\\nwill be words like “the,” “of,” “and”; these high frequency, “content free” words are called\\nstop words) since they occur in so many documents and do little to indicate whether an\\nemail is spam or non-spam.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 43}, page_content='43\\nthe Naive Bayes assumption is an extremely strong assumptions, the resulting\\nalgorithm works well on many problems.\\nOur model is parameterized by φj|y=1 = p(xj = 1|y = 1), φj|y=0 = p(xj =\\n1|y = 0), and φy = p(y = 1). As usual, given a training set {(x(i), y(i)); i =\\n1, . . . , n}, we can write down the joint likelihood of the data:\\nL(φy, φj|y=0, φj|y=1) =\\nn\\nY\\ni=1\\np(x(i), y(i)).\\nMaximizing this with respect to φy, φj|y=0 and φj|y=1 gives the maximum\\nlikelihood estimates:\\nφj|y=1\\n=\\nPn\\ni=1 1{x(i)\\nj\\n= 1 ∧y(i) = 1}\\nPn\\ni=1 1{y(i) = 1}\\nφj|y=0\\n=\\nPn\\ni=1 1{x(i)\\nj\\n= 1 ∧y(i) = 0}\\nPn\\ni=1 1{y(i) = 0}\\nφy\\n=\\nPn\\ni=1 1{y(i) = 1}\\nn\\nIn the equations above, the “∧” symbol means “and.” The parameters have\\na very natural interpretation. For instance, φj|y=1 is just the fraction of the\\nspam (y = 1) emails in which word j does appear.\\nHaving ﬁt all these parameters, to make a prediction on a new example\\nwith features x, we then simply calculate\\np(y = 1|x)\\n=\\np(x|y = 1)p(y = 1)\\np(x)\\n=\\n\\x10Qd\\nj=1 p(xj|y = 1)\\n\\x11\\np(y = 1)\\n\\x10Qd\\nj=1 p(xj|y = 1)\\n\\x11\\np(y = 1) +\\n\\x10Qd\\nj=1 p(xj|y = 0)\\n\\x11\\np(y = 0)\\n,\\nand pick whichever class has the higher posterior probability.\\nLastly, we note that while we have developed the Naive Bayes algorithm\\nmainly for the case of problems where the features xj are binary-valued, the\\ngeneralization to where xj can take values in {1, 2, . . . , kj} is straightforward.\\nHere, we would simply model p(xj|y) as multinomial rather than as Bernoulli.\\nIndeed, even if some original input attribute (say, the living area of a house,\\nas in our earlier example) were continuous valued, it is quite common to\\ndiscretize it—that is, turn it into a small set of discrete values—and apply\\nNaive Bayes. For instance, if we use some feature xj to represent living area,\\nwe might discretize the continuous values as follows:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 44}, page_content='44\\nLiving area (sq. feet)\\n< 400\\n400-800\\n800-1200\\n1200-1600\\n>1600\\nxi\\n1\\n2\\n3\\n4\\n5\\nThus, for a house with living area 890 square feet, we would set the value\\nof the corresponding feature xj to 3. We can then apply the Naive Bayes\\nalgorithm, and model p(xj|y) with a multinomial distribution, as described\\npreviously.\\nWhen the original, continuous-valued attributes are not well-\\nmodeled by a multivariate normal distribution, discretizing the features and\\nusing Naive Bayes (instead of GDA) will often result in a better classiﬁer.\\n4.2.1\\nLaplace smoothing\\nThe Naive Bayes algorithm as we have described it will work fairly well\\nfor many problems, but there is a simple change that makes it work much\\nbetter, especially for text classiﬁcation. Let’s brieﬂy discuss a problem with\\nthe algorithm in its current form, and then talk about how we can ﬁx it.\\nConsider spam/email classiﬁcation, and let’s suppose that, we are in the\\nyear of 20xx, after completing CS229 and having done excellent work on the\\nproject, you decide around May 20xx to submit work you did to the NeurIPS\\nconference for publication.3 Because you end up discussing the conference\\nin your emails, you also start getting messages with the word “neurips”\\nin it. But this is your ﬁrst NeurIPS paper, and until this time, you had\\nnot previously seen any emails containing the word “neurips”; in particular\\n“neurips” did not ever appear in your training set of spam/non-spam emails.\\nAssuming that “neurips” was the 35000th word in the dictionary, your Naive\\nBayes spam ﬁlter therefore had picked its maximum likelihood estimates of\\nthe parameters φ35000|y to be\\nφ35000|y=1\\n=\\nPn\\ni=1 1{x(i)\\n35000 = 1 ∧y(i) = 1}\\nPn\\ni=1 1{y(i) = 1}\\n= 0\\nφ35000|y=0\\n=\\nPn\\ni=1 1{x(i)\\n35000 = 1 ∧y(i) = 0}\\nPn\\ni=1 1{y(i) = 0}\\n= 0\\nI.e., because it has never seen “neurips” before in either spam or non-spam\\ntraining examples, it thinks the probability of seeing it in either type of email\\nis zero. Hence, when trying to decide if one of these messages containing\\n3NeurIPS is one of the top machine learning conferences. The deadline for submitting\\na paper is typically in May-June.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 45}, page_content='45\\n“neurips” is spam, it calculates the class posterior probabilities, and obtains\\np(y = 1|x)\\n=\\nQd\\nj=1 p(xj|y = 1)p(y = 1)\\nQd\\nj=1 p(xj|y = 1)p(y = 1) + Qd\\nj=1 p(xj|y = 0)p(y = 0)\\n=\\n0\\n0.\\nThis is because each of the terms “Qd\\nj=1 p(xj|y)” includes a term p(x35000|y) =\\n0 that is multiplied into it. Hence, our algorithm obtains 0/0, and doesn’t\\nknow how to make a prediction.\\nStating the problem more broadly, it is statistically a bad idea to esti-\\nmate the probability of some event to be zero just because you haven’t seen\\nit before in your ﬁnite training set. Take the problem of estimating the mean\\nof a multinomial random variable z taking values in {1, . . . , k}. We can pa-\\nrameterize our multinomial with φj = p(z = j). Given a set of n independent\\nobservations {z(1), . . . , z(n)}, the maximum likelihood estimates are given by\\nφj =\\nPn\\ni=1 1{z(i) = j}\\nn\\n.\\nAs we saw previously, if we were to use these maximum likelihood estimates,\\nthen some of the φj’s might end up as zero, which was a problem. To avoid\\nthis, we can use Laplace smoothing, which replaces the above estimate\\nwith\\nφj = 1 + Pn\\ni=1 1{z(i) = j}\\nk + n\\n.\\nHere, we’ve added 1 to the numerator, and k to the denominator. Note that\\nPk\\nj=1 φj = 1 still holds (check this yourself!), which is a desirable property\\nsince the φj’s are estimates for probabilities that we know must sum to 1.\\nAlso, φj ̸= 0 for all values of j, solving our problem of probabilities being\\nestimated as zero. Under certain (arguably quite strong) conditions, it can\\nbe shown that the Laplace smoothing actually gives the optimal estimator\\nof the φj’s.\\nReturning to our Naive Bayes classiﬁer, with Laplace smoothing, we\\ntherefore obtain the following estimates of the parameters:\\nφj|y=1\\n=\\n1 + Pn\\ni=1 1{x(i)\\nj\\n= 1 ∧y(i) = 1}\\n2 + Pn\\ni=1 1{y(i) = 1}\\nφj|y=0\\n=\\n1 + Pn\\ni=1 1{x(i)\\nj\\n= 1 ∧y(i) = 0}\\n2 + Pn\\ni=1 1{y(i) = 0}'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 46}, page_content='46\\n(In practice, it usually doesn’t matter much whether we apply Laplace smooth-\\ning to φy or not, since we will typically have a fair fraction each of spam and\\nnon-spam messages, so φy will be a reasonable estimate of p(y = 1) and will\\nbe quite far from 0 anyway.)\\n4.2.2\\nEvent models for text classiﬁcation\\nTo close oﬀour discussion of generative learning algorithms, let’s talk about\\none more model that is speciﬁcally for text classiﬁcation. While Naive Bayes\\nas we’ve presented it will work well for many classiﬁcation problems, for text\\nclassiﬁcation, there is a related model that does even better.\\nIn the speciﬁc context of text classiﬁcation, Naive Bayes as presented uses\\nthe what’s called the Bernoulli event model (or sometimes multi-variate\\nBernoulli event model). In this model, we assumed that the way an email\\nis generated is that ﬁrst it is randomly determined (according to the class\\npriors p(y)) whether a spammer or non-spammer will send you your next\\nmessage. Then, the person sending the email runs through the dictionary,\\ndeciding whether to include each word j in that email independently and\\naccording to the probabilities p(xj = 1|y) = φj|y. Thus, the probability of a\\nmessage was given by p(y) Qd\\nj=1 p(xj|y).\\nHere’s a diﬀerent model, called the Multinomial event model.\\nTo\\ndescribe this model, we will use a diﬀerent notation and set of features for\\nrepresenting emails. We let xj denote the identity of the j-th word in the\\nemail. Thus, xj is now an integer taking values in {1, . . . , |V |}, where |V |\\nis the size of our vocabulary (dictionary). An email of d words is now rep-\\nresented by a vector (x1, x2, . . . , xd) of length d; note that d can vary for\\ndiﬀerent documents. For instance, if an email starts with “A NeurIPS . . . ,”\\nthen x1 = 1 (“a” is the ﬁrst word in the dictionary), and x2 = 35000 (if\\n“neurips” is the 35000th word in the dictionary).\\nIn the multinomial event model, we assume that the way an email is\\ngenerated is via a random process in which spam/non-spam is ﬁrst deter-\\nmined (according to p(y)) as before. Then, the sender of the email writes the\\nemail by ﬁrst generating x1 from some multinomial distribution over words\\n(p(x1|y)). Next, the second word x2 is chosen independently of x1 but from\\nthe same multinomial distribution, and similarly for x3, x4, and so on, until\\nall d words of the email have been generated. Thus, the overall probability of\\na message is given by p(y) Qd\\nj=1 p(xj|y). Note that this formula looks like the\\none we had earlier for the probability of a message under the Bernoulli event\\nmodel, but that the terms in the formula now mean very diﬀerent things. In\\nparticular xj|y is now a multinomial, rather than a Bernoulli distribution.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 47}, page_content='47\\nThe parameters for our new model are φy = p(y) as before, φk|y=1 =\\np(xj = k|y = 1) (for any j) and φk|y=0 = p(xj = k|y = 0). Note that we have\\nassumed that p(xj|y) is the same for all values of j (i.e., that the distribution\\naccording to which a word is generated does not depend on its position j\\nwithin the email).\\nIf we are given a training set {(x(i), y(i)); i = 1, . . . , n} where x(i) =\\n(x(i)\\n1 , x(i)\\n2 , . . . , x(i)\\ndi ) (here, di is the number of words in the i-training example),\\nthe likelihood of the data is given by\\nL(φy, φk|y=0, φk|y=1)\\n=\\nn\\nY\\ni=1\\np(x(i), y(i))\\n=\\nn\\nY\\ni=1\\n di\\nY\\nj=1\\np(x(i)\\nj |y; φk|y=0, φk|y=1)\\n!\\np(y(i); φy).\\nMaximizing this yields the maximum likelihood estimates of the parameters:\\nφk|y=1\\n=\\nPn\\ni=1\\nPdi\\nj=1 1{x(i)\\nj\\n= k ∧y(i) = 1}\\nPn\\ni=1 1{y(i) = 1}di\\nφk|y=0\\n=\\nPn\\ni=1\\nPdi\\nj=1 1{x(i)\\nj\\n= k ∧y(i) = 0}\\nPn\\ni=1 1{y(i) = 0}di\\nφy\\n=\\nPn\\ni=1 1{y(i) = 1}\\nn\\n.\\nIf we were to apply Laplace smoothing (which is needed in practice for good\\nperformance) when estimating φk|y=0 and φk|y=1, we add 1 to the numerators\\nand |V | to the denominators, and obtain:\\nφk|y=1\\n=\\n1 + Pn\\ni=1\\nPdi\\nj=1 1{x(i)\\nj\\n= k ∧y(i) = 1}\\n|V | + Pn\\ni=1 1{y(i) = 1}di\\nφk|y=0\\n=\\n1 + Pn\\ni=1\\nPdi\\nj=1 1{x(i)\\nj\\n= k ∧y(i) = 0}\\n|V | + Pn\\ni=1 1{y(i) = 0}di\\n.\\nWhile not necessarily the very best classiﬁcation algorithm, the Naive Bayes\\nclassiﬁer often works surprisingly well. It is often also a very good “ﬁrst thing\\nto try,” given its simplicity and ease of implementation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 48}, page_content='Chapter 5\\nKernel methods\\n5.1\\nFeature maps\\nRecall that in our discussion about linear regression, we considered the prob-\\nlem of predicting the price of a house (denoted by y) from the living area of\\nthe house (denoted by x), and we ﬁt a linear function of x to the training\\ndata. What if the price y can be more accurately represented as a non-linear\\nfunction of x? In this case, we need a more expressive family of models than\\nlinear models.\\nWe start by considering ﬁtting cubic functions y = θ3x3 +θ2x2 +θ1x+θ0.\\nIt turns out that we can view the cubic function as a linear function over\\nthe a diﬀerent set of feature variables (deﬁned below). Concretely, let the\\nfunction φ : R →R4 be deﬁned as\\nφ(x) =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\nx\\nx2\\nx3\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb∈R4.\\n(5.1)\\nLet θ ∈R4 be the vector containing θ0, θ1, θ2, θ3 as entries. Then we can\\nrewrite the cubic function in x as:\\nθ3x3 + θ2x2 + θ1x + θ0 = θTφ(x)\\nThus, a cubic function of the variable x can be viewed as a linear function\\nover the variables φ(x). To distinguish between these two sets of variables,\\nin the context of kernel methods, we will call the “original” input value the\\ninput attributes of a problem (in this case, x, the living area). When the\\n48'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 49}, page_content='49\\noriginal input is mapped to some new set of quantities φ(x), we will call those\\nnew quantities the features variables. (Unfortunately, diﬀerent authors use\\ndiﬀerent terms to describe these two things in diﬀerent contexts.) We will\\ncall φ a feature map, which maps the attributes to the features.\\n5.2\\nLMS (least mean squares) with features\\nWe will derive the gradient descent algorithm for ﬁtting the model θTφ(x).\\nFirst recall that for ordinary least square problem where we were to ﬁt θTx,\\nthe batch gradient descent update is (see the ﬁrst lecture note for its deriva-\\ntion):\\nθ := θ + α\\nn\\nX\\ni=1\\n\\x00y(i) −hθ(x(i))\\n\\x01\\nx(i)\\n:= θ + α\\nn\\nX\\ni=1\\n\\x00y(i) −θTx(i)\\x01\\nx(i).\\n(5.2)\\nLet φ : Rd →Rp be a feature map that maps attribute x (in Rd) to the\\nfeatures φ(x) in Rp. (In the motivating example in the previous subsection,\\nwe have d = 1 and p = 4.) Now our goal is to ﬁt the function θTφ(x), with\\nθ being a vector in Rp instead of Rd. We can replace all the occurrences of\\nx(i) in the algorithm above by φ(x(i)) to obtain the new update:\\nθ := θ + α\\nn\\nX\\ni=1\\n\\x00y(i) −θTφ(x(i))\\n\\x01\\nφ(x(i))\\n(5.3)\\nSimilarly, the corresponding stochastic gradient descent update rule is\\nθ := θ + α\\n\\x00y(i) −θTφ(x(i))\\n\\x01\\nφ(x(i))\\n(5.4)\\n5.3\\nLMS with the kernel trick\\nThe gradient descent update, or stochastic gradient update above becomes\\ncomputationally expensive when the features φ(x) is high-dimensional. For\\nexample, consider the direct extension of the feature map in equation (5.1)\\nto high-dimensional input x: suppose x ∈Rd, and let φ(x) be the vector that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 50}, page_content='50\\ncontains all the monomials of x with degree ≤3\\nφ(x) =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\nx1\\nx2\\n...\\nx2\\n1\\nx1x2\\nx1x3\\n...\\nx2x1\\n...\\nx3\\n1\\nx2\\n1x2\\n...\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\n(5.5)\\nThe dimension of the features φ(x) is on the order of d3.1 This is a pro-\\nhibitively long vector for computational purpose — when d = 1000, each\\nupdate requires at least computing and storing a 10003 = 109 dimensional\\nvector, which is 106 times slower than the update rule for for ordinary least\\nsquares updates (5.2).\\nIt may appear at ﬁrst that such d3 runtime per update and memory usage\\nare inevitable, because the vector θ itself is of dimension p ≈d3, and we may\\nneed to update every entry of θ and store it. However, we will introduce the\\nkernel trick with which we will not need to store θ explicitly, and the runtime\\ncan be signiﬁcantly improved.\\nFor simplicity, we assume the initialize the value θ = 0, and we focus\\non the iterative update (5.3). The main observation is that at any time, θ\\ncan be represented as a linear combination of the vectors φ(x(1)), . . . , φ(x(n)).\\nIndeed, we can show this inductively as follows. At initialization, θ = 0 =\\nPn\\ni=1 0 · φ(x(i)). Assume at some point, θ can be represented as\\nθ =\\nn\\nX\\ni=1\\nβiφ(x(i))\\n(5.6)\\n1Here, for simplicity, we include all the monomials with repetitions (so that, e.g., x1x2x3\\nand x2x3x1 both appear in φ(x)). Therefore, there are totally 1 + d + d2 + d3 entries in\\nφ(x).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 51}, page_content='51\\nfor some β1, . . . , βn ∈R. Then we claim that in the next round, θ is still a\\nlinear combination of φ(x(1)), . . . , φ(x(n)) because\\nθ := θ + α\\nn\\nX\\ni=1\\n\\x00y(i) −θTφ(x(i))\\n\\x01\\nφ(x(i))\\n=\\nn\\nX\\ni=1\\nβiφ(x(i)) + α\\nn\\nX\\ni=1\\n\\x00y(i) −θTφ(x(i))\\n\\x01\\nφ(x(i))\\n=\\nn\\nX\\ni=1\\n(βi + α\\n\\x00y(i) −θTφ(x(i))\\n\\x01\\n)\\n|\\n{z\\n}\\nnew βi\\nφ(x(i))\\n(5.7)\\nYou may realize that our general strategy is to implicitly represent the p-\\ndimensional vector θ by a set of coeﬃcients β1, . . . , βn. Towards doing this,\\nwe derive the update rule of the coeﬃcients β1, . . . , βn. Using the equation\\nabove, we see that the new βi depends on the old one via\\nβi := βi + α\\n\\x00y(i) −θTφ(x(i))\\n\\x01\\n(5.8)\\nHere we still have the old θ on the RHS of the equation. Replacing θ by\\nθ = Pn\\nj=1 βjφ(x(j)) gives\\n∀i ∈{1, . . . , n}, βi := βi + α\\n \\ny(i) −\\nn\\nX\\nj=1\\nβjφ(x(j))\\nTφ(x(i))\\n!\\nWe often rewrite φ(x(j))\\nTφ(x(i)) as ⟨φ(x(j)), φ(x(i))⟩to emphasize that it’s the\\ninner product of the two feature vectors. Viewing βi’s as the new representa-\\ntion of θ, we have successfully translated the batch gradient descent algorithm\\ninto an algorithm that updates the value of β iteratively. It may appear that\\nat every iteration, we still need to compute the values of ⟨φ(x(j)), φ(x(i))⟩for\\nall pairs of i, j, each of which may take roughly O(p) operation. However,\\ntwo important properties come to rescue:\\n1. We can pre-compute the pairwise inner products ⟨φ(x(j)), φ(x(i))⟩for all\\npairs of i, j before the loop starts.\\n2. For the feature map φ deﬁned in (5.5) (or many other interesting fea-\\nture maps), computing ⟨φ(x(j)), φ(x(i))⟩can be eﬃcient and does not'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 52}, page_content='52\\nnecessarily require computing φ(x(i)) explicitly. This is because:\\n⟨φ(x), φ(z)⟩= 1 +\\nd\\nX\\ni=1\\nxizi +\\nX\\ni,j∈{1,...,d}\\nxixjzizj +\\nX\\ni,j,k∈{1,...,d}\\nxixjxkzizjzk\\n= 1 +\\nd\\nX\\ni=1\\nxizi +\\n \\nd\\nX\\ni=1\\nxizi\\n!2\\n+\\n \\nd\\nX\\ni=1\\nxizi\\n!3\\n= 1 + ⟨x, z⟩+ ⟨x, z⟩2 + ⟨x, z⟩3\\n(5.9)\\nTherefore, to compute ⟨φ(x), φ(z)⟩, we can ﬁrst compute ⟨x, z⟩with\\nO(d) time and then take another constant number of operations to com-\\npute 1 + ⟨x, z⟩+ ⟨x, z⟩2 + ⟨x, z⟩3.\\nAs you will see, the inner products between the features ⟨φ(x), φ(z)⟩are\\nessential here. We deﬁne the Kernel corresponding to the feature map φ as\\na function that maps X × X →R satisfying: 2\\nK(x, z) ≜⟨φ(x), φ(z)⟩\\n(5.10)\\nTo wrap up the discussion, we write the down the ﬁnal algorithm as\\nfollows:\\n1. Compute all the values K(x(i), x(j)) ≜⟨φ(x(i)), φ(x(j))⟩using equa-\\ntion (5.9) for all i, j ∈{1, . . . , n}. Set β := 0.\\n2. Loop:\\n∀i ∈{1, . . . , n}, βi := βi + α\\n \\ny(i) −\\nn\\nX\\nj=1\\nβjK(x(i), x(j))\\n!\\n(5.11)\\nOr in vector notation, letting K be the n × n matrix with Kij =\\nK(x(i), x(j)), we have\\nβ := β + α(⃗y −Kβ)\\nWith the algorithm above, we can update the representation β of the\\nvector θ eﬃciently with O(n) time per update. Finally, we need to show that\\n2Recall that X is the space of the input x. In our running example, X = Rd'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 53}, page_content='53\\nthe knowledge of the representation β suﬃces to compute the prediction\\nθTφ(x). Indeed, we have\\nθTφ(x) =\\nn\\nX\\ni=1\\nβiφ(x(i))\\nTφ(x) =\\nn\\nX\\ni=1\\nβiK(x(i), x)\\n(5.12)\\nYou may realize that fundamentally all we need to know about the feature\\nmap φ(·) is encapsulated in the corresponding kernel function K(·, ·). We\\nwill expand on this in the next section.\\n5.4\\nProperties of kernels\\nIn the last subsection, we started with an explicitly deﬁned feature map φ,\\nwhich induces the kernel function K(x, z) ≜⟨φ(x), φ(z)⟩. Then we saw that\\nthe kernel function is so intrinsic so that as long as the kernel function is\\ndeﬁned, the whole training algorithm can be written entirely in the language\\nof the kernel without referring to the feature map φ, so can the prediction of\\na test example x (equation (5.12).)\\nTherefore, it would be tempted to deﬁne other kernel function K(·, ·) and\\nrun the algorithm (5.11). Note that the algorithm (5.11) does not need to\\nexplicitly access the feature map φ, and therefore we only need to ensure the\\nexistence of the feature map φ, but do not necessarily need to be able to\\nexplicitly write φ down.\\nWhat kinds of functions K(·, ·) can correspond to some feature map φ? In\\nother words, can we tell if there is some feature mapping φ so that K(x, z) =\\nφ(x)Tφ(z) for all x, z?\\nIf we can answer this question by giving a precise characterization of valid\\nkernel functions, then we can completely change the interface of selecting\\nfeature maps φ to the interface of selecting kernel function K. Concretely,\\nwe can pick a function K, verify that it satisﬁes the characterization (so\\nthat there exists a feature map φ that K corresponds to), and then we can\\nrun update rule (5.11). The beneﬁt here is that we don’t have to be able\\nto compute φ or write it down analytically, and we only need to know its\\nexistence. We will answer this question at the end of this subsection after\\nwe go through several concrete examples of kernels.\\nSuppose x, z ∈Rd, and let’s ﬁrst consider the function K(·, ·) deﬁned as:\\nK(x, z) = (xTz)2.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 54}, page_content='54\\nWe can also write this as\\nK(x, z)\\n=\\n \\nd\\nX\\ni=1\\nxizi\\n!  \\nd\\nX\\nj=1\\nxjzj\\n!\\n=\\nd\\nX\\ni=1\\nd\\nX\\nj=1\\nxixjzizj\\n=\\nd\\nX\\ni,j=1\\n(xixj)(zizj)\\nThus, we see that K(x, z) = ⟨φ(x), φ(z)⟩is the kernel function that corre-\\nsponds to the the feature mapping φ given (shown here for the case of d = 3)\\nby\\nφ(x) =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx1x1\\nx1x2\\nx1x3\\nx2x1\\nx2x2\\nx2x3\\nx3x1\\nx3x2\\nx3x3\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nRevisiting the computational eﬃciency perspective of kernel, note that whereas\\ncalculating the high-dimensional φ(x) requires O(d2) time, ﬁnding K(x, z)\\ntakes only O(d) time—linear in the dimension of the input attributes.\\nFor another related example, also consider K(·, ·) deﬁned by\\nK(x, z)\\n=\\n(xTz + c)2\\n=\\nd\\nX\\ni,j=1\\n(xixj)(zizj) +\\nd\\nX\\ni=1\\n(\\n√\\n2cxi)(\\n√\\n2czi) + c2.\\n(Check this yourself.) This function K is a kernel function that corresponds'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 55}, page_content='55\\nto the feature mapping (again shown for d = 3)\\nφ(x) =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx1x1\\nx1x2\\nx1x3\\nx2x1\\nx2x2\\nx2x3\\nx3x1\\nx3x2\\nx3x3\\n√\\n2cx1\\n√\\n2cx2\\n√\\n2cx3\\nc\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n,\\nand the parameter c controls the relative weighting between the xi (ﬁrst\\norder) and the xixj (second order) terms.\\nMore broadly, the kernel K(x, z) = (xTz + c)k corresponds to a feature\\nmapping to an\\n\\x00d+k\\nk\\n\\x01\\nfeature space, corresponding of all monomials of the\\nform xi1xi2 . . . xik that are up to order k. However, despite working in this\\nO(dk)-dimensional space, computing K(x, z) still takes only O(d) time, and\\nhence we never need to explicitly represent feature vectors in this very high\\ndimensional feature space.\\nKernels as similarity metrics.\\nNow, let’s talk about a slightly diﬀerent\\nview of kernels. Intuitively, (and there are things wrong with this intuition,\\nbut nevermind), if φ(x) and φ(z) are close together, then we might expect\\nK(x, z) = φ(x)Tφ(z) to be large. Conversely, if φ(x) and φ(z) are far apart—\\nsay nearly orthogonal to each other—then K(x, z) = φ(x)Tφ(z) will be small.\\nSo, we can think of K(x, z) as some measurement of how similar are φ(x)\\nand φ(z), or of how similar are x and z.\\nGiven this intuition, suppose that for some learning problem that you’re\\nworking on, you’ve come up with some function K(x, z) that you think might\\nbe a reasonable measure of how similar x and z are. For instance, perhaps\\nyou chose\\nK(x, z) = exp\\n\\x12\\n−||x −z||2\\n2σ2\\n\\x13\\n.\\nThis is a reasonable measure of x and z’s similarity, and is close to 1 when\\nx and z are close, and near 0 when x and z are far apart. Does there exist'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 56}, page_content='56\\na feature map φ such that the kernel K deﬁned above satisﬁes K(x, z) =\\nφ(x)Tφ(z)? In this particular example, the answer is yes. This kernel is called\\nthe Gaussian kernel, and corresponds to an inﬁnite dimensional feature\\nmapping φ. We will give a precise characterization about what properties\\na function K needs to satisfy so that it can be a valid kernel function that\\ncorresponds to some feature map φ.\\nNecessary conditions for valid kernels.\\nSuppose for now that K is\\nindeed a valid kernel corresponding to some feature mapping φ, and we will\\nﬁrst see what properties it satisﬁes. Now, consider some ﬁnite set of n points\\n(not necessarily the training set) {x(1), . . . , x(n)}, and let a square, n-by-n\\nmatrix K be deﬁned so that its (i, j)-entry is given by Kij = K(x(i), x(j)).\\nThis matrix is called the kernel matrix. Note that we’ve overloaded the\\nnotation and used K to denote both the kernel function K(x, z) and the\\nkernel matrix K, due to their obvious close relationship.\\nNow, if K is a valid kernel, then Kij = K(x(i), x(j)) = φ(x(i))Tφ(x(j)) =\\nφ(x(j))Tφ(x(i)) = K(x(j), x(i)) = Kji, and hence K must be symmetric. More-\\nover, letting φk(x) denote the k-th coordinate of the vector φ(x), we ﬁnd that\\nfor any vector z, we have\\nzTKz\\n=\\nX\\ni\\nX\\nj\\nziKijzj\\n=\\nX\\ni\\nX\\nj\\nziφ(x(i))Tφ(x(j))zj\\n=\\nX\\ni\\nX\\nj\\nzi\\nX\\nk\\nφk(x(i))φk(x(j))zj\\n=\\nX\\nk\\nX\\ni\\nX\\nj\\nziφk(x(i))φk(x(j))zj\\n=\\nX\\nk\\n X\\ni\\nziφk(x(i))\\n!2\\n≥\\n0.\\nThe second-to-last step uses the fact that P\\ni,j aiaj = (P\\ni ai)2 for ai =\\nziφk(x(i)). Since z was arbitrary, this shows that K is positive semi-deﬁnite\\n(K ≥0).\\nHence, we’ve shown that if K is a valid kernel (i.e., if it corresponds to\\nsome feature mapping φ), then the corresponding kernel matrix K ∈Rn×n\\nis symmetric positive semideﬁnite.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 57}, page_content='57\\nSuﬃcient conditions for valid kernels.\\nMore generally, the condition\\nabove turns out to be not only a necessary, but also a suﬃcient, condition\\nfor K to be a valid kernel (also called a Mercer kernel). The following result\\nis due to Mercer.3\\nTheorem (Mercer).\\nLet K : Rd × Rd 7→R be given.\\nThen for K\\nto be a valid (Mercer) kernel, it is necessary and suﬃcient that for any\\n{x(1), . . . , x(n)}, (n < ∞), the corresponding kernel matrix is symmetric pos-\\nitive semi-deﬁnite.\\nGiven a function K, apart from trying to ﬁnd a feature mapping φ that\\ncorresponds to it, this theorem therefore gives another way of testing if it is\\na valid kernel. You’ll also have a chance to play with these ideas more in\\nproblem set 2.\\nIn class, we also brieﬂy talked about a couple of other examples of ker-\\nnels. For instance, consider the digit recognition problem, in which given\\nan image (16x16 pixels) of a handwritten digit (0-9), we have to ﬁgure out\\nwhich digit it was. Using either a simple polynomial kernel K(x, z) = (xTz)k\\nor the Gaussian kernel, SVMs were able to obtain extremely good perfor-\\nmance on this problem.\\nThis was particularly surprising since the input\\nattributes x were just 256-dimensional vectors of the image pixel intensity\\nvalues, and the system had no prior knowledge about vision, or even about\\nwhich pixels are adjacent to which other ones. Another example that we\\nbrieﬂy talked about in lecture was that if the objects x that we are trying\\nto classify are strings (say, x is a list of amino acids, which strung together\\nform a protein), then it seems hard to construct a reasonable, “small” set of\\nfeatures for most learning algorithms, especially if diﬀerent strings have dif-\\nferent lengths. However, consider letting φ(x) be a feature vector that counts\\nthe number of occurrences of each length-k substring in x. If we’re consid-\\nering strings of English letters, then there are 26k such strings. Hence, φ(x)\\nis a 26k dimensional vector; even for moderate values of k, this is probably\\ntoo big for us to eﬃciently work with. (e.g., 264 ≈460000.) However, using\\n(dynamic programming-ish) string matching algorithms, it is possible to ef-\\nﬁciently compute K(x, z) = φ(x)Tφ(z), so that we can now implicitly work\\nin this 26k-dimensional feature space, but without ever explicitly computing\\nfeature vectors in this space.\\n3Many texts present Mercer’s theorem in a slightly more complicated form involving\\nL2 functions, but when the input attributes take values in Rd, the version given here is\\nequivalent.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 58}, page_content='58\\nApplication of kernel methods: We’ve seen the application of kernels\\nto linear regression. In the next part, we will introduce the support vector\\nmachines to which kernels can be directly applied. dwell too much longer on\\nit here. In fact, the idea of kernels has signiﬁcantly broader applicability than\\nlinear regression and SVMs. Speciﬁcally, if you have any learning algorithm\\nthat you can write in terms of only inner products ⟨x, z⟩between input\\nattribute vectors, then by replacing this with K(x, z) where K is a kernel,\\nyou can “magically” allow your algorithm to work eﬃciently in the high\\ndimensional feature space corresponding to K. For instance, this kernel trick\\ncan be applied with the perceptron to derive a kernel perceptron algorithm.\\nMany of the algorithms that we’ll see later in this class will also be amenable\\nto this method, which has come to be known as the “kernel trick.”'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 59}, page_content='Chapter 6\\nSupport vector machines\\nThis set of notes presents the Support Vector Machine (SVM) learning al-\\ngorithm. SVMs are among the best (and many believe are indeed the best)\\n“oﬀ-the-shelf” supervised learning algorithms. To tell the SVM story, we’ll\\nneed to ﬁrst talk about margins and the idea of separating data with a large\\n“gap.” Next, we’ll talk about the optimal margin classiﬁer, which will lead\\nus into a digression on Lagrange duality. We’ll also see kernels, which give\\na way to apply SVMs eﬃciently in very high dimensional (such as inﬁnite-\\ndimensional) feature spaces, and ﬁnally, we’ll close oﬀthe story with the\\nSMO algorithm, which gives an eﬃcient implementation of SVMs.\\n6.1\\nMargins: intuition\\nWe’ll start our story on SVMs by talking about margins. This section will\\ngive the intuitions about margins and about the “conﬁdence” of our predic-\\ntions; these ideas will be made formal in Section 6.3.\\nConsider logistic regression, where the probability p(y = 1|x; θ) is mod-\\neled by hθ(x) = g(θTx). We then predict “1” on an input x if and only if\\nhθ(x) ≥0.5, or equivalently, if and only if θTx ≥0. Consider a positive\\ntraining example (y = 1). The larger θTx is, the larger also is hθ(x) = p(y =\\n1|x; θ), and thus also the higher our degree of “conﬁdence” that the label is 1.\\nThus, informally we can think of our prediction as being very conﬁdent that\\ny = 1 if θTx ≫0. Similarly, we think of logistic regression as conﬁdently\\npredicting y = 0, if θTx ≪0. Given a training set, again informally it seems\\nthat we’d have found a good ﬁt to the training data if we can ﬁnd θ so that\\nθTx(i) ≫0 whenever y(i) = 1, and θTx(i) ≪0 whenever y(i) = 0, since this\\nwould reﬂect a very conﬁdent (and correct) set of classiﬁcations for all the\\n59'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 60}, page_content='60\\ntraining examples. This seems to be a nice goal to aim for, and we’ll soon\\nformalize this idea using the notion of functional margins.\\nFor a diﬀerent type of intuition, consider the following ﬁgure, in which x’s\\nrepresent positive training examples, o’s denote negative training examples,\\na decision boundary (this is the line given by the equation θTx = 0, and\\nis also called the separating hyperplane) is also shown, and three points\\nhave also been labeled A, B and C.\\n\\x00\\x01\\n\\x00\\x01\\n\\x00\\x01\\nB\\nA\\nC\\nNotice that the point A is very far from the decision boundary. If we are\\nasked to make a prediction for the value of y at A, it seems we should be\\nquite conﬁdent that y = 1 there. Conversely, the point C is very close to\\nthe decision boundary, and while it’s on the side of the decision boundary\\non which we would predict y = 1, it seems likely that just a small change to\\nthe decision boundary could easily have caused out prediction to be y = 0.\\nHence, we’re much more conﬁdent about our prediction at A than at C. The\\npoint B lies in-between these two cases, and more broadly, we see that if\\na point is far from the separating hyperplane, then we may be signiﬁcantly\\nmore conﬁdent in our predictions. Again, informally we think it would be\\nnice if, given a training set, we manage to ﬁnd a decision boundary that\\nallows us to make all correct and conﬁdent (meaning far from the decision\\nboundary) predictions on the training examples. We’ll formalize this later\\nusing the notion of geometric margins.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 61}, page_content='61\\n6.2\\nNotation (option reading)\\nTo make our discussion of SVMs easier, we’ll ﬁrst need to introduce a new\\nnotation for talking about classiﬁcation.\\nWe will be considering a linear\\nclassiﬁer for a binary classiﬁcation problem with labels y and features x.\\nFrom now, we’ll use y ∈{−1, 1} (instead of {0, 1}) to denote the class labels.\\nAlso, rather than parameterizing our linear classiﬁer with the vector θ, we\\nwill use parameters w, b, and write our classiﬁer as\\nhw,b(x) = g(wTx + b).\\nHere, g(z) = 1 if z ≥0, and g(z) = −1 otherwise. This “w, b” notation\\nallows us to explicitly treat the intercept term b separately from the other\\nparameters. (We also drop the convention we had previously of letting x0 = 1\\nbe an extra coordinate in the input feature vector.) Thus, b takes the role of\\nwhat was previously θ0, and w takes the role of [θ1 . . . θd]T.\\nNote also that, from our deﬁnition of g above, our classiﬁer will directly\\npredict either 1 or −1 (cf. the perceptron algorithm), without ﬁrst going\\nthrough the intermediate step of estimating p(y = 1) (which is what logistic\\nregression does).\\n6.3\\nFunctional and geometric margins (op-\\ntion reading)\\nLet’s formalize the notions of the functional and geometric margins. Given a\\ntraining example (x(i), y(i)), we deﬁne the functional margin of (w, b) with\\nrespect to the training example as\\nˆγ(i) = y(i)(wTx(i) + b).\\nNote that if y(i) = 1, then for the functional margin to be large (i.e., for\\nour prediction to be conﬁdent and correct), we need wTx(i) + b to be a large\\npositive number. Conversely, if y(i) = −1, then for the functional margin\\nto be large, we need wTx(i) + b to be a large negative number. Moreover, if\\ny(i)(wTx(i) + b) > 0, then our prediction on this example is correct. (Check\\nthis yourself.) Hence, a large functional margin represents a conﬁdent and a\\ncorrect prediction.\\nFor a linear classiﬁer with the choice of g given above (taking values in\\n{−1, 1}), there’s one property of the functional margin that makes it not a\\nvery good measure of conﬁdence, however. Given our choice of g, we note that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 62}, page_content='62\\nif we replace w with 2w and b with 2b, then since g(wTx+b) = g(2wTx+2b),\\nthis would not change hw,b(x) at all. I.e., g, and hence also hw,b(x), depends\\nonly on the sign, but not on the magnitude, of wTx + b. However, replacing\\n(w, b) with (2w, 2b) also results in multiplying our functional margin by a\\nfactor of 2. Thus, it seems that by exploiting our freedom to scale w and b,\\nwe can make the functional margin arbitrarily large without really changing\\nanything meaningful. Intuitively, it might therefore make sense to impose\\nsome sort of normalization condition such as that ||w||2 = 1; i.e., we might\\nreplace (w, b) with (w/||w||2, b/||w||2), and instead consider the functional\\nmargin of (w/||w||2, b/||w||2). We’ll come back to this later.\\nGiven a training set S = {(x(i), y(i)); i = 1, . . . , n}, we also deﬁne the\\nfunction margin of (w, b) with respect to S as the smallest of the functional\\nmargins of the individual training examples. Denoted by ˆγ, this can therefore\\nbe written:\\nˆγ = min\\ni=1,...,n ˆγ(i).\\nNext, let’s talk about geometric margins. Consider the picture below:\\nw\\nA\\nγ\\nB\\n(i)\\nThe decision boundary corresponding to (w, b) is shown, along with the\\nvector w. Note that w is orthogonal (at 90◦) to the separating hyperplane.\\n(You should convince yourself that this must be the case.)\\nConsider the\\npoint at A, which represents the input x(i) of some training example with\\nlabel y(i) = 1. Its distance to the decision boundary, γ(i), is given by the line\\nsegment AB.\\nHow can we ﬁnd the value of γ(i)? Well, w/||w|| is a unit-length vector\\npointing in the same direction as w. Since A represents x(i), we therefore'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 63}, page_content='63\\nﬁnd that the point B is given by x(i) −γ(i) · w/||w||. But this point lies on\\nthe decision boundary, and all points x on the decision boundary satisfy the\\nequation wTx + b = 0. Hence,\\nwT\\n\\x12\\nx(i) −γ(i) w\\n||w||\\n\\x13\\n+ b = 0.\\nSolving for γ(i) yields\\nγ(i) = wTx(i) + b\\n||w||\\n=\\n\\x12 w\\n||w||\\n\\x13T\\nx(i) +\\nb\\n||w||.\\nThis was worked out for the case of a positive training example at A in the\\nﬁgure, where being on the “positive” side of the decision boundary is good.\\nMore generally, we deﬁne the geometric margin of (w, b) with respect to a\\ntraining example (x(i), y(i)) to be\\nγ(i) = y(i)\\n \\x12 w\\n||w||\\n\\x13T\\nx(i) +\\nb\\n||w||\\n!\\n.\\nNote that if ||w|| = 1, then the functional margin equals the geometric\\nmargin—this thus gives us a way of relating these two diﬀerent notions of\\nmargin. Also, the geometric margin is invariant to rescaling of the parame-\\nters; i.e., if we replace w with 2w and b with 2b, then the geometric margin\\ndoes not change. This will in fact come in handy later. Speciﬁcally, because\\nof this invariance to the scaling of the parameters, when trying to ﬁt w and b\\nto training data, we can impose an arbitrary scaling constraint on w without\\nchanging anything important; for instance, we can demand that ||w|| = 1, or\\n|w1| = 5, or |w1 + b| + |w2| = 2, and any of these can be satisﬁed simply by\\nrescaling w and b.\\nFinally, given a training set S = {(x(i), y(i)); i = 1, . . . , n}, we also deﬁne\\nthe geometric margin of (w, b) with respect to S to be the smallest of the\\ngeometric margins on the individual training examples:\\nγ = min\\ni=1,...,n γ(i).\\n6.4\\nThe optimal margin classiﬁer (option read-\\ning)\\nGiven a training set, it seems from our previous discussion that a natural\\ndesideratum is to try to ﬁnd a decision boundary that maximizes the (ge-\\nometric) margin, since this would reﬂect a very conﬁdent set of predictions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 64}, page_content='64\\non the training set and a good “ﬁt” to the training data. Speciﬁcally, this\\nwill result in a classiﬁer that separates the positive and the negative training\\nexamples with a “gap” (geometric margin).\\nFor now, we will assume that we are given a training set that is linearly\\nseparable; i.e., that it is possible to separate the positive and negative ex-\\namples using some separating hyperplane. How will we ﬁnd the one that\\nachieves the maximum geometric margin? We can pose the following opti-\\nmization problem:\\nmaxγ,w,b\\nγ\\ns.t.\\ny(i)(wTx(i) + b) ≥γ, i = 1, . . . , n\\n||w|| = 1.\\nI.e., we want to maximize γ, subject to each training example having func-\\ntional margin at least γ. The ||w|| = 1 constraint moreover ensures that the\\nfunctional margin equals to the geometric margin, so we are also guaranteed\\nthat all the geometric margins are at least γ. Thus, solving this problem will\\nresult in (w, b) with the largest possible geometric margin with respect to the\\ntraining set.\\nIf we could solve the optimization problem above, we’d be done. But the\\n“||w|| = 1” constraint is a nasty (non-convex) one, and this problem certainly\\nisn’t in any format that we can plug into standard optimization software to\\nsolve. So, let’s try transforming the problem into a nicer one. Consider:\\nmaxˆγ,w,b\\nˆγ\\n||w||\\ns.t.\\ny(i)(wTx(i) + b) ≥ˆγ, i = 1, . . . , n\\nHere, we’re going to maximize ˆγ/||w||, subject to the functional margins all\\nbeing at least ˆγ. Since the geometric and functional margins are related by\\nγ = ˆγ/||w|, this will give us the answer we want. Moreover, we’ve gotten rid\\nof the constraint ||w|| = 1 that we didn’t like. The downside is that we now\\nhave a nasty (again, non-convex) objective\\nˆγ\\n||w|| function; and, we still don’t\\nhave any oﬀ-the-shelf software that can solve this form of an optimization\\nproblem.\\nLet’s keep going. Recall our earlier discussion that we can add an arbi-\\ntrary scaling constraint on w and b without changing anything. This is the\\nkey idea we’ll use now. We will introduce the scaling constraint that the\\nfunctional margin of w, b with respect to the training set must be 1:\\nˆγ = 1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 65}, page_content='65\\nSince multiplying w and b by some constant results in the functional margin\\nbeing multiplied by that same constant, this is indeed a scaling constraint,\\nand can be satisﬁed by rescaling w, b. Plugging this into our problem above,\\nand noting that maximizing ˆγ/||w|| = 1/||w|| is the same thing as minimizing\\n||w||2, we now have the following optimization problem:\\nminw,b\\n1\\n2||w||2\\ns.t.\\ny(i)(wTx(i) + b) ≥1, i = 1, . . . , n\\nWe’ve now transformed the problem into a form that can be eﬃciently\\nsolved. The above is an optimization problem with a convex quadratic ob-\\njective and only linear constraints. Its solution gives us the optimal mar-\\ngin classiﬁer. This optimization problem can be solved using commercial\\nquadratic programming (QP) code.1\\nWhile we could call the problem solved here, what we will instead do is\\nmake a digression to talk about Lagrange duality. This will lead us to our\\noptimization problem’s dual form, which will play a key role in allowing us to\\nuse kernels to get optimal margin classiﬁers to work eﬃciently in very high\\ndimensional spaces. The dual form will also allow us to derive an eﬃcient\\nalgorithm for solving the above optimization problem that will typically do\\nmuch better than generic QP software.\\n6.5\\nLagrange duality (optional reading)\\nLet’s temporarily put aside SVMs and maximum margin classiﬁers, and talk\\nabout solving constrained optimization problems.\\nConsider a problem of the following form:\\nminw\\nf(w)\\ns.t.\\nhi(w) = 0, i = 1, . . . , l.\\nSome of you may recall how the method of Lagrange multipliers can be used\\nto solve it. (Don’t worry if you haven’t seen it before.) In this method, we\\ndeﬁne the Lagrangian to be\\nL(w, β) = f(w) +\\nl\\nX\\ni=1\\nβihi(w)\\n1You may be familiar with linear programming, which solves optimization problems\\nthat have linear objectives and linear constraints. QP software is also widely available,\\nwhich allows convex quadratic objectives and linear constraints.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 66}, page_content='66\\nHere, the βi’s are called the Lagrange multipliers. We would then ﬁnd\\nand set L’s partial derivatives to zero:\\n∂L\\n∂wi\\n= 0;\\n∂L\\n∂βi\\n= 0,\\nand solve for w and β.\\nIn this section, we will generalize this to constrained optimization prob-\\nlems in which we may have inequality as well as equality constraints. Due to\\ntime constraints, we won’t really be able to do the theory of Lagrange duality\\njustice in this class,2 but we will give the main ideas and results, which we\\nwill then apply to our optimal margin classiﬁer’s optimization problem.\\nConsider the following, which we’ll call the primal optimization problem:\\nminw\\nf(w)\\ns.t.\\ngi(w) ≤0, i = 1, . . . , k\\nhi(w) = 0, i = 1, . . . , l.\\nTo solve it, we start by deﬁning the generalized Lagrangian\\nL(w, α, β) = f(w) +\\nk\\nX\\ni=1\\nαigi(w) +\\nl\\nX\\ni=1\\nβihi(w).\\nHere, the αi’s and βi’s are the Lagrange multipliers. Consider the quantity\\nθP(w) =\\nmax\\nα,β : αi≥0 L(w, α, β).\\nHere, the “P” subscript stands for “primal.” Let some w be given. If w\\nviolates any of the primal constraints (i.e., if either gi(w) > 0 or hi(w) ̸= 0\\nfor some i), then you should be able to verify that\\nθP(w)\\n=\\nmax\\nα,β : αi≥0 f(w) +\\nk\\nX\\ni=1\\nαigi(w) +\\nl\\nX\\ni=1\\nβihi(w)\\n(6.1)\\n=\\n∞.\\n(6.2)\\nConversely, if the constraints are indeed satisﬁed for a particular value of w,\\nthen θP(w) = f(w). Hence,\\nθP(w) =\\n\\x1a f(w)\\nif w satisﬁes primal constraints\\n∞\\notherwise.\\n2Readers interested in learning more about this topic are encouraged to read, e.g., R.\\nT. Rockarfeller (1970), Convex Analysis, Princeton University Press.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 67}, page_content='67\\nThus, θP takes the same value as the objective in our problem for all val-\\nues of w that satisﬁes the primal constraints, and is positive inﬁnity if the\\nconstraints are violated. Hence, if we consider the minimization problem\\nmin\\nw θP(w) = min\\nw\\nmax\\nα,β : αi≥0 L(w, α, β),\\nwe see that it is the same problem (i.e., and has the same solutions as) our\\noriginal, primal problem. For later use, we also deﬁne the optimal value of\\nthe objective to be p∗= minw θP(w); we call this the value of the primal\\nproblem.\\nNow, let’s look at a slightly diﬀerent problem. We deﬁne\\nθD(α, β) = min\\nw L(w, α, β).\\nHere, the “D” subscript stands for “dual.” Note also that whereas in the\\ndeﬁnition of θP we were optimizing (maximizing) with respect to α, β, here\\nwe are minimizing with respect to w.\\nWe can now pose the dual optimization problem:\\nmax\\nα,β : αi≥0 θD(α, β) =\\nmax\\nα,β : αi≥0 min\\nw L(w, α, β).\\nThis is exactly the same as our primal problem shown above, except that the\\norder of the “max” and the “min” are now exchanged. We also deﬁne the\\noptimal value of the dual problem’s objective to be d∗= maxα,β : αi≥0 θD(w).\\nHow are the primal and the dual problems related? It can easily be shown\\nthat\\nd∗=\\nmax\\nα,β : αi≥0 min\\nw L(w, α, β) ≤min\\nw\\nmax\\nα,β : αi≥0 L(w, α, β) = p∗.\\n(You should convince yourself of this; this follows from the “max min” of a\\nfunction always being less than or equal to the “min max.”) However, under\\ncertain conditions, we will have\\nd∗= p∗,\\nso that we can solve the dual problem in lieu of the primal problem. Let’s\\nsee what these conditions are.\\nSuppose f and the gi’s are convex,3 and the hi’s are aﬃne.4 Suppose\\nfurther that the constraints gi are (strictly) feasible; this means that there\\nexists some w so that gi(w) < 0 for all i.\\n3When f has a Hessian, then it is convex if and only if the Hessian is positive semi-\\ndeﬁnite. For instance, f(w) = wT w is convex; similarly, all linear (and aﬃne) functions\\nare also convex. (A function f can also be convex without being diﬀerentiable, but we\\nwon’t need those more general deﬁnitions of convexity here.)\\n4I.e., there exists ai, bi, so that hi(w) = aT\\ni w + bi. “Aﬃne” means the same thing as\\nlinear, except that we also allow the extra intercept term bi.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 68}, page_content='68\\nUnder our above assumptions, there must exist w∗, α∗, β∗so that w∗is the\\nsolution to the primal problem, α∗, β∗are the solution to the dual problem,\\nand moreover p∗= d∗= L(w∗, α∗, β∗). Moreover, w∗, α∗and β∗satisfy the\\nKarush-Kuhn-Tucker (KKT) conditions, which are as follows:\\n∂\\n∂wi\\nL(w∗, α∗, β∗)\\n=\\n0, i = 1, . . . , d\\n(6.3)\\n∂\\n∂βi\\nL(w∗, α∗, β∗)\\n=\\n0, i = 1, . . . , l\\n(6.4)\\nα∗\\ni gi(w∗)\\n=\\n0, i = 1, . . . , k\\n(6.5)\\ngi(w∗)\\n≤\\n0, i = 1, . . . , k\\n(6.6)\\nα∗\\n≥\\n0, i = 1, . . . , k\\n(6.7)\\nMoreover, if some w∗, α∗, β∗satisfy the KKT conditions, then it is also a solution to the primal and dual\\nproblems.\\nWe draw attention to Equation (6.5), which is called the KKT dual\\ncomplementarity condition. Speciﬁcally, it implies that if α∗\\ni > 0, then\\ngi(w∗) = 0. (I.e., the “gi(w) ≤0” constraint is active, meaning it holds with\\nequality rather than with inequality.) Later on, this will be key for showing\\nthat the SVM has only a small number of “support vectors”; the KKT dual\\ncomplementarity condition will also give us our convergence test when we\\ntalk about the SMO algorithm.\\n6.6\\nOptimal margin classiﬁers: the dual form\\n(option reading)\\nNote: The equivalence of optimization problem (6.8) and the optimization\\nproblem (6.12), and the relationship between the primary and dual variables\\nin equation (6.10) are the most important take home messages of this section.\\nPreviously, we posed the following (primal) optimization problem for ﬁnd-\\ning the optimal margin classiﬁer:\\nminw,b\\n1\\n2||w||2\\n(6.8)\\ns.t.\\ny(i)(wTx(i) + b) ≥1, i = 1, . . . , n\\nWe can write the constraints as\\ngi(w) = −y(i)(wTx(i) + b) + 1 ≤0.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 69}, page_content='69\\nWe have one such constraint for each training example. Note that from the\\nKKT dual complementarity condition, we will have αi > 0 only for the train-\\ning examples that have functional margin exactly equal to one (i.e., the ones\\ncorresponding to constraints that hold with equality, gi(w) = 0). Consider\\nthe ﬁgure below, in which a maximum margin separating hyperplane is shown\\nby the solid line.\\nThe points with the smallest margins are exactly the ones closest to the\\ndecision boundary; here, these are the three points (one negative and two pos-\\nitive examples) that lie on the dashed lines parallel to the decision boundary.\\nThus, only three of the αi’s—namely, the ones corresponding to these three\\ntraining examples—will be non-zero at the optimal solution to our optimiza-\\ntion problem. These three points are called the support vectors in this\\nproblem. The fact that the number of support vectors can be much smaller\\nthan the size the training set will be useful later.\\nLet’s move on. Looking ahead, as we develop the dual form of the prob-\\nlem, one key idea to watch out for is that we’ll try to write our algorithm\\nin terms of only the inner product ⟨x(i), x(j)⟩(think of this as (x(i))Tx(j))\\nbetween points in the input feature space. The fact that we can express our\\nalgorithm in terms of these inner products will be key when we apply the\\nkernel trick.\\nWhen we construct the Lagrangian for our optimization problem we have:\\nL(w, b, α) = 1\\n2||w||2 −\\nn\\nX\\ni=1\\nαi\\n\\x02\\ny(i)(wTx(i) + b) −1\\n\\x03\\n.\\n(6.9)\\nNote that there’re only “αi” but no “βi” Lagrange multipliers, since the\\nproblem has only inequality constraints.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 70}, page_content='70\\nLet’s ﬁnd the dual form of the problem.\\nTo do so, we need to ﬁrst\\nminimize L(w, b, α) with respect to w and b (for ﬁxed α), to get θD, which\\nwe’ll do by setting the derivatives of L with respect to w and b to zero. We\\nhave:\\n∇wL(w, b, α) = w −\\nn\\nX\\ni=1\\nαiy(i)x(i) = 0\\nThis implies that\\nw =\\nn\\nX\\ni=1\\nαiy(i)x(i).\\n(6.10)\\nAs for the derivative with respect to b, we obtain\\n∂\\n∂bL(w, b, α) =\\nn\\nX\\ni=1\\nαiy(i) = 0.\\n(6.11)\\nIf we take the deﬁnition of w in Equation (6.10) and plug that back into\\nthe Lagrangian (Equation 6.9), and simplify, we get\\nL(w, b, α) =\\nn\\nX\\ni=1\\nαi −1\\n2\\nn\\nX\\ni,j=1\\ny(i)y(j)αiαj(x(i))Tx(j) −b\\nn\\nX\\ni=1\\nαiy(i).\\nBut from Equation (6.11), the last term must be zero, so we obtain\\nL(w, b, α) =\\nn\\nX\\ni=1\\nαi −1\\n2\\nn\\nX\\ni,j=1\\ny(i)y(j)αiαj(x(i))Tx(j).\\nRecall that we got to the equation above by minimizing L with respect to\\nw and b. Putting this together with the constraints αi ≥0 (that we always\\nhad) and the constraint (6.11), we obtain the following dual optimization\\nproblem:\\nmaxα\\nW(α) =\\nn\\nX\\ni=1\\nαi −1\\n2\\nn\\nX\\ni,j=1\\ny(i)y(j)αiαj⟨x(i), x(j)⟩.\\n(6.12)\\ns.t.\\nαi ≥0, i = 1, . . . , n\\nn\\nX\\ni=1\\nαiy(i) = 0,\\nYou should also be able to verify that the conditions required for p∗= d∗\\nand the KKT conditions (Equations 6.3–6.7) to hold are indeed satisﬁed in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 71}, page_content='71\\nour optimization problem. Hence, we can solve the dual in lieu of solving\\nthe primal problem.\\nSpeciﬁcally, in the dual problem above, we have a\\nmaximization problem in which the parameters are the αi’s. We’ll talk later\\nabout the speciﬁc algorithm that we’re going to use to solve the dual problem,\\nbut if we are indeed able to solve it (i.e., ﬁnd the α’s that maximize W(α)\\nsubject to the constraints), then we can use Equation (6.10) to go back and\\nﬁnd the optimal w’s as a function of the α’s. Having found w∗, by considering\\nthe primal problem, it is also straightforward to ﬁnd the optimal value for\\nthe intercept term b as\\nb∗= −maxi:y(i)=−1 w∗Tx(i) + mini:y(i)=1 w∗Tx(i)\\n2\\n.\\n(6.13)\\n(Check for yourself that this is correct.)\\nBefore moving on, let’s also take a more careful look at Equation (6.10),\\nwhich gives the optimal value of w in terms of (the optimal value of) α.\\nSuppose we’ve ﬁt our model’s parameters to a training set, and now wish to\\nmake a prediction at a new point input x. We would then calculate wTx + b,\\nand predict y = 1 if and only if this quantity is bigger than zero.\\nBut\\nusing (6.10), this quantity can also be written:\\nwTx + b\\n=\\n n\\nX\\ni=1\\nαiy(i)x(i)\\n!T\\nx + b\\n(6.14)\\n=\\nn\\nX\\ni=1\\nαiy(i)⟨x(i), x⟩+ b.\\n(6.15)\\nHence, if we’ve found the αi’s, in order to make a prediction, we have to\\ncalculate a quantity that depends only on the inner product between x and\\nthe points in the training set. Moreover, we saw earlier that the αi’s will all\\nbe zero except for the support vectors. Thus, many of the terms in the sum\\nabove will be zero, and we really need to ﬁnd only the inner products between\\nx and the support vectors (of which there is often only a small number) in\\norder calculate (6.15) and make our prediction.\\nBy examining the dual form of the optimization problem, we gained sig-\\nniﬁcant insight into the structure of the problem, and were also able to write\\nthe entire algorithm in terms of only inner products between input feature\\nvectors. In the next section, we will exploit this property to apply the ker-\\nnels to our classiﬁcation problem. The resulting algorithm, support vector\\nmachines, will be able to eﬃciently learn in very high dimensional spaces.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 72}, page_content='72\\n6.7\\nRegularization and the non-separable case\\n(optional reading)\\nThe derivation of the SVM as presented so far assumed that the data is\\nlinearly separable. While mapping data to a high dimensional feature space\\nvia φ does generally increase the likelihood that the data is separable, we\\ncan’t guarantee that it always will be so. Also, in some cases it is not clear\\nthat ﬁnding a separating hyperplane is exactly what we’d want to do, since\\nthat might be susceptible to outliers.\\nFor instance, the left ﬁgure below\\nshows an optimal margin classiﬁer, and when a single outlier is added in the\\nupper-left region (right ﬁgure), it causes the decision boundary to make a\\ndramatic swing, and the resulting classiﬁer has a much smaller margin.\\nTo make the algorithm work for non-linearly separable datasets as well\\nas be less sensitive to outliers, we reformulate our optimization (using ℓ1\\nregularization) as follows:\\nminγ,w,b\\n1\\n2||w||2 + C\\nn\\nX\\ni=1\\nξi\\ns.t.\\ny(i)(wTx(i) + b) ≥1 −ξi, i = 1, . . . , n\\nξi ≥0, i = 1, . . . , n.\\nThus, examples are now permitted to have (functional) margin less than 1,\\nand if an example has functional margin 1 −ξi (with ξ > 0), we would pay\\na cost of the objective function being increased by Cξi. The parameter C\\ncontrols the relative weighting between the twin goals of making the ||w||2\\nsmall (which we saw earlier makes the margin large) and of ensuring that\\nmost examples have functional margin at least 1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 73}, page_content='73\\nAs before, we can form the Lagrangian:\\nL(w, b, ξ, α, r) = 1\\n2wTw + C\\nn\\nX\\ni=1\\nξi −\\nn\\nX\\ni=1\\nαi\\n\\x02\\ny(i)(xTw + b) −1 + ξi\\n\\x03\\n−\\nn\\nX\\ni=1\\nriξi.\\nHere, the αi’s and ri’s are our Lagrange multipliers (constrained to be ≥0).\\nWe won’t go through the derivation of the dual again in detail, but after\\nsetting the derivatives with respect to w and b to zero as before, substituting\\nthem back in, and simplifying, we obtain the following dual form of the\\nproblem:\\nmaxα\\nW(α) =\\nn\\nX\\ni=1\\nαi −1\\n2\\nn\\nX\\ni,j=1\\ny(i)y(j)αiαj⟨x(i), x(j)⟩\\ns.t.\\n0 ≤αi ≤C, i = 1, . . . , n\\nn\\nX\\ni=1\\nαiy(i) = 0,\\nAs before, we also have that w can be expressed in terms of the αi’s as\\ngiven in Equation (6.10), so that after solving the dual problem, we can con-\\ntinue to use Equation (6.15) to make our predictions. Note that, somewhat\\nsurprisingly, in adding ℓ1 regularization, the only change to the dual prob-\\nlem is that what was originally a constraint that 0 ≤αi has now become\\n0 ≤αi ≤C. The calculation for b∗also has to be modiﬁed (Equation 6.13 is\\nno longer valid); see the comments in the next section/Platt’s paper.\\nAlso, the KKT dual-complementarity conditions (which in the next sec-\\ntion will be useful for testing for the convergence of the SMO algorithm)\\nare:\\nαi = 0\\n⇒\\ny(i)(wTx(i) + b) ≥1\\n(6.16)\\nαi = C\\n⇒\\ny(i)(wTx(i) + b) ≤1\\n(6.17)\\n0 < αi < C\\n⇒\\ny(i)(wTx(i) + b) = 1.\\n(6.18)\\nNow, all that remains is to give an algorithm for actually solving the dual\\nproblem, which we will do in the next section.\\n6.8\\nThe SMO algorithm (optional reading)\\nThe SMO (sequential minimal optimization) algorithm, due to John Platt,\\ngives an eﬃcient way of solving the dual problem arising from the derivation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 74}, page_content='74\\nof the SVM. Partly to motivate the SMO algorithm, and partly because it’s\\ninteresting in its own right, let’s ﬁrst take another digression to talk about\\nthe coordinate ascent algorithm.\\n6.8.1\\nCoordinate ascent\\nConsider trying to solve the unconstrained optimization problem\\nmax\\nα\\nW(α1, α2, . . . , αn).\\nHere, we think of W as just some function of the parameters αi’s, and for now\\nignore any relationship between this problem and SVMs. We’ve already seen\\ntwo optimization algorithms, gradient ascent and Newton’s method. The\\nnew algorithm we’re going to consider here is called coordinate ascent:\\nLoop until convergence: {\\nFor i = 1, . . . , n, {\\nαi := arg maxˆαi W(α1, . . . , αi−1, ˆαi, αi+1, . . . , αn).\\n}\\n}\\nThus, in the innermost loop of this algorithm, we will hold all the variables\\nexcept for some αi ﬁxed, and reoptimize W with respect to just the parameter\\nαi. In the version of this method presented here, the inner-loop reoptimizes\\nthe variables in order α1, α2, . . . , αn, α1, α2, . . .. (A more sophisticated version\\nmight choose other orderings; for instance, we may choose the next variable\\nto update according to which one we expect to allow us to make the largest\\nincrease in W(α).)\\nWhen the function W happens to be of such a form that the “arg max”\\nin the inner loop can be performed eﬃciently, then coordinate ascent can be\\na fairly eﬃcient algorithm. Here’s a picture of coordinate ascent in action:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 75}, page_content='75\\n−2\\n−1.5\\n−1\\n−0.5\\n0\\n0.5\\n1\\n1.5\\n2\\n2.5\\n−2\\n−1.5\\n−1\\n−0.5\\n0\\n0.5\\n1\\n1.5\\n2\\n2.5\\nThe ellipses in the ﬁgure are the contours of a quadratic function that\\nwe want to optimize. Coordinate ascent was initialized at (2, −2), and also\\nplotted in the ﬁgure is the path that it took on its way to the global maximum.\\nNotice that on each step, coordinate ascent takes a step that’s parallel to one\\nof the axes, since only one variable is being optimized at a time.\\n6.8.2\\nSMO\\nWe close oﬀthe discussion of SVMs by sketching the derivation of the SMO\\nalgorithm.\\nHere’s the (dual) optimization problem that we want to solve:\\nmaxα\\nW(α) =\\nn\\nX\\ni=1\\nαi −1\\n2\\nn\\nX\\ni,j=1\\ny(i)y(j)αiαj⟨x(i), x(j)⟩.\\n(6.19)\\ns.t.\\n0 ≤αi ≤C, i = 1, . . . , n\\n(6.20)\\nn\\nX\\ni=1\\nαiy(i) = 0.\\n(6.21)\\nLet’s say we have set of αi’s that satisfy the constraints (6.20-6.21). Now,\\nsuppose we want to hold α2, . . . , αn ﬁxed, and take a coordinate ascent step\\nand reoptimize the objective with respect to α1. Can we make any progress?\\nThe answer is no, because the constraint (6.21) ensures that\\nα1y(1) = −\\nn\\nX\\ni=2\\nαiy(i).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 76}, page_content='76\\nOr, by multiplying both sides by y(1), we equivalently have\\nα1 = −y(1)\\nn\\nX\\ni=2\\nαiy(i).\\n(This step used the fact that y(1) ∈{−1, 1}, and hence (y(1))2 = 1.) Hence,\\nα1 is exactly determined by the other αi’s, and if we were to hold α2, . . . , αn\\nﬁxed, then we can’t make any change to α1 without violating the con-\\nstraint (6.21) in the optimization problem.\\nThus, if we want to update some subject of the αi’s, we must update at\\nleast two of them simultaneously in order to keep satisfying the constraints.\\nThis motivates the SMO algorithm, which simply does the following:\\nRepeat till convergence {\\n1. Select some pair αi and αj to update next (using a heuristic that\\ntries to pick the two that will allow us to make the biggest progress\\ntowards the global maximum).\\n2. Reoptimize W(α) with respect to αi and αj, while holding all the\\nother αk’s (k ̸= i, j) ﬁxed.\\n}\\nTo test for convergence of this algorithm, we can check whether the KKT\\nconditions (Equations 6.16-6.18) are satisﬁed to within some tol. Here, tol is\\nthe convergence tolerance parameter, and is typically set to around 0.01 to\\n0.001. (See the paper and pseudocode for details.)\\nThe key reason that SMO is an eﬃcient algorithm is that the update to\\nαi, αj can be computed very eﬃciently. Let’s now brieﬂy sketch the main\\nideas for deriving the eﬃcient update.\\nLet’s say we currently have some setting of the αi’s that satisfy the con-\\nstraints (6.20-6.21), and suppose we’ve decided to hold α3, . . . , αn ﬁxed, and\\nwant to reoptimize W(α1, α2, . . . , αn) with respect to α1 and α2 (subject to\\nthe constraints). From (6.21), we require that\\nα1y(1) + α2y(2) = −\\nn\\nX\\ni=3\\nαiy(i).\\nSince the right hand side is ﬁxed (as we’ve ﬁxed α3, . . . αn), we can just let\\nit be denoted by some constant ζ:\\nα1y(1) + α2y(2) = ζ.\\n(6.22)\\nWe can thus picture the constraints on α1 and α2 as follows:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 77}, page_content='77\\nα2\\nα1\\nα1\\nα2\\nC\\nC\\n(1)+\\n(2)\\ny\\ny =ζ\\nH\\nL\\nFrom the constraints (6.20), we know that α1 and α2 must lie within the box\\n[0, C]×[0, C] shown. Also plotted is the line α1y(1) +α2y(2) = ζ, on which we\\nknow α1 and α2 must lie. Note also that, from these constraints, we know\\nL ≤α2 ≤H; otherwise, (α1, α2) can’t simultaneously satisfy both the box\\nand the straight line constraint. In this example, L = 0. But depending on\\nwhat the line α1y(1) + α2y(2) = ζ looks like, this won’t always necessarily be\\nthe case; but more generally, there will be some lower-bound L and some\\nupper-bound H on the permissible values for α2 that will ensure that α1, α2\\nlie within the box [0, C] × [0, C].\\nUsing Equation (6.22), we can also write α1 as a function of α2:\\nα1 = (ζ −α2y(2))y(1).\\n(Check this derivation yourself; we again used the fact that y(1) ∈{−1, 1} so\\nthat (y(1))2 = 1.) Hence, the objective W(α) can be written\\nW(α1, α2, . . . , αn) = W((ζ −α2y(2))y(1), α2, . . . , αn).\\nTreating α3, . . . , αn as constants, you should be able to verify that this is\\njust some quadratic function in α2. I.e., this can also be expressed in the\\nform aα2\\n2 + bα2 + c for some appropriate a, b, and c. If we ignore the “box”\\nconstraints (6.20) (or, equivalently, that L ≤α2 ≤H), then we can easily\\nmaximize this quadratic function by setting its derivative to zero and solving.\\nWe’ll let αnew,unclipped\\n2\\ndenote the resulting value of α2. You should also be\\nable to convince yourself that if we had instead wanted to maximize W with\\nrespect to α2 but subject to the box constraint, then we can ﬁnd the resulting\\nvalue optimal simply by taking αnew,unclipped\\n2\\nand “clipping” it to lie in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 78}, page_content='78\\n[L, H] interval, to get\\nαnew\\n2\\n=\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nH\\nif αnew,unclipped\\n2\\n> H\\nαnew,unclipped\\n2\\nif L ≤αnew,unclipped\\n2\\n≤H\\nL\\nif αnew,unclipped\\n2\\n< L\\nFinally, having found the αnew\\n2\\n, we can use Equation (6.22) to go back and\\nﬁnd the optimal value of αnew\\n1\\n.\\nThere’re a couple more details that are quite easy but that we’ll leave you\\nto read about yourself in Platt’s paper: One is the choice of the heuristics\\nused to select the next αi, αj to update; the other is how to update b as the\\nSMO algorithm is run.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 79}, page_content='Part II\\nDeep learning\\n79'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 80}, page_content='Chapter 7\\nDeep learning\\nWe now begin our study of deep learning. In this set of notes, we give an\\noverview of neural networks, discuss vectorization and discuss training neural\\nnetworks with backpropagation.\\n7.1\\nSupervised learning with non-linear mod-\\nels\\nIn the supervised learning setting (predicting y from the input x), suppose\\nour model/hypothesis is hθ(x). In the past lectures, we have considered the\\ncases when hθ(x) = θ⊤x (in linear regression) or hθ(x) = θ⊤φ(x) (where φ(x)\\nis the feature map). A commonality of these two models is that they are\\nlinear in the parameters θ. Next we will consider learning general family of\\nmodels that are non-linear in both the parameters θ and the inputs x. The\\nmost common non-linear models are neural networks, which we will deﬁne\\nstaring from the next section. For this section, it suﬃces to think hθ(x) as\\nan abstract non-linear model.1\\nSuppose {(x(i), y(i))}n\\ni=1 are the training examples.\\nWe will deﬁne the\\nnonlinear model and the loss/cost function for learning it.\\nRegression problems.\\nFor simplicity, we start with the case where the\\noutput is a real number, that is, y(i) ∈R, and thus the model hθ also outputs\\na real number hθ(x) ∈R. We deﬁne the least square cost function for the\\n1If a concrete example is helpful, perhaps think about the model hθ(x) = θ2\\n1x2\\n1 +θ2\\n2x2\\n2 +\\n· · · + θ2\\ndx2\\nd in this subsection, even though it’s not a neural network.\\n80'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 81}, page_content='81\\ni-th example (x(i), y(i)) as\\nJ(i)(θ) = 1\\n2(hθ(x(i)) −y(i))2 ,\\n(7.1)\\nand deﬁne the mean-square cost function for the dataset as\\nJ(θ) = 1\\nn\\nn\\nX\\ni=1\\nJ(i)(θ) ,\\n(7.2)\\nwhich is same as in linear regression except that we introduce a constant\\n1/n in front of the cost function to be consistent with the convention. Note\\nthat multiplying the cost function with a scalar will not change the local\\nminima or global minima of the cost function. Also note that the underlying\\nparameterization for hθ(x) is diﬀerent from the case of linear regression,\\neven though the form of the cost function is the same mean-squared loss.\\nThroughout the notes, we use the words “loss” and “cost” interchangeably.\\nBinary classiﬁcation.\\nNext we deﬁne the model and loss function for\\nbinary classiﬁcation. Suppose the inputs x ∈Rd. Let ¯hθ : Rd →R be a\\nparameterized model (the analog of θ⊤x in logistic linear regression). We\\ncall the output ¯hθ(x) ∈R the logit. Analogous to Section 2.1, we use the\\nlogistic function g(·) to turn the logit ¯hθ(x) to a probability hθ(x) ∈[0, 1]:\\nhθ(x) = g(¯hθ(x)) = 1/(1 + exp(−¯hθ(x)) .\\n(7.3)\\nWe model the conditional distribution of y given x and θ by\\nP(y = 1 | x; θ)\\n=\\nhθ(x)\\nP(y = 0 | x; θ)\\n=\\n1 −hθ(x)\\nFollowing the same derivation in Section 2.1 and using the derivation in\\nRemark 2.1.1, the negative likelihood loss function is equal to:\\nJ(i)(θ) = −log p(y(i) | x(i); θ) = ℓlogistic(¯hθ(x(i)), y(i))\\n(7.4)\\nAs done in equation (7.2), the total loss function is also deﬁned as the average\\nof the loss function over individual training examples, J(θ) = 1\\nn\\nPn\\ni=1 J(i)(θ).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 82}, page_content='82\\nMulti-class classiﬁcation.\\nFollowing Section 2.3, we consider a classiﬁca-\\ntion problem where the response variable y can take on any one of k values,\\ni.e. y ∈{1, 2, . . . , k}. Let ¯hθ : Rd →Rk be a parameterized model. We\\ncall the outputs ¯hθ(x) ∈Rk the logits. Each logit corresponds to the predic-\\ntion for one of the k classes. Analogous to Section 2.3, we use the softmax\\nfunction to turn the logits ¯hθ(x) into a probability vector with non-negative\\nentries that sum up to 1:\\nP(y = j | x; θ) =\\nexp(¯hθ(x)j)\\nPk\\ns=1 exp(¯hθ(x)s)\\n,\\n(7.5)\\nwhere ¯hθ(x)s denotes the s-th coordinate of ¯hθ(x).\\nSimilarly to Section 2.3, the loss function for a single training example\\n(x(i), y(i)) is its negative log-likelihood:\\nJ(i)(θ) = −log p(y(i) | x(i); θ) = −log\\n \\nexp(¯hθ(x(i))y(i))\\nPk\\ns=1 exp(¯hθ(x(i))s)\\n!\\n.\\n(7.6)\\nUsing the notations of Section 2.3, we can simply write in an abstract way:\\nJ(i)(θ) = ℓce(¯hθ(x(i)), y(i)).\\n(7.7)\\nThe loss function is also deﬁned as the average of the loss function of indi-\\nvidual training examples, J(θ) = 1\\nn\\nPn\\ni=1 J(i)(θ).\\nWe also note that the approach above can also be generated to any con-\\nditional probabilistic model where we have an exponential distribution for\\ny, Exponential-family(y; η), where η = ¯hθ(x) is a parameterized nonlinear\\nfunction of x. However, the most widely used situations are the three cases\\ndiscussed above.\\nOptimizers (SGD).\\nCommonly, people use gradient descent (GD), stochas-\\ntic gradient (SGD), or their variants to optimize the loss function J(θ). GD’s\\nupdate rule can be written as2\\nθ := θ −α∇θJ(θ)\\n(7.8)\\nwhere α > 0 is often referred to as the learning rate or step size. Next, we\\nintroduce a version of the SGD (Algorithm 1), which is lightly diﬀerent from\\nthat in the ﬁrst lecture notes.\\n2Recall that, as deﬁned in the previous lecture notes, we use the notation “a := b” to\\ndenote an operation (in a computer program) in which we set the value of a variable a to\\nbe equal to the value of b. In other words, this operation overwrites a with the value of\\nb. In contrast, we will write “a = b” when we are asserting a statement of fact, that the\\nvalue of a is equal to the value of b.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 83}, page_content='83\\nAlgorithm 1 Stochastic Gradient Descent\\n1: Hyperparameter: learning rate α, number of total iteration niter.\\n2: Initialize θ randomly.\\n3: for i = 1 to niter do\\n4:\\nSample j uniformly from {1, . . . , n}, and update θ by\\nθ := θ −α∇θJ(j)(θ)\\n(7.9)\\nOftentimes computing the gradient of B examples simultaneously for the\\nparameter θ can be faster than computing B gradients separately due to\\nhardware parallelization. Therefore, a mini-batch version of SGD is most\\ncommonly used in deep learning, as shown in Algorithm 2. There are also\\nother variants of the SGD or mini-batch SGD with slightly diﬀerent sampling\\nschemes.\\nAlgorithm 2 Mini-batch Stochastic Gradient Descent\\n1: Hyperparameters: learning rate α, batch size B, # iterations niter.\\n2: Initialize θ randomly\\n3: for i = 1 to niter do\\n4:\\nSample B examples j1, . . . , jB (without replacement) uniformly from\\n{1, . . . , n}, and update θ by\\nθ := θ −α\\nB\\nB\\nX\\nk=1\\n∇θJ(jk)(θ)\\n(7.10)\\nWith these generic algorithms, a typical deep learning model is learned\\nwith the following steps. 1. Deﬁne a neural network parametrization hθ(x),\\nwhich we will introduce in Section 7.2, and 2. write the backpropagation\\nalgorithm to compute the gradient of the loss function J(j)(θ) eﬃciently,\\nwhich will be covered in Section 7.4, and 3. run SGD or mini-batch SGD (or\\nother gradient-based optimizers) with the loss function J(θ).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 84}, page_content='84\\n7.2\\nNeural networks\\nNeural networks refer to a broad type of non-linear models/parametrizations\\n¯hθ(x) that involve combinations of matrix multiplications and other entry-\\nwise non-linear operations. To have a uniﬁed treatment for regression prob-\\nlem and classiﬁcation problem, here we consider ¯hθ(x) as the output of the\\nneural network. For regression problem, the ﬁnal prediction hθ(x) = ¯hθ(x),\\nand for classiﬁcation problem, ¯hθ(x) is the logits and the predicted probability\\nwill be hθ(x) = 1/(1+exp(−¯hθ(x)) (see equation 7.3) for binary classiﬁcation\\nor hθ(x) = softmax(¯hθ(x)) for multi-class classiﬁcation (see equation 7.5).\\nWe will start small and slowly build up a neural network, step by step.\\nA Neural Network with a Single Neuron.\\nRecall the housing price\\nprediction problem from before: given the size of the house, we want to\\npredict the price. We will use it as a running example in this subsection.\\nPreviously, we ﬁt a straight line to the graph of size vs. housing price.\\nNow, instead of ﬁtting a straight line, we wish to prevent negative housing\\nprices by setting the absolute minimum price as zero. This produces a “kink”\\nin the graph as shown in Figure 7.1. How do we represent such a function\\nwith a single kink as ¯hθ(x) with unknown parameter? (After doing so, we\\ncan invoke the machinery in Section 7.1.)\\nWe deﬁne a parameterized function ¯hθ(x) with input x, parameterized by\\nθ, which outputs the price of the house y. Formally, ¯hθ : x →y. Perhaps\\none of the simplest parametrization would be\\n¯hθ(x) = max(wx + b, 0), where θ = (w, b) ∈R2\\n(7.11)\\nHere ¯hθ(x) returns a single value: (wx+b) or zero, whichever is greater. In\\nthe context of neural networks, the function max{t, 0} is called a ReLU (pro-\\nnounced “ray-lu”), or rectiﬁed linear unit, and often denoted by ReLU(t) ≜\\nmax{t, 0}.\\nGenerally, a one-dimensional non-linear function that maps R to R such as\\nReLU is often referred to as an activation function. The model ¯hθ(x) is said\\nto have a single neuron partly because it has a single non-linear activation\\nfunction. (We will discuss more about why a non-linear activation is called\\nneuron.)\\nWhen the input x ∈Rd has multiple dimensions, a neural network with\\na single neuron can be written as\\n¯hθ(x) = ReLU(w⊤x + b), where w ∈Rd, b ∈R, and θ = (w, b)\\n(7.12)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 85}, page_content='85\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\n3500\\n4000\\n4500\\n5000\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n800\\n900\\n1000\\nhousing prices\\nsquare feet\\nprice (in $1000)\\nFigure 7.1: Housing prices with a “kink” in the graph.\\nThe term b is often referred to as the “bias”, and the vector w is referred\\nto as the weight vector. Such a neural network has 1 layer. (We will deﬁne\\nwhat multiple layers mean in the sequel.)\\nStacking Neurons.\\nA more complex neural network may take the single\\nneuron described above and “stack” them together such that one neuron\\npasses its output as input into the next neuron, resulting in a more complex\\nfunction.\\nLet us now deepen the housing prediction example. In addition to the size\\nof the house, suppose that you know the number of bedrooms, the zip code\\nand the wealth of the neighborhood. Building neural networks is analogous\\nto Lego bricks: you take individual bricks and stack them together to build\\ncomplex structures. The same applies to neural networks: we take individual\\nneurons and stack them together to create complex neural networks.\\nGiven these features (size, number of bedrooms, zip code, and wealth),\\nwe might then decide that the price of the house depends on the maximum\\nfamily size it can accommodate. Suppose the family size is a function of the\\nsize of the house and number of bedrooms (see Figure 7.2). The zip code\\nmay provide additional information such as how walkable the neighborhood\\nis (i.e., can you walk to the grocery store or do you need to drive everywhere).\\nCombining the zip code with the wealth of the neighborhood may predict\\nthe quality of the local elementary school. Given these three derived features\\n(family size, walkable, school quality), we may conclude that the price of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 86}, page_content='86\\nhome ultimately depends on these three features.\\nFamily Size\\nSchool Quality\\nWalkable\\nSize\\n# Bedrooms\\nZip Code\\nWealth\\nPrice\\ny\\nFigure 7.2: Diagram of a small neural network for predicting housing prices.\\nFormally, the input to a neural network is a set of input features\\nx1, x2, x3, x4. We denote the intermediate variables for “family size”, “walk-\\nable”, and “school quality” by a1, a2, a3 (these ai’s are often referred to as\\n“hidden units” or “hidden neurons”). We represent each of the ai’s as a neu-\\nral network with a single neuron with a subset of x1, . . . , x4 as inputs. Then\\nas in Figure 7.1, we will have the parameterization:\\na1 = ReLU(θ1x1 + θ2x2 + θ3)\\na2 = ReLU(θ4x3 + θ5)\\na3 = ReLU(θ6x3 + θ7x4 + θ8)\\nwhere (θ1, · · · , θ8) are parameters. Now we represent the ﬁnal output ¯hθ(x)\\nas another linear function with a1, a2, a3 as inputs, and we get3\\n¯hθ(x) = θ9a1 + θ10a2 + θ11a3 + θ12\\n(7.13)\\nwhere θ contains all the parameters (θ1, · · · , θ12).\\nNow we represent the output as a quite complex function of x with pa-\\nrameters θ. Then you can use this parametrization ¯hθ with the machinery of\\nSection 7.1 to learn the parameters θ.\\nInspiration from Biological Neural Networks.\\nAs the name suggests,\\nartiﬁcial neural networks were inspired by biological neural networks. The\\nhidden units a1, . . . , am correspond to the neurons in a biological neural net-\\nwork, and the parameters θi’s correspond to the synapses.\\nHowever, it’s\\nunclear how similar the modern deep artiﬁcial neural networks are to the bi-\\nological ones. For example, perhaps not many neuroscientists think biological\\n3Typically, for multi-layer neural network, at the end, near the output, we don’t apply\\nReLU, especially when the output is not necessarily a positive number.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 87}, page_content='87\\nneural networks could have 1000 layers, while some modern artiﬁcial neural\\nnetworks do (we will elaborate more on the notion of layers.) Moreover, it’s\\nan open question whether human brains update their neural networks in a\\nway similar to the way that computer scientists learn artiﬁcial neural net-\\nworks (using backpropagation, which we will introduce in the next section.).\\nTwo-layer Fully-Connected Neural Networks.\\nWe constructed the\\nneural network in equation (7.13) using a signiﬁcant amount of prior knowl-\\nedge/belief about how the “family size”, “walkable”, and “school quality” are\\ndetermined by the inputs. We implicitly assumed that we know the family\\nsize is an important quantity to look at and that it can be determined by\\nonly the “size” and “# bedrooms”. Such a prior knowledge might not be\\navailable for other applications. It would be more ﬂexible and general to have\\na generic parameterization. A simple way would be to write the intermediate\\nvariable a1 as a function of all x1, . . . , x4:\\na1 = ReLU(w⊤\\n1 x + b1), where w1 ∈R4 and b1 ∈R\\n(7.14)\\na2 = ReLU(w⊤\\n2 x + b2), where w2 ∈R4 and b2 ∈R\\na3 = ReLU(w⊤\\n3 x + b3), where w3 ∈R4 and b3 ∈R\\nWe still deﬁne ¯hθ(x) using equation (7.13) with a1, a2, a3 being deﬁned as\\nabove. Thus we have a so-called fully-connected neural network because\\nall the intermediate variables ai’s depend on all the inputs xi’s.\\nFor full generality, a two-layer fully-connected neural network with m\\nhidden units and d dimensional input x ∈Rd is deﬁned as\\n∀j ∈[1, ..., m],\\nzj = w[1]\\nj\\n⊤x + b[1]\\nj\\nwhere w[1]\\nj\\n∈Rd, b[1]\\nj ∈R\\n(7.15)\\naj = ReLU(zj),\\na = [a1, . . . , am]⊤∈Rm\\n¯hθ(x) = w[2]⊤a + b[2] where w[2] ∈Rm, b[2] ∈R,\\n(7.16)\\nNote that by default the vectors in Rd are viewed as column vectors, and\\nin particular a is a column vector with components a1, a2, ..., am. The indices\\n[1] and [2] are used to distinguish two sets of parameters: the w[1]\\nj ’s (each of\\nwhich is a vector in Rd) and w[2] (which is a vector in Rm). We will have\\nmore of these later.\\nVectorization.\\nBefore we introduce neural networks with more layers and\\nmore complex structures, we will simplify the expressions for neural networks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 88}, page_content='88\\nwith more matrix and vector notations. Another important motivation of\\nvectorization is the speed perspective in the implementation. In order to\\nimplement a neural network eﬃciently, one must be careful when using for\\nloops. The most natural way to implement equation (7.15) in code is perhaps\\nto use a for loop. In practice, the dimensionalities of the inputs and hidden\\nunits are high. As a result, code will run very slowly if you use for loops.\\nLeveraging the parallelism in GPUs is/was crucial for the progress of deep\\nlearning.\\nThis gave rise to vectorization. Instead of using for loops, vectorization\\ntakes advantage of matrix algebra and highly optimized numerical linear\\nalgebra packages (e.g., BLAS) to make neural network computations run\\nquickly. Before the deep learning era, a for loop may have been suﬃcient\\non smaller datasets, but modern deep networks and state-of-the-art datasets\\nwill be infeasible to run with for loops.\\nWe vectorize the two-layer fully-connected neural network as below. We\\ndeﬁne a weight matrix W [1] in Rm×d as the concatenation of all the vectors\\nw[1]\\nj ’s in the following way:\\nW [1] =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n— w[1]\\n1\\n⊤—\\n— w[1]\\n2\\n⊤—\\n...\\n— w[1]\\nm\\n⊤—\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈Rm×d\\n(7.17)\\nNow by the deﬁnition of matrix vector multiplication, we can write z =\\n[z1, . . . , zm]⊤∈Rm as\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nz1\\n...\\n...\\nzm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n| {z }\\nz ∈Rm×1\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n— w[1]\\n1\\n⊤—\\n— w[1]\\n2\\n⊤—\\n...\\n— w[1]\\nm\\n⊤—\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n|\\n{z\\n}\\nW [1] ∈Rm×d\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx1\\nx2\\n...\\nxd\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n| {z }\\nx ∈Rd×1\\n+\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nb[1]\\n1\\nb[1]\\n2...\\nb[1]\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n| {z }\\nb[1] ∈Rm×1\\n(7.18)\\nOr succinctly,\\nz = W [1]x + b[1]\\n(7.19)\\nWe remark again that a vector in Rd in this notes, following the conventions\\npreviously established, is automatically viewed as a column vector, and can'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 89}, page_content='89\\nalso be viewed as a d × 1 dimensional matrix. (Note that this is diﬀerent\\nfrom numpy where a vector is viewed as a row vector in broadcasting.)\\nComputing the activations a ∈Rm from z ∈Rm involves an element-\\nwise non-linear application of the ReLU function, which can be computed in\\nparallel eﬃciently. Overloading ReLU for element-wise application of ReLU\\n(meaning, for a vector t ∈Rd, ReLU(t) is a vector such that ReLU(t)i =\\nReLU(ti)), we have\\na = ReLU(z)\\n(7.20)\\nDeﬁne W [2] = [w[2]⊤] ∈R1×m similarly.\\nThen, the model in equa-\\ntion (7.16) can be summarized as\\na = ReLU(W [1]x + b[1])\\n¯hθ(x) = W [2]a + b[2]\\n(7.21)\\nHere θ consists of W [1], W [2] (often referred to as the weight matrices) and\\nb[1], b[2] (referred to as the biases). The collection of W [1], b[1] is referred to as\\nthe ﬁrst layer, and W [2], b[2] the second layer. The activation a is referred to as\\nthe hidden layer. A two-layer neural network is also called one-hidden-layer\\nneural network.\\nMulti-layer fully-connected neural networks.\\nWith this succinct no-\\ntations, we can stack more layers to get a deeper fully-connected neu-\\nral network.\\nLet r be the number of layers (weight matrices).\\nLet\\nW [1], . . . , W [r], b[1], . . . , b[r] be the weight matrices and biases of all the layers.\\nThen a multi-layer neural network can be written as\\na[1] = ReLU(W [1]x + b[1])\\na[2] = ReLU(W [2]a[1] + b[2])\\n· · ·\\na[r−1] = ReLU(W [r−1]a[r−2] + b[r−1])\\n¯hθ(x) = W [r]a[r−1] + b[r]\\n(7.22)\\nWe note that the weight matrices and biases need to have compatible\\ndimensions for the equations above to make sense. If a[k] has dimension mk,\\nthen the weight matrix W [k] should be of dimension mk ×mk−1, and the bias\\nb[k] ∈Rmk. Moreover, W [1] ∈Rm1×d and W [r] ∈R1×mr−1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 90}, page_content='90\\nThe total number of neurons in the network is m1 + · · · + mr, and the\\ntotal number of parameters in this network is (d+1)m1 +(m1 +1)m2 +· · ·+\\n(mr−1 + 1)mr.\\nSometimes for notational consistency we also write a[0] = x, and a[r] =\\nhθ(x). Then we have simple recursion that\\na[k] = ReLU(W [k]a[k−1] + b[k]), ∀k = 1, . . . , r −1\\n(7.23)\\nNote that this would have be true for k = r if there were an additional\\nReLU in equation (7.22), but often people like to make the last layer linear\\n(aka without a ReLU) so that negative outputs are possible and it’s easier\\nto interpret the last layer as a linear model. (More on the interpretability at\\nthe “connection to kernel method” paragraph of this section.)\\nOther activation functions.\\nThe activation function ReLU can be re-\\nplaced by many other non-linear function σ(·) that maps R to R such as\\nσ(z) =\\n1\\n1 + e−z\\n(sigmoid)\\n(7.24)\\nσ(z) = ez −e−z\\nez + e−z\\n(tanh)\\n(7.25)\\nσ(z) = max{z, γz}, γ ∈(0, 1)\\n(leaky ReLU)\\n(7.26)\\nσ(z) = z\\n2\\n\\x14\\n1 + erf( z\\n√\\n2)\\n\\x15\\n(GELU)\\n(7.27)\\nσ(z) = 1\\nβ log(1 + exp(βz)), β > 0\\n(Softplus)\\n(7.28)\\nThe activation functions are plotted in Figure 7.3. Sigmoid and tanh are\\nless and less used these days partly because their are bounded from both sides\\nand the gradient of them vanishes as z goes to both positive and negative\\ninﬁnity (whereas all the other activation functions still have gradients as the\\ninput goes to positive inﬁnity.)\\nSoftplus is not used very often either in\\npractice and can be viewed as a smoothing of the ReLU so that it has a\\nproper second order derivative. GELU and leaky ReLU are both variants of\\nReLU but they have some non-zero gradient even when the input is negative.\\nGELU (or its slight variant) is used in NLP models such as BERT and GPT\\n(which we will discuss in Chapter 14.)\\nWhy do we not use the identity function for σ(z)?\\nThat is, why\\nnot use σ(z) = z? Assume for sake of argument that b[1] and b[2] are zeros.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 91}, page_content='91\\nFigure 7.3: Activation functions in deep learning.\\nSuppose σ(z) = z, then for two-layer neural network, we have that\\n¯hθ(x) = W [2]a[1]\\n(7.29)\\n= W [2]σ(z[1])\\nby deﬁnition\\n(7.30)\\n= W [2]z[1]\\nsince σ(z) = z\\n(7.31)\\n= W [2]W [1]x\\nfrom Equation (7.18)\\n(7.32)\\n= ˜Wx\\nwhere ˜W = W [2]W [1]\\n(7.33)\\nNotice how W [2]W [1] collapsed into ˜W.\\nThis is because applying a linear function to another linear function will\\nresult in a linear function over the original input (i.e., you can construct a ˜W\\nsuch that ˜Wx = W [2]W [1]x). This loses much of the representational power\\nof the neural network as often times the output we are trying to predict\\nhas a non-linear relationship with the inputs. Without non-linear activation\\nfunctions, the neural network will simply perform linear regression.\\nConnection to the Kernel Method.\\nIn the previous lectures, we covered\\nthe concept of feature maps. Recall that the main motivation for feature\\nmaps is to represent functions that are non-linear in the input x by θ⊤φ(x),\\nwhere θ are the parameters and φ(x), the feature map, is a handcrafted\\nfunction non-linear in the raw input x.\\nThe performance of the learning\\nalgorithms can signiﬁcantly depends on the choice of the feature map φ(x).\\nOftentimes people use domain knowledge to design the feature map φ(x) that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 92}, page_content='92\\nsuits the particular applications. The process of choosing the feature maps\\nis often referred to as feature engineering.\\nWe can view deep learning as a way to automatically learn the right\\nfeature map (sometimes also referred to as “the representation”) as follows.\\nSuppose we denote by β the collection of the parameters in a fully-connected\\nneural networks (equation (7.22)) except those in the last layer. Then we\\ncan abstract right a[r−1] as a function of the input x and the parameters in\\nβ: a[r−1] = φβ(x). Now we can write the model as\\n¯hθ(x) = W [r]φβ(x) + b[r]\\n(7.34)\\nWhen β is ﬁxed, then φβ(·) can viewed as a feature map, and therefore ¯hθ(x)\\nis just a linear model over the features φβ(x). However, we will train the\\nneural networks, both the parameters in β and the parameters W [r], b[r] are\\noptimized, and therefore we are not learning a linear model in the feature\\nspace, but also learning a good feature map φβ(·) itself so that it’s possi-\\nble to predict accurately with a linear model on top of the feature map.\\nTherefore, deep learning tends to depend less on the domain knowledge of\\nthe particular applications and requires often less feature engineering. The\\npenultimate layer a[r] is often (informally) referred to as the learned features\\nor representations in the context of deep learning.\\nIn the example of house price prediction, a fully-connected neural network\\ndoes not need us to specify the intermediate quantity such “family size”, and\\nmay automatically discover some useful features in the last penultimate layer\\n(the activation a[r−1]), and use them to linearly predict the housing price.\\nOften the feature map / representation obtained from one datasets (that is,\\nthe function φβ(·) can be also useful for other datasets, which indicates they\\ncontain essential information about the data. However, oftentimes, the neural\\nnetwork will discover complex features which are very useful for predicting\\nthe output but may be diﬃcult for a human to understand or interpret. This\\nis why some people refer to neural networks as a black box, as it can be\\ndiﬃcult to understand the features it has discovered.\\n7.3\\nModules in Modern Neural Networks\\nThe multi-layer neural network introduced in equation (7.22) of Section 7.2\\nis often called multi-layer perceptron (MLP) these days. Modern neural net-\\nworks used in practice are often much more complex and consist of multiple\\nbuilding blocks or multiple layers of building blocks. In this section, we will'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 93}, page_content='93\\nintroduce some of the other building blocks and discuss possible ways to\\ncombine them.\\nFirst, each matrix multiplication can be viewed as a building block. Con-\\nsider a matrix multiplication operation with parameters (W, b) where W is\\nthe weight matrix and b is the bias vector, operating on an input z,\\nMMW,b(z) = Wz + b .\\n(7.35)\\nNote that we implicitly assume all the dimensions are chosen to be compat-\\nible. We will also drop the subscripts under MM when they are clear in the\\ncontext or just for convenience when they are not essential to the discussion.\\nThen, the MLP can be written as as a composition of multiple matrix\\nmultiplication modules and nonlinear activation modules (which can also be\\nviewed as a building block):\\nMLP(x) = MMW [r],b[r](σ(MMW [r−1],b[r−1](σ(· · · MMW [1],b[1](x)))).\\n(7.36)\\nAlternatively, when we drop the subscripts that indicate the parameters for\\nconvenience, we can write\\nMLP(x) = MM(σ(MMσ(· · · MM(x)))).\\n(7.37)\\nNote that in this lecture notes, by default, all the modules have diﬀerent\\nsets of parameters, and the dimensions of the parameters are chosen such\\nthat the composition is meaningful.\\nLarger modules can be deﬁned via smaller modules as well, e.g., one\\nactivation layer σ and a matrix multiplication layer MM are often combined\\nand called a “layer” in many papers. People often draw the architecture\\nwith the basic modules in a ﬁgure by indicating the dependency between\\nthese modules. E.g., see an illustration of an MLP in Figure 7.4, Left.\\nResidual connections.\\nOne of the very inﬂuential neural network archi-\\ntecture for vision application is ResNet, which uses the residual connections\\nthat are essentially used in almost all large-scale deep learning architectures\\nthese days. Using our notation above, a very much simpliﬁed residual block\\ncan be deﬁned as\\nRes(z) = z + σ(MM(σ(MM(z)))).\\n(7.38)\\nA much simpliﬁed ResNet is a composition of many residual blocks followed\\nby a matrix multiplication,\\nResNet-S(x) = MM(Res(Res(· · · Res(x)))).\\n(7.39)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 94}, page_content='94\\n𝑥\\nLayer 𝑟−1\\nLayer 𝑖\\n...\\nLayer 1\\nMLP(𝑥)\\n...\\nLayer 𝑖\\nMM![\"],#[\"]\\n𝜎\\nMM![$],#[$]\\n𝑥\\nRes\\nRes\\n...\\nRes\\nResNet-S(𝑥)\\n...\\nRes\\nMM\\n𝜎\\nMM\\n𝜎\\nFigure 7.4: Illustrative Figures for Architecture.\\nLeft: An MLP with r\\nlayers. Right: A residual network.\\nWe also draw the dependency of these modules in Figure 7.4, Right.\\nWe note that the ResNet-S is still not the same as the ResNet architec-\\nture introduced in the seminal paper [He et al., 2016] because ResNet uses\\nconvolution layers instead of vanilla matrix multiplication, and adds batch\\nnormalization between convolutions and activations. We will introduce con-\\nvolutional layers and some variants of batch normalization below. ResNet-S\\nand layer normalization are part of the Transformer architecture that are\\nwidely used in modern large language models.\\nLayer normalization.\\nLayer normalization, denoted by LN in this text,\\nis a module that maps a vector z ∈Rm to a more normalized vector LN(z) ∈\\nRm. It is oftentimes used after the nonlinear activations.\\nWe ﬁrst deﬁne a sub-module of the layer normalization, denoted by LN-S.\\nLN-S(z) =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nz1−ˆµ\\nˆσ\\nz2−ˆµ\\nˆσ...\\nzm−ˆµ\\nˆσ\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb,\\n(7.40)\\nwhere ˆµ =\\nPm\\ni=1 zi\\nm\\nis the empirical mean of the vector z and ˆσ =\\nq Pm\\ni=1(zi−ˆµ2)\\nm\\nis the empirical standard deviation of the entries of z.4 Intuitively, LN-S(z)\\nis a vector that is normalized to having empirical mean zero and empirical\\nstandard deviation 1.\\n4Note that we divide by m instead of m −1 in the empirical standard deviation here\\nbecause we are interested in making the output of LN-S(z) have sum of squares equal to\\n1 (as opposed to estimating the standard deviation in statistics.)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 95}, page_content='95\\nOftentimes zero mean and standard deviation 1 is not the most desired\\nnormalization scheme, and thus layernorm introduces to parameters learnable\\nscalars β and γ as the desired mean and standard deviation, and use an aﬃne\\ntransformation to turn the output of LN-S(z) into a vector with mean β and\\nstandard deviation γ.\\nLN(z) = β + γ · LN-S(z) =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nβ + γ\\n\\x00 z1−ˆµ\\nˆσ\\n\\x01\\nβ + γ\\n\\x00 z2−ˆµ\\nˆσ\\n\\x01\\n...\\nβ + γ\\n\\x00 zm−ˆµ\\nˆσ\\n\\x01\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb.\\n(7.41)\\nHere the ﬁrst occurrence of β should be technically interpreted as a vector\\nwith all the entries being β. in We also note that ˆµ and ˆσ are also functions\\nof z and shouldn’t be treated as constants when computing the derivatives of\\nlayernorm. Moreover, β and γ are learnable parameters and thus layernorm\\nis a parameterized module (as opposed to the activation layer which doesn’t\\nhave any parameters.)\\nScaling-invariant property. One important property of layer normalization\\nis that it will make the model invariant to scaling of the parameters in the\\nfollowing sense. Suppose we consider composing LN with MMW,b and get\\na subnetwork LN(MMW,b(z)). Then, we have that the output of this sub-\\nnetwork does not change when the parameter in MMW,b is scaled:\\nLN(MMαW,αb(z)) = LN(MMW,b(z)), ∀α > 0.\\n(7.42)\\nTo see this, we ﬁrst know that LN-S(·) is scale-invariant\\nLN-S(αz) =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nαz1−αˆµ\\nαˆσ\\nαz2−αˆµ\\nαˆσ...\\nαzm−αˆµ\\nαˆσ\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nz1−ˆµ\\nˆσ\\nz2−ˆµ\\nˆσ...\\nzm−ˆµ\\nˆσ\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb= LN-S(z).\\n(7.43)\\nThen we have\\nLN(MMαW,αb(z)) = β + γLN-S(MMαW,αb(z))\\n(7.44)\\n= β + γLN-S(αMMW,b(z))\\n(7.45)\\n= β + γLN-S(MMW,b(z))\\n(7.46)\\n= LN(MMW,b(z)).\\n(7.47)\\nDue to this property, most of the modern DL architectures for large-scale\\ncomputer vision and language applications have the following scale-invariant'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 96}, page_content='96\\nproperty w.r.t all the weights that are not at the last layer. Suppose the\\nnetwork f has last layer’ weights Wlast, and all the rest of the weights are\\ndenote by W. Then, we have fWlast,αW(x) = fWlast,W(x) for all α > 0. Here,\\nthe last layers weights are special because there are typically no layernorm\\nor batchnorm after the last layer’s weights.\\nOther normalization layers. There are several other normalization layers that\\naim to normalize the intermediate layers of the neural networks to a more\\nﬁxed and controllable scaling, such as batch-normalization [?], and group\\nnormalization [?]. Batch normalization and group normalization are more\\noften used in computer vision applications whereas layer norm is used more\\noften in language applications.\\nConvolutional Layers.\\nConvolutional Neural Networks are neural net-\\nworks that consist of convolution layers (and many other modules), and are\\nparticularly useful for computer vision applications. For the simplicity of\\nexposition, we focus on 1-D convolution in this text and only brieﬂy mention\\n2-D convolution informally at the end of this subsection. (2-D convolution\\nis more suitable for images which have two dimensions. 1-D convolution is\\nalso used in natural language processing.)\\nWe start by introducing a simpliﬁed version of the 1-D convolution layer,\\ndenoted by Conv1D-S(·) which is a type of matrix multiplication layer with\\na special structure. The parameters of Conv1D-S are a ﬁlter vector w ∈Rk\\nwhere k is called the ﬁlter size (oftentimes k ≪m), and a bias scalar b.\\nOftentimes the ﬁlter is also called a kernel (but it does not have much to do\\nwith the kernel in kernel method.) For simplicity, we assume k = 2ℓ+ 1 is\\nan odd number. We ﬁrst pad zeros to the input vector z in the sense that we\\nlet z1−ℓ= z1−ℓ+1 = .. = z0 = 0 and zm+1 = zm+2 = .. = zm+ℓ= 0, and treat\\nz as an (m + 2ℓ)-dimension vector. Conv1D-S outputs a vector of dimension\\nRm where each output dimension is a linear combination of subsets of zj’s\\nwith coeﬃcients from w,\\nConv1D-S(z)i = w1zi−ℓ+ w2zi−ℓ+1 + · · · + w2ℓ+1zi+ℓ=\\n2ℓ+1\\nX\\nj=1\\nwjzi−ℓ+(j−1).\\n(7.48)\\nTherefore, one can view Conv1D-S as a matrix multiplication with shared'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 97}, page_content='97\\nparameters: Conv1D-S(z) = Qz, where\\nQ =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nwℓ+1\\n· · ·\\nw2ℓ+1\\n0\\n0\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n0\\nwℓ\\n· · ·\\nw2ℓ\\nw2ℓ+1\\n0\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n0\\n...\\nw1\\n· · ·\\nwℓ+1\\n· · ·\\n· · ·\\n· · ·\\nw2ℓ+1\\n0\\n· · ·\\n· · ·\\n· · ·\\n0\\n0\\nw1\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\nw2ℓ\\nw2ℓ+1\\n0\\n· · ·\\n· · ·\\n0\\n...\\n...\\n0\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n0\\nw1\\n· · ·\\n· · ·\\nw2ℓ+1\\n...\\n0\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n0\\nw1\\n· · ·\\nwℓ+1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\n(7.49)\\nNote that Qi,j = Qi−1,j−1 for all i, j ∈{2, . . . , m}, and thus convoluation is a\\nmatrix multiplication with parameter sharing. We also note that computing\\nthe convolution only takes O(km) times but computing a generic matrix\\nmultiplication takes O(m2) time. Convolution has k parameters but generic\\nmatrix multiplication will have m2 parameters. Thus convolution is supposed\\nto be much more eﬃcient than a generic matrix multiplication (as long as\\nthe additional structure imposed does not hurt the ﬂexibility of the model\\nto ﬁt the data).\\nWe also note that in practice there are many variants of the convolutional\\nlayers that we deﬁne here, e.g., there are other ways to pad zeros or sometimes\\nthe dimension of the output of the convolutional layers could be diﬀerent from\\nthe input. We omit some of this subtleties here for simplicity.\\nThe convolutional layers used in practice have also many “channels” and\\nthe simpliﬁed version above corresponds to the 1-channel version. Formally,\\nConv1D takes in C vectors z1, . . . , zC ∈Rm as inputs, where C is referred\\nto as the number of channels.\\nIn other words, the more general version,\\ndenoted by Conv1D, takes in a matrix as input, which is the concatenation\\nof z1, . . . , zC and has dimension m×C. It can output C′ vectors of dimension\\nm, denoted by Conv1D(z)1, . . . , Conv1D(z)C′, where C′ is referred to as the\\noutput channel, or equivalently a matrix of dimension m × C′. Each of the\\noutput is a sum of the simpliﬁed convolutions applied on various channels.\\n∀i ∈[C′], Conv1D(z)i =\\nC\\nX\\nj=1\\nConv1D-Si,j(zj).\\n(7.50)\\nNote that each Conv1D-Si,j are modules with diﬀerent parameters, and\\nthus the total number of parameters is k (the number of parameters in a\\nConv1D-S) ×CC′ (the number of Conv1D-Si.j’s) = kCC′. In contrast, a\\ngeneric linear mapping from Rm×C and Rm×C′ has m2CC′ parameters. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 98}, page_content='98\\nparameters can also be represented as a three-dimensional tensor of dimen-\\nsion k × C × C′.\\n2-D convolution (brief). A 2-D convolution with one channel, denoted by\\nConv2D-S, is analogous to the Conv1D-S, but takes a 2-dimensional input\\nz ∈Rm×m and applies a ﬁlter of size k × k, and outputs Conv2D-S(z) ∈\\nRm×m.\\nThe full 2-D convolutional layer, denoted by Conv2D, takes in\\na sequence of matrices z1, . . . , zC ∈Rm×m, or equivalently a 3-D ten-\\nsor z = (z1, . . . , zC) ∈Rm×m×C and outputs a sequence of matrices,\\nConv2D(z)1, . . . , Conv2D(z)C′ ∈Rm×m, which can also be viewed as a 3D\\ntensor in Rm×m×C′. Each channel of the output is sum of the outcomes of\\napplying Conv2D-S layers on all the input channels.\\n∀i ∈[C′], Conv2D(z)i =\\nC\\nX\\nj=1\\nConv2D-Si,j(zj).\\n(7.51)\\nBecause there are CC′ number of Conv2D-S modules and each of the\\nConv2D-S module has k2 parameters, the total number of parameters is\\nCC′k2.\\nThe parameters can also be viewed as a 4D tensor of dimension\\nC × C′ × k × k.\\n7.4\\nBackpropagation\\nIn this section, we introduce backpropgation or auto-diﬀerentiation, which\\ncomputes the gradient of the loss ∇J(θ) eﬃciently. We will start with an\\ninformal theorem that states that as long as a real-valued function f can be\\neﬃciently computed/evaluated by a diﬀerentiable network or circuit, then its\\ngradient can be eﬃciently computed in a similar time. We will then show\\nhow to do this concretely for neural networks.\\nBecause the formality of the general theorem is not the main focus here,\\nwe will introduce the terms with informal deﬁnitions. By a diﬀerentiable\\ncircuit or a diﬀerentiable network, we mean a composition of a sequence of\\ndiﬀerentiable arithmetic operations (additions, subtraction, multiplication,\\ndivisions, etc) and elementary diﬀerentiable functions (ReLU, exp, log, sin,\\ncos, etc.). Let the size of the circuit be the total number of such operations\\nand elementary functions. We assume that each of the operations and func-\\ntions, and their derivatives or partial derivatives ecan be computed in O(1)\\ntime.\\nTheorem 7.4.1: [backpropagation or auto-diﬀerentiation, informally stated]\\nSuppose a diﬀerentiable circuit of size N computes a real-valued function'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 99}, page_content='99\\nf : Rℓ→R. Then, the gradient ∇f can be computed in time O(N), by a\\ncircuit of size O(N).5\\nWe note that the loss function J(j)(θ) for j-th example can be indeed\\ncomputed by a sequence of operations and functions involving additions,\\nsubtraction, multiplications, and non-linear activations. Thus the theorem\\nsuggests that we should be able to compute the ∇J(j)(θ) in a similar time\\nto that for computing J(j)(θ) itself. This does not only apply to the fully-\\nconnected neural network introduced in the Section 7.2, but also many other\\ntypes of neural networks that uses more advance modules.\\nWe remark that auto-diﬀerentiation or backpropagation is already imple-\\nmented in all the deep learning packages such as tensorﬂow and pytorch, and\\nthus in practice, in most of cases a researcher does not need to write their\\nbackpropagation algorithms. However, understanding it is very helpful for\\ngaining insights into the working of deep learning.\\nOrganization of the rest of the section. In Section 7.4.1, we will start review-\\ning the basic Chain rule with a new perspective that is particularly useful\\nfor understanding backpropgation. Section 7.4.2 will introduce the general\\nstrategy for backpropagation. Section 7.4.2 will discuss how to compute the\\nso-called backward function for basic modules used in neural networks, and\\nSection 7.4.4 will put everything together to get a concrete backprop algo-\\nrithm for MLPs.\\n7.4.1\\nPreliminaries on partial derivatives\\nSuppose a scalar variable J depend on some variables z (which could be a\\nscalar, matrix, or high-order tensor), we write ∂J\\n∂z as the partial derivatives\\nof J w.r.t to the variable z. We stress that the convention here is that ∂J\\n∂z\\nhas exactly the same dimension as z itself. For example, if z ∈Rm×n, then\\n∂J\\n∂z ∈Rm×n, and the (i, j)-entry of ∂J\\n∂z is equal to\\n∂J\\n∂zij .\\nRemark 7.4.2: When both J and z are not scalars, the partial derivatives of\\nJ w.r.t z becomes either a matrix or tensor and the notation becomes some-\\nwhat tricky. Besides the mathematical or notational challenges in dealing\\n5We note if the output of the function f does not depend on some of the input co-\\nordinates, then we set by default the gradient w.r.t that coordinate to zero. Setting to\\nzero does not count towards the total runtime here in our accounting scheme. This is why\\nwhen N ≤ℓ, we can compute the gradient in O(N) time, which might be potentially even\\nless than ℓ.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 100}, page_content='100\\nwith these partial derivatives of multi-variate functions, they are also expen-\\nsive to compute and store, and thus rarely explicitly constructed empirically.\\nThe experience of authors of this note is that it’s generally more productive\\nto think only about derivatives of scalar function w.r.t to vector, matrices,\\nor tensors. For example, in this note, we will not deal with derivatives of\\nmulti-variate functions.\\nChain rule.\\nWe review the chain rule in calculus but with a perspective\\nand notions that are more relevant for auto-diﬀerentiation.\\nConsider a scalar variable J which is obtained by the composition of f\\nand g on some variable z,\\nz ∈Rm\\nu = g(z) ∈Rn\\nJ = f(u) ∈R .\\n(7.52)\\nThe same derivations below can be easily extend to the cases when z and u\\nare matrices or tensors; but we insist that the ﬁnal variable J is a scalar. (See\\nalso Remark 7.4.2.) Let u = (u1, . . . , un) and let g(z) = (g1(z), · · · , gn(z)).\\nThen, the standard chain rule gives us that\\n∀i ∈{1, . . . , m},\\n∂J\\n∂zi\\n=\\nn\\nX\\nj=1\\n∂J\\n∂uj\\n· ∂gj\\n∂zi\\n.\\n(7.53)\\nAlternatively, when z and u are both vectors, in a vectorized notation:\\n∂J\\n∂z =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n∂g1\\n∂z1\\n· · ·\\n∂gn\\n∂z1\\n...\\n...\\n...\\n∂g1\\n∂zm\\n· · ·\\n∂gn\\n∂zm\\n\\uf8f9\\n\\uf8fa\\uf8fb· ∂J\\n∂u .\\n(7.54)\\nIn other words, the backward function is always a linear map from ∂J\\n∂u to\\n∂J\\n∂z , though note that the mapping itself can depend on z in complex ways.\\nThe matrix on the RHS of (7.54) is actually the transpose of the Jacobian\\nmatrix of the function g. However, we do not discuss in-depth about Jacobian\\nmatrices to avoid complications. Part of the reason is that when z is a matrix\\n(or tensor), to write an analog of equation (7.54), one has to either ﬂatten z\\ninto a vector or introduce additional notations on tensor-matrix product. In\\nthis sense, equation (7.53) is more convenient and eﬀective to use in all cases.\\nFor example, when z ∈Rr×s is a matrix, we can easily rewrite equation (7.53)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 101}, page_content='101\\nto\\n∀i, k,\\n∂J\\n∂zik\\n=\\nn\\nX\\nj=1\\n∂J\\n∂uj\\n· ∂gj\\n∂zik\\n.\\n(7.55)\\nwhich will indeed be used in some of the derivations in Section 7.4.3.\\nKey interpretation of the chain rule.\\nWe can view the formula above (equa-\\ntion (7.53) or (7.54)) as a way to compute ∂J\\n∂z from ∂J\\n∂u. Consider the following\\nabstract problem. Suppose J depends on z via u as deﬁned in equation (7.52).\\nHowever, suppose the function f is not given or the function f is complex,\\nbut we are given the value of ∂J\\n∂u. Then, the formula in equation (7.54) gives\\nus a way to compute ∂J\\n∂z from ∂J\\n∂u.\\n∂J\\n∂u\\nchain rule, formula (7.54)\\n====================⇒\\nonly requires info about g(·) and z\\n∂J\\n∂z .\\n(7.56)\\nMoreover, this formula only involves knowledge about g (more precisely ∂gj\\n∂zi ).\\nWe will repeatedly use this fact in situations where g is a building blocks of\\na complex network f.\\nEmpirically, it’s often useful to modularized the mapping in (7.53) or\\n(7.54) into a black-box, and mathematically it’s also convenient to deﬁne a\\nnotation for it.6 We use B[g, z] to deﬁne the function that maps ∂J\\n∂u to ∂J\\n∂z ,\\nand write\\n∂J\\n∂z = B[g, z]\\n\\x12∂J\\n∂u\\n\\x13\\n.\\n(7.57)\\nWe call B[g, z] the backward function for the module g. Note that when z\\nis ﬁxed, B[g, z] is merely a linear map from Rn to Rm. Using equation (7.53),\\nwe have\\n(B[g, z](v))i =\\nm\\nX\\nj=1\\n∂gj\\n∂zi\\n· vj .\\n(7.58)\\nOr in vectorized notation, using (7.54), we have\\nB[g, z](v) =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n∂g1\\n∂z1\\n· · ·\\n∂gn\\n∂z1\\n...\\n...\\n...\\n∂g1\\n∂zm\\n· · ·\\n∂gn\\n∂zm\\n\\uf8f9\\n\\uf8fa\\uf8fb· v .\\n(7.59)\\n6e.g., the function is the .backward() method of the module in pytorch.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 102}, page_content='102\\nand therefore B[g, z] can be viewed as a matrix. However, in reality, z will be\\nchanging and thus the backward mapping has to be recomputed for diﬀerent\\nz’s while g is often ﬁxed. Thus, empirically, the backward function B[g, z](v)\\nis often viewed as a function which takes in z (=the input to g) and v (=a\\nvector that is supposed to be the gradient of some variable J w.r.t to the\\noutput of g) as the inputs, and outputs a vector that is supposed to be the\\ngradient of J w.r.t to z.\\n7.4.2\\nGeneral strategy of backpropagation\\nWe discuss the general strategy of auto-diﬀerentiation in this section to build\\na high-level understanding. Then, we will instantiate the approach to con-\\ncrete neural networks. We take the viewpoint that neural networks are com-\\nplex compositions of small building blocks such as MM, σ, Conv2D, LN,\\netc., deﬁned in Section 7.3. Note that the losses (e.g., mean-squared loss, or\\nthe cross-entropy loss) can also be abstractly viewed as additional modules.\\nThus, we can abstractly write the loss function J (on a single example (x, y))\\nas a composition of many modules:7\\nJ = Mk(Mk−1(· · · M1(x))) .\\n(7.60)\\nFor example, for a binary classiﬁcation problem with a MLP ¯hθ(x) (de-\\nﬁned in equation (7.36) and (7.37)), the loss function has ber written in the\\nform of equation (7.60) with M1 = MMW [1],b[1], M2 = σ, M3 = MMW [2],b[2],\\n. . . , and Mk−1 = MMW [r],b[r] and Mk = ℓlogistic.\\nWe can see from this example that some modules involve parameters, and\\nother modules might only involve a ﬁxed set of operations. For generality,\\nwe assume that eachj Mi involves a set of parameters θ[i], though θ[i] could\\npossibly be an empty set when Mi is a ﬁxed operation such as the nonlinear\\nactivations. We will discuss more on the granularity of the modularization,\\nbut so far we assume all the modules Mi’s are simple enough.\\nWe introduce the intermediate variables for the computation in (7.60).\\n7Technically, we should write J = Mk(Mk−1(· · · M1(x)), y). However, y is treated as a\\nconstant for the purpose of computing the derivatives w.r.t to the parameters, and thus\\nwe can view it as part of Mk for the sake of simplicity of notations.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 103}, page_content='103\\nLet\\nu[0] = x\\nu[1] = M1(u[0])\\nu[2] = M2(u[1])\\n...\\nJ = u[k] = Mk(u[k−1]) .\\n(F)\\nBackpropgation consists of two passes, the forward pass and backward\\npass. In the forward pass, the algorithm simply computes u[1], . . . , u[k] from\\ni = 1, . . . , k, sequentially using the deﬁnition in (F), and save all the in-\\ntermediate variables u[i]’s in the memory.\\nIn the backward pass, we ﬁrst compute the derivatives w.r.t to the\\nintermediate variables, that is,\\n∂J\\n∂u[k], . . . ,\\n∂J\\n∂u[1], sequentially in this backward\\norder, and then compute the derivatives of the parameters\\n∂J\\n∂θ[i] from\\n∂J\\n∂u[i] and\\nu[i−1]. These two type of computations can be also interleaved with each\\nother because\\n∂J\\n∂θ[i] only depends on\\n∂J\\n∂u[i] and u[i−1] but not any\\n∂J\\n∂u[k] with\\nk < i.\\nWe ﬁrst see why\\n∂J\\n∂u[i−1] can be computed eﬃciently from\\n∂J\\n∂u[i] and u[i−1]\\nby invoking the discussion in Section 7.4.1 on the chain rule.\\nWe in-\\nstantiate the discussion by setting u = u[i] and z = u[i−1], and f(u) =\\nMk(Mk−1(· · · Mi+1(u[i]))), and g(·) = Mi(·). Note that f is very complex\\nbut we don’t need any concrete information about f. Then, the conclusive\\nequation (7.56) corresponds to\\n∂J\\n∂u[i]\\nchain rule\\n==========================⇒\\nonly requires info about Mi(·) and u[i−1]\\n∂J\\n∂u[i−1].\\n(7.61)\\nMore precisely, we can write, following equation (7.57)\\n∂J\\n∂u[i−1] = B[Mi, u[i−1]]\\n\\x12 ∂J\\n∂u[i]\\n\\x13\\n.\\n(B1)\\nInstantiating the chain rule with z = θ[i] and u = u[i], we also have\\n∂J\\n∂θ[i] = B[Mi, θ[i]]\\n\\x12 ∂J\\n∂u[i]\\n\\x13\\n.\\n(B2)\\nSee Figure 7.5 for an illustration of the algorithm.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 104}, page_content='104\\n𝑥\\n. . .\\n𝑀!\\n𝐽\\n. . .\\n𝑀\"\\n𝑢[!]\\n𝑢[\"%!]\\n𝜕𝐽\\n𝜕𝐽\\nℬ[𝑀\", 𝑢[\"%!]]\\n𝜕𝐽\\n𝜕𝑢!\"#\\n𝑢[&]\\n𝑀&\\n𝑢[&%!]\\nℬ[𝑀&, 𝑢[&%!]]\\n𝜕𝐽\\n𝜕𝑢$\"#\\n𝜕𝐽\\n𝜕𝑢$\\n. . .\\n𝜕𝐽\\n𝜕𝑢#\\n. . .\\nForward pass\\nBackward pass\\nℬ[𝑀!, 𝜃! ]\\n𝜕𝐽\\n𝜕𝜃#\\nℬ[𝑀&, 𝜃& ]\\n𝜕𝐽\\n𝜕𝜃$\\nℬ[𝑀\", 𝜃\" ]\\n𝜕𝐽\\n𝜕𝜃!\\nFigure 7.5: Back-propagation.\\nRemark 7.4.3: [Computational eﬃciency and granularity of the modules]\\nThe main underlying purpose of treating a complex network as compositions\\nof small modules is that small modules tend to have eﬃciently implementable\\nbackward function. In fact, the backward functions of all the atomic modules\\nsuch as addition, multiplication and ReLU can be computed as eﬃciently as\\nthe the evaluation of these modules (up to multiplicative constant factor).\\nUsing this fact, we can prove Theorem 7.4.1 by viewing neural networks as\\ncompositions of many atomic operations, and invoking the backpropagation\\ndiscussed above. However, in practice, it’s oftentimes more convenient to\\nmodularize the networks using modules on the level of matrix multiplication,\\nlayernorm, etc. As we will see, naive implementation of these operations’\\nbackward functions also have the same runtime as the evaluation of these\\nfunctions.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 105}, page_content='105\\n7.4.3\\nBackward functions for basic modules\\nUsing the general strategy in Section 7.4.2, it suﬃces to compute the back-\\nward function for all modules Mi’s used in the networks. We compute the\\nbackward function for the basic module MM, activations σ, and loss functions\\nin this section.\\nBackward function for MM. Suppose MMW,b(z) = Wz + b is a matrix multi-\\nplication module where z ∈Rm and W ∈Rn×m. Then, using equation (7.59),\\nwe have for v ∈Rn\\nB[MM, z](v) =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n∂(Wz+b)1\\n∂z1\\n· · ·\\n∂(Wz+b)n\\n∂z1\\n...\\n...\\n...\\n∂(Wz+b)1\\n∂zm\\n· · ·\\n∂(Wz+b)n\\n∂zm\\n\\uf8f9\\n\\uf8fa\\uf8fbv .\\n(7.62)\\nUsing the fact that ∀i ∈[m], j ∈[n],∂(Wz+b)j\\n∂zi\\n=\\n∂bj+Pm\\nk=1 Wjkzk\\n∂zi\\n= Wji, we\\nhave\\nB[MM, z](v) = W ⊤v ∈Rm .\\n(7.63)\\nIn the derivation above, we have treated MM as a function of z. If we treat\\nMM as a function of W and b, then we can also compute the backward\\nfunction for the parameter variables W and b. It’s less convenient to use\\nequation (7.59) because the variable W is a matrix and the matrix in (7.59)\\nwill be a 4-th order tensor that is challenging for us to mathematically write\\ndown. We use (7.58) instead:\\n(B[MM, W](v))ij =\\nm\\nX\\nk=1\\n∂(Wz + b)k\\n∂Wij\\n· vk =\\nm\\nX\\nk=1\\n∂Pm\\ns=1 Wkszs\\n∂Wij\\n· vk = vizj .\\n(7.64)\\nIn vectorized notation, we have\\nB[MM, W](v) = vz⊤∈Rn××m .\\n(7.65)\\nUsing equation (7.59) for the variable b, we have,\\nB[MM, b](v) =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n∂(Wz+b)1\\n∂b1\\n· · ·\\n∂(Wz+b)n\\n∂b1\\n...\\n...\\n...\\n∂(Wz+b)1\\n∂bn\\n· · ·\\n∂(Wz+b)n\\n∂bn\\n\\uf8f9\\n\\uf8fa\\uf8fbv = v .\\n(7.66)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 106}, page_content='106\\nHere we used that ∂(Wz+b)j\\n∂bi\\n= 0 if i ̸= j and ∂(Wz+b)j\\n∂bi\\n= 1 if i = j.\\nThe computational eﬃciency for computing the backward function is\\nO(mn), the same as evaluating the result of matrix multiplication up to\\nconstant factor.\\nBackward function for the activations. Suppose M(z) = σ(z) where σ is an\\nelement-wise activation function and z ∈Rm. Then, using equation (7.59),\\nwe have\\nB[σ, z](v) =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n∂σ(z1)\\n∂z1\\n· · ·\\n∂σ(zm)\\n∂z1\\n...\\n...\\n...\\n∂σ(z1)\\n∂zm\\n· · ·\\n∂σ(zm)\\n∂zm\\n\\uf8f9\\n\\uf8fa\\uf8fbv\\n(7.67)\\n= diag(σ′(z1), · · · , σ′(zm))v\\n(7.68)\\n= σ′(z) ⊙v ∈Rm .\\n(7.69)\\nHere, we used the fact that ∂σ(zj)\\n∂zi\\n= 0 when j ̸= i, diag(λ1, . . . , λm) denotes\\nthe diagonal matrix with λ1, . . . , λm on the diagonal, and ⊙denotes the\\nelement-wise product of two vectors with the same dimension, and σ′(·) is\\nthe element-wise application of the derivative of the activation function σ.\\nRegarding computation eﬃciency, we note that at the ﬁrst sight, equa-\\ntion (7.67) appears to indicate the backward function takes O(m2) time, but\\nequation (7.69) shows that it’s implementable in O(m) time (which is the\\nsame as the time for evaluating of the function.) We are not supposed to be\\nsurprised by that the possibility of simplifying equation (7.67) to (7.69)—if\\nwe use smaller modules, that is, treating the vector-to-vector nonlinear ac-\\ntivation as m scalar-to-scalar non-linear activation, then it’s more obvious\\nthat the backward pass should have similar time to the forward pass.\\nBackward function for loss functions.\\nWhen a module M takes in a vector\\nz and outputs a scalar, by equation (7.59), the backward function takes in a\\nscalar v and outputs a vector with entries (B[M, z](v))i = ∂M\\n∂zi v. Therefore,\\nin vectorized notation, B[M, z](v) = ∂M\\n∂z · v.\\nRecall that squared loss ℓMSE(z, y) =\\n1\\n2(z −y)2. Thus, B[ℓMSE, z](v) =\\n∂1\\n2 (z−y)2\\n∂z\\n· v = (z −y) · v.\\nFor logistics loss, by equation (2.6), we have\\nB[ℓlogistic, t](v) = ∂ℓlogistic(t, y)\\n∂t\\n· v = (1/(1 + exp(−t)) −y) · v .\\n(7.70)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 107}, page_content='107\\nFor cross-entropy loss, by equation (2.17), we have\\nB[ℓce, t](v) = ∂ℓce(t, y)\\n∂t\\n· v = (φ −ey) · v ,\\n(7.71)\\nwhere φ = softmax(t).\\n7.4.4\\nBack-propagation for MLPs\\nGiven the backward functions for every module needed in evaluating the loss\\nof an MLP, we follow the strategy in Section 7.4.2 to compute the gradient\\nof the loss w.r.t to the hidden activations and the parameters.\\nWe consider the an r-layer MLP with a logistic loss. The loss function\\ncan be computed via a sequence of operations (that is, the forward pass),\\nz[1] = MMW [1],b[1](x),\\na[1] = σ(z[1])\\nz[2] = MMW [2],b[2](a[1])\\na[2] = σ(z[2])\\n...\\nz[r] = MMW [r],b[r](a[r−1])\\nJ = ℓlogistic(z[r], y) .\\n(7.72)\\nWe apply the backward function sequentially in a backward order. First, we\\nhave that\\n∂J\\n∂z[r] = B[ℓlogistic, z[r]]\\n\\x12∂J\\n∂J\\n\\x13\\n= B[ℓlogistic, z[r]](1) .\\n(7.73)\\nThen, we iteratively compute\\n∂J\\n∂a[i] and\\n∂J\\n∂z[i]’s by repeatedly invoking the chain\\nrule (equation (7.58)),\\n∂J\\n∂a[r−1] = B[MM, a[r−1]]\\n\\x12 ∂J\\n∂z[r]\\n\\x13\\n∂J\\n∂z[r−1] = B[σ, z[r−1]]\\n\\x12\\n∂J\\n∂a[r−1]\\n\\x13\\n...\\n∂J\\n∂z[1] = B[σ, z[1]]\\n\\x12 ∂J\\n∂a[1]\\n\\x13\\n.\\n(7.74)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 108}, page_content='108\\nNumerically, we compute these quantities by repeatedly invoking equa-\\ntions (7.69) and (7.63) with diﬀerent choices of variables.\\nWe note that the intermediate values of a[i] and z[i] are used in the back-\\npropagation (equation (7.74)), and therefore these values need to be stored\\nin the memory after the forward pass.\\nNext, we compute the gradient of the parameters by invoking equa-\\ntions (7.65) and (7.66),\\n∂J\\n∂W [r] = B[MM, W [r]]\\n\\x12 ∂J\\n∂z[r]\\n\\x13\\n∂J\\n∂b[r] = B[MM, b[r]]\\n\\x12 ∂J\\n∂z[r]\\n\\x13\\n...\\n∂J\\n∂W [1] = B[MM, W [1]]\\n\\x12 ∂J\\n∂z[1]\\n\\x13\\n∂J\\n∂b[1] = B[MM, b[1]]\\n\\x12 ∂J\\n∂z[1]\\n\\x13\\n.\\n(7.75)\\nWe also note that the block of computations in equations (7.75) can be\\ninterleaved with the block of computation in equations (7.74) because the\\n∂J\\n∂W [i] and\\n∂J\\n∂b[i] can be computed as soon as\\n∂J\\n∂z[i] is computed.\\nPutting all of these together,\\nand explicitly invoking the equa-\\ntions (7.72), (7.74) and (7.75), we have the following algorithm (Algorithm 3).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 109}, page_content='109\\nAlgorithm 3 Back-propagation for multi-layer neural networks.\\n1: Forward pass. Compute and store the values of a[k]’s, z[k]’s, and J\\nusing the equations (7.72).\\n2: Backward pass. Compute the gradient of loss J with respect to z[r]:\\n∂J\\n∂z[r] = B[ℓlogistic, z[r]](1) =\\n\\x001/(1 + exp(−z[r])) −y\\n\\x01\\n.\\n(7.76)\\n3: for k = r −1 to 0 do\\n4:\\nCompute the gradient with respect to parameters W [k+1] and b[k+1].\\n∂J\\n∂W [k+1] = B[MM, W [k+1]]\\n\\x12\\n∂J\\n∂z[k+1]\\n\\x13\\n=\\n∂J\\n∂z[k+1]a[k]⊤.\\n(7.77)\\n∂J\\n∂b[k+1] = B[MM, b[k+1]]\\n\\x12\\n∂J\\n∂z[k+1]\\n\\x13\\n=\\n∂J\\n∂z[k+1] .\\n(7.78)\\n5:\\nWhen k ≥1, compute the gradient with respect to z[k] and a[k].\\n∂J\\n∂a[k] = B[σ, a[k]]\\n\\x12\\n∂J\\n∂z[k+1]\\n\\x13\\n= W [k+1]⊤\\n∂J\\n∂z[k+1] .\\n(7.79)\\n∂J\\n∂z[k] = B[σ, z[k]]\\n\\x12 ∂J\\n∂a[k]\\n\\x13\\n= σ′(z[k]) ⊙∂J\\n∂a[k] .\\n(7.80)\\n7.5\\nVectorization over training examples\\nAs we discussed in Section 7.1, in the implementation of neural networks,\\nwe will leverage the parallelism across the multiple examples. This means\\nthat we will need to write the forward pass (the evaluation of the outputs)\\nof the neural network and the backward pass (backpropagation) for multiple'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 110}, page_content='110\\ntraining examples in matrix notation.\\nThe basic idea.\\nThe basic idea is simple. Suppose you have a training\\nset with three examples x(1), x(2), x(3). The ﬁrst-layer activations for each\\nexample are as follows:\\nz[1](1) = W [1]x(1) + b[1]\\nz[1](2) = W [1]x(2) + b[1]\\nz[1](3) = W [1]x(3) + b[1]\\nNote the diﬀerence between square brackets [·], which refer to the layer num-\\nber, and parenthesis (·), which refer to the training example number. In-\\ntuitively, one would implement this using a for loop. It turns out, we can\\nvectorize these operations as well. First, deﬁne:\\nX =\\n\\uf8ee\\n\\uf8f0\\n|\\n|\\n|\\nx(1)\\nx(2)\\nx(3)\\n|\\n|\\n|\\n\\uf8f9\\n\\uf8fb∈Rd×3\\n(7.81)\\nNote that we are stacking training examples in columns and not rows. We\\ncan then combine this into a single uniﬁed formulation:\\nZ[1] =\\n\\uf8ee\\n\\uf8f0\\n|\\n|\\n|\\nz[1](1)\\nz[1](2)\\nz[1](3)\\n|\\n|\\n|\\n\\uf8f9\\n\\uf8fb= W [1]X + b[1]\\n(7.82)\\nYou may notice that we are attempting to add b[1] ∈R4×1 to W [1]X ∈\\nR4×3. Strictly following the rules of linear algebra, this is not allowed. In\\npractice however, this addition is performed using broadcasting. We create\\nan intermediate ˜b[1] ∈R4×3:\\n˜b[1] =\\n\\uf8ee\\n\\uf8f0\\n|\\n|\\n|\\nb[1]\\nb[1]\\nb[1]\\n|\\n|\\n|\\n\\uf8f9\\n\\uf8fb\\n(7.83)\\nWe can then perform the computation: Z[1] = W [1]X + ˜b[1]. Often times, it\\nis not necessary to explicitly construct ˜b[1]. By inspecting the dimensions in\\n(7.82), you can assume b[1] ∈R4×1 is correctly broadcast to W [1]X ∈R4×3.\\nThe matricization approach as above can easily generalize to multiple\\nlayers, with one subtlety though, as discussed below.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 111}, page_content='111\\nComplications/Subtlety in the Implementation.\\nAll the deep learn-\\ning packages or implementations put the data points in the rows of a data\\nmatrix. (If the data point itself is a matrix or tensor, then the data are con-\\ncentrated along the zero-th dimension.) However, most of the deep learning\\npapers use a similar notation to these notes where the data points are treated\\nas column vectors.8 There is a simple conversion to deal with the mismatch:\\nin the implementation, all the columns become row vectors, row vectors be-\\ncome column vectors, all the matrices are transposed, and the orders of the\\nmatrix multiplications are ﬂipped. In the example above, using the row ma-\\njor convention, the data matrix is X ∈R3×d, the ﬁrst layer weight matrix\\nhas dimensionality d × m (instead of m × d as in the two layer neural net\\nsection), and the bias vector b[1] ∈R1×m. The computation for the hidden\\nactivation becomes\\nZ[1] = XW [1] + b[1] ∈R3×m\\n(7.84)\\n8The instructor suspects that this is mostly because in mathematics we naturally mul-\\ntiply a matrix to a vector on the left hand side.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 112}, page_content='Part III\\nGeneralization and\\nregularization\\n112'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 113}, page_content='Chapter 8\\nGeneralization\\nThis chapter discusses tools to analyze and understand the generaliza-\\ntion of machine learning models, i.e, their performances on unseen test\\nexamples.\\nRecall that for supervised learning problems, given a train-\\ning dataset {(x(i), y(i))}n\\ni=1, we typically learn a model hθ by minimizing a\\nloss/cost function J(θ), which encourages hθ to ﬁt the data.\\nE.g., when\\nthe loss function is the least square loss (aka mean squared error), we have\\nJ(θ) = 1\\nn\\nPn\\ni=1(y(i) −hθ(x(i)))2. This loss function for training purposes is\\noftentimes referred to as the training loss/error/cost.\\nHowever, minimizing the training loss is not our ultimate goal—it is\\nmerely our approach towards the goal of learning a predictive model. The\\nmost important evaluation metric of a model is the loss on unseen test exam-\\nples, which is oftentimes referred to as the test error. Formally, we sample a\\ntest example (x, y) from the so-called test distribution D, and measure the\\nmodel’s error on it, by, e.g., the mean squared error, (hθ(x) −y)2. The ex-\\npected loss/error over the randomness of the test example is called the test\\nloss/error,1\\nL(θ) = E(x,y)∼D[(y −hθ(x))2]\\n(8.1)\\nNote that the measurement of the error involves computing the expectation,\\nand in practice, it can be approximated by the average error on many sampled\\ntest examples, which are referred to as the test dataset. Note that the key\\ndiﬀerence here between training and test datasets is that the test examples\\n1In theoretical and statistical literature, we oftentimes call the uniform distribution\\nover the training set {(x(i), y(i))}n\\ni=1, denoted by bD, an empirical distribution, and call\\nD the population distribution. Partly because of this, the training loss is also referred\\nto as the empirical loss/risk/error, and the test loss is also referred to as the population\\nloss/risk/error.\\n113'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 114}, page_content='114\\nare unseen, in the sense that the training procedure has not used the test\\nexamples. In classical statistical learning settings, the training examples are\\nalso drawn from the same distribution as the test distribution D, but still\\nthe test examples are unseen by the learning procedure whereas the training\\nexamples are seen.2\\nBecause of this key diﬀerence between training and test datasets, even\\nif they are both drawn from the same distribution D, the test error is not\\nnecessarily always close to the training error.3 As a result, successfully min-\\nimizing the training error may not always lead to a small test error. We\\ntypically say the model overﬁts the data if the model predicts accurately on\\nthe training dataset but doesn’t generalize well to other test examples, that\\nis, if the training error is small but the test error is large. We say the model\\nunderﬁts the data if the training error is relatively large4 (and in this case,\\ntypically the test error is also relatively large.)\\nThis chapter studies how the test error is inﬂuenced by the learning pro-\\ncedure, especially the choice of model parameterizations. We will decompose\\nthe test error into “bias” and “variance” terms and study how each of them is\\naﬀected by the choice of model parameterizations and their tradeoﬀs. Using\\nthe bias-variance tradeoﬀ, we will discuss when overﬁtting and underﬁtting\\nwill occur and be avoided.\\nWe will also discuss the double descent phe-\\nnomenon in Section 8.2 and some classical theoretical results in Section 8.3.\\n2These days, researchers have increasingly been more interested in the setting with\\n“domain shift”, that is, the training distribution and test distribution are diﬀerent.\\n3the diﬀerence between test error and training error is often referred to as the gener-\\nalization gap. The term generalization error in some literature means the test error, and\\nin some other literature means the generalization gap.\\n4e.g., larger than the intrinsic noise level of the data in regression problems.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 115}, page_content='115\\n8.1\\nBias-variance tradeoﬀ\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nx\\n0.0\\n0.5\\n1.0\\n1.5\\ny\\ntraining dataset\\ntraining data\\nground truth h *\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nx\\n0.0\\n0.5\\n1.0\\n1.5\\ny\\ntest dataset\\ntest data\\nground truth h *\\nFigure 8.1: A running example of training and test dataset for this section.\\nAs an illustrating example, we consider the following training dataset and\\ntest dataset, which are also shown in Figure 8.1. The training inputs x(i)’s are\\nrandomly chosen and the outputs y(i) are generated by y(i) = h⋆(x(i)) + ξ(i)\\nwhere the function h⋆(·) is a quadratic function and is shown in Figure 8.1\\nas the solid line, and ξ(i) is the a observation noise assumed to be generated\\nfrom ∼N(0, σ2).\\nA test example (x, y) also has the same input-output\\nrelationship y = h⋆(x) + ξ where ξ ∼N(0, σ2). It’s impossible to predict the\\nnoise ξ, and therefore essentially our goal is to recover the function h⋆(·).\\nWe will consider the test error of learning various types of models. When\\ntalking about linear regression, we discussed the problem of whether to ﬁt\\na “simple” model such as the linear “y = θ0 + θ1x,” or a more “complex”\\nmodel such as the polynomial “y = θ0 + θ1x + · · · θ5x5.”\\nWe start with ﬁtting a linear model, as shown in Figure 8.2. The best\\nﬁtted linear model cannot predict y from x accurately even on the training\\ndataset, let alone on the test dataset. This is because the true relationship\\nbetween y and x is not linear—any linear model is far away from the true\\nfunction h⋆(·). As a result, the training error is large and this is a typical\\nsituation of underﬁtting.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 116}, page_content='116\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nx\\n0.0\\n0.5\\n1.0\\n1.5\\ny\\ntraining data\\nbest fit linear model\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nx\\n0.0\\n0.5\\n1.0\\n1.5\\ny\\ntest data\\nbest fit linear model\\nFigure 8.2: The best ﬁt linear model has large training and test errors.\\nThe issue cannot be mitigated with more training examples—even with\\na very large amount of, or even inﬁnite training examples, the best ﬁtted\\nlinear model is still inaccurate and fails to capture the structure of the data\\n(Figure 8.3). Even if the noise is not present in the training data, the issue\\nstill occurs (Figure 8.4). Therefore, the fundamental bottleneck here is the\\nlinear model family’s inability to capture the structure in the data—linear\\nmodels cannot represent the true quadratic function h⋆—, but not the lack of\\nthe data. Informally, we deﬁne the bias of a model to be the test error even\\nif we were to ﬁt it to a very (say, inﬁnitely) large training dataset. Thus, in\\nthis case, the linear model suﬀers from large bias, and underﬁts (i.e., fails to\\ncapture structure exhibited by) the data.\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nx\\n0.0\\n0.5\\n1.0\\n1.5\\ny\\nfitting linear models on a large dataset\\ntraining data\\nground truth h *\\nbest fit linear model\\nFigure 8.3:\\nThe best ﬁt linear\\nmodel on a much larger dataset\\nstill has a large training error.\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nx\\n0.0\\n0.5\\n1.0\\n1.5\\ny\\nfitting linear models on a noiseless dataset\\ntraining data\\nground truth h *\\nbest fit linear model\\nFigure 8.4:\\nThe best ﬁt linear\\nmodel on a noiseless dataset also\\nhas a large training/test error.\\nNext, we ﬁt a 5th-degree polynomial to the data. Figure 8.5 shows that\\nit fails to learn a good model either. However, the failure pattern is diﬀerent\\nfrom the linear model case. Speciﬁcally, even though the learnt 5th-degree'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 117}, page_content='117\\npolynomial did a very good job predicting y(i)’s from x(i)’s for training ex-\\namples, it does not work well on test examples (Figure 8.5). In other words,\\nthe model learnt from the training set does not generalize well to other test\\nexamples—the test error is high. Contrary to the behavior of linear models,\\nthe bias of the 5-th degree polynomials is small—if we were to ﬁt a 5-th de-\\ngree polynomial to an extremely large dataset, the resulting model would be\\nclose to a quadratic function and be accurate (Figure 8.6). This is because\\nthe family of 5-th degree polynomials contains all the quadratic functions\\n(setting θ5 = θ4 = θ3 = 0 results in a quadratic function), and, therefore,\\n5-th degree polynomials are in principle capable of capturing the structure\\nof the data.\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nx\\n0.0\\n0.5\\n1.0\\n1.5\\ny\\ntraining data\\nbest fit 5-th degree model\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nx\\n0.0\\n0.5\\n1.0\\n1.5\\ny\\ntest data\\nground truth h *\\nbest fit 5-th degree model\\nFigure 8.5: Best ﬁt 5-th degree polynomial has zero training error, but still\\nhas a large test error and does not recover the the ground truth. This is a\\nclassic situation of overﬁtting.\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nx\\n0.0\\n0.5\\n1.0\\n1.5\\ny\\ntraining data\\nbest fit 5-th degree model\\nground truth h *\\nfitting 5-th degree model on large dataset\\nFigure 8.6: The best ﬁt 5-th degree polynomial on a huge dataset nearly\\nrecovers the ground-truth—suggesting that the culprit in Figure 8.5 is the\\nvariance (or lack of data) but not bias.\\nThe failure of ﬁtting 5-th degree polynomials can be captured by another'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 118}, page_content='118\\ncomponent of the test error, called variance of a model ﬁtting procedure.\\nSpeciﬁcally, when ﬁtting a 5-th degree polynomial as in Figure 8.7, there is a\\nlarge risk that we’re ﬁtting patterns in the data that happened to be present\\nin our small, ﬁnite training set, but that do not reﬂect the wider pattern of\\nthe relationship between x and y. These “spurious” patterns in the training\\nset are (mostly) due to the observation noise ξ(i), and ﬁtting these spurious\\npatters results in a model with large test error. In this case, we say the model\\nhas a large variance.\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nx\\n0.0\\n0.5\\n1.0\\n1.5\\ny\\ntraining data\\nbest fit 5-th degree model\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nx\\n0.0\\n0.5\\n1.0\\n1.5\\ny\\ntraining data\\nbest fit 5-th degree model\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nx\\n0.0\\n0.5\\n1.0\\n1.5\\ny\\ntraining data\\nbest fit 5-th degree model\\nfitting 5-th degree model on different datasets\\nFigure 8.7: The best ﬁt 5-th degree models on three diﬀerent datasets gen-\\nerated from the same distribution behave quite diﬀerently, suggesting the\\nexistence of a large variance.\\nThe variance can be intuitively (and mathematically, as shown in Sec-\\ntion 8.1.1) characterized by the amount of variations across models learnt\\non multiple diﬀerent training datasets (drawn from the same underlying dis-\\ntribution). The “spurious patterns” are speciﬁc to the randomness of the\\nnoise (and inputs) in a particular dataset, and thus are diﬀerent across mul-\\ntiple training datasets. Therefore, overﬁtting to the “spurious patterns” of\\nmultiple datasets should result in very diﬀerent models. Indeed, as shown\\nin Figure 8.7, the models learned on the three diﬀerent training datasets are\\nquite diﬀerent, overﬁtting to the “spurious patterns” of each datasets.\\nOften, there is a tradeoﬀbetween bias and variance. If our model is too\\n“simple” and has very few parameters, then it may have large bias (but small\\nvariance), and it typically may suﬀer from underﬁttng. If it is too “complex”\\nand has very many parameters, then it may suﬀer from large variance (but\\nhave smaller bias), and thus overﬁtting. See Figure 8.8 for a typical tradeoﬀ\\nbetween bias and variance.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 119}, page_content='119\\nModel Complexity\\nError\\nBias2\\nVariance\\nTest Error (= Bias2 +Variance)\\nOptimal Tradeoff\\nFigure 8.8: An illustration of the typical bias-variance tradeoﬀ.\\nAs we will see formally in Section 8.1.1, the test error can be decomposed\\nas a summation of bias and variance. This means that the test error will\\nhave a convex curve as the model complexity increases, and in practice we\\nshould tune the model complexity to achieve the best tradeoﬀ. For instance,\\nin the example above, ﬁtting a quadratic function does better than either of\\nthe extremes of a ﬁrst or a 5-th degree polynomial, as shown in Figure 8.9.\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nx\\n0.0\\n0.5\\n1.0\\n1.5\\ny\\ntraining data\\nbest fit quadratic model\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nx\\n0.0\\n0.5\\n1.0\\n1.5\\ny\\ntest data\\nbest fit quadratic model\\nground truth h *\\nFigure 8.9: Best ﬁt quadratic model has small training and test error because\\nquadratic model achieves a better tradeoﬀ.\\nInterestingly, the bias-variance tradeoﬀcurves or the test error curves\\ndo not universally follow the shape in Figure 8.8, at least not universally\\nwhen the model complexity is simply measured by the number of parameters.\\n(We will discuss the so-called double descent phenomenon in Section 8.2.)\\nNevertheless, the principle of bias-variance tradeoﬀis perhaps still the ﬁrst\\nresort when analyzing and predicting the behavior of test errors.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 120}, page_content='120\\n8.1.1\\nA mathematical decomposition (for regression)\\nTo formally state the bias-variance tradeoﬀfor regression problems, we con-\\nsider the following setup (which is an extension of the beginning paragraph\\nof Section 8.1).\\n• Draw a training dataset S = {x(i), y(i)}n\\ni=1 such that y(i) = h⋆(x(i))+ξ(i)\\nwhere ξ(i) ∈N(0, σ2).\\n• Train a model on the dataset S, denoted by ˆhS.\\n• Take a test example (x, y) such that y = h⋆(x) + ξ where ξ ∼N(0, σ2),\\nand measure the expected test error (averaged over the random draw of\\nthe training set S and the randomness of ξ)56\\nMSE(x) = ES,ξ[(y −hS(x))2]\\n(8.2)\\nWe will decompose the MSE into a bias and variance term. We start by\\nstating a following simple mathematical tool that will be used twice below.\\nClaim 8.1.1: Suppose A and B are two independent real random variables\\nand E[A] = 0. Then, E[(A + B)2] = E[A2] + E[B2].\\nAs a corollary, because a random variable A is independent with a con-\\nstant c, when E[A] = 0, we have E[(A + c)2] = E[A2] + c2.\\nThe proof of the claim follows from expanding the square: E[(A + B)2] =\\nE[A2] + E[B2] + 2E[AB] = E[A2] + E[B2]. Here we used the independence to\\nshow that E[AB] = E[A]E[B] = 0.\\nUsing Claim 8.1.1 with A = ξ and B = h⋆(x) −ˆhS(x), we have\\nMSE(x) = E[(y −hS(x))2] = E[(ξ + (h⋆(x) −hS(x)))2]\\n(8.3)\\n= E[ξ2] + E[(h⋆(x) −hS(x))2] (by Claim 8.1.1)\\n= σ2 + E[(h⋆(x) −hS(x))2]\\n(8.4)\\nThen, let’s deﬁne havg(x) = ES[hS(x)] as the “average model”—the model\\nobtained by drawing an inﬁnite number of datasets, training on them, and\\naveraging their predictions on x. Note that havg is a hypothetical model for\\nanalytical purposes that can not be obtained in reality (because we don’t\\n5For simplicity, the test input x is considered to be ﬁxed here, but the same conceptual\\nmessage holds when we average over the choice of x’s.\\n6The subscript under the expectation symbol is to emphasize the variables that are\\nconsidered as random by the expectation operation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 121}, page_content='121\\nhave inﬁnite number of datasets). It turns out that for many cases, havg\\nis (approximately) equal to the the model obtained by training on a single\\ndataset with inﬁnite samples. Thus, we can also intuitively interpret havg this\\nway, which is consistent with our intuitive deﬁnition of bias in the previous\\nsubsection.\\nWe can further decompose MSE(x) by letting c = h⋆(x)−havg(x) (which is\\na constant that does not depend on the choice of S!) and A = havg(x)−hS(x)\\nin the corollary part of Claim 8.1.1:\\nMSE(x) = σ2 + E[(h⋆(x) −hS(x))2]\\n(8.5)\\n= σ2 + (h⋆(x) −havg(x))2 + E[(havg −hS(x))2]\\n(8.6)\\n=\\nσ2\\n|{z}\\nunavoidable\\n+ (h⋆(x) −havg(x))2\\n|\\n{z\\n}\\n≜bias2\\n+ var(hS(x))\\n|\\n{z\\n}\\n≜variance\\n(8.7)\\nWe call the second term the bias (square) and the third term the variance. As\\ndiscussed before, the bias captures the part of the error that are introduced\\ndue to the lack of expressivity of the model. Recall that havg can be thought\\nof as the best possible model learned even with inﬁnite data. Thus, the bias is\\nnot due to the lack of data, but is rather caused by that the family of models\\nfundamentally cannot approximate the h⋆. For example, in the illustrating\\nexample in Figure 8.2, because any linear model cannot approximate the\\ntrue quadratic function h⋆, neither can havg, and thus the bias term has to\\nbe large.\\nThe variance term captures how the random nature of the ﬁnite dataset\\nintroduces errors in the learned model. It measures the sensitivity of the\\nlearned model to the randomness in the dataset. It often decreases as the\\nsize of the dataset increases.\\nThere is nothing we can do about the ﬁrst term σ2 as we can not predict\\nthe noise ξ by deﬁnition.\\nFinally, we note that the bias-variance decomposition for classiﬁcation\\nis much less clear than for regression problems.\\nThere have been several\\nproposals, but there is as yet no agreement on what is the “right” and/or\\nthe most useful formalism.\\n8.2\\nThe double descent phenomenon\\nModel-wise double descent. Recent works have demonstrated that the\\ntest error can present a “double descent” phenomenon in a range of machine'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 122}, page_content='122\\nlearning models including linear models and deep neural networks.7\\nThe\\nconventional wisdom, as discussed in Section 8.1, is that as we increase the\\nmodel complexity, the test error ﬁrst decreases and then increases, as illus-\\ntrated in Figure 8.8. However, in many cases, we empirically observe that\\nthe test error can have a second descent—it ﬁrst decreases, then increases\\nto a peak around when the model size is large enough to ﬁt all the training\\ndata very well, and then decreases again in the so-called overparameterized\\nregime, where the number of parameters is larger than the number of data\\npoints. See Figure 8.10 for an illustration of the typical curves of test errors\\nagainst model complexity (measured by the number of parameters). To some\\nextent, the overparameterized regime with the second descent is considered as\\nnew to the machine learning community—partly because lightly-regularized,\\noverparameterized models are only extensively used in the deep learning era.\\nA practical implication of the phenomenon is that one should not hold back\\nfrom scaling into and experimenting with over-parametrized models because\\nthe test error may well decrease again to a level even smaller than the previ-\\nous lowest point. Actually, in many cases, larger overparameterized models\\nalways lead to a better test performance (meaning there won’t be a second\\nascent after the second descent).\\n# parameters\\ntest error\\ntypically when # parameters\\nis sufficient to fit the data\\nclassical regime:\\nbias-variance tradeoff\\nmodern regime:\\nover-parameterization\\nFigure 8.10: A typical model-wise double descent phenomenon. As the num-\\nber of parameters increases, the test error ﬁrst decreases when the number of\\nparameters is smaller than the training data. Then in the overparameterized\\nregime, the test error decreases again.\\n7The discovery of the phenomenon perhaps dates back to Opper [1995, 2001], and has\\nbeen recently popularized by Belkin et al. [2020], Hastie et al. [2019], etc.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 123}, page_content='123\\nSample-wise double descent.\\nA priori, we would expect that more\\ntraining examples always lead to smaller test errors—more samples give\\nstrictly more information for the algorithm to learn from. However, recent\\nwork [Nakkiran, 2019] observes that the test error is not monotonically de-\\ncreasing as we increase the sample size. Instead, as shown in Figure 8.11, the\\ntest error decreases, and then increases and peaks around when the number\\nof examples (denoted by n) is similar to the number of parameters (denoted\\nby d), and then decreases again. We refer to this as the sample-wise dou-\\nble descent phenomenon. To some extent, sample-wise double descent and\\nmodel-wise double descent are essentially describing similar phenomena—the\\ntest error is peaked when n ≈d.\\nExplanation and mitigation strategy.\\nThe sample-wise double descent,\\nor, in particular, the peak of test error at n ≈d, suggests that the existing\\ntraining algorithms evaluated in these experiments are far from optimal when\\nn ≈d. We will be better oﬀby tossing away some examples and run the\\nalgorithms with a smaller sample size to steer clear of the peak. In other\\nwords, in principle, there are other algorithms that can achieve smaller test\\nerror when n ≈d, but the algorithms evaluated in these experiments fail to\\ndo so. The sub-optimality of the learning procedure appears to be the culprit\\nof the peak in both sample-wise and model-wise double descent.\\nIndeed, with an optimally-tuned regularization (which will be discussed\\nmore in Section 9), the test error in the n ≈d regime can be dramatically\\nimproved, and the model-wise and sample-wise double descent are both mit-\\nigated. See Figure 8.11.\\nThe intuition above only explains the peak in the model-wise and sample-\\nwise double descent, but does not explain the second descent in the model-\\nwise double descent—why overparameterized models are able to generalize\\nso well. The theoretical understanding of overparameterized models is an ac-\\ntive research area with many recent advances. A typical explanation is that\\nthe commonly-used optimizers such as gradient descent provide an implicit\\nregularization eﬀect (which will be discussed in more detail in Section 9.2).\\nIn other words, even in the overparameterized regime and with an unregular-\\nized loss function, the model is still implicitly regularized, and thus exhibits\\na better test performance than an arbitrary solution that ﬁts the data. For\\nexample, for linear models, when n ≪d, the gradient descent optimizer with\\nzero initialization ﬁnds the minimum norm solution that ﬁts the data (in-\\nstead of an arbitrary solution that ﬁts the data), and the minimum norm reg-\\nularizer turns out to be a suﬃciently good for the overparameterized regime\\n(but it’s not a good regularizer when n ≈d, resulting in the peak of test'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 124}, page_content='124\\nerror).\\n0\\n200\\n400\\n600\\n800\\n1000\\nNum Samples\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\nT\\nest Error\\nT\\nest Error vs. # Samples\\nT\\nest Error\\nFigure 8.11:\\nLeft: The sample-wise double descent phenomenon for linear\\nmodels. Right: The sample-wise double descent with diﬀerent regularization\\nstrength for linear models. Using the optimal regularization parameter λ\\n(optimally tuned for each n, shown in green solid curve) mitigates double\\ndescent. Setup: The data distribution of (x, y) is x ∼N(0, Id) and y ∼\\nx⊤β + N(0, σ2) where d = 500, σ = 0.5 and ∥β∥2 = 1.8\\nFinally, we also remark that the double descent phenomenon has been\\nmostly observed when the model complexity is measured by the number of\\nparameters. It is unclear if and when the number of parameters is the best\\ncomplexity measure of a model. For example, in many situations, the norm\\nof the models is used as a complexity measure. As shown in Figure 8.12\\nright, for a particular linear case, if we plot the test error against the norm\\nof the learnt model, the double descent phenomenon no longer occurs. This\\nis partly because the norm of the learned model is also peaked around n ≈d\\n(See Figure 8.12 (middle) or Belkin et al. [2019], Mei and Montanari [2022],\\nand discussions in Section 10.8 of James et al. [2021]).\\nFor deep neural\\nnetworks, the correct complexity measure is even more elusive. The study of\\ndouble descent phenomenon is an active research topic.\\n8The ﬁgure is reproduced from Figure 1 of Nakkiran et al. [2020]. Similar phenomenon\\nare also observed in Hastie et al. [2022], Mei and Montanari [2022]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 125}, page_content='125\\n0\\n250\\n500\\n750\\n1000\\n# parameters\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\ntest error\\ntest error vs. # params\\n0\\n200\\n400\\n600\\n800\\n1000\\n# parameters\\n0\\n10\\n20\\n30\\n40\\nnorm\\nnorm vs. # params\\n0\\n10\\n20\\n30\\n40\\nnorm\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\ntest error\\nd=n\\n# parameters\\ntest error vs. norm\\n0\\n200\\n400\\n600\\n800\\n1000\\nFigure 8.12: Left: The double descent phenomenon, where the number of pa-\\nrameters is used as the model complexity. Middle: The norm of the learned\\nmodel is peaked around n ≈d. Right: The test error against the norm of\\nthe learnt model. The color bar indicate the number of parameters and the\\narrows indicates the direction of increasing model size. Their relationship\\nare closer to the convention wisdom than to a double descent. Setup: We\\nconsider a linear regression with a ﬁxed dataset of size n = 500. The input\\nx is a random ReLU feature on Fashion-MNIST, and output y ∈R10 is the\\none-hot label. This is the same setting as in Section 5.2 of Nakkiran et al.\\n[2020].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 126}, page_content='126\\n8.3\\nSample\\ncomplexity\\nbounds\\n(optional\\nreadings)\\n8.3.1\\nPreliminaries\\nIn this set of notes, we begin our foray into learning theory. Apart from\\nbeing interesting and enlightening in its own right, this discussion will also\\nhelp us hone our intuitions and derive rules of thumb about how to best\\napply learning algorithms in diﬀerent settings. We will also seek to answer\\na few questions: First, can we make formal the bias/variance tradeoﬀthat\\nwas just discussed? This will also eventually lead us to talk about model\\nselection methods, which can, for instance, automatically decide what order\\npolynomial to ﬁt to a training set. Second, in machine learning it’s really\\ngeneralization error that we care about, but most learning algorithms ﬁt their\\nmodels to the training set. Why should doing well on the training set tell us\\nanything about generalization error? Speciﬁcally, can we relate error on the\\ntraining set to generalization error? Third and ﬁnally, are there conditions\\nunder which we can actually prove that learning algorithms will work well?\\nWe start with two simple but very useful lemmas.\\nLemma. (The union bound). Let A1, A2, . . . , Ak be k diﬀerent events (that\\nmay not be independent). Then\\nP(A1 ∪· · · ∪Ak) ≤P(A1) + . . . + P(Ak).\\nIn probability theory, the union bound is usually stated as an axiom\\n(and thus we won’t try to prove it), but it also makes intuitive sense: The\\nprobability of any one of k events happening is at most the sum of the\\nprobabilities of the k diﬀerent events.\\nLemma. (Hoeﬀding inequality) Let Z1, . . . , Zn be n independent and iden-\\ntically distributed (iid) random variables drawn from a Bernoulli(φ) distri-\\nbution. I.e., P(Zi = 1) = φ, and P(Zi = 0) = 1 −φ. Let ˆφ = (1/n) Pn\\ni=1 Zi\\nbe the mean of these random variables, and let any γ > 0 be ﬁxed. Then\\nP(|φ −ˆφ| > γ) ≤2 exp(−2γ2n)\\nThis lemma (which in learning theory is also called the Chernoﬀbound)\\nsays that if we take ˆφ—the average of n Bernoulli(φ) random variables—to\\nbe our estimate of φ, then the probability of our being far from the true value\\nis small, so long as n is large. Another way of saying this is that if you have\\na biased coin whose chance of landing on heads is φ, then if you toss it n'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 127}, page_content='127\\ntimes and calculate the fraction of times that it came up heads, that will be\\na good estimate of φ with high probability (if n is large).\\nUsing just these two lemmas, we will be able to prove some of the deepest\\nand most important results in learning theory.\\nTo simplify our exposition, let’s restrict our attention to binary classiﬁca-\\ntion in which the labels are y ∈{0, 1}. Everything we’ll say here generalizes\\nto other problems, including regression and multi-class classiﬁcation.\\nWe assume we are given a training set S = {(x(i), y(i)); i = 1, . . . , n} of size\\nn, where the training examples (x(i), y(i)) are drawn iid from some probability\\ndistribution D. For a hypothesis h, we deﬁne the training error (also called\\nthe empirical risk or empirical error in learning theory) to be\\nˆε(h) = 1\\nn\\nn\\nX\\ni=1\\n1{h(x(i)) ̸= y(i)}.\\nThis is just the fraction of training examples that h misclassiﬁes. When we\\nwant to make explicit the dependence of ˆε(h) on the training set S, we may\\nalso write this a ˆεS(h). We also deﬁne the generalization error to be\\nε(h) = P(x,y)∼D(h(x) ̸= y).\\nI.e. this is the probability that, if we now draw a new example (x, y) from\\nthe distribution D, h will misclassify it.\\nNote that we have assumed that the training data was drawn from the\\nsame distribution D with which we’re going to evaluate our hypotheses (in\\nthe deﬁnition of generalization error). This is sometimes also referred to as\\none of the PAC assumptions.9\\nConsider the setting of linear classiﬁcation, and let hθ(x) = 1{θTx ≥0}.\\nWhat’s a reasonable way of ﬁtting the parameters θ? One approach is to try\\nto minimize the training error, and pick\\nˆθ = arg min\\nθ\\nˆε(hθ).\\nWe call this process empirical risk minimization (ERM), and the resulting\\nhypothesis output by the learning algorithm is ˆh = hˆθ. We think of ERM\\nas the most “basic” learning algorithm, and it will be this algorithm that we\\n9PAC stands for “probably approximately correct,” which is a framework and set of\\nassumptions under which numerous results on learning theory were proved. Of these, the\\nassumption of training and testing on the same distribution, and the assumption of the\\nindependently drawn training examples, were the most important.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 128}, page_content='128\\nfocus on in these notes. (Algorithms such as logistic regression can also be\\nviewed as approximations to empirical risk minimization.)\\nIn our study of learning theory, it will be useful to abstract away from\\nthe speciﬁc parameterization of hypotheses and from issues such as whether\\nwe’re using a linear classiﬁer. We deﬁne the hypothesis class H used by a\\nlearning algorithm to be the set of all classiﬁers considered by it. For linear\\nclassiﬁcation, H = {hθ : hθ(x) = 1{θTx ≥0}, θ ∈Rd+1} is thus the set of\\nall classiﬁers over X (the domain of the inputs) where the decision boundary\\nis linear. More broadly, if we were studying, say, neural networks, then we\\ncould let H be the set of all classiﬁers representable by some neural network\\narchitecture.\\nEmpirical risk minimization can now be thought of as a minimization over\\nthe class of functions H, in which the learning algorithm picks the hypothesis:\\nˆh = arg min\\nh∈H ˆε(h)\\n8.3.2\\nThe case of ﬁnite H\\nLet’s start by considering a learning problem in which we have a ﬁnite hy-\\npothesis class H = {h1, . . . , hk} consisting of k hypotheses. Thus, H is just a\\nset of k functions mapping from X to {0, 1}, and empirical risk minimization\\nselects ˆh to be whichever of these k functions has the smallest training error.\\nWe would like to give guarantees on the generalization error of ˆh. Our\\nstrategy for doing so will be in two parts: First, we will show that ˆε(h) is a\\nreliable estimate of ε(h) for all h. Second, we will show that this implies an\\nupper-bound on the generalization error of ˆh.\\nTake any one, ﬁxed, hi ∈H. Consider a Bernoulli random variable Z\\nwhose distribution is deﬁned as follows. We’re going to sample (x, y) ∼D.\\nThen, we set Z = 1{hi(x) ̸= y}. I.e., we’re going to draw one example,\\nand let Z indicate whether hi misclassiﬁes it. Similarly, we also deﬁne Zj =\\n1{hi(x(j)) ̸= y(j)}. Since our training set was drawn iid from D, Z and the\\nZj’s have the same distribution.\\nWe see that the misclassiﬁcation probability on a randomly drawn\\nexample—that is, ε(h)—is exactly the expected value of Z (and Zj). More-\\nover, the training error can be written\\nˆε(hi) = 1\\nn\\nn\\nX\\nj=1\\nZj.\\nThus, ˆε(hi) is exactly the mean of the n random variables Zj that are drawn\\niid from a Bernoulli distribution with mean ε(hi). Hence, we can apply the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 129}, page_content='129\\nHoeﬀding inequality, and obtain\\nP(|ε(hi) −ˆε(hi)| > γ) ≤2 exp(−2γ2n).\\nThis shows that, for our particular hi, training error will be close to\\ngeneralization error with high probability, assuming n is large. But we don’t\\njust want to guarantee that ε(hi) will be close to ˆε(hi) (with high probability)\\nfor just only one particular hi.\\nWe want to prove that this will be true\\nsimultaneously for all h ∈H. To do so, let Ai denote the event that |ε(hi) −\\nˆε(hi)| > γ. We’ve already shown that, for any particular Ai, it holds true\\nthat P(Ai) ≤2 exp(−2γ2n). Thus, using the union bound, we have that\\nP(∃h ∈H.|ε(hi) −ˆε(hi)| > γ)\\n=\\nP(A1 ∪· · · ∪Ak)\\n≤\\nk\\nX\\ni=1\\nP(Ai)\\n≤\\nk\\nX\\ni=1\\n2 exp(−2γ2n)\\n=\\n2k exp(−2γ2n)\\nIf we subtract both sides from 1, we ﬁnd that\\nP(¬∃h ∈H.|ε(hi) −ˆε(hi)| > γ)\\n=\\nP(∀h ∈H.|ε(hi) −ˆε(hi)| ≤γ)\\n≥\\n1 −2k exp(−2γ2n)\\n(The “¬” symbol means “not.”)\\nSo, with probability at least 1 −\\n2k exp(−2γ2n), we have that ε(h) will be within γ of ˆε(h) for all h ∈H.\\nThis is called a uniform convergence result, because this is a bound that\\nholds simultaneously for all (as opposed to just one) h ∈H.\\nIn the discussion above, what we did was, for particular values of n and\\nγ, give a bound on the probability that for some h ∈H, |ε(h) −ˆε(h)| > γ.\\nThere are three quantities of interest here: n, γ, and the probability of error;\\nwe can bound either one in terms of the other two.\\nFor instance, we can ask the following question: Given γ and some δ > 0,\\nhow large must n be before we can guarantee that with probability at least\\n1 −δ, training error will be within γ of generalization error? By setting\\nδ = 2k exp(−2γ2n) and solving for n, [you should convince yourself this is\\nthe right thing to do!], we ﬁnd that if\\nn ≥\\n1\\n2γ2 log 2k\\nδ ,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 130}, page_content='130\\nthen with probability at least 1 −δ, we have that |ε(h) −ˆε(h)| ≤γ for all\\nh ∈H. (Equivalently, this shows that the probability that |ε(h) −ˆε(h)| > γ\\nfor some h ∈H is at most δ.)\\nThis bound tells us how many training\\nexamples we need in order make a guarantee. The training set size n that\\na certain method or algorithm requires in order to achieve a certain level of\\nperformance is also called the algorithm’s sample complexity.\\nThe key property of the bound above is that the number of training\\nexamples needed to make this guarantee is only logarithmic in k, the number\\nof hypotheses in H. This will be important later.\\nSimilarly, we can also hold n and δ ﬁxed and solve for γ in the previous\\nequation, and show [again, convince yourself that this is right!] that with\\nprobability 1 −δ, we have that for all h ∈H,\\n|ˆε(h) −ε(h)| ≤\\nr\\n1\\n2n log 2k\\nδ .\\nNow, let’s assume that uniform convergence holds, i.e., that |ε(h)−ˆε(h)| ≤\\nγ for all h ∈H. What can we prove about the generalization of our learning\\nalgorithm that picked ˆh = arg minh∈H ˆε(h)?\\nDeﬁne h∗= arg minh∈H ε(h) to be the best possible hypothesis in H. Note\\nthat h∗is the best that we could possibly do given that we are using H, so\\nit makes sense to compare our performance to that of h∗. We have:\\nε(ˆh)\\n≤\\nˆε(ˆh) + γ\\n≤\\nˆε(h∗) + γ\\n≤\\nε(h∗) + 2γ\\nThe ﬁrst line used the fact that |ε(ˆh)−ˆε(ˆh)| ≤γ (by our uniform convergence\\nassumption). The second used the fact that ˆh was chosen to minimize ˆε(h),\\nand hence ˆε(ˆh) ≤ˆε(h) for all h, and in particular ˆε(ˆh) ≤ˆε(h∗). The third\\nline used the uniform convergence assumption again, to show that ˆε(h∗) ≤\\nε(h∗) + γ. So, what we’ve shown is the following: If uniform convergence\\noccurs, then the generalization error of ˆh is at most 2γ worse than the best\\npossible hypothesis in H!\\nLet’s put all this together into a theorem.\\nTheorem. Let |H| = k, and let any n, δ be ﬁxed. Then with probability at\\nleast 1 −δ, we have that\\nε(ˆh) ≤\\n\\x12\\nmin\\nh∈H ε(h)\\n\\x13\\n+ 2\\nr\\n1\\n2n log 2k\\nδ .'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 131}, page_content='131\\nThis is proved by letting γ equal the √· term, using our previous argu-\\nment that uniform convergence occurs with probability at least 1 −δ, and\\nthen noting that uniform convergence implies ε(h) is at most 2γ higher than\\nε(h∗) = minh∈H ε(h) (as we showed previously).\\nThis also quantiﬁes what we were saying previously saying about the\\nbias/variance tradeoﬀin model selection. Speciﬁcally, suppose we have some\\nhypothesis class H, and are considering switching to some much larger hy-\\npothesis class H′ ⊇H. If we switch to H′, then the ﬁrst term minh ε(h)\\ncan only decrease (since we’d then be taking a min over a larger set of func-\\ntions). Hence, by learning using a larger hypothesis class, our “bias” can\\nonly decrease. However, if k increases, then the second 2√· term would also\\nincrease. This increase corresponds to our “variance” increasing when we use\\na larger hypothesis class.\\nBy holding γ and δ ﬁxed and solving for n like we did before, we can also\\nobtain the following sample complexity bound:\\nCorollary.\\nLet |H| = k, and let any δ, γ be ﬁxed.\\nThen for ε(ˆh) ≤\\nminh∈H ε(h) + 2γ to hold with probability at least 1 −δ, it suﬃces that\\nn\\n≥\\n1\\n2γ2 log 2k\\nδ\\n=\\nO\\n\\x12 1\\nγ2 log k\\nδ\\n\\x13\\n,\\n8.3.3\\nThe case of inﬁnite H\\nWe have proved some useful theorems for the case of ﬁnite hypothesis classes.\\nBut many hypothesis classes, including any parameterized by real numbers\\n(as in linear classiﬁcation) actually contain an inﬁnite number of functions.\\nCan we prove similar results for this setting?\\nLet’s start by going through something that is not the “right” argument.\\nBetter and more general arguments exist, but this will be useful for honing\\nour intuitions about the domain.\\nSuppose we have an H that is parameterized by d real numbers. Since we\\nare using a computer to represent real numbers, and IEEE double-precision\\nﬂoating point (double’s in C) uses 64 bits to represent a ﬂoating point num-\\nber, this means that our learning algorithm, assuming we’re using double-\\nprecision ﬂoating point, is parameterized by 64d bits. Thus, our hypothesis\\nclass really consists of at most k = 264d diﬀerent hypotheses. From the Corol-\\nlary at the end of the previous section, we therefore ﬁnd that, to guarantee'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 132}, page_content='132\\nε(ˆh) ≤ε(h∗)+2γ, with to hold with probability at least 1−δ, it suﬃces that\\nn ≥O\\n\\x10\\n1\\nγ2 log 264d\\nδ\\n\\x11\\n= O\\n\\x10\\nd\\nγ2 log 1\\nδ\\n\\x11\\n= Oγ,δ(d). (The γ, δ subscripts indicate\\nthat the last big-O is hiding constants that may depend on γ and δ.) Thus,\\nthe number of training examples needed is at most linear in the parameters\\nof the model.\\nThe fact that we relied on 64-bit ﬂoating point makes this argument not\\nentirely satisfying, but the conclusion is nonetheless roughly correct: If what\\nwe try to do is minimize training error, then in order to learn “well” using a\\nhypothesis class that has d parameters, generally we’re going to need on the\\norder of a linear number of training examples in d.\\n(At this point, it’s worth noting that these results were proved for an al-\\ngorithm that uses empirical risk minimization. Thus, while the linear depen-\\ndence of sample complexity on d does generally hold for most discriminative\\nlearning algorithms that try to minimize training error or some approxima-\\ntion to training error, these conclusions do not always apply as readily to\\ndiscriminative learning algorithms. Giving good theoretical guarantees on\\nmany non-ERM learning algorithms is still an area of active research.)\\nThe other part of our previous argument that’s slightly unsatisfying is\\nthat it relies on the parameterization of H. Intuitively, this doesn’t seem like\\nit should matter: We had written the class of linear classiﬁers as hθ(x) =\\n1{θ0 + θ1x1 + · · · θdxd ≥0}, with n + 1 parameters θ0, . . . , θd. But it could\\nalso be written hu,v(x) = 1{(u2\\n0 −v2\\n0) + (u2\\n1 −v2\\n1)x1 + · · · (u2\\nd −v2\\nd)xd ≥0}\\nwith 2d + 2 parameters ui, vi. Yet, both of these are just deﬁning the same\\nH: The set of linear classiﬁers in d dimensions.\\nTo derive a more satisfying argument, let’s deﬁne a few more things.\\nGiven a set S = {x(i), . . . , x(D)} (no relation to the training set) of points\\nx(i) ∈X, we say that H shatters S if H can realize any labeling on S.\\nI.e., if for any set of labels {y(1), . . . , y(D)}, there exists some h ∈H so that\\nh(x(i)) = y(i) for all i = 1, . . . D.\\nGiven a hypothesis class H, we then deﬁne its Vapnik-Chervonenkis\\ndimension, written VC(H), to be the size of the largest set that is shattered\\nby H. (If H can shatter arbitrarily large sets, then VC(H) = ∞.)\\nFor instance, consider the following set of three points:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 133}, page_content='133\\n\\x00\\x01\\n\\x00\\x01\\n\\x00\\x01\\nx\\nx1\\n2\\nCan the set H of linear classiﬁers in two dimensions (h(x) = 1{θ0+θ1x1+\\nθ2x2 ≥0}) can shatter the set above? The answer is yes. Speciﬁcally, we\\nsee that, for any of the eight possible labelings of these points, we can ﬁnd a\\nlinear classiﬁer that obtains “zero training error” on them:\\nx\\nx1\\n2\\nx\\nx1\\n2\\nx\\nx1\\n2\\nx\\nx1\\n2\\nx\\nx1\\n2\\nx\\nx1\\n2\\nx\\nx1\\n2\\nx\\nx1\\n2\\nMoreover, it is possible to show that there is no set of 4 points that this\\nhypothesis class can shatter. Thus, the largest set that H can shatter is of\\nsize 3, and hence VC(H) = 3.\\nNote that the VC dimension of H here is 3 even though there may be\\nsets of size 3 that it cannot shatter. For instance, if we had a set of three\\npoints lying in a straight line (left ﬁgure), then there is no way to ﬁnd a linear\\nseparator for the labeling of the three points shown below (right ﬁgure):'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 134}, page_content='134\\nx\\nx1\\n2\\n\\x00\\x01\\n\\x00\\x01\\n\\x00\\x01\\nx\\nx1\\n2\\nIn order words, under the deﬁnition of the VC dimension, in order to\\nprove that VC(H) is at least D, we need to show only that there’s at least\\none set of size D that H can shatter.\\nThe following theorem, due to Vapnik, can then be shown. (This is, many\\nwould argue, the most important theorem in all of learning theory.)\\nTheorem. Let H be given, and let D = VC(H). Then with probability at\\nleast 1 −δ, we have that for all h ∈H,\\n|ε(h) −ˆε(h)| ≤O\\n r\\nD\\nn log n\\nD + 1\\nn log 1\\nδ\\n!\\n.\\nThus, with probability at least 1 −δ, we also have that:\\nε(ˆh) ≤ε(h∗) + O\\n r\\nD\\nn log n\\nD + 1\\nn log 1\\nδ\\n!\\n.\\nIn other words, if a hypothesis class has ﬁnite VC dimension, then uniform\\nconvergence occurs as n becomes large. As before, this allows us to give a\\nbound on ε(h) in terms of ε(h∗). We also have the following corollary:\\nCorollary. For |ε(h) −ˆε(h)| ≤γ to hold for all h ∈H (and hence ε(ˆh) ≤\\nε(h∗) + 2γ) with probability at least 1 −δ, it suﬃces that n = Oγ,δ(D).\\nIn other words, the number of training examples needed to learn “well”\\nusing H is linear in the VC dimension of H. It turns out that, for “most”\\nhypothesis classes, the VC dimension (assuming a “reasonable” parameter-\\nization) is also roughly linear in the number of parameters. Putting these\\ntogether, we conclude that for a given hypothesis class H (and for an algo-\\nrithm that tries to minimize training error), the number of training examples\\nneeded to achieve generalization error close to that of the optimal classiﬁer\\nis usually roughly linear in the number of parameters of H.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 135}, page_content='Chapter 9\\nRegularization and model\\nselection\\n9.1\\nRegularization\\nRecall that as discussed in Section 8.1, overftting is typically a result of using\\ntoo complex models, and we need to choose a proper model complexity to\\nachieve the optimal bias-variance tradeoﬀ. When the model complexity is\\nmeasured by the number of parameters, we can vary the size of the model\\n(e.g., the width of a neural net). However, the correct, informative complex-\\nity measure of the models can be a function of the parameters (e.g., ℓ2 norm\\nof the parameters), which may not necessarily depend on the number of pa-\\nrameters. In such cases, we will use regularization, an important technique\\nin machine learning, control the model complexity and prevent overﬁtting.\\nRegularization typically involves adding an additional term, called a reg-\\nularizer and denoted by R(θ) here, to the training loss/cost function:\\nJλ(θ) = J(θ) + λR(θ)\\n(9.1)\\nHere Jλ is often called the regularized loss, and λ ≥0 is called the regular-\\nization parameter. The regularizer R(θ) is a nonnegative function (in almost\\nall cases). In classical methods, R(θ) is purely a function of the parameter θ,\\nbut some modern approach allows R(θ) to depend on the training dataset.1\\nThe regularizer R(θ) is typically chosen to be some measure of the com-\\nplexity of the model θ. Thus, when using the regularized loss, we aim to\\nﬁnd a model that both ﬁt the data (a small loss J(θ)) and have a small\\n1Here our notations generally omit the dependency on the training dataset for\\nsimplicity—we write J(θ) even though it obviously needs to depend on the training dataset.\\n135'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 136}, page_content='136\\nmodel complexity (a small R(θ)). The balance between the two objectives is\\ncontrolled by the regularization parameter λ. When λ = 0, the regularized\\nloss is equivalent to the original loss. When λ is a suﬃciently small positive\\nnumber, minimizing the regularized loss is eﬀectively minimizing the original\\nloss with the regularizer as the tie-breaker. When the regularizer is extremely\\nlarge, then the original loss is not eﬀective (and likely the model will have a\\nlarge bias.)\\nThe most commonly used regularization is perhaps ℓ2 regularization,\\nwhere R(θ) =\\n1\\n2∥θ∥2\\n2.\\nIt encourages the optimizer to ﬁnd a model with\\nsmall ℓ2 norm. In deep learning, it’s oftentimes referred to as weight de-\\ncay, because gradient descent with learning rate η on the regularized loss\\nRλ(θ) is equivalent to shrinking/decaying θ by a scalar factor of 1 −ηλ and\\nthen applying the standard gradient\\nθ ←θ −η∇Jλ(θ) = θ −ηλθ −η∇J(θ)\\n=\\n(1 −λη)θ\\n|\\n{z\\n}\\ndecaying weights\\n−η∇J(θ)\\n(9.2)\\nBesides encouraging simpler models, regularization can also impose in-\\nductive biases or structures on the model parameters. For example, suppose\\nwe had a prior belief that the number of non-zeros in the ground-truth model\\nparameters is small,2—which is oftentimes called sparsity of the model—, we\\ncan impose a regularization on the number of non-zeros in θ, denoted by\\n∥θ∥0, to leverage such a prior belief. Imposing additional structure of the\\nparameters narrows our search space and makes the complexity of the model\\nfamily smaller,—e.g., the family of sparse models can be thought of as having\\nlower complexity than the family of all models—, and thus tends to lead to a\\nbetter generalization. On the other hand, imposing additional structure may\\nrisk increasing the bias. For example, if we regularize the sparsity strongly\\nbut no sparse models can predict the label accurately, we will suﬀer from\\nlarge bias (analogously to the situation when we use linear models to learn\\ndata than can only be represented by quadratic functions in Section 8.1.)\\nThe sparsity of the parameters is not a continuous function of the param-\\neters, and thus we cannot optimize it with (stochastic) gradient descent. A\\ncommon relaxation is to use R(θ) = ∥θ∥1 as a continuous surrogate.3\\n2For linear models, this means the model just uses a few coordinates of the inputs to\\nmake an accurate prediction.\\n3There has been a rich line of theoretical work that explains why ∥θ∥1 is a good sur-\\nrogate for encouraging sparsity, but it’s beyond the scope of this course. An intuition is:\\nassuming the parameter is on the unit sphere, the parameter with smallest ℓ1 norm also'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 137}, page_content='137\\nThe R(θ) = ∥θ∥1 (also called LASSO) and R(θ) =\\n1\\n2∥θ∥2\\n2 are perhaps\\namong the most commonly used regularizers for linear models. Other norm\\nand powers of norms are sometimes also used. The ℓ2 norm regularization is\\nmuch more commonly used with kernel methods because ℓ1 regularization is\\ntypically not compatible with the kernel trick (the optimal solution cannot\\nbe written as functions of inner products of features.)\\nIn deep learning, the most commonly used regularizer is ℓ2 regularization\\nor weight decay. Other common ones include dropout, data augmentation,\\nregularizing the spectral norm of the weight matrices, and regularizing the\\nLipschitzness of the model, etc. Regularization in deep learning is an ac-\\ntive research area, and it’s known that there is another implicit source of\\nregularization, as discussed in the next section.\\n9.2\\nImplicit regularization eﬀect (optional\\nreading)\\nThe implicit regularization eﬀect of optimizers, or implicit bias or algorithmic\\nregularization, is a new concept/phenomenon observed in the deep learning\\nera. It largely refers to that the optimizers can implicitly impose structures\\non parameters beyond what has been imposed by the regularized loss.\\nIn most classical settings, the loss or regularized loss has a unique global\\nminimum, and thus any reasonable optimizer should converge to that global\\nminimum and cannot impose any additional preferences. However, in deep\\nlearning, oftentimes the loss or regularized loss has more than one (approx-\\nimate) global minima, and diﬀerence optimizers may converge to diﬀerent\\nglobal minima. Though these global minima have the same or similar train-\\ning losses, they may be of diﬀerent nature and have dramatically diﬀerent\\ngeneralization performance. See Figures 9.1 and 9.2 and its caption for an\\nillustration and some experiment results. For example, it’s possible that one\\nglobal minimum gives a much more Lipschitz or sparse model than others\\nand thus has a better test error. It turns out that many commonly-used op-\\ntimizers (or their components) prefer or bias towards ﬁnding global minima\\nof certain properties, leading to a better test performance.\\nhappen to be the sparsest parameter with only 1 non-zero coordinate. Thus, sparsity and\\nℓ1 norm gives the same extremal points to some extent.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 138}, page_content='138\\nθ\\nloss\\nFigure 9.1: An Illustration that diﬀerent global minima of the training loss\\ncan have diﬀerent test performance.\\nFigure 9.2: Left: Performance of neural networks trained by two diﬀerent\\nlearning rates schedules on the CIFAR-10 dataset. Although both exper-\\niments used exactly the same regularized losses and the optimizers ﬁt the\\ntraining data perfectly, the models’ generalization performance diﬀer much.\\nRight: On a diﬀerent synthetic dataset, optimizers with diﬀerent initializa-\\ntions have the same training error but diﬀerent generalization performance.4\\nIn summary, the takehome message here is that the choice of optimizer\\ndoes not only aﬀect minimizing the training loss, but also imposes implicit\\nregularization and aﬀects the generalization of the model. Even if your cur-\\nrent optimizer already converges to a small training error perfectly, you may\\nstill need to tune your optimizer for a better generalization, .\\n4The setting is the same as in Woodworth et al. [2020], HaoChen et al. [2020]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 139}, page_content='139\\nOne may wonder which components of the optimizers bias towards what\\ntype of global minima and what type of global minima may generalize bet-\\nter.\\nThese are open questions that researchers are actively investigating.\\nEmpirical and theoretical research have oﬀered some clues and heuristics.\\nIn many (but deﬁnitely far from all) situations, among those setting where\\noptimization can succeed in minimizing the training loss, the use of larger\\ninitial learning rate, smaller initialization, smaller batch size, and momen-\\ntum appears to help with biasing towards more generalizable solutions. A\\nconjecture (that can be proven in certain simpliﬁed case) is that stochas-\\nticity in the optimization process help the optimizer to ﬁnd ﬂatter global\\nminima (global minima where the curvature of the loss is small), and ﬂat\\nglobal minima tend to give more Lipschitz models and better generalization.\\nCharacterizing the implicit regularization eﬀect formally is still a challenging\\nopen research question.\\n9.3\\nModel selection via cross validation\\nSuppose we are trying select among several diﬀerent models for a learning\\nproblem. For instance, we might be using a polynomial regression model\\nhθ(x) = g(θ0 + θ1x + θ2x2 + · · · + θkxk), and wish to decide if k should be\\n0, 1, . . . , or 10. How can we automatically select a model that represents\\na good tradeoﬀbetween the twin evils of bias and variance5? Alternatively,\\nsuppose we want to automatically choose the bandwidth parameter τ for\\nlocally weighted regression, or the parameter C for our ℓ1-regularized SVM.\\nHow can we do that?\\nFor the sake of concreteness, in these notes we assume we have some\\nﬁnite set of models M = {M1, . . . , Md} that we’re trying to select among.\\nFor instance, in our ﬁrst example above, the model Mi would be an i-th\\ndegree polynomial regression model. (The generalization to inﬁnite M is\\nnot hard.6) Alternatively, if we are trying to decide between using an SVM,\\na neural network or logistic regression, then M may contain these models.\\n5Given that we said in the previous set of notes that bias and variance are two very\\ndiﬀerent beasts, some readers may be wondering if we should be calling them “twin” evils\\nhere. Perhaps it’d be better to think of them as non-identical twins. The phrase “the\\nfraternal twin evils of bias and variance” doesn’t have the same ring to it, though.\\n6If we are trying to choose from an inﬁnite set of models, say corresponding to the\\npossible values of the bandwidth τ ∈R+, we may discretize τ and consider only a ﬁnite\\nnumber of possible values for it. More generally, most of the algorithms described here\\ncan all be viewed as performing optimization search in the space of models, and we can\\nperform this search over inﬁnite model classes as well.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 140}, page_content='140\\nCross validation. Lets suppose we are, as usual, given a training set S.\\nGiven what we know about empirical risk minimization, here’s what might\\ninitially seem like a algorithm, resulting from using empirical risk minimiza-\\ntion for model selection:\\n1. Train each model Mi on S, to get some hypothesis hi.\\n2. Pick the hypotheses with the smallest training error.\\nThis algorithm does not work. Consider choosing the degree of a poly-\\nnomial. The higher the degree of the polynomial, the better it will ﬁt the\\ntraining set S, and thus the lower the training error. Hence, this method will\\nalways select a high-variance, high-degree polynomial model, which we saw\\npreviously is often poor choice.\\nHere’s an algorithm that works better. In hold-out cross validation\\n(also called simple cross validation), we do the following:\\n1. Randomly split S into Strain (say, 70% of the data) and Scv (the remain-\\ning 30%). Here, Scv is called the hold-out cross validation set.\\n2. Train each model Mi on Strain only, to get some hypothesis hi.\\n3. Select and output the hypothesis hi that had the smallest error ˆεScv(hi)\\non the hold out cross validation set. (Here ˆεScv(h) denotes the average\\nerror of h on the set of examples in Scv.) The error on the hold out\\nvalidation set is also referred to as the validation error.\\nBy testing/validating on a set of examples Scv that the models were not\\ntrained on, we obtain a better estimate of each hypothesis hi’s true general-\\nization/test error. Thus, this approach is essentially picking the model with\\nthe smallest estimated generalization/test error. The size of the validation\\nset depends on the total number of available examples. Usually, somewhere\\nbetween 1/4−1/3 of the data is used in the hold out cross validation set, and\\n30% is a typical choice. However, when the total dataset is huge, validation\\nset can be a smaller fraction of the total examples as long as the absolute\\nnumber of validation examples is decent. For example, for the ImageNet\\ndataset that has about 1M training images, the validation set is sometimes\\nset to be 50K images, which is only about 5% of the total examples.\\nOptionally, step 3 in the algorithm may also be replaced with selecting\\nthe model Mi according to arg mini ˆεScv(hi), and then retraining Mi on the\\nentire training set S. (This is often a good idea, with one exception being\\nlearning algorithms that are be very sensitive to perturbations of the initial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 141}, page_content='141\\nconditions and/or data. For these methods, Mi doing well on Strain does not\\nnecessarily mean it will also do well on Scv, and it might be better to forgo\\nthis retraining step.)\\nThe disadvantage of using hold out cross validation is that it “wastes”\\nabout 30% of the data. Even if we were to take the optional step of retraining\\nthe model on the entire training set, it’s still as if we’re trying to ﬁnd a good\\nmodel for a learning problem in which we had 0.7n training examples, rather\\nthan n training examples, since we’re testing models that were trained on\\nonly 0.7n examples each time. While this is ﬁne if data is abundant and/or\\ncheap, in learning problems in which data is scarce (consider a problem with\\nn = 20, say), we’d like to do something better.\\nHere is a method, called k-fold cross validation, that holds out less\\ndata each time:\\n1. Randomly split S into k disjoint subsets of m/k training examples each.\\nLets call these subsets S1, . . . , Sk.\\n2. For each model Mi, we evaluate it as follows:\\nFor j = 1, . . . , k\\nTrain the model Mi on S1 ∪· · · ∪Sj−1 ∪Sj+1 ∪· · · Sk (i.e., train\\non all the data except Sj) to get some hypothesis hij.\\nTest the hypothesis hij on Sj, to get ˆεSj(hij).\\nThe estimated generalization error of model Mi is then calculated\\nas the average of the ˆεSj(hij)’s (averaged over j).\\n3. Pick the model Mi with the lowest estimated generalization error, and\\nretrain that model on the entire training set S. The resulting hypothesis\\nis then output as our ﬁnal answer.\\nA typical choice for the number of folds to use here would be k = 10.\\nWhile the fraction of data held out each time is now 1/k—much smaller\\nthan before—this procedure may also be more computationally expensive\\nthan hold-out cross validation, since we now need train to each model k\\ntimes.\\nWhile k = 10 is a commonly used choice, in problems in which data is\\nreally scarce, sometimes we will use the extreme choice of k = m in order\\nto leave out as little data as possible each time. In this setting, we would\\nrepeatedly train on all but one of the training examples in S, and test on that\\nheld-out example. The resulting m = k errors are then averaged together to\\nobtain our estimate of the generalization error of a model. This method has'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 142}, page_content='142\\nits own name; since we’re holding out one training example at a time, this\\nmethod is called leave-one-out cross validation.\\nFinally, even though we have described the diﬀerent versions of cross vali-\\ndation as methods for selecting a model, they can also be used more simply to\\nevaluate a single model or algorithm. For example, if you have implemented\\nsome learning algorithm and want to estimate how well it performs for your\\napplication (or if you have invented a novel learning algorithm and want to\\nreport in a technical paper how well it performs on various test sets), cross\\nvalidation would give a reasonable way of doing so.\\n9.4\\nBayesian statistics and regularization\\nIn this section, we will talk about one more tool in our arsenal for our battle\\nagainst overﬁtting.\\nAt the beginning of the quarter, we talked about parameter ﬁtting using\\nmaximum likelihood estimation (MLE), and chose our parameters according\\nto\\nθMLE = arg max\\nθ\\nn\\nY\\ni=1\\np(y(i)|x(i); θ).\\nThroughout our subsequent discussions, we viewed θ as an unknown param-\\neter of the world. This view of the θ as being constant-valued but unknown\\nis taken in frequentist statistics. In the frequentist this view of the world, θ\\nis not random—it just happens to be unknown—and it’s our job to come up\\nwith statistical procedures (such as maximum likelihood) to try to estimate\\nthis parameter.\\nAn alternative way to approach our parameter estimation problems is to\\ntake the Bayesian view of the world, and think of θ as being a random\\nvariable whose value is unknown.\\nIn this approach, we would specify a\\nprior distribution p(θ) on θ that expresses our “prior beliefs” about the\\nparameters. Given a training set S = {(x(i), y(i))}n\\ni=1, when we are asked to\\nmake a prediction on a new value of x, we can then compute the posterior\\ndistribution on the parameters\\np(θ|S)\\n=\\np(S|θ)p(θ)\\np(S)\\n=\\n\\x00Qn\\ni=1 p(y(i)|x(i), θ)\\n\\x01\\np(θ)\\nR\\nθ (Qn\\ni=1 p(y(i)|x(i), θ)p(θ)) dθ\\n(9.3)\\nIn the equation above, p(y(i)|x(i), θ) comes from whatever model you’re using'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 143}, page_content='143\\nfor your learning problem. For example, if you are using Bayesian logistic re-\\ngression, then you might choose p(y(i)|x(i), θ) = hθ(x(i))y(i)(1−hθ(x(i)))(1−y(i)),\\nwhere hθ(x(i)) = 1/(1 + exp(−θTx(i))).7\\nWhen we are given a new test example x and asked to make it prediction\\non it, we can compute our posterior distribution on the class label using the\\nposterior distribution on θ:\\np(y|x, S) =\\nZ\\nθ\\np(y|x, θ)p(θ|S)dθ\\n(9.4)\\nIn the equation above, p(θ|S) comes from Equation (9.3). Thus, for example,\\nif the goal is to the predict the expected value of y given x, then we would\\noutput8\\nE[y|x, S] =\\nZ\\ny\\nyp(y|x, S)dy\\nThe procedure that we’ve outlined here can be thought of as doing “fully\\nBayesian” prediction, where our prediction is computed by taking an average\\nwith respect to the posterior p(θ|S) over θ. Unfortunately, in general it is\\ncomputationally very diﬃcult to compute this posterior distribution. This is\\nbecause it requires taking integrals over the (usually high-dimensional) θ as\\nin Equation (9.3), and this typically cannot be done in closed-form.\\nThus, in practice we will instead approximate the posterior distribution\\nfor θ. One common approximation is to replace our posterior distribution for\\nθ (as in Equation 9.4) with a single point estimate. The MAP (maximum\\na posteriori) estimate for θ is given by\\nθMAP = arg max\\nθ\\nn\\nY\\ni=1\\np(y(i)|x(i), θ)p(θ).\\n(9.5)\\nNote that this is the same formulas as for the MLE (maximum likelihood)\\nestimate for θ, except for the prior p(θ) term at the end.\\nIn practical applications, a common choice for the prior p(θ) is to assume\\nthat θ ∼N(0, τ 2I). Using this choice of prior, the ﬁtted parameters θMAP will\\nhave smaller norm than that selected by maximum likelihood. In practice,\\nthis causes the Bayesian MAP estimate to be less susceptible to overﬁtting\\nthan the ML estimate of the parameters.\\nFor example, Bayesian logistic\\nregression turns out to be an eﬀective algorithm for text classiﬁcation, even\\nthough in text classiﬁcation we usually have d ≫n.\\n7Since we are now viewing θ as a random variable, it is okay to condition on it value,\\nand write “p(y|x, θ)” instead of “p(y|x; θ).”\\n8The integral below would be replaced by a summation if y is discrete-valued.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 144}, page_content='Part IV\\nUnsupervised learning\\n144'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 145}, page_content='Chapter 10\\nClustering and the k-means\\nalgorithm\\nIn the clustering problem, we are given a training set {x(1), . . . , x(n)}, and\\nwant to group the data into a few cohesive “clusters.”\\nHere, x(i) ∈Rd\\nas usual; but no labels y(i) are given. So, this is an unsupervised learning\\nproblem.\\nThe k-means clustering algorithm is as follows:\\n1. Initialize cluster centroids µ1, µ2, . . . , µk ∈Rd randomly.\\n2. Repeat until convergence: {\\nFor every i, set\\nc(i) := arg min\\nj\\n||x(i) −µj||2.\\nFor each j, set\\nµj :=\\nPn\\ni=1 1{c(i) = j}x(i)\\nPn\\ni=1 1{c(i) = j} .\\n}\\nIn the algorithm above, k (a parameter of the algorithm) is the number\\nof clusters we want to ﬁnd; and the cluster centroids µj represent our current\\nguesses for the positions of the centers of the clusters. To initialize the cluster\\ncentroids (in step 1 of the algorithm above), we could choose k training\\nexamples randomly, and set the cluster centroids to be equal to the values of\\nthese k examples. (Other initialization methods are also possible.)\\nThe inner-loop of the algorithm repeatedly carries out two steps: (i)\\n“Assigning” each training example x(i) to the closest cluster centroid µj, and\\n145'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 146}, page_content='146\\nFigure 10.1: K-means algorithm. Training examples are shown as dots, and\\ncluster centroids are shown as crosses. (a) Original dataset. (b) Random ini-\\ntial cluster centroids (in this instance, not chosen to be equal to two training\\nexamples). (c-f) Illustration of running two iterations of k-means. In each\\niteration, we assign each training example to the closest cluster centroid\\n(shown by “painting” the training examples the same color as the cluster\\ncentroid to which is assigned); then we move each cluster centroid to the\\nmean of the points assigned to it. (Best viewed in color.) Images courtesy\\nMichael Jordan.\\n(ii) Moving each cluster centroid µj to the mean of the points assigned to it.\\nFigure 10.1 shows an illustration of running k-means.\\nIs the k-means algorithm guaranteed to converge? Yes it is, in a certain\\nsense. In particular, let us deﬁne the distortion function to be:\\nJ(c, µ) =\\nn\\nX\\ni=1\\n||x(i) −µc(i)||2\\nThus, J measures the sum of squared distances between each training exam-\\nple x(i) and the cluster centroid µc(i) to which it has been assigned. It can\\nbe shown that k-means is exactly coordinate descent on J. Speciﬁcally, the\\ninner-loop of k-means repeatedly minimizes J with respect to c while holding\\nµ ﬁxed, and then minimizes J with respect to µ while holding c ﬁxed. Thus,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 147}, page_content='147\\nJ must monotonically decrease, and the value of J must converge. (Usu-\\nally, this implies that c and µ will converge too. In theory, it is possible for\\nk-means to oscillate between a few diﬀerent clusterings—i.e., a few diﬀerent\\nvalues for c and/or µ—that have exactly the same value of J, but this almost\\nnever happens in practice.)\\nThe distortion function J is a non-convex function, and so coordinate\\ndescent on J is not guaranteed to converge to the global minimum. In other\\nwords, k-means can be susceptible to local optima. Very often k-means will\\nwork ﬁne and come up with very good clusterings despite this. But if you\\nare worried about getting stuck in bad local minima, one common thing to\\ndo is run k-means many times (using diﬀerent random initial values for the\\ncluster centroids µj). Then, out of all the diﬀerent clusterings found, pick\\nthe one that gives the lowest distortion J(c, µ).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 148}, page_content='Chapter 11\\nEM algorithms\\nIn this set of notes, we discuss the EM (Expectation-Maximization) algorithm\\nfor density estimation.\\n11.1\\nEM for mixture of Gaussians\\nSuppose that we are given a training set {x(1), . . . , x(n)} as usual. Since we\\nare in the unsupervised learning setting, these points do not come with any\\nlabels.\\nWe wish to model the data by specifying a joint distribution p(x(i), z(i)) =\\np(x(i)|z(i))p(z(i)). Here, z(i) ∼Multinomial(φ) (where φj ≥0, Pk\\nj=1 φj = 1,\\nand the parameter φj gives p(z(i) = j)), and x(i)|z(i) = j ∼N(µj, Σj). We\\nlet k denote the number of values that the z(i)’s can take on. Thus, our\\nmodel posits that each x(i) was generated by randomly choosing z(i) from\\n{1, . . . , k}, and then x(i) was drawn from one of k Gaussians depending on\\nz(i). This is called the mixture of Gaussians model. Also, note that the\\nz(i)’s are latent random variables, meaning that they’re hidden/unobserved.\\nThis is what will make our estimation problem diﬃcult.\\nThe parameters of our model are thus φ, µ and Σ. To estimate them, we\\ncan write down the likelihood of our data:\\nℓ(φ, µ, Σ)\\n=\\nn\\nX\\ni=1\\nlog p(x(i); φ, µ, Σ)\\n=\\nn\\nX\\ni=1\\nlog\\nk\\nX\\nz(i)=1\\np(x(i)|z(i); µ, Σ)p(z(i); φ).\\nHowever, if we set to zero the derivatives of this formula with respect to\\n148'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 149}, page_content='149\\nthe parameters and try to solve, we’ll ﬁnd that it is not possible to ﬁnd the\\nmaximum likelihood estimates of the parameters in closed form. (Try this\\nyourself at home.)\\nThe random variables z(i) indicate which of the k Gaussians each x(i)\\nhad come from. Note that if we knew what the z(i)’s were, the maximum\\nlikelihood problem would have been easy. Speciﬁcally, we could then write\\ndown the likelihood as\\nℓ(φ, µ, Σ) =\\nn\\nX\\ni=1\\nlog p(x(i)|z(i); µ, Σ) + log p(z(i); φ).\\nMaximizing this with respect to φ, µ and Σ gives the parameters:\\nφj\\n=\\n1\\nn\\nn\\nX\\ni=1\\n1{z(i) = j},\\nµj\\n=\\nPn\\ni=1 1{z(i) = j}x(i)\\nPn\\ni=1 1{z(i) = j} ,\\nΣj\\n=\\nPn\\ni=1 1{z(i) = j}(x(i) −µj)(x(i) −µj)T\\nPn\\ni=1 1{z(i) = j}\\n.\\nIndeed, we see that if the z(i)’s were known, then maximum likelihood\\nestimation becomes nearly identical to what we had when estimating the\\nparameters of the Gaussian discriminant analysis model, except that here\\nthe z(i)’s playing the role of the class labels.1\\nHowever, in our density estimation problem, the z(i)’s are not known.\\nWhat can we do?\\nThe EM algorithm is an iterative algorithm that has two main steps.\\nApplied to our problem, in the E-step, it tries to “guess” the values of the\\nz(i)’s. In the M-step, it updates the parameters of our model based on our\\nguesses. Since in the M-step we are pretending that the guesses in the ﬁrst\\npart were correct, the maximization becomes easy. Here’s the algorithm:\\nRepeat until convergence: {\\n(E-step) For each i, j, set\\nw(i)\\nj\\n:= p(z(i) = j|x(i); φ, µ, Σ)\\n1There are other minor diﬀerences in the formulas here from what we’d obtained in\\nPS1 with Gaussian discriminant analysis, ﬁrst because we’ve generalized the z(i)’s to be\\nmultinomial rather than Bernoulli, and second because here we are using a diﬀerent Σj\\nfor each Gaussian.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 150}, page_content='150\\n(M-step) Update the parameters:\\nφj\\n:=\\n1\\nn\\nn\\nX\\ni=1\\nw(i)\\nj ,\\nµj\\n:=\\nPn\\ni=1 w(i)\\nj x(i)\\nPn\\ni=1 w(i)\\nj\\n,\\nΣj\\n:=\\nPn\\ni=1 w(i)\\nj (x(i) −µj)(x(i) −µj)T\\nPn\\ni=1 w(i)\\nj\\n}\\nIn the E-step, we calculate the posterior probability of our parameters\\nthe z(i)’s, given the x(i) and using the current setting of our parameters. I.e.,\\nusing Bayes rule, we obtain:\\np(z(i) = j|x(i); φ, µ, Σ) =\\np(x(i)|z(i) = j; µ, Σ)p(z(i) = j; φ)\\nPk\\nl=1 p(x(i)|z(i) = l; µ, Σ)p(z(i) = l; φ)\\nHere, p(x(i)|z(i) = j; µ, Σ) is given by evaluating the density of a Gaussian\\nwith mean µj and covariance Σj at x(i); p(z(i) = j; φ) is given by φj, and so\\non. The values w(i)\\nj\\ncalculated in the E-step represent our “soft” guesses2 for\\nthe values of z(i).\\nAlso, you should contrast the updates in the M-step with the formulas we\\nhad when the z(i)’s were known exactly. They are identical, except that in-\\nstead of the indicator functions “1{z(i) = j}” indicating from which Gaussian\\neach datapoint had come, we now instead have the w(i)\\nj ’s.\\nThe EM-algorithm is also reminiscent of the K-means clustering algo-\\nrithm, except that instead of the “hard” cluster assignments c(i), we instead\\nhave the “soft” assignments w(i)\\nj . Similar to K-means, it is also susceptible\\nto local optima, so reinitializing at several diﬀerent initial parameters may\\nbe a good idea.\\nIt’s clear that the EM algorithm has a very natural interpretation of\\nrepeatedly trying to guess the unknown z(i)’s; but how did it come about,\\nand can we make any guarantees about it, such as regarding its convergence?\\nIn the next set of notes, we will describe a more general view of EM, one\\n2The term “soft” refers to our guesses being probabilities and taking values in [0, 1]; in\\ncontrast, a “hard” guess is one that represents a single best guess (such as taking values\\nin {0, 1} or {1, . . . , k}).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 151}, page_content='151\\nthat will allow us to easily apply it to other estimation problems in which\\nthere are also latent variables, and which will allow us to give a convergence\\nguarantee.\\n11.2\\nJensen’s inequality\\nWe begin our discussion with a very useful result called Jensen’s inequality\\nLet f be a function whose domain is the set of real numbers. Recall that\\nf is a convex function if f ′′(x) ≥0 (for all x ∈R). In the case of f taking\\nvector-valued inputs, this is generalized to the condition that its hessian H\\nis positive semi-deﬁnite (H ≥0). If f ′′(x) > 0 for all x, then we say f is\\nstrictly convex (in the vector-valued case, the corresponding statement is\\nthat H must be positive deﬁnite, written H > 0). Jensen’s inequality can\\nthen be stated as follows:\\nTheorem. Let f be a convex function, and let X be a random variable.\\nThen:\\nE[f(X)] ≥f(EX).\\nMoreover, if f is strictly convex, then E[f(X)] = f(EX) holds true if and\\nonly if X = E[X] with probability 1 (i.e., if X is a constant).\\nRecall our convention of occasionally dropping the parentheses when writ-\\ning expectations, so in the theorem above, f(EX) = f(E[X]).\\nFor an interpretation of the theorem, consider the ﬁgure below.\\na\\nE[X]\\nb\\nf(a)\\nf(b)\\nf(EX)\\nE[f(X)]\\nf\\nHere, f is a convex function shown by the solid line. Also, X is a random\\nvariable that has a 0.5 chance of taking the value a, and a 0.5 chance of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 152}, page_content='152\\ntaking the value b (indicated on the x-axis). Thus, the expected value of X\\nis given by the midpoint between a and b.\\nWe also see the values f(a), f(b) and f(E[X]) indicated on the y-axis.\\nMoreover, the value E[f(X)] is now the midpoint on the y-axis between f(a)\\nand f(b). From our example, we see that because f is convex, it must be the\\ncase that E[f(X)] ≥f(EX).\\nIncidentally, quite a lot of people have trouble remembering which way\\nthe inequality goes, and remembering a picture like this is a good way to\\nquickly ﬁgure out the answer.\\nRemark. Recall that f is [strictly] concave if and only if −f is [strictly]\\nconvex (i.e., f ′′(x) ≤0 or H ≤0). Jensen’s inequality also holds for concave\\nfunctions f, but with the direction of all the inequalities reversed (E[f(X)] ≤\\nf(EX), etc.).\\n11.3\\nGeneral EM algorithms\\nSuppose we have an estimation problem in which we have a training set\\n{x(1), . . . , x(n)} consisting of n independent examples. We have a latent vari-\\nable model p(x, z; θ) with z being the latent variable (which for simplicity is\\nassumed to take ﬁnite number of values). The density for x can be obtained\\nby marginalized over the latent variable z:\\np(x; θ) =\\nX\\nz\\np(x, z; θ)\\n(11.1)\\nWe wish to ﬁt the parameters θ by maximizing the log-likelihood of the\\ndata, deﬁned by\\nℓ(θ)\\n=\\nn\\nX\\ni=1\\nlog p(x(i); θ)\\n(11.2)\\nWe can rewrite the objective in terms of the joint density p(x, z; θ) by\\nℓ(θ)\\n=\\nn\\nX\\ni=1\\nlog p(x(i); θ)\\n(11.3)\\n=\\nn\\nX\\ni=1\\nlog\\nX\\nz(i)\\np(x(i), z(i); θ).\\n(11.4)\\nBut, explicitly ﬁnding the maximum likelihood estimates of the parameters\\nθ may be hard since it will result in diﬃcult non-convex optimization prob-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 153}, page_content='153\\nlems.3 Here, the z(i)’s are the latent random variables; and it is often the case\\nthat if the z(i)’s were observed, then maximum likelihood estimation would\\nbe easy.\\nIn such a setting, the EM algorithm gives an eﬃcient method for max-\\nimum likelihood estimation. Maximizing ℓ(θ) explicitly might be diﬃcult,\\nand our strategy will be to instead repeatedly construct a lower-bound on ℓ\\n(E-step), and then optimize that lower-bound (M-step).4\\nIt turns out that the summation Pn\\ni=1 is not essential here, and towards a\\nsimpler exposition of the EM algorithm, we will ﬁrst consider optimizing the\\nthe likelihood log p(x) for a single example x. After we derive the algorithm\\nfor optimizing log p(x), we will convert it to an algorithm that works for n\\nexamples by adding back the sum to each of the relevant equations. Thus,\\nnow we aim to optimize log p(x; θ) which can be rewritten as\\nlog p(x; θ) = log\\nX\\nz\\np(x, z; θ)\\n(11.5)\\nLet Q be a distribution over the possible values of z. That is, P\\nz Q(z) = 1,\\nQ(z) ≥0).\\nConsider the following:5\\nlog p(x; θ)\\n=\\nlog\\nX\\nz\\np(x, z; θ)\\n=\\nlog\\nX\\nz\\nQ(z)p(x, z; θ)\\nQ(z)\\n(11.6)\\n≥\\nX\\nz\\nQ(z) log p(x, z; θ)\\nQ(z)\\n(11.7)\\nThe last step of this derivation used Jensen’s inequality.\\nSpeciﬁcally,\\nf(x) = log x is a concave function, since f ′′(x) = −1/x2 < 0 over its domain\\n3It’s mostly an empirical observation that the optimization problem is diﬃcult to op-\\ntimize.\\n4Empirically, the E-step and M-step can often be computed more eﬃciently than op-\\ntimizing the function ℓ(·) directly. However, it doesn’t necessarily mean that alternating\\nthe two steps can always converge to the global optimum of ℓ(·). Even for mixture of\\nGaussians, the EM algorithm can either converge to a global optimum or get stuck, de-\\npending on the properties of the training data. Empirically, for real-world data, often EM\\ncan converge to a solution with relatively high likelihood (if not the optimum), and the\\ntheory behind it is still largely not understood.\\n5If z were continuous, then Q would be a density, and the summations over z in our\\ndiscussion are replaced with integrals over z.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 154}, page_content='154\\nx ∈R+. Also, the term\\nX\\nz\\nQ(z)\\n\\x14p(x, z; θ)\\nQ(z)\\n\\x15\\nin the summation is just an expectation of the quantity [p(x, z; θ)/Q(z)] with\\nrespect to z drawn according to the distribution given by Q.6 By Jensen’s\\ninequality, we have\\nf\\n\\x12\\nEz∼Q\\n\\x14p(x, z; θ)\\nQ(z)\\n\\x15\\x13\\n≥Ez∼Q\\n\\x14\\nf\\n\\x12p(x, z; θ)\\nQ(z)\\n\\x13\\x15\\n,\\nwhere the “z ∼Q” subscripts above indicate that the expectations are with\\nrespect to z drawn from Q. This allowed us to go from Equation (11.6) to\\nEquation (11.7).\\nNow, for any distribution Q, the formula (11.7) gives a lower-bound on\\nlog p(x; θ). There are many possible choices for the Q’s. Which should we\\nchoose? Well, if we have some current guess θ of the parameters, it seems\\nnatural to try to make the lower-bound tight at that value of θ. I.e., we will\\nmake the inequality above hold with equality at our particular value of θ.\\nTo make the bound tight for a particular value of θ, we need for the step\\ninvolving Jensen’s inequality in our derivation above to hold with equality.\\nFor this to be true, we know it is suﬃcient that the expectation be taken\\nover a “constant”-valued random variable. I.e., we require that\\np(x, z; θ)\\nQ(z)\\n= c\\nfor some constant c that does not depend on z. This is easily accomplished\\nby choosing\\nQ(z) ∝p(x, z; θ).\\nActually, since we know P\\nz Q(z) = 1 (because it is a distribution), this\\nfurther tells us that\\nQ(z)\\n=\\np(x, z; θ)\\nP\\nz p(x, z; θ)\\n=\\np(x, z; θ)\\np(x; θ)\\n=\\np(z|x; θ)\\n(11.8)\\n6We note that the notion p(x,z;θ)\\nQ(z)\\nonly makes sense if Q(z) ̸= 0 whenever p(x, z; θ) ̸= 0.\\nHere we implicitly assume that we only consider those Q with such a property.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 155}, page_content='155\\nThus, we simply set the Q’s to be the posterior distribution of the z’s given\\nx and the setting of the parameters θ.\\nIndeed, we can directly verify that when Q(z) = p(z|x; θ), then equa-\\ntion (11.7) is an equality because\\nX\\nz\\nQ(z) log p(x, z; θ)\\nQ(z)\\n=\\nX\\nz\\np(z|x; θ) log p(x, z; θ)\\np(z|x; θ)\\n=\\nX\\nz\\np(z|x; θ) log p(z|x; θ)p(x; θ)\\np(z|x; θ)\\n=\\nX\\nz\\np(z|x; θ) log p(x; θ)\\n= log p(x; θ)\\nX\\nz\\np(z|x; θ)\\n= log p(x; θ)\\n(because P\\nz p(z|x; θ) = 1)\\nFor convenience, we call the expression in Equation (11.7) the evidence\\nlower bound (ELBO) and we denote it by\\nELBO(x; Q, θ) =\\nX\\nz\\nQ(z) log p(x, z; θ)\\nQ(z)\\n(11.9)\\nWith this equation, we can re-write equation (11.7) as\\n∀Q, θ, x,\\nlog p(x; θ) ≥ELBO(x; Q, θ)\\n(11.10)\\nIntuitively, the EM algorithm alternatively updates Q and θ by a) set-\\nting Q(z) = p(z|x; θ) following Equation (11.8) so that ELBO(x; Q, θ) =\\nlog p(x; θ) for x and the current θ, and b) maximizing ELBO(x; Q, θ) w.r.t θ\\nwhile ﬁxing the choice of Q.\\nRecall that all the discussion above was under the assumption that we\\naim to optimize the log-likelihood log p(x; θ) for a single example x. It turns\\nout that with multiple training examples, the basic idea is the same and we\\nonly needs to take a sum over examples at relevant places. Next, we will\\nbuild the evidence lower bound for multiple training examples and make the\\nEM algorithm formal.\\nRecall we have a training set {x(1), . . . , x(n)}. Note that the optimal choice\\nof Q is p(z|x; θ), and it depends on the particular example x. Therefore here\\nwe will introduce n distributions Q1, . . . , Qn, one for each example x(i). For\\neach example x(i), we can build the evidence lower bound\\nlog p(x(i); θ) ≥ELBO(x(i); Qi, θ) =\\nX\\nz(i)\\nQi(z(i)) log p(x(i), z(i); θ)\\nQi(z(i))'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 156}, page_content='156\\nTaking sum over all the examples, we obtain a lower bound for the log-\\nlikelihood\\nℓ(θ) ≥\\nX\\ni\\nELBO(x(i); Qi, θ)\\n(11.11)\\n=\\nX\\ni\\nX\\nz(i)\\nQi(z(i)) log p(x(i), z(i); θ)\\nQi(z(i))\\nFor any set of distributions Q1, . . . , Qn, the formula (11.11) gives a lower-\\nbound on ℓ(θ), and analogous to the argument around equation (11.8), the\\nQi that attains equality satisﬁes\\nQi(z(i))\\n=\\np(z(i)|x(i); θ)\\nThus, we simply set the Qi’s to be the posterior distribution of the z(i)’s\\ngiven x(i) with the current setting of the parameters θ.\\nNow, for this choice of the Qi’s, Equation (11.11) gives a lower-bound on\\nthe loglikelihood ℓthat we’re trying to maximize. This is the E-step. In the\\nM-step of the algorithm, we then maximize our formula in Equation (11.11)\\nwith respect to the parameters to obtain a new setting of the θ’s. Repeatedly\\ncarrying out these two steps gives us the EM algorithm, which is as follows:\\nRepeat until convergence {\\n(E-step) For each i, set\\nQi(z(i)) := p(z(i)|x(i); θ).\\n(M-step) Set\\nθ := arg max\\nθ\\nn\\nX\\ni=1\\nELBO(x(i); Qi, θ)\\n= arg max\\nθ\\nX\\ni\\nX\\nz(i)\\nQi(z(i)) log p(x(i), z(i); θ)\\nQi(z(i))\\n.\\n(11.12)\\n}\\nHow do we know if this algorithm will converge? Well, suppose θ(t) and\\nθ(t+1) are the parameters from two successive iterations of EM. We will now\\nprove that ℓ(θ(t)) ≤ℓ(θ(t+1)), which shows EM always monotonically im-\\nproves the log-likelihood. The key to showing this result lies in our choice of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 157}, page_content='157\\nthe Qi’s. Speciﬁcally, on the iteration of EM in which the parameters had\\nstarted out as θ(t), we would have chosen Q(t)\\ni (z(i)) := p(z(i)|x(i); θ(t)). We\\nsaw earlier that this choice ensures that Jensen’s inequality, as applied to get\\nEquation (11.11), holds with equality, and hence\\nℓ(θ(t)) =\\nn\\nX\\ni=1\\nELBO(x(i); Q(t)\\ni , θ(t))\\n(11.13)\\nThe parameters θ(t+1) are then obtained by maximizing the right hand side\\nof the equation above. Thus,\\nℓ(θ(t+1)) ≥\\nn\\nX\\ni=1\\nELBO(x(i); Q(t)\\ni , θ(t+1))\\n(because ineqaulity (11.11) holds for all Q and θ)\\n≥\\nn\\nX\\ni=1\\nELBO(x(i); Q(t)\\ni , θ(t))\\n(see reason below)\\n= ℓ(θ(t))\\n(by equation (11.13))\\nwhere the last inequality follows from that θ(t+1) is chosen explicitly to be\\narg max\\nθ\\nn\\nX\\ni=1\\nELBO(x(i); Q(t)\\ni , θ)\\nHence, EM causes the likelihood to converge monotonically. In our de-\\nscription of the EM algorithm, we said we’d run it until convergence. Given\\nthe result that we just showed, one reasonable convergence test would be\\nto check if the increase in ℓ(θ) between successive iterations is smaller than\\nsome tolerance parameter, and to declare convergence if EM is improving\\nℓ(θ) too slowly.\\nRemark. If we deﬁne (by overloading ELBO(·))\\nELBO(Q, θ) =\\nn\\nX\\ni=1\\nELBO(x(i); Qi, θ) =\\nX\\ni\\nX\\nz(i)\\nQi(z(i)) log p(x(i), z(i); θ)\\nQi(z(i))\\n(11.14)\\nthen we know ℓ(θ) ≥ELBO(Q, θ) from our previous derivation. The EM\\ncan also be viewed an alternating maximization algorithm on ELBO(Q, θ),\\nin which the E-step maximizes it with respect to Q (check this yourself), and\\nthe M-step maximizes it with respect to θ.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 158}, page_content='158\\n11.3.1\\nOther interpretation of ELBO\\nLet ELBO(x; Q, θ) = P\\nz Q(z) log p(x,z;θ)\\nQ(z)\\nbe deﬁned as in equation (11.9).\\nThere are several other forms of ELBO. First, we can rewrite\\nELBO(x; Q, θ) = Ez∼Q[log p(x, z; θ)] −Ez∼Q[log Q(z)]\\n= Ez∼Q[log p(x|z; θ)] −DKL(Q∥pz)\\n(11.15)\\nwhere we use pz to denote the marginal distribution of z (under the distri-\\nbution p(x, z; θ)), and DKL() denotes the KL divergence\\nDKL(Q∥pz) =\\nX\\nz\\nQ(z) log Q(z)\\np(z)\\n(11.16)\\nIn many cases, the marginal distribution of z does not depend on the param-\\neter θ. In this case, we can see that maximizing ELBO over θ is equivalent\\nto maximizing the ﬁrst term in (11.15). This corresponds to maximizing the\\nconditional likelihood of x conditioned on z, which is often a simpler question\\nthan the original question.\\nAnother form of ELBO(·) is (please verify yourself)\\nELBO(x; Q, θ) = log p(x) −DKL(Q∥pz|x)\\n(11.17)\\nwhere pz|x is the conditional distribution of z given x under the parameter\\nθ. This forms shows that the maximizer of ELBO(Q, θ) over Q is obtained\\nwhen Q = pz|x, which was shown in equation (11.8) before.\\n11.4\\nMixture of Gaussians revisited\\nArmed with our general deﬁnition of the EM algorithm, let’s go back to our\\nold example of ﬁtting the parameters φ, µ and Σ in a mixture of Gaussians.\\nFor the sake of brevity, we carry out the derivations for the M-step updates\\nonly for φ and µj, and leave the updates for Σj as an exercise for the reader.\\nThe E-step is easy. Following our algorithm derivation above, we simply\\ncalculate\\nw(i)\\nj\\n= Qi(z(i) = j) = P(z(i) = j|x(i); φ, µ, Σ).\\nHere, “Qi(z(i) = j)” denotes the probability of z(i) taking the value j under\\nthe distribution Qi.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 159}, page_content='159\\nNext, in the M-step, we need to maximize, with respect to our parameters\\nφ, µ, Σ, the quantity\\nn\\nX\\ni=1\\nX\\nz(i)\\nQi(z(i)) log p(x(i), z(i); φ, µ, Σ)\\nQi(z(i))\\n=\\nn\\nX\\ni=1\\nk\\nX\\nj=1\\nQi(z(i) = j) log p(x(i)|z(i) = j; µ, Σ)p(z(i) = j; φ)\\nQi(z(i) = j)\\n=\\nn\\nX\\ni=1\\nk\\nX\\nj=1\\nw(i)\\nj log\\n1\\n(2π)d/2|Σj|1/2 exp\\n\\x00−1\\n2(x(i) −µj)TΣ−1\\nj (x(i) −µj)\\n\\x01\\n· φj\\nw(i)\\nj\\nLet’s maximize this with respect to µl. If we take the derivative with respect\\nto µl, we ﬁnd\\n∇µl\\nn\\nX\\ni=1\\nk\\nX\\nj=1\\nw(i)\\nj log\\n1\\n(2π)d/2|Σj|1/2 exp\\n\\x00−1\\n2(x(i) −µj)TΣ−1\\nj (x(i) −µj)\\n\\x01\\n· φj\\nw(i)\\nj\\n=\\n−∇µl\\nn\\nX\\ni=1\\nk\\nX\\nj=1\\nw(i)\\nj\\n1\\n2(x(i) −µj)TΣ−1\\nj (x(i) −µj)\\n=\\n1\\n2\\nn\\nX\\ni=1\\nw(i)\\nl ∇µl2µT\\nl Σ−1\\nl x(i) −µT\\nl Σ−1\\nl µl\\n=\\nn\\nX\\ni=1\\nw(i)\\nl\\n\\x00Σ−1\\nl x(i) −Σ−1\\nl µl\\n\\x01\\nSetting this to zero and solving for µl therefore yields the update rule\\nµl :=\\nPn\\ni=1 w(i)\\nl x(i)\\nPn\\ni=1 w(i)\\nl\\n,\\nwhich was what we had in the previous set of notes.\\nLet’s do one more example, and derive the M-step update for the param-\\neters φj. Grouping together only the terms that depend on φj, we ﬁnd that\\nwe need to maximize\\nn\\nX\\ni=1\\nk\\nX\\nj=1\\nw(i)\\nj log φj.\\nHowever, there is an additional constraint that the φj’s sum to 1, since they\\nrepresent the probabilities φj = p(z(i) = j; φ). To deal with the constraint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 160}, page_content='160\\nthat Pk\\nj=1 φj = 1, we construct the Lagrangian\\nL(φ) =\\nn\\nX\\ni=1\\nk\\nX\\nj=1\\nw(i)\\nj log φj + β(\\nk\\nX\\nj=1\\nφj −1),\\nwhere β is the Lagrange multiplier.7 Taking derivatives, we ﬁnd\\n∂\\n∂φj\\nL(φ) =\\nn\\nX\\ni=1\\nw(i)\\nj\\nφj\\n+ β\\nSetting this to zero and solving, we get\\nφj =\\nPn\\ni=1 w(i)\\nj\\n−β\\nI.e., φj ∝Pn\\ni=1 w(i)\\nj . Using the constraint that P\\nj φj = 1, we easily ﬁnd\\nthat −β = Pn\\ni=1\\nPk\\nj=1 w(i)\\nj\\n= Pn\\ni=1 1 = n. (This used the fact that w(i)\\nj\\n=\\nQi(z(i) = j), and since probabilities sum to 1, P\\nj w(i)\\nj\\n= 1.) We therefore\\nhave our M-step updates for the parameters φj:\\nφj := 1\\nn\\nn\\nX\\ni=1\\nw(i)\\nj .\\nThe derivation for the M-step updates to Σj are also entirely straightfor-\\nward.\\n11.5\\nVariational\\ninference\\nand\\nvariational\\nauto-encoder (optional reading)\\nLoosely speaking, variational auto-encoder Kingma and Welling [2013] gen-\\nerally refers to a family of algorithms that extend the EM algorithms to more\\ncomplex models parameterized by neural networks. It extends the technique\\nof variational inference with the additional “re-parametrization trick” which\\nwill be introduced below. Variational auto-encoder may not give the best\\nperformance for many datasets, but it contains several central ideas about\\nhow to extend EM algorithms to high-dimensional continuous latent variables\\n7We don’t need to worry about the constraint that φj ≥0, because as we’ll shortly see,\\nthe solution we’ll ﬁnd from this derivation will automatically satisfy that anyway.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 161}, page_content='161\\nwith non-linear models. Understanding it will likely give you the language\\nand backgrounds to understand various recent papers related to it.\\nAs a running example, we will consider the following parameterization of\\np(x, z; θ) by a neural network. Let θ be the collection of the weights of a\\nneural network g(z; θ) that maps z ∈Rk to Rd. Let\\nz ∼N(0, Ik×k)\\n(11.18)\\nx|z ∼N(g(z; θ), σ2Id×d)\\n(11.19)\\nHere Ik×k denotes identity matrix of dimension k by k, and σ is a scalar that\\nwe assume to be known for simplicity.\\nFor the Gaussian mixture models in Section 11.4, the optimal choice of\\nQ(z) = p(z|x; θ) for each ﬁxed θ, that is the posterior distribution of z,\\ncan be analytically computed. In many more complex models such as the\\nmodel (11.19), it’s intractable to compute the exact the posterior distribution\\np(z|x; θ).\\nRecall that from equation (11.10), ELBO is always a lower bound for any\\nchoice of Q, and therefore, we can also aim for ﬁnding an approximation of\\nthe true posterior distribution. Often, one has to use some particular form\\nto approximate the true posterior distribution. Let Q be a family of Q’s that\\nwe are considering, and we will aim to ﬁnd a Q within the family of Q that is\\nclosest to the true posterior distribution. To formalize, recall the deﬁnition of\\nthe ELBO lower bound as a function of Q and θ deﬁned in equation (11.14)\\nELBO(Q, θ) =\\nn\\nX\\ni=1\\nELBO(x(i); Qi, θ) =\\nX\\ni\\nX\\nz(i)\\nQi(z(i)) log p(x(i), z(i); θ)\\nQi(z(i))\\nRecall\\nthat\\nEM\\ncan\\nbe\\nviewed\\nas\\nalternating\\nmaximization\\nof\\nELBO(Q, θ). Here instead, we optimize the EBLO over Q ∈Q\\nmax\\nQ∈Q max\\nθ\\nELBO(Q, θ)\\n(11.20)\\nNow the next question is what form of Q (or what structural assumptions\\nto make about Q) allows us to eﬃciently maximize the objective above. When\\nthe latent variable z are high-dimensional discrete variables, one popular as-\\nsumption is the mean ﬁeld assumption, which assumes that Qi(z) gives a\\ndistribution with independent coordinates, or in other words, Qi can be de-\\ncomposed into Qi(z) = Q1\\ni (z1) · · · Qk\\ni (zk). There are tremendous applications\\nof mean ﬁeld assumptions to learning generative models with discrete latent\\nvariables, and we refer to Blei et al. [2017] for a survey of these models and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 162}, page_content='162\\ntheir impact to a wide range of applications including computational biology,\\ncomputational neuroscience, social sciences. We will not get into the details\\nabout the discrete latent variable cases, and our main focus is to deal with\\ncontinuous latent variables, which requires not only mean ﬁeld assumptions,\\nbut additional techniques.\\nWhen z ∈Rk is a continuous latent variable, there are several decisions to\\nmake towards successfully optimizing (11.20). First we need to give a succinct\\nrepresentation of the distribution Qi because it is over an inﬁnite number of\\npoints. A natural choice is to assume Qi is a Gaussian distribution with some\\nmean and variance. We would also like to have more succinct representation\\nof the means of Qi of all the examples. Note that Qi(z(i)) is supposed to\\napproximate p(z(i)|x(i); θ). It would make sense let all the means of the Qi’s\\nbe some function of x(i). Concretely, let q(·; φ), v(·; φ) be two functions that\\nmap from dimension d to k, which are parameterized by φ and ψ, we assume\\nthat\\nQi = N(q(x(i); φ), diag(v(x(i); ψ))2)\\n(11.21)\\nHere diag(w) means the k × k matrix with the entries of w ∈Rk on the\\ndiagonal. In other words, the distribution Qi is assumed to be a Gaussian\\ndistribution with independent coordinates, and the mean and standard de-\\nviations are governed by q and v. Often in variational auto-encoder, q and v\\nare chosen to be neural networks.8 In recent deep learning literature, often\\nq, v are called encoder (in the sense of encoding the data into latent code),\\nwhereas g(z; θ) if often referred to as the decoder.\\nWe remark that Qi of such form in many cases are very far from a good ap-\\nproximation of the true posterior distribution. However, some approximation\\nis necessary for feasible optimization. In fact, the form of Qi needs to satisfy\\nother requirements (which happened to be satisﬁed by the form (11.21))\\nBefore optimizing the ELBO, let’s ﬁrst verify whether we can eﬃciently\\nevaluate the value of the ELBO for ﬁxed Q of the form (11.21) and θ. We\\nrewrite the ELBO as a function of φ, ψ, θ by\\nELBO(φ, ψ, θ) =\\nn\\nX\\ni=1\\nEz(i)∼Qi\\n\\x14\\nlog p(x(i), z(i); θ)\\nQi(z(i))\\n\\x15\\n,\\n(11.22)\\nwhere Qi = N(q(x(i); φ), diag(v(x(i); ψ))2)\\nNote that to evaluate Qi(z(i)) inside the expectation, we should be able to\\ncompute the density of Qi.\\nTo estimate the expectation Ez(i)∼Qi, we\\n8q and v can also share parameters. We sweep this level of details under the rug in this\\nnote.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 163}, page_content='163\\nshould be able to sample from distribution Qi so that we can build an\\nempirical estimator with samples. It happens that for Gaussian distribution\\nQi = N(q(x(i); φ), diag(v(x(i); ψ))2), we are able to be both eﬃciently.\\nNow let’s optimize the ELBO. It turns out that we can run gradient ascent\\nover φ, ψ, θ instead of alternating maximization. There is no strong need to\\ncompute the maximum over each variable at a much greater cost. (For Gaus-\\nsian mixture model in Section 11.4, computing the maximum is analytically\\nfeasible and relatively cheap, and therefore we did alternating maximization.)\\nMathematically, let η be the learning rate, the gradient ascent step is\\nθ := θ + η∇θELBO(φ, ψ, θ)\\nφ := φ + η∇φELBO(φ, ψ, θ)\\nψ := ψ + η∇ψELBO(φ, ψ, θ)\\nComputing the gradient over θ is simple because\\n∇θELBO(φ, ψ, θ) = ∇θ\\nn\\nX\\ni=1\\nEz(i)∼Qi\\n\\x14\\nlog p(x(i), z(i); θ)\\nQi(z(i))\\n\\x15\\n= ∇θ\\nn\\nX\\ni=1\\nEz(i)∼Qi\\n\\x02\\nlog p(x(i), z(i); θ)\\n\\x03\\n=\\nn\\nX\\ni=1\\nEz(i)∼Qi\\n\\x02\\n∇θ log p(x(i), z(i); θ)\\n\\x03\\n,\\n(11.23)\\nBut computing the gradient over φ and ψ is tricky because the sam-\\npling distribution Qi depends on φ and ψ.\\n(Abstractly speaking, the is-\\nsue we face can be simpliﬁed as the problem of computing the gradi-\\nent Ez∼Qφ[f(φ)] with respect to variable φ.\\nWe know that in general,\\n∇Ez∼Qφ[f(φ)] ̸= Ez∼Qφ[∇f(φ)] because the dependency of Qφ on φ has to be\\ntaken into account as well. )\\nThe idea that comes to rescue is the so-called re-parameterization\\ntrick: we rewrite z(i) ∼Qi = N(q(x(i); φ), diag(v(x(i); ψ))2) in an equivalent\\nway:\\nz(i) = q(x(i); φ) + v(x(i); ψ) ⊙ξ(i) where ξ(i) ∼N(0, Ik×k)\\n(11.24)\\nHere x ⊙y denotes the entry-wise product of two vectors of the same\\ndimension. Here we used the fact that x ∼N(µ, σ2) is equivalent to that\\nx = µ+ξσ with ξ ∼N(0, 1). We mostly just used this fact in every dimension\\nsimultaneously for the random variable z(i) ∼Qi.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 164}, page_content='164\\nWith this re-parameterization, we have that\\nEz(i)∼Qi\\n\\x14\\nlog p(x(i), z(i); θ)\\nQi(z(i))\\n\\x15\\n(11.25)\\n= Eξ(i)∼N(0,1)\\n\\x14\\nlog p(x(i), q(x(i); φ) + v(x(i); ψ) ⊙ξ(i); θ)\\nQi(q(x(i); φ) + v(x(i); ψ) ⊙ξ(i))\\n\\x15\\nIt follows that\\n∇φEz(i)∼Qi\\n\\x14\\nlog p(x(i), z(i); θ)\\nQi(z(i))\\n\\x15\\n= ∇φEξ(i)∼N(0,1)\\n\\x14\\nlog p(x(i), q(x(i); φ) + v(x(i); ψ) ⊙ξ(i); θ)\\nQi(q(x(i); φ) + v(x(i); ψ) ⊙ξ(i))\\n\\x15\\n= Eξ(i)∼N(0,1)\\n\\x14\\n∇φ log p(x(i), q(x(i); φ) + v(x(i); ψ) ⊙ξ(i); θ)\\nQi(q(x(i); φ) + v(x(i); ψ) ⊙ξ(i))\\n\\x15\\nWe can now sample multiple copies of ξ(i)’s to estimate the the expecta-\\ntion in the RHS of the equation above.9 We can estimate the gradient with\\nrespect to ψ similarly, and with these, we can implement the gradient ascent\\nalgorithm to optimize the ELBO over φ, ψ, θ.\\nThere are not many high-dimensional distributions with analytically com-\\nputable density function are known to be re-parameterizable. We refer to\\nKingma and Welling [2013] for a few other choices that can replace Gaussian\\ndistribution.\\n9Empirically people sometimes just use one sample to estimate it for maximum com-\\nputational eﬃciency.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 165}, page_content='Chapter 12\\nPrincipal components analysis\\nIn this set of notes, we will develop a method, Principal Components Analysis\\n(PCA), that tries to identify the subspace in which the data approximately\\nlies. PCA is computationally eﬃcient: it will require only an eigenvector\\ncalculation (easily done with the eig function in Matlab).\\nSuppose we are given a dataset {x(i); i = 1, . . . , n} of attributes of n dif-\\nferent types of automobiles, such as their maximum speed, turn radius, and\\nso on. Let x(i) ∈Rd for each i (d ≪n). But unknown to us, two diﬀerent\\nattributes—some xi and xj—respectively give a car’s maximum speed mea-\\nsured in miles per hour, and the maximum speed measured in kilometers per\\nhour. These two attributes are therefore almost linearly dependent, up to\\nonly small diﬀerences introduced by rounding oﬀto the nearest mph or kph.\\nThus, the data really lies approximately on an n −1 dimensional subspace.\\nHow can we automatically detect, and perhaps remove, this redundancy?\\nFor a less contrived example, consider a dataset resulting from a survey of\\npilots for radio-controlled helicopters, where x(i)\\n1 is a measure of the piloting\\nskill of pilot i, and x(i)\\n2\\ncaptures how much he/she enjoys ﬂying. Because\\nRC helicopters are very diﬃcult to ﬂy, only the most committed students,\\nones that truly enjoy ﬂying, become good pilots.\\nSo, the two attributes\\nx1 and x2 are strongly correlated.\\nIndeed, we might posit that that the\\ndata actually likes along some diagonal axis (the u1 direction) capturing the\\nintrinsic piloting “karma” of a person, with only a small amount of noise\\nlying oﬀthis axis. (See ﬁgure.) How can we automatically compute this u1\\ndirection?\\n165'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 166}, page_content='166\\nx1\\nx2 (enjoyment)\\n(skill)\\n1\\nu\\nu\\n2\\nWe will shortly develop the PCA algorithm. But prior to running PCA\\nper se, typically we ﬁrst preprocess the data by normalizing each feature\\nto have mean 0 and variance 1. We do this by subtracting the mean and\\ndividing by the empirical standard deviation:\\nx(i)\\nj\\n←x(i)\\nj −µj\\nσj\\nwhere µj = 1\\nn\\nPn\\ni=1 x(i)\\nj\\nand σ2\\nj = 1\\nn\\nPn\\ni=1(x(i)\\nj −µj)2 are the mean variance of\\nfeature j, respectively.\\nSubtracting µj zeros out the mean and may be omitted for data known\\nto have zero mean (for instance, time series corresponding to speech or other\\nacoustic signals). Dividing by the standard deviation σj rescales each coor-\\ndinate to have unit variance, which ensures that diﬀerent attributes are all\\ntreated on the same “scale.” For instance, if x1 was cars’ maximum speed in\\nmph (taking values in the high tens or low hundreds) and x2 were the num-\\nber of seats (taking values around 2-4), then this renormalization rescales\\nthe diﬀerent attributes to make them more comparable. This rescaling may\\nbe omitted if we had a priori knowledge that the diﬀerent attributes are all\\non the same scale. One example of this is if each data point represented a\\ngrayscale image, and each x(i)\\nj\\ntook a value in {0, 1, . . . , 255} corresponding\\nto the intensity value of pixel j in image i.\\nNow, having normalized our data, how do we compute the “major axis\\nof variation” u—that is, the direction on which the data approximately lies?\\nOne way is to pose this problem as ﬁnding the unit vector u so that when'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 167}, page_content='167\\nthe data is projected onto the direction corresponding to u, the variance of\\nthe projected data is maximized. Intuitively, the data starts oﬀwith some\\namount of variance/information in it. We would like to choose a direction u\\nso that if we were to approximate the data as lying in the direction/subspace\\ncorresponding to u, as much as possible of this variance is still retained.\\nConsider the following dataset, on which we have already carried out the\\nnormalization steps:\\nNow, suppose we pick u to correspond the the direction shown in the\\nﬁgure below. The circles denote the projections of the original data onto this\\nline.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 168}, page_content='168\\n\\x00\\x00\\x01\\n\\x01\\n\\x00\\x00\\x00\\x00\\x01\\x01\\n\\x01\\x01\\n\\x00\\x01\\n\\x00\\x00\\x00\\x00\\x01\\x01\\n\\x01\\x01\\n\\x00\\x01\\n\\x00\\x00\\x00\\x00\\x01\\x01\\n\\x01\\x01\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\n\\x01\\x01\\n\\x01\\x01\\n\\x01\\x01\\n\\x01\\x01\\n\\x01\\x01\\n\\x01\\x01\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\n\\x01\\x01\\n\\x01\\x01\\n\\x01\\x01\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\n\\x01\\x01\\x01\\n\\x01\\x01\\x01\\n\\x01\\x01\\x01\\n\\x01\\x01\\x01\\n\\x01\\x01\\x01\\n\\x01\\x01\\x01\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\n\\x01\\x01\\n\\x01\\x01\\n\\x01\\x01\\nWe see that the projected data still has a fairly large variance, and the\\npoints tend to be far from zero. In contrast, suppose had instead picked the\\nfollowing direction:\\n\\x00\\x00\\x01\\x01\\n\\x00\\x00\\x00\\x00\\x01\\x01\\n\\x01\\x01\\n\\x00\\x01\\n\\x00\\x00\\x00\\x00\\x01\\x01\\n\\x01\\x01\\n\\x00\\x00\\x01\\n\\x01\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\nHere, the projections have a signiﬁcantly smaller variance, and are much\\ncloser to the origin.\\nWe would like to automatically select the direction u corresponding to\\nthe ﬁrst of the two ﬁgures shown above. To formalize this, note that given a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 169}, page_content='169\\nunit vector u and a point x, the length of the projection of x onto u is given\\nby xTu. I.e., if x(i) is a point in our dataset (one of the crosses in the plot),\\nthen its projection onto u (the corresponding circle in the ﬁgure) is distance\\nxTu from the origin. Hence, to maximize the variance of the projections, we\\nwould like to choose a unit-length u so as to maximize:\\n1\\nn\\nn\\nX\\ni=1\\n(x(i)Tu)2 = 1\\nn\\nn\\nX\\ni=1\\nuTx(i)x(i)Tu\\n= uT\\n \\n1\\nn\\nn\\nX\\ni=1\\nx(i)x(i)T\\n!\\nu.\\nWe easily recognize that the maximizing this subject to ∥u∥2 = 1 gives the\\nprincipal eigenvector of Σ =\\n1\\nn\\nPn\\ni=1 x(i)x(i)T, which is just the empirical\\ncovariance matrix of the data (assuming it has zero mean).1\\nTo summarize, we have found that if we wish to ﬁnd a 1-dimensional\\nsubspace with with to approximate the data, we should choose u to be the\\nprincipal eigenvector of Σ. More generally, if we wish to project our data\\ninto a k-dimensional subspace (k < d), we should choose u1, . . . , uk to be the\\ntop k eigenvectors of Σ. The ui’s now form a new, orthogonal basis for the\\ndata.2\\nThen, to represent x(i) in this basis, we need only compute the corre-\\nsponding vector\\ny(i) =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nuT\\n1 x(i)\\nuT\\n2 x(i)\\n...\\nuT\\nk x(i)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb∈Rk.\\nThus, whereas x(i) ∈Rd, the vector y(i) now gives a lower, k-dimensional,\\napproximation/representation for x(i). PCA is therefore also referred to as\\na dimensionality reduction algorithm. The vectors u1, . . . , uk are called\\nthe ﬁrst k principal components of the data.\\nRemark. Although we have shown it formally only for the case of k = 1,\\nusing well-known properties of eigenvectors it is straightforward to show that\\n1If you haven’t seen this before, try using the method of Lagrange multipliers to max-\\nimize uT Σu subject to that uT u = 1. You should be able to show that Σu = λu, for some\\nλ, which implies u is an eigenvector of Σ, with eigenvalue λ.\\n2Because Σ is symmetric, the ui’s will (or always can be chosen to be) orthogonal to\\neach other.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 170}, page_content='170\\nof all possible orthogonal bases u1, . . . , uk, the one that we have chosen max-\\nimizes P\\ni ∥y(i)∥2\\n2. Thus, our choice of a basis preserves as much variability\\nas possible in the original data.\\nPCA can also be derived by picking the basis that minimizes the ap-\\nproximation error arising from projecting the data onto the k-dimensional\\nsubspace spanned by them. (See more in homework.)\\nPCA has many applications; we will close our discussion with a few exam-\\nples. First, compression—representing x(i)’s with lower dimension y(i)’s—is\\nan obvious application. If we reduce high dimensional data to k = 2 or 3 di-\\nmensions, then we can also plot the y(i)’s to visualize the data. For instance,\\nif we were to reduce our automobiles data to 2 dimensions, then we can plot\\nit (one point in our plot would correspond to one car type, say) to see what\\ncars are similar to each other and what groups of cars may cluster together.\\nAnother standard application is to preprocess a dataset to reduce its\\ndimension before running a supervised learning learning algorithm with the\\nx(i)’s as inputs.\\nApart from computational beneﬁts, reducing the data’s\\ndimension can also reduce the complexity of the hypothesis class considered\\nand help avoid overﬁtting (e.g., linear classiﬁers over lower dimensional input\\nspaces will have smaller VC dimension).\\nLastly, as in our RC pilot example, we can also view PCA as a noise\\nreduction algorithm.\\nIn our example it, estimates the intrinsic “piloting\\nkarma” from the noisy measures of piloting skill and enjoyment. In class, we\\nalso saw the application of this idea to face images, resulting in eigenfaces\\nmethod. Here, each point x(i) ∈R100×100 was a 10000 dimensional vector,\\nwith each coordinate corresponding to a pixel intensity value in a 100x100\\nimage of a face. Using PCA, we represent each image x(i) with a much lower-\\ndimensional y(i). In doing so, we hope that the principal components we\\nfound retain the interesting, systematic variations between faces that capture\\nwhat a person really looks like, but not the “noise” in the images introduced\\nby minor lighting variations, slightly diﬀerent imaging conditions, and so on.\\nWe then measure distances between faces i and j by working in the reduced\\ndimension, and computing ∥y(i) −y(j)∥2. This resulted in a surprisingly good\\nface-matching and retrieval algorithm.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 171}, page_content='Chapter 13\\nIndependent components\\nanalysis\\nOur next topic is Independent Components Analysis (ICA). Similar to PCA,\\nthis will ﬁnd a new basis in which to represent our data. However, the goal\\nis very diﬀerent.\\nAs a motivating example, consider the “cocktail party problem.” Here, d\\nspeakers are speaking simultaneously at a party, and any microphone placed\\nin the room records only an overlapping combination of the d speakers’ voices.\\nBut lets say we have d diﬀerent microphones placed in the room, and because\\neach microphone is a diﬀerent distance from each of the speakers, it records a\\ndiﬀerent combination of the speakers’ voices. Using these microphone record-\\nings, can we separate out the original d speakers’ speech signals?\\nTo formalize this problem, we imagine that there is some data s ∈Rd\\nthat is generated via d independent sources. What we observe is\\nx = As,\\nwhere A is an unknown square matrix called the mixing matrix. Repeated\\nobservations gives us a dataset {x(i); i = 1, . . . , n}, and our goal is to recover\\nthe sources s(i) that had generated our data (x(i) = As(i)).\\nIn our cocktail party problem, s(i) is an d-dimensional vector, and s(i)\\nj\\nis\\nthe sound that speaker j was uttering at time i. Also, x(i) in an d-dimensional\\nvector, and x(i)\\nj\\nis the acoustic reading recorded by microphone j at time i.\\nLet W = A−1 be the unmixing matrix. Our goal is to ﬁnd W, so\\nthat given our microphone recordings x(i), we can recover the sources by\\ncomputing s(i) = Wx(i). For notational convenience, we also let wT\\ni denote\\n171'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 172}, page_content='172\\nthe i-th row of W, so that\\nW =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n— wT\\n1 —\\n...\\n— wT\\nd —\\n\\uf8f9\\n\\uf8fa\\uf8fb.\\nThus, wi ∈Rd, and the j-th source can be recovered as s(i)\\nj\\n= wT\\nj x(i).\\n13.1\\nICA ambiguities\\nTo what degree can W = A−1 be recovered? If we have no prior knowledge\\nabout the sources and the mixing matrix, it is easy to see that there are some\\ninherent ambiguities in A that are impossible to recover, given only the x(i)’s.\\nSpeciﬁcally, let P be any d-by-d permutation matrix. This means that\\neach row and each column of P has exactly one “1.” Here are some examples\\nof permutation matrices:\\nP =\\n\\uf8ee\\n\\uf8f0\\n0\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n1\\n\\uf8f9\\n\\uf8fb;\\nP =\\n\\x14 0\\n1\\n1\\n0\\n\\x15\\n;\\nP =\\n\\x14 1\\n0\\n0\\n1\\n\\x15\\n.\\nIf z is a vector, then Pz is another vector that contains a permuted version\\nof z’s coordinates. Given only the x(i)’s, there will be no way to distinguish\\nbetween W and PW. Speciﬁcally, the permutation of the original sources is\\nambiguous, which should be no surprise. Fortunately, this does not matter\\nfor most applications.\\nFurther, there is no way to recover the correct scaling of the wi’s. For in-\\nstance, if A were replaced with 2A, and every s(i) were replaced with (0.5)s(i),\\nthen our observed x(i) = 2A · (0.5)s(i) would still be the same. More broadly,\\nif a single column of A were scaled by a factor of α, and the corresponding\\nsource were scaled by a factor of 1/α, then there is again no way to determine\\nthat this had happened given only the x(i)’s. Thus, we cannot recover the\\n“correct” scaling of the sources. However, for the applications that we are\\nconcerned with—including the cocktail party problem—this ambiguity also\\ndoes not matter. Speciﬁcally, scaling a speaker’s speech signal s(i)\\nj\\nby some\\npositive factor α aﬀects only the volume of that speaker’s speech. Also, sign\\nchanges do not matter, and s(i)\\nj\\nand −s(i)\\nj\\nsound identical when played on a\\nspeaker. Thus, if the wi found by an algorithm is scaled by any non-zero real\\nnumber, the corresponding recovered source si = wT\\ni x will be scaled by the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 173}, page_content='173\\nsame factor; but this usually does not matter. (These comments also apply\\nto ICA for the brain/MEG data that we talked about in class.)\\nAre these the only sources of ambiguity in ICA? It turns out that they\\nare, so long as the sources si are non-Gaussian. To see what the diﬃculty is\\nwith Gaussian data, consider an example in which n = 2, and s ∼N(0, I).\\nHere, I is the 2x2 identity matrix. Note that the contours of the density of\\nthe standard normal distribution N(0, I) are circles centered on the origin,\\nand the density is rotationally symmetric.\\nNow, suppose we observe some x = As, where A is our mixing matrix.\\nThen, the distribution of x will be Gaussian, x ∼N(0, AAT), since\\nEs∼N(0,I)[x] = E[As] = AE[s] = 0\\nCov[x] = Es∼N(0,I)[xxT] = E[AssTAT] = AE[ssT]AT = A · Cov[s] · AT = AAT\\nNow, let R be an arbitrary orthogonal (less formally, a rotation/reﬂection)\\nmatrix, so that RRT = RTR = I, and let A′ = AR. Then if the data had\\nbeen mixed according to A′ instead of A, we would have instead observed\\nx′ = A′s. The distribution of x′ is also Gaussian, x′ ∼N(0, AAT), since\\nEs∼N(0,I)[x′(x′)T] = E[A′ssT(A′)T] = E[ARssT(AR)T] = ARRTAT = AAT.\\nHence, whether the mixing matrix is A or A′, we would observe data from\\na N(0, AAT) distribution. Thus, there is no way to tell if the sources were\\nmixed using A and A′. There is an arbitrary rotational component in the\\nmixing matrix that cannot be determined from the data, and we cannot\\nrecover the original sources.\\nOur argument above was based on the fact that the multivariate standard\\nnormal distribution is rotationally symmetric. Despite the bleak picture that\\nthis paints for ICA on Gaussian data, it turns out that, so long as the data is\\nnot Gaussian, it is possible, given enough data, to recover the d independent\\nsources.\\n13.2\\nDensities and linear transformations\\nBefore moving on to derive the ICA algorithm proper, we ﬁrst digress brieﬂy\\nto talk about the eﬀect of linear transformations on densities.\\nSuppose a random variable s is drawn according to some density ps(s).\\nFor simplicity, assume for now that s ∈R is a real number. Now, let the\\nrandom variable x be deﬁned according to x = As (here, x ∈R, A ∈R). Let\\npx be the density of x. What is px?\\nLet W = A−1. To calculate the “probability” of a particular value of x,\\nit is tempting to compute s = Wx, then then evaluate ps at that point, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 174}, page_content='174\\nconclude that “px(x) = ps(Wx).” However, this is incorrect. For example,\\nlet s ∼Uniform[0, 1], so ps(s) = 1{0 ≤s ≤1}. Now, let A = 2, so x = 2s.\\nClearly, x is distributed uniformly in the interval [0, 2]. Thus, its density is\\ngiven by px(x) = (0.5)1{0 ≤x ≤2}. This does not equal ps(Wx), where\\nW = 0.5 = A−1. Instead, the correct formula is px(x) = ps(Wx)|W|.\\nMore generally, if s is a vector-valued distribution with density ps, and\\nx = As for a square, invertible matrix A, then the density of x is given by\\npx(x) = ps(Wx) · |W|,\\nwhere W = A−1.\\nRemark. If you’re seen the result that A maps [0, 1]d to a set of volume |A|,\\nthen here’s another way to remember the formula for px given above, that also\\ngeneralizes our previous 1-dimensional example. Speciﬁcally, let A ∈Rd×d be\\ngiven, and let W = A−1 as usual. Also let C1 = [0, 1]d be the d-dimensional\\nhypercube, and deﬁne C2 = {As : s ∈C1} ⊆Rd to be the image of C1\\nunder the mapping given by A. Then it is a standard result in linear algebra\\n(and, indeed, one of the ways of deﬁning determinants) that the volume of\\nC2 is given by |A|. Now, suppose s is uniformly distributed in [0, 1]d, so its\\ndensity is ps(s) = 1{s ∈C1}. Then clearly x will be uniformly distributed\\nin C2. Its density is therefore found to be px(x) = 1{x ∈C2}/vol(C2) (since\\nit must integrate over C2 to 1). But using the fact that the determinant\\nof the inverse of a matrix is just the inverse of the determinant, we have\\n1/vol(C2) = 1/|A| = |A−1| = |W|. Thus, px(x) = 1{x ∈C2}|W| = 1{Wx ∈\\nC1}|W| = ps(Wx)|W|.\\n13.3\\nICA algorithm\\nWe are now ready to derive an ICA algorithm. We describe an algorithm\\nby Bell and Sejnowski, and we give an interpretation of their algorithm as a\\nmethod for maximum likelihood estimation. (This is diﬀerent from their orig-\\ninal interpretation involving a complicated idea called the infomax principal\\nwhich is no longer necessary given the modern understanding of ICA.)\\nWe suppose that the distribution of each source sj is given by a density\\nps, and that the joint distribution of the sources s is given by\\np(s) =\\nd\\nY\\nj=1\\nps(sj).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 175}, page_content='175\\nNote that by modeling the joint distribution as a product of marginals, we\\ncapture the assumption that the sources are independent. Using our formulas\\nfrom the previous section, this implies the following density on x = As =\\nW −1s:\\np(x) =\\nd\\nY\\nj=1\\nps(wT\\nj x) · |W|.\\nAll that remains is to specify a density for the individual sources ps.\\nRecall that, given a real-valued random variable z, its cumulative distri-\\nbution function (cdf) F is deﬁned by F(z0) = P(z ≤z0) =\\nR z0\\n−∞pz(z)dz and\\nthe density is the derivative of the cdf: pz(z) = F ′(z).\\nThus, to specify a density for the si’s, all we need to do is to specify some\\ncdf for it. A cdf has to be a monotonic function that increases from zero\\nto one. Following our previous discussion, we cannot choose the Gaussian\\ncdf, as ICA doesn’t work on Gaussian data. What we’ll choose instead as\\na reasonable “default” cdf that slowly increases from 0 to 1, is the sigmoid\\nfunction g(s) = 1/(1 + e−s). Hence, ps(s) = g′(s).1\\nThe square matrix W is the parameter in our model. Given a training\\nset {x(i); i = 1, . . . , n}, the log likelihood is given by\\nℓ(W) =\\nn\\nX\\ni=1\\n \\nd\\nX\\nj=1\\nlog g′(wT\\nj x(i)) + log |W|\\n!\\n.\\nWe would like to maximize this in terms W. By taking derivatives and using\\nthe fact (from the ﬁrst set of notes) that ∇W|W| = |W|(W −1)T, we easily\\nderive a stochastic gradient ascent learning rule. For a training example x(i),\\nthe update rule is:\\nW := W + α\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 −2g(wT\\n1 x(i))\\n1 −2g(wT\\n2 x(i))\\n...\\n1 −2g(wT\\nd x(i))\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fbx(i)T + (W T)−1\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8,\\n1If you have prior knowledge that the sources’ densities take a certain form, then it\\nis a good idea to substitute that in here.\\nBut in the absence of such knowledge, the\\nsigmoid function can be thought of as a reasonable default that seems to work well for\\nmany problems. Also, the presentation here assumes that either the data x(i) has been\\npreprocessed to have zero mean, or that it can naturally be expected to have zero mean\\n(such as acoustic signals). This is necessary because our assumption that ps(s) = g′(s)\\nimplies E[s] = 0 (the derivative of the logistic function is a symmetric function, and\\nhence gives a density corresponding to a random variable with zero mean), which implies\\nE[x] = E[As] = 0.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 176}, page_content='176\\nwhere α is the learning rate.\\nAfter the algorithm converges, we then compute s(i) = Wx(i) to recover\\nthe original sources.\\nRemark. When writing down the likelihood of the data, we implicitly as-\\nsumed that the x(i)’s were independent of each other (for diﬀerent values\\nof i; note this issue is diﬀerent from whether the diﬀerent coordinates of\\nx(i) are independent), so that the likelihood of the training set was given\\nby Q\\ni p(x(i); W). This assumption is clearly incorrect for speech data and\\nother time series where the x(i)’s are dependent, but it can be shown that\\nhaving correlated training examples will not hurt the performance of the al-\\ngorithm if we have suﬃcient data. However, for problems where successive\\ntraining examples are correlated, when implementing stochastic gradient as-\\ncent, it sometimes helps accelerate convergence if we visit training examples\\nin a randomly permuted order. (I.e., run stochastic gradient ascent on a\\nrandomly shuﬄed copy of the training set.)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 177}, page_content='Chapter 14\\nSelf-supervised learning and\\nfoundation models\\nDespite its huge success, supervised learning with neural networks typically\\nrelies on the availability of a labeled dataset of decent size, which is some-\\ntimes costly to collect. Recently, AI and machine learning are undergoing a\\nparadigm shift with the rise of models (e.g., BERT [Devlin et al., 2019] and\\nGPT-3 [Brown et al., 2020]) that are pre-trained on broad data at scale and\\nare adaptable to a wide range of downstream tasks. These models, called\\nfoundation models by Bommasani et al. [2021], oftentimes leverage massive\\nunlabeled data so that much fewer labeled data in the downstream tasks are\\nneeded. Moreover, though foundation models are based on standard deep\\nlearning and transfer learning, their scale results in new emergent capabil-\\nities. These models are typically (pre-)trained by self-supervised learning\\nmethods where the supervisions/labels come from parts of the inputs.\\nThis chapter will introduce the paradigm of foundation models and basic\\nrelated concepts.\\n14.1\\nPretraining and adaptation\\nThe foundation models paradigm consists of two phases: pretraining (or sim-\\nply training) and adaptation. We ﬁrst pretrain a large model on a massive\\nunlabeled dataset (e.g., billions of unlabeled images).1 Then, we adapt the\\npretrained model to a downstream task (e.g., detecting cancer from scan im-\\nages). These downstream tasks are often prediction tasks with limited or\\n1Sometimes, pretraining can involve large-scale labeled datasets as well (e.g., the Ima-\\ngeNet dataset).\\n177'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 178}, page_content='178\\neven no labeled data. The intuition is that the pretrained models learn good\\nrepresentations that capture intrinsic semantic structure/ information about\\nthe data, and the adaptation phase customizes the model to a particular\\ndownstream task by, e.g., retrieving the information speciﬁc to it. For ex-\\nample, a model pretrained on massive unlabeled image data may learn good\\ngeneral visual representations/features, and we adapt the representations to\\nsolve biomedical imagining tasks.\\nWe formalize the two phases below.\\nPretraining.\\nSuppose\\nwe\\nhave\\nan\\nunlabeled\\npretraining\\ndataset\\n{x(1), x(2) · · · , x(n)} that consists of n examples in Rd. Let φθ be a model that\\nis parameterized by θ and maps the input x to some m-dimensional represen-\\ntation φθ(x). (People also call φθ(x) ∈Rm the embedding or features of the\\nexample x.) We pretrain the model θ with a pretraining loss, which is often\\nan average of loss functions on all the examples: Lpre(θ) = 1\\nn\\nPn\\ni=1 ℓpre(θ, x(i)).\\nHere ℓpre is a so-called self-supervised loss on a single datapoint x(i), because\\nas shown later, e.g., in Section 14.3, the “supervision” comes from the data\\npoint x(i) itself. It is also possible that the pretraining loss is not a sum\\nof losses on individual examples. We will discuss two pretraining losses in\\nSection 14.2 and Section 14.3.\\nWe use some optimizers (mostly likely SGD or ADAM [Kingma and Ba,\\n2014]) to minimize Lpre(θ). We denote the obtained pretrained model by ˆθ.\\nAdaptation. For a downstream task, we usually have a labeled dataset\\n{(x(1)\\ntask, y(1)\\ntask), · · · , (x(ntask)\\ntask\\n, y(ntask)\\ntask\\n)} with ntask examples. The setting when\\nntask = 0 is called zero-shot learning—the downstream task doesn’t have any\\nlabeled examples. When ntask is relatively small (say, between 1 and 50), the\\nsetting is called few-shot learning. It’s also pretty common to have a larger\\nntask on the order of ranging from hundreds to tens of thousands.\\nAn adaptation algorithm generally takes in a downstream dataset and the\\npretrained model ˆθ, and outputs a variant of ˆθ that solves the downstream\\ntask. We will discuss below two popular and general adaptation methods,\\nlinear probe and ﬁnetuning. In addition, two other methods speciﬁc to lan-\\nguage problems are introduced in 14.3.2.\\nThe linear probe approach uses a linear head on top of the representation\\nto predict the downstream labels. Mathematically, the adapted model out-\\nputs w⊤φˆθ(x), where w ∈Rm is a parameter to be learned, and ˆθ is exactly\\nthe pretrained model (ﬁxed). We can use SGD (or other optimizers) to train'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 179}, page_content='179\\nw on the downstream task loss to predict the task label\\nmin\\nw∈Rm\\n1\\nntask\\nntask\\nX\\ni=1\\nℓtask(y(i)\\ntask, w⊤φˆθ(x(i)\\ntask))\\n(14.1)\\nE.g.,\\nif the downstream task is a regression problem,\\nwe will have\\nℓtask(ytask, w⊤φˆθ(xtask)) = (ytask −w⊤φˆθ(xtask))2.\\nThe ﬁnetuning algorithm uses a similar structure for the downstream\\nprediction model, but also further ﬁnetunes the pretrained model (instead\\nof keeping it ﬁxed). Concretely, the prediction model is w⊤φθ(x) with pa-\\nrameters w and θ. We optimize both w and θ to ﬁt the downstream data,\\nbut initialize θ with the pretrained model ˆθ. The linear head w is usually\\ninitialized randomly.\\nminimize\\nw,θ\\n1\\nntask\\nntask\\nX\\ni=1\\nℓtask(y(i)\\ntask, w⊤φθ(x(i)\\ntask))\\n(14.2)\\nwith initialization\\nw ←random vector\\n(14.3)\\nθ ←ˆθ\\n(14.4)\\nVarious other adaptation methods exists and are sometimes specialized\\nto the particular pretraining methods. We will discuss one of them in Sec-\\ntion 14.3.2.\\n14.2\\nPretraining methods in computer vision\\nThis section introduces two concrete pretraining methods for computer vi-\\nsion: supervised pretraining and contrastive learning.\\nSupervised pretraining.\\nHere, the pretraining dataset is a large-scale\\nlabeled dataset (e.g., ImageNet), and the pretrained models are simply a\\nneural network trained with vanilla supervised learning (with the last layer\\nbeing removed). Concretely, suppose we write the learned neural network as\\nUφˆθ(x), where U is the last (fully-connected) layer parameters, ˆθ corresponds\\nto the parameters of all the other layers, and φˆθ(x) are the penultimate\\nactivations layer (which serves as the representation). We simply discard U\\nand use φˆθ(x) as the pretrained model.\\nContrastive learning. Contrastive learning is a self-supervised pretraining\\nmethod that uses only unlabeled data. The main intuition is that a good\\nrepresentation function φθ(·) should map semantically similar images to sim-\\nilar representations, and that random pair of images should generally have'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 180}, page_content='180\\ndistinct representations. E.g., we may want to map images of two huskies to\\nsimilar representations, but a husky and an elephant should have diﬀerent\\nrepresentations. One deﬁnition of similarity is that images from the same\\nclass are similar. Using this deﬁnition will result in the so-called supervised\\ncontrastive algorithms that work well when labeled pretraining datasets are\\navailable.\\nWithout labeled data, we can use data augmentation to generate a pair\\nof “similar” augmented images given an original image x. Data augmenta-\\ntion typically means that we apply random cropping, ﬂipping, and/or color\\ntransformation on the original image x to generate a variant. We can take\\ntwo random augmentations, denoted by ˆx and ˜x, of the same original image\\nx, and call them a positive pair. We observe that positive pairs of images\\nare often semantically related because they are augmentations of the same\\nimage. We will design a loss function for θ such that the representations of\\na positive pair, φθ(ˆx), φθ(˜x), as close to each other as possible.\\nOn the other hand, we can also take another random image z from the\\npretraining dataset and generate an augmentation ˆz from z. Note that (ˆx, ˆz)\\nare from diﬀerent images; therefore, with a good chance, they are not seman-\\ntically related. We call (ˆx, ˆz) a negative or random pair.2 We will design a\\nloss to push the representation of random pairs, φθ(ˆx), φθ(ˆz), far away from\\neach other.\\nThere are many recent algorithms based on the contrastive learning prin-\\nciple, and here we introduce SIMCLR [Chen et al., 2020] as an concrete\\nexample. The loss function is deﬁned on a batch of examples (x1, · · · , x(B))\\nwith batch size B. The algorithm computes two random augmentations for\\neach example x(i) in the batch, denoted by ˆx(i) and ˜x(i). As a result, we\\nhave the augmented batch of 2B examples: ˆx1, · · · , ˆx(B), ˜x1, · · · , ˜x(B). The\\nSIMCLR loss is deﬁned as3\\nLpre(θ) = −\\nB\\nX\\ni=1\\nlog\\nexp\\n\\x00φθ(ˆx(i))⊤φθ(˜x(i))\\n\\x01\\nexp (φθ(ˆx(i))⊤φθ(˜x(i))) + P\\nj̸=i exp (φθ(ˆx(i))⊤φθ(˜x(j))).\\nThe intuition is as follows. The loss is increasing in φθ(ˆx(i))⊤φθ(˜x(j)), and\\nthus minimizing the loss encourages φθ(ˆx(i))⊤φθ(˜x(j)) to be small, making\\nφθ(ˆx(i)) far away from φθ(˜x(j)). On the other hand, the loss is decreasing in\\n2Random pair may be a more accurate term because it’s still possible (though not\\nlikely) that x and z are semantically related, so are ˆx and ˆz. But in the literature, the\\nterm negative pair seems to be also common.\\n3This is a variant and simpliﬁcation of the original loss that does not change the essence\\n(but may change the eﬃciency slightly).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 181}, page_content='181\\nφθ(ˆx(i))⊤φθ(˜x(i)), and thus minimizing the loss encourages φθ(ˆx(i))⊤φθ(˜x(i))\\nto be large, resulting in φθ(ˆx(i)) and φθ(˜x(i)) to be close.4\\n14.3\\nPretrained large language models\\nNatural language processing is another area where pretraining models are\\nparticularly successful. In language problems, an example typically corre-\\nsponds to a document or generally a sequence/trunk of words,5 denoted\\nby x = (x1, · · · , xT) where T is the length of the document/sequence,\\nxi ∈{1, · · · , V } are words in the document, and V is the vocabulary size.6\\nA language model is a probabilistic model representing the probability of\\na document, denoted by p(x1, · · · , xT). This probability distribution is very\\ncomplex because its support size is V T — exponential in the length of the\\ndocument. Instead of modeling the distribution of a document itself, we can\\napply the chain rule of conditional probability to decompose it as follows:\\np(x1, · · · , xT) = p(x1)p(x2|x1) · · · p(xT|x1, · · · , xT−1).\\n(14.5)\\nNow the support size of each of the conditional probability p(xt|x1, · · · , xt−1)\\nis V .\\nWe will model the conditional probability p(xt|x1, · · · , xt−1) as a function\\nof x1, . . . , xt−1 parameterized by some parameter θ.\\nA parameterized model takes in numerical inputs and therefore we ﬁrst\\nintroduce embeddings or representations fo the words. Let ei ∈Rd be the\\nembedding of the word i ∈{1, 2, · · · , V }. We call [e1, · · · , eV ] ∈Rd×V the\\nembedding matrix.\\nThe most commonly used model is Transformer [Vaswani et al., 2017]. In\\nthis subsection, we will introduce the input-output interface of a Transformer,\\nbut treat the intermediate computation in the Transformer as a blackbox. We\\nrefer the students to the transformer paper or more advanced courses for more\\ndetails. As shown in Figure 14.1, given a document (x1, · · · , xT), we ﬁrst\\ntranslate the sequence of discrete variables into a sequence of corresponding\\n4To see this, you can verify that the function −log\\np\\np+q is decreasing in p, and increasing\\nin q when p, q > 0.\\n5In the practical implementations, typically all the data are concatenated into a single\\nsequence in some order, and each example typically corresponds a sub-sequence of consec-\\nutive words which may corresponds to a subset of a document or may span across multiple\\ndocuments.\\n6Technically, words may be decomposed into tokens which could be words or sub-words\\n(combinations of letters), but this note omits this technicality. In fact most commons words\\nare a single token themselves.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 182}, page_content='182\\nword embeddings (ex1, · · · , exT ).\\nWe also introduce a ﬁxed special token\\nx0 = ⊥in the vocabulary with corresponding embedding ex0 to mark the\\nbeginning of a document.\\nThen, the word embeddings are passed into a\\nTransformer model, which takes in a sequence of vectors (ex0, ex1, · · · , exT )\\nand outputs a sequence of vectors (u1, u2, · · · , uT+1), where ut ∈RV will be\\ninterpreted as the logits for the probability distribution of the next word.\\nHere we use the autoregressive version of the Transformers which by design\\nensures ut only depends on x1, · · · , xt−1 (note that this property does not\\nhold in masked language models [Devlin et al., 2019] where the losses are\\nalso diﬀerent.) We view the whole mapping from x’s to u’s a blackbox in\\nthis subsection and call it a Transformer, denoted it by fθ, where θ include\\nboth the parameters in the Transformer and the input embeddings. We write\\nut = fθ(x0, x1, . . . , xt−1) where fθ denotes the mapping from the input to the\\noutputs.\\n𝑥!\\n𝑥\"\\n𝑥#\\n𝑒$!\\n𝑒$\"\\n𝑒$#\\n…\\nTransformer \\n𝑓%(𝑥)\\n𝑥&\\n𝑒$$\\n𝑢\"\\n𝑢\\'\\n𝑢#(!\\n𝑢!\\n…\\nFigure 14.1: The inputs and outputs of a Transformer model.\\nThe conditional probability p(xt|x1, · · · , xt−1) is the softtmax of the logits:\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\np(xt = 1|x1 · · · , xt−1)\\np(xt = 2|x1 · · · , xt−1)\\n...\\np(xt = V |x1 · · · , xt−1)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb= softmax(ut) ∈RV\\n(14.6)\\n= softmax(fθ(x0, . . . , xt−1))\\n(14.7)\\nWe train the Transformer parameter θ by minimizing the negative log-\\nlikelihood of seeing the data under the probabilistic model deﬁned by θ,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 183}, page_content='183\\nwhich is the cross-entropy loss on the logitis.\\nloss(θ) = 1\\nT\\nT\\nX\\nt=1\\n−log(pθ(xt|x1, . . . , xt−1))\\n(14.8)\\n= 1\\nT\\nT\\nX\\nt=1\\nℓce(fθ(x0, x1, · · · , xt−1), xt)\\n= 1\\nT\\nT\\nX\\nt=1\\n−log(softmax(fθ(x0, x1, · · · , xt−1))xt) .\\nAutoregressive text decoding / generation.\\nGiven a autoregressive\\nTransformer, we can simply sample text from it sequentially. Given a pre-\\nﬁx x1, . . . xt, we generate text completion xt+1, . . . xT sequentially using the\\nconditional distribution.\\nxt+1 ∼softmax(fθ(x0, x1, · · · , xt))\\n(14.9)\\nxt+2 ∼softmax(fθ(x0, x1, · · · , xt+1))\\n(14.10)\\n. . .\\n(14.11)\\nxT ∼softmax(fθ(x0, x1, · · · , xT−1)) .\\n(14.12)\\nNote that each generated token is used as the input to the model when gen-\\nerating the following tokens. In practice, people often introduce a parameter\\nτ > 0 named temperature to further adjust the entropy/sharpness of the\\ngenerated distribution,\\nxt+1 ∼softmax(fθ(x0, x1, · · · , xt)/τ)\\n(14.13)\\nxt+2 ∼softmax(fθ(x0, x1, · · · , xt+1)/τ)\\n(14.14)\\n. . .\\n(14.15)\\nxT ∼softmax(fθ(x0, x1, · · · , xT−1)/τ) .\\n(14.16)\\nWhen τ = 1, the text is sampled from the original conditional probability\\ndeﬁned by the model. With a decreasing τ, the generated text gradually\\nbecomes more “deterministic”. τ →0 reduces to greedy decoding, where we\\ngenerate the most probable next token from the conditional probability.\\n14.3.1\\nZero-shot learning and in-context learning\\nFor language models, there are many ways to adapt a pretrained model to\\ndownstream tasks. In this notes, we discuss three of them: ﬁnetuning, zero-\\nshot learning, and in-context learning.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 184}, page_content='184\\nFinetuning is not very common for the autoregressive language models that\\nwe introduced in Section 14.3 but much more common for other variants\\nsuch as masked language models which has similar input-output interfaces\\nbut are pretrained diﬀerently [Devlin et al., 2019]. The ﬁnetuning method is\\nthe same as introduced generally in Section 14.1—the only question is how\\nwe deﬁne the prediction task with an additional linear head. One option\\nis to treat cT+1 = φθ(x1, · · · , xT) as the representation and use w⊤cT+1 =\\nw⊤φθ(x1, · · · , xT) to predict task label.\\nAs described in Section 14.1, we\\ninitialize θ to the pretrained model ˆθ and then optimize both w and θ.\\nZero-shot adaptation or zero-shot learning is the setting where there is no\\ninput-output pairs from the downstream tasks. For language problems tasks,\\ntypically the task is formatted as a question or a cloze test form via natural\\nlanguage. For example, we can format an example as a question:\\nxtask = (xtask,1, · · · , xtask,T) = “Is the speed of light a universal constant?”\\nThen, we compute the most likely next word predicted by the lan-\\nguage model given this question, that is, computing argmaxxT +1p(xT+1 |\\nxtask,1, · · · , xtask,T). In this case, if the most likely next word xT+1 is “No”,\\nthen we solve the task. (The speed of light is only a constant in vacuum).\\nWe note that there are many ways to decode the answer from the language\\nmodels, e.g., instead of computing the argmax, we may use the language\\nmodel to generate a few words word. It is an active research question to ﬁnd\\nthe best way to utilize the language models.\\nIn-context learning is mostly used for few-shot settings where we have a\\nfew labeled examples (x(1)\\ntask, y(1)\\ntask), · · · , (x(ntask)\\ntask\\n, y(ntask)\\ntask\\n). Given a test example\\nxtest, we construct a document (x1, · · · , xT), which is more commonly called\\na “prompt” in this context, by concatenating the labeled examples and the\\ntext example in some format. For example, we may construct the prompt as\\nfollows\\nx1, · · · , xT\\n=\\n“Q: 2 ∼3 = ?\\nx(1)\\ntask\\nA: 5\\ny(1)\\ntask\\nQ: 6 ∼7 = ?\\nx(2)\\ntask\\nA: 13\\ny(2)\\ntask\\n· · ·\\nQ: 15 ∼2 = ?”\\nxtest'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 185}, page_content='185\\nThen, we let the pretrained model generate the most likely xT+1, xT+2, · · · .\\nIn this case, if the model can “learn” that the symbol ∼means addition from\\nthe few examples, we will obtain the following which suggests the answer is\\n17.\\nxT+1, xT+2, · · · = “A: 17”.\\nThe area of foundation models is very new and quickly growing. The notes\\nhere only attempt to introduce these models on a conceptual level with a\\nsigniﬁcant amount of simpliﬁcation. We refer the readers to other materials,\\ne.g., Bommasani et al. [2021], for more details.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 186}, page_content='Part V\\nReinforcement Learning and\\nControl\\n186'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 187}, page_content='Chapter 15\\nReinforcement learning\\nWe now begin our study of reinforcement learning and adaptive control.\\nIn supervised learning, we saw algorithms that tried to make their outputs\\nmimic the labels y given in the training set. In that setting, the labels gave\\nan unambiguous “right answer” for each of the inputs x. In contrast, for\\nmany sequential decision making and control problems, it is very diﬃcult to\\nprovide this type of explicit supervision to a learning algorithm. For example,\\nif we have just built a four-legged robot and are trying to program it to walk,\\nthen initially we have no idea what the “correct” actions to take are to make\\nit walk, and so do not know how to provide explicit supervision for a learning\\nalgorithm to try to mimic.\\nIn the reinforcement learning framework, we will instead provide our al-\\ngorithms only a reward function, which indicates to the learning agent when\\nit is doing well, and when it is doing poorly. In the four-legged walking ex-\\nample, the reward function might give the robot positive rewards for moving\\nforwards, and negative rewards for either moving backwards or falling over.\\nIt will then be the learning algorithm’s job to ﬁgure out how to choose actions\\nover time so as to obtain large rewards.\\nReinforcement learning has been successful in applications as diverse as\\nautonomous helicopter ﬂight, robot legged locomotion, cell-phone network\\nrouting, marketing strategy selection, factory control, and eﬃcient web-page\\nindexing. Our study of reinforcement learning will begin with a deﬁnition of\\nthe Markov decision processes (MDP), which provides the formalism in\\nwhich RL problems are usually posed.\\n187'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 188}, page_content='188\\n15.1\\nMarkov decision processes\\nA Markov decision process is a tuple (S, A, {Psa}, γ, R), where:\\n• S is a set of states. (For example, in autonomous helicopter ﬂight, S\\nmight be the set of all possible positions and orientations of the heli-\\ncopter.)\\n• A is a set of actions. (For example, the set of all possible directions in\\nwhich you can push the helicopter’s control sticks.)\\n• Psa are the state transition probabilities.\\nFor each state s ∈S and\\naction a ∈A, Psa is a distribution over the state space. We’ll say more\\nabout this later, but brieﬂy, Psa gives the distribution over what states\\nwe will transition to if we take action a in state s.\\n• γ ∈[0, 1) is called the discount factor.\\n• R : S × A 7→R is the reward function. (Rewards are sometimes also\\nwritten as a function of a state S only, in which case we would have\\nR : S 7→R).\\nThe dynamics of an MDP proceeds as follows: We start in some state s0,\\nand get to choose some action a0 ∈A to take in the MDP. As a result of our\\nchoice, the state of the MDP randomly transitions to some successor state\\ns1, drawn according to s1 ∼Ps0a0. Then, we get to pick another action a1.\\nAs a result of this action, the state transitions again, now to some s2 ∼Ps1a1.\\nWe then pick a2, and so on. . . . Pictorially, we can represent this process as\\nfollows:\\ns0\\na0\\n−→s1\\na1\\n−→s2\\na2\\n−→s3\\na3\\n−→. . .\\nUpon visiting the sequence of states s0, s1, . . . with actions a0, a1, . . ., our\\ntotal payoﬀis given by\\nR(s0, a0) + γR(s1, a1) + γ2R(s2, a2) + · · · .\\nOr, when we are writing rewards as a function of the states only, this becomes\\nR(s0) + γR(s1) + γ2R(s2) + · · · .\\nFor most of our development, we will use the simpler state-rewards R(s),\\nthough the generalization to state-action rewards R(s, a) oﬀers no special\\ndiﬃculties.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 189}, page_content='189\\nOur goal in reinforcement learning is to choose actions over time so as to\\nmaximize the expected value of the total payoﬀ:\\nE\\n\\x02\\nR(s0) + γR(s1) + γ2R(s2) + · · ·\\n\\x03\\nNote that the reward at timestep t is discounted by a factor of γt. Thus, to\\nmake this expectation large, we would like to accrue positive rewards as soon\\nas possible (and postpone negative rewards as long as possible). In economic\\napplications where R(·) is the amount of money made, γ also has a natural\\ninterpretation in terms of the interest rate (where a dollar today is worth\\nmore than a dollar tomorrow).\\nA policy is any function π : S 7→A mapping from the states to the\\nactions. We say that we are executing some policy π if, whenever we are\\nin state s, we take action a = π(s). We also deﬁne the value function for\\na policy π according to\\nV π(s) = E\\n\\x02\\nR(s0) + γR(s1) + γ2R(s2) + · · ·\\n\\x0c\\x0c s0 = s, π].\\nV π(s) is simply the expected sum of discounted rewards upon starting in\\nstate s, and taking actions according to π.1\\nGiven a ﬁxed policy π, its value function V π satisﬁes the Bellman equa-\\ntions:\\nV π(s) = R(s) + γ\\nX\\ns′∈S\\nPsπ(s)(s′)V π(s′).\\nThis says that the expected sum of discounted rewards V π(s) for starting\\nin s consists of two terms: First, the immediate reward R(s) that we get\\nright away simply for starting in state s, and second, the expected sum of\\nfuture discounted rewards. Examining the second term in more detail, we\\nsee that the summation term above can be rewritten Es′∼Psπ(s)[V π(s′)]. This\\nis the expected sum of discounted rewards for starting in state s′, where s′\\nis distributed according Psπ(s), which is the distribution over where we will\\nend up after taking the ﬁrst action π(s) in the MDP from state s. Thus, the\\nsecond term above gives the expected sum of discounted rewards obtained\\nafter the ﬁrst step in the MDP.\\nBellman’s equations can be used to eﬃciently solve for V π. Speciﬁcally,\\nin a ﬁnite-state MDP (|S| < ∞), we can write down one such equation for\\nV π(s) for every state s. This gives us a set of |S| linear equations in |S|\\nvariables (the unknown V π(s)’s, one for each state), which can be eﬃciently\\nsolved for the V π(s)’s.\\n1This notation in which we condition on π isn’t technically correct because π isn’t a\\nrandom variable, but this is quite standard in the literature.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 190}, page_content='190\\nWe also deﬁne the optimal value function according to\\nV ∗(s) = max\\nπ\\nV π(s).\\n(15.1)\\nIn other words, this is the best possible expected sum of discounted rewards\\nthat can be attained using any policy. There is also a version of Bellman’s\\nequations for the optimal value function:\\nV ∗(s) = R(s) + max\\na∈A γ\\nX\\ns′∈S\\nPsa(s′)V ∗(s′).\\n(15.2)\\nThe ﬁrst term above is the immediate reward as before. The second term\\nis the maximum over all actions a of the expected future sum of discounted\\nrewards we’ll get upon after action a. You should make sure you understand\\nthis equation and see why it makes sense.\\nWe also deﬁne a policy π∗: S 7→A as follows:\\nπ∗(s) = arg max\\na∈A\\nX\\ns′∈S\\nPsa(s′)V ∗(s′).\\n(15.3)\\nNote that π∗(s) gives the action a that attains the maximum in the “max”\\nin Equation (15.2).\\nIt is a fact that for every state s and every policy π, we have\\nV ∗(s) = V π∗(s) ≥V π(s).\\nThe ﬁrst equality says that the V π∗, the value function for π∗, is equal to the\\noptimal value function V ∗for every state s. Further, the inequality above\\nsays that π∗’s value is at least a large as the value of any other other policy.\\nIn other words, π∗as deﬁned in Equation (15.3) is the optimal policy.\\nNote that π∗has the interesting property that it is the optimal policy\\nfor all states s. Speciﬁcally, it is not the case that if we were starting in\\nsome state s then there’d be some optimal policy for that state, and if we\\nwere starting in some other state s′ then there’d be some other policy that’s\\noptimal policy for s′. The same policy π∗attains the maximum in Equa-\\ntion (15.1) for all states s. This means that we can use the same policy π∗\\nno matter what the initial state of our MDP is.\\n15.2\\nValue iteration and policy iteration\\nWe now describe two eﬃcient algorithms for solving ﬁnite-state MDPs. For\\nnow, we will consider only MDPs with ﬁnite state and action spaces (|S| <'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 191}, page_content='191\\n∞, |A| < ∞). In this section, we will also assume that we know the state\\ntransition probabilities {Psa} and the reward function R.\\nThe ﬁrst algorithm, value iteration, is as follows:\\nAlgorithm 4 Value Iteration\\n1: For each state s, initialize V (s) := 0.\\n2: for until convergence do\\n3:\\nFor every state, update\\nV (s) := R(s) + max\\na∈A γ\\nX\\ns′\\nPsa(s′)V (s′).\\n(15.4)\\nThis algorithm can be thought of as repeatedly trying to update the\\nestimated value function using Bellman Equations (15.2).\\nThere are two possible ways of performing the updates in the inner loop of\\nthe algorithm. In the ﬁrst, we can ﬁrst compute the new values for V (s) for\\nevery state s, and then overwrite all the old values with the new values. This\\nis called a synchronous update. In this case, the algorithm can be viewed as\\nimplementing a “Bellman backup operator” that takes a current estimate of\\nthe value function, and maps it to a new estimate. (See homework problem\\nfor details.)\\nAlternatively, we can also perform asynchronous updates.\\nHere, we would loop over the states (in some order), updating the values one\\nat a time.\\nUnder either synchronous or asynchronous updates, it can be shown that\\nvalue iteration will cause V to converge to V ∗. Having found V ∗, we can\\nthen use Equation (15.3) to ﬁnd the optimal policy.\\nApart from value iteration, there is a second standard algorithm for ﬁnd-\\ning an optimal policy for an MDP. The policy iteration algorithm proceeds\\nas follows:\\nThus, the inner-loop repeatedly computes the value function for the cur-\\nrent policy, and then updates the policy using the current value function.\\n(The policy π found in step (b) is also called the policy that is greedy with\\nrespect to V .) Note that step (a) can be done via solving Bellman’s equa-\\ntions as described earlier, which in the case of a ﬁxed policy, is just a set of\\n|S| linear equations in |S| variables.\\nAfter at most a ﬁnite number of iterations of this algorithm, V will con-\\nverge to V ∗, and π will converge to π∗.2\\n2Note that value iteration cannot reach the exact V ∗in a ﬁnite number of iterations,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 192}, page_content='192\\nAlgorithm 5 Policy Iteration\\n1: Initialize π randomly.\\n2: for until convergence do\\n3:\\nLet V := V π.\\n▷typically by linear system solver\\n4:\\nFor each state s, let\\nπ(s) := arg max\\na∈A\\nX\\ns′\\nPsa(s′)V (s′).\\nBoth value iteration and policy iteration are standard algorithms for solv-\\ning MDPs, and there isn’t currently universal agreement over which algo-\\nrithm is better.\\nFor small MDPs, policy iteration is often very fats and\\nconverges with very few iterations.\\nHowever, for MDPs with large state\\nspaces, solving for V π explicitly would involve solving a large system of lin-\\near equations, and could be diﬃcult (and note that one has to solve the\\nlinear system multiple times in policy iteration). In these problems, value\\niteration may be preferred. For this reason, in practice value iteration seems\\nto be used more often than policy iteration. For some more discussions on\\nthe comparison and connection of value iteration and policy iteration, please\\nsee Section 15.5.\\n15.3\\nLearning a model for an MDP\\nSo far, we have discussed MDPs and algorithms for MDPs assuming that the\\nstate transition probabilities and rewards are known. In many realistic prob-\\nlems, we are not given state transition probabilities and rewards explicitly,\\nbut must instead estimate them from data. (Usually, S, A and γ are known.)\\nFor example, suppose that, for the inverted pendulum problem (see prob-\\nwhereas policy iteration with an exact linear system solver, can. This is because when\\nthe actions space and policy space are discrete and ﬁnite, and once the policy reaches the\\noptimal policy in policy iteration, then it will not change at all. On the other hand, even\\nthough value iteration will converge to the V ∗, but there is always some non-zero error in\\nthe learned value function.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 193}, page_content='193\\nlem set 4), we had a number of trials in the MDP, that proceeded as follows:\\ns(1)\\n0\\na(1)\\n0\\n−→s(1)\\n1\\na(1)\\n1\\n−→s(1)\\n2\\na(1)\\n2\\n−→s(1)\\n3\\na(1)\\n3\\n−→. . .\\ns(2)\\n0\\na(2)\\n0\\n−→s(2)\\n1\\na(2)\\n1\\n−→s(2)\\n2\\na(2)\\n2\\n−→s(2)\\n3\\na(2)\\n3\\n−→. . .\\n. . .\\nHere, s(j)\\ni\\nis the state we were at time i of trial j, and a(j)\\ni\\nis the cor-\\nresponding action that was taken from that state. In practice, each of the\\ntrials above might be run until the MDP terminates (such as if the pole falls\\nover in the inverted pendulum problem), or it might be run for some large\\nbut ﬁnite number of timesteps.\\nGiven this “experience” in the MDP consisting of a number of trials,\\nwe can then easily derive the maximum likelihood estimates for the state\\ntransition probabilities:\\nPsa(s′) = #times took we action a in state s and got to s′\\n#times we took action a in state s\\n(15.5)\\nOr, if the ratio above is “0/0”—corresponding to the case of never having\\ntaken action a in state s before—the we might simply estimate Psa(s′) to be\\n1/|S|. (I.e., estimate Psa to be the uniform distribution over all states.)\\nNote that, if we gain more experience (observe more trials) in the MDP,\\nthere is an eﬃcient way to update our estimated state transition probabilities\\nusing the new experience. Speciﬁcally, if we keep around the counts for both\\nthe numerator and denominator terms of (15.5), then as we observe more\\ntrials, we can simply keep accumulating those counts. Computing the ratio\\nof these counts then given our estimate of Psa.\\nUsing a similar procedure, if R is unknown, we can also pick our estimate\\nof the expected immediate reward R(s) in state s to be the average reward\\nobserved in state s.\\nHaving learned a model for the MDP, we can then use either value it-\\neration or policy iteration to solve the MDP using the estimated transition\\nprobabilities and rewards. For example, putting together model learning and\\nvalue iteration, here is one possible algorithm for learning in an MDP with\\nunknown state transition probabilities:\\n1. Initialize π randomly.\\n2. Repeat {\\n(a) Execute π in the MDP for some number of trials.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 194}, page_content='194\\n(b) Using the accumulated experience in the MDP, update our esti-\\nmates for Psa (and R, if applicable).\\n(c) Apply value iteration with the estimated state transition probabil-\\nities and rewards to get a new estimated value function V .\\n(d) Update π to be the greedy policy with respect to V .\\n}\\nWe note that, for this particular algorithm, there is one simple optimiza-\\ntion that can make it run much more quickly. Speciﬁcally, in the inner loop\\nof the algorithm where we apply value iteration, if instead of initializing value\\niteration with V = 0, we initialize it with the solution found during the pre-\\nvious iteration of our algorithm, then that will provide value iteration with\\na much better initial starting point and make it converge more quickly.\\n15.4\\nContinuous state MDPs\\nSo far, we’ve focused our attention on MDPs with a ﬁnite number of states.\\nWe now discuss algorithms for MDPs that may have an inﬁnite number of\\nstates. For example, for a car, we might represent the state as (x, y, θ, ˙x, ˙y, ˙θ),\\ncomprising its position (x, y); orientation θ; velocity in the x and y directions\\n˙x and ˙y; and angular velocity ˙θ. Hence, S = R6 is an inﬁnite set of states,\\nbecause there is an inﬁnite number of possible positions and orientations\\nfor the car.3 Similarly, the inverted pendulum you saw in PS4 has states\\n(x, θ, ˙x, ˙θ), where θ is the angle of the pole. And, a helicopter ﬂying in 3d\\nspace has states of the form (x, y, z, φ, θ, ψ, ˙x, ˙y, ˙z, ˙φ, ˙θ, ˙ψ), where here the roll\\nφ, pitch θ, and yaw ψ angles specify the 3d orientation of the helicopter.\\nIn this section, we will consider settings where the state space is S = Rd,\\nand describe ways for solving such MDPs.\\n15.4.1\\nDiscretization\\nPerhaps the simplest way to solve a continuous-state MDP is to discretize\\nthe state space, and then to use an algorithm like value iteration or policy\\niteration, as described previously.\\nFor example, if we have 2d states (s1, s2), we can use a grid to discretize\\nthe state space:\\n3Technically, θ is an orientation and so the range of θ is better written θ ∈[−π, π) than\\nθ ∈R; but for our purposes, this distinction is not important.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 195}, page_content='195\\n[t]\\nHere, each grid cell represents a separate discrete state ¯s.\\nWe can\\nthen approximate the continuous-state MDP via a discrete-state one\\n( ¯S, A, {P¯sa}, γ, R), where ¯S is the set of discrete states, {P¯sa} are our state\\ntransition probabilities over the discrete states, and so on. We can then use\\nvalue iteration or policy iteration to solve for the V ∗(¯s) and π∗(¯s) in the\\ndiscrete state MDP ( ¯S, A, {P¯sa}, γ, R). When our actual system is in some\\ncontinuous-valued state s ∈S and we need to pick an action to execute, we\\ncompute the corresponding discretized state ¯s, and execute action π∗(¯s).\\nThis discretization approach can work well for many problems. However,\\nthere are two downsides. First, it uses a fairly naive representation for V ∗\\n(and π∗). Speciﬁcally, it assumes that the value function is takes a constant\\nvalue over each of the discretization intervals (i.e., that the value function is\\npiecewise constant in each of the gridcells).\\nTo better understand the limitations of such a representation, consider a\\nsupervised learning problem of ﬁtting a function to this dataset:\\n[t]\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n1.5\\n2\\n2.5\\n3\\n3.5\\n4\\n4.5\\n5\\n5.5\\nx\\ny'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 196}, page_content='196\\nClearly, linear regression would do ﬁne on this problem. However, if we\\ninstead discretize the x-axis, and then use a representation that is piecewise\\nconstant in each of the discretization intervals, then our ﬁt to the data would\\nlook like this:\\n[t]\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n1.5\\n2\\n2.5\\n3\\n3.5\\n4\\n4.5\\n5\\n5.5\\nx\\ny\\nThis piecewise constant representation just isn’t a good representation for\\nmany smooth functions. It results in little smoothing over the inputs, and no\\ngeneralization over the diﬀerent grid cells. Using this sort of representation,\\nwe would also need a very ﬁne discretization (very small grid cells) to get a\\ngood approximation.\\nA second downside of this representation is called the curse of dimen-\\nsionality. Suppose S = Rd, and we discretize each of the d dimensions of the\\nstate into k values. Then the total number of discrete states we have is kd.\\nThis grows exponentially quickly in the dimension of the state space d, and\\nthus does not scale well to large problems. For example, with a 10d state, if\\nwe discretize each state variable into 100 values, we would have 10010 = 1020\\ndiscrete states, which is far too many to represent even on a modern desktop\\ncomputer.\\nAs a rule of thumb, discretization usually works extremely well for 1d\\nand 2d problems (and has the advantage of being simple and quick to im-\\nplement). Perhaps with a little bit of cleverness and some care in choosing\\nthe discretization method, it often works well for problems with up to 4d\\nstates. If you’re extremely clever, and somewhat lucky, you may even get it\\nto work for some 6d problems. But it very rarely works for problems any\\nhigher dimensional than that.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 197}, page_content='197\\n15.4.2\\nValue function approximation\\nWe now describe an alternative method for ﬁnding policies in continuous-\\nstate MDPs, in which we approximate V ∗directly, without resorting to dis-\\ncretization. This approach, called value function approximation, has been\\nsuccessfully applied to many RL problems.\\nUsing a model or simulator\\nTo develop a value function approximation algorithm, we will assume that\\nwe have a model, or simulator, for the MDP. Informally, a simulator is\\na black-box that takes as input any (continuous-valued) state st and action\\nat, and outputs a next-state st+1 sampled according to the state transition\\nprobabilities Pstat:\\n[t]\\nThere are several ways that one can get such a model. One is to use\\nphysics simulation. For example, the simulator for the inverted pendulum\\nin PS4 was obtained by using the laws of physics to calculate what position\\nand orientation the cart/pole will be in at time t + 1, given the current state\\nat time t and the action a taken, assuming that we know all the parameters\\nof the system such as the length of the pole, the mass of the pole, and so\\non. Alternatively, one can also use an oﬀ-the-shelf physics simulation software\\npackage which takes as input a complete physical description of a mechanical\\nsystem, the current state st and action at, and computes the state st+1 of the\\nsystem a small fraction of a second into the future.4\\nAn alternative way to get a model is to learn one from data collected in\\nthe MDP. For example, suppose we execute n trials in which we repeatedly\\ntake actions in an MDP, each trial for T timesteps. This can be done picking\\nactions at random, executing some speciﬁc policy, or via some other way of\\n4Open Dynamics Engine (http://www.ode.com) is one example of a free/open-source\\nphysics simulator that can be used to simulate systems like the inverted pendulum, and\\nthat has been a reasonably popular choice among RL researchers.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 198}, page_content='198\\nchoosing actions. We would then observe n state sequences like the following:\\ns(1)\\n0\\na(1)\\n0\\n−→s(1)\\n1\\na(1)\\n1\\n−→s(1)\\n2\\na(1)\\n2\\n−→· · ·\\na(1)\\nT −1\\n−→s(1)\\nT\\ns(2)\\n0\\na(2)\\n0\\n−→s(2)\\n1\\na(2)\\n1\\n−→s(2)\\n2\\na(2)\\n2\\n−→· · ·\\na(2)\\nT −1\\n−→s(2)\\nT\\n· · ·\\ns(n)\\n0\\na(n)\\n0\\n−→s(n)\\n1\\na(n)\\n1\\n−→s(n)\\n2\\na(n)\\n2\\n−→· · ·\\na(n)\\nT −1\\n−→s(n)\\nT\\nWe can then apply a learning algorithm to predict st+1 as a function of st\\nand at.\\nFor example, one may choose to learn a linear model of the form\\nst+1 = Ast + Bat,\\n(15.6)\\nusing an algorithm similar to linear regression. Here, the parameters of the\\nmodel are the matrices A and B, and we can estimate them using the data\\ncollected from our n trials, by picking\\narg min\\nA,B\\nn\\nX\\ni=1\\nT−1\\nX\\nt=0\\n\\r\\r\\rs(i)\\nt+1 −\\n\\x10\\nAs(i)\\nt\\n+ Ba(i)\\nt\\n\\x11\\r\\r\\r\\n2\\n2 .\\nWe could also potentially use other loss functions for learning the model.\\nFor example, it has been found in recent work Luo et al. [2018] that using\\n∥· ∥2 norm (without the square) may be helpful in certain cases.\\nHaving learned A and B, one option is to build a deterministic model,\\nin which given an input st and at, the output st+1 is exactly determined.\\nSpeciﬁcally, we always compute st+1 according to Equation (15.6). Alter-\\nnatively, we may also build a stochastic model, in which st+1 is a random\\nfunction of the inputs, by modeling it as\\nst+1 = Ast + Bat + ϵt,\\nwhere here ϵt is a noise term, usually modeled as ϵt ∼N(0, Σ). (The covari-\\nance matrix Σ can also be estimated from data in a straightforward way.)\\nHere, we’ve written the next-state st+1 as a linear function of the current\\nstate and action; but of course, non-linear functions are also possible. Specif-\\nically, one can learn a model st+1 = Aφs(st) + Bφa(at), where φs and φa are\\nsome non-linear feature mappings of the states and actions. Alternatively,\\none can also use non-linear learning algorithms, such as locally weighted lin-\\near regression, to learn to estimate st+1 as a function of st and at. These\\napproaches can also be used to build either deterministic or stochastic sim-\\nulators of an MDP.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 199}, page_content='199\\nFitted value iteration\\nWe now describe the ﬁtted value iteration algorithm for approximating\\nthe value function of a continuous state MDP. In the sequel, we will assume\\nthat the problem has a continuous state space S = Rd, but that the action\\nspace A is small and discrete.5\\nRecall that in value iteration, we would like to perform the update\\nV (s)\\n:=\\nR(s) + γ max\\na\\nZ\\ns′ Psa(s′)V (s′)ds′\\n(15.7)\\n=\\nR(s) + γ max\\na\\nEs′∼Psa[V (s′)]\\n(15.8)\\n(In Section 15.2, we had written the value iteration update with a summation\\nV (s) := R(s) + γ maxa\\nP\\ns′ Psa(s′)V (s′) rather than an integral over states;\\nthe new notation reﬂects that we are now working in continuous states rather\\nthan discrete states.)\\nThe main idea of ﬁtted value iteration is that we are going to approxi-\\nmately carry out this step, over a ﬁnite sample of states s(1), . . . , s(n). Specif-\\nically, we will use a supervised learning algorithm—linear regression in our\\ndescription below—to approximate the value function as a linear or non-linear\\nfunction of the states:\\nV (s) = θTφ(s).\\nHere, φ is some appropriate feature mapping of the states.\\nFor each state s in our ﬁnite sample of n states, ﬁtted value iteration\\nwill ﬁrst compute a quantity y(i), which will be our approximation to R(s) +\\nγ maxa Es′∼Psa[V (s′)] (the right hand side of Equation 15.8). Then, it will\\napply a supervised learning algorithm to try to get V (s) close to R(s) +\\nγ maxa Es′∼Psa[V (s′)] (or, in other words, to try to get V (s) close to y(i)).\\nIn detail, the algorithm is as follows:\\n1. Randomly sample n states s(1), s(2), . . . s(n) ∈S.\\n2. Initialize θ := 0.\\n3. Repeat {\\nFor i = 1, . . . , n {\\n5In practice, most MDPs have much smaller action spaces than state spaces. E.g., a car\\nhas a 6d state space, and a 2d action space (steering and velocity controls); the inverted\\npendulum has a 4d state space, and a 1d action space; a helicopter has a 12d state space,\\nand a 4d action space. So, discretizing this set of actions is usually less of a problem than\\ndiscretizing the state space would have been.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 200}, page_content='200\\nFor each action a ∈A {\\nSample s′\\n1, . . . , s′\\nk ∼Ps(i)a (using a model of the MDP).\\nSet q(a) = 1\\nk\\nPk\\nj=1 R(s(i)) + γV (s′\\nj)\\n// Hence, q(a) is an estimate of R(s(i)) +\\nγEs′∼Ps(i)a[V (s′)].\\n}\\nSet y(i) = maxa q(a).\\n// Hence,\\ny(i)\\nis an estimate of R(s(i)) +\\nγ maxa Es′∼Ps(i)a[V (s′)].\\n}\\n// In the original value iteration algorithm (over discrete states)\\n// we updated the value function according to V (s(i)) := y(i).\\n// In this algorithm, we want V (s(i)) ≈y(i), which we’ll achieve\\n// using supervised learning (linear regression).\\nSet θ := arg minθ\\n1\\n2\\nPn\\ni=1\\n\\x00θTφ(s(i)) −y(i)\\x012\\n}\\nAbove, we had written out ﬁtted value iteration using linear regression\\nas the algorithm to try to make V (s(i)) close to y(i). That step of the algo-\\nrithm is completely analogous to a standard supervised learning (regression)\\nproblem in which we have a training set (x(1), y(1)), (x(2), y(2)), . . . , (x(n), y(n)),\\nand want to learn a function mapping from x to y; the only diﬀerence is that\\nhere s plays the role of x. Even though our description above used linear re-\\ngression, clearly other regression algorithms (such as locally weighted linear\\nregression) can also be used.\\nUnlike value iteration over a discrete set of states, ﬁtted value iteration\\ncannot be proved to always to converge. However, in practice, it often does\\nconverge (or approximately converge), and works well for many problems.\\nNote also that if we are using a deterministic simulator/model of the MDP,\\nthen ﬁtted value iteration can be simpliﬁed by setting k = 1 in the algorithm.\\nThis is because the expectation in Equation (15.8) becomes an expectation\\nover a deterministic distribution, and so a single example is suﬃcient to\\nexactly compute that expectation. Otherwise, in the algorithm above, we\\nhad to draw k samples, and average to try to approximate that expectation\\n(see the deﬁnition of q(a), in the algorithm pseudo-code).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 201}, page_content='201\\nFinally, ﬁtted value iteration outputs V , which is an approximation to\\nV ∗. This implicitly deﬁnes our policy. Speciﬁcally, when our system is in\\nsome state s, and we need to choose an action, we would like to choose the\\naction\\narg max\\na\\nEs′∼Psa[V (s′)]\\n(15.9)\\nThe process for computing/approximating this is similar to the inner-loop of\\nﬁtted value iteration, where for each action, we sample s′\\n1, . . . , s′\\nk ∼Psa to\\napproximate the expectation. (And again, if the simulator is deterministic,\\nwe can set k = 1.)\\nIn practice, there are often other ways to approximate this step as well.\\nFor example, one very common case is if the simulator is of the form st+1 =\\nf(st, at) + ϵt, where f is some deterministic function of the states (such as\\nf(st, at) = Ast + Bat), and ϵ is zero-mean Gaussian noise. In this case, we\\ncan pick the action given by\\narg max\\na\\nV (f(s, a)).\\nIn other words, here we are just setting ϵt = 0 (i.e., ignoring the noise in\\nthe simulator), and setting k = 1.\\nEquivalent, this can be derived from\\nEquation (15.9) using the approximation\\nEs′[V (s′)]\\n≈\\nV (Es′[s′])\\n(15.10)\\n=\\nV (f(s, a)),\\n(15.11)\\nwhere here the expectation is over the random s′ ∼Psa. So long as the noise\\nterms ϵt are small, this will usually be a reasonable approximation.\\nHowever, for problems that don’t lend themselves to such approximations,\\nhaving to sample k|A| states using the model, in order to approximate the\\nexpectation above, can be computationally expensive.\\n15.5\\nConnections between Policy and Value\\nIteration (Optional)\\nIn the policy iteration, line 3 of Algorithm 5, we typically use linear system\\nsolver to compute V π.\\nAlternatively, one can also the iterative Bellman\\nupdates, similarly to the value iteration, to evaluate V π, as in the Procedure\\nVE(·) in Line 1 of Algorithm 6 below. Here if we take option 1 in Line 2 of\\nthe Procedure VE, then the diﬀerence between the Procedure VE from the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 202}, page_content='202\\nAlgorithm 6 Variant of Policy Iteration\\n1: procedure VE(π, k)\\n▷To evaluate V π\\n2:\\nOption 1: initialize V (s) := 0; Option 2: Initialize from the current\\nV in the main algorithm.\\n3:\\nfor i = 0 to k −1 do\\n4:\\nFor every state s, update\\nV (s) := R(s) + γ\\nX\\ns′\\nPsπ(s)(s′)V (s′).\\n(15.12)\\nreturn V\\n5:\\nRequire: hyperparameter k.\\n6: Initialize π randomly.\\n7: for until convergence do\\n8:\\nLet V = VE(π, k).\\n9:\\nFor each state s, let\\nπ(s) := arg max\\na∈A\\nX\\ns′\\nPsa(s′)V (s′).\\n(15.13)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 203}, page_content='203\\nvalue iteration (Algorithm 4) is that on line 4, the procedure is using the\\naction from π instead of the greedy action.\\nUsing the Procedure VE, we can build Algorithm 6, which is a variant\\nof policy iteration that serves an intermediate algorithm that connects pol-\\nicy iteration and value iteration. Here we are going to use option 2 in VE\\nto maximize the re-use of knowledge learned before. One can verify indeed\\nthat if we take k = 1 and use option 2 in Line 2 in Algorithm 6, then Algo-\\nrithm 6 is semantically equivalent to value iteration (Algorithm 4). In other\\nwords, both Algorithm 6 and value iteration interleave the updates in (15.13)\\nand (15.12). Algorithm 6 alternate between k steps of update (15.12) and\\none step of (15.13), whereas value iteration alternates between 1 steps of up-\\ndate (15.12) and one step of (15.13). Therefore generally Algorithm 6 should\\nnot be faster than value iteration, because assuming that update (15.12)\\nand (15.13) are equally useful and time-consuming, then the optimal balance\\nof the update frequencies could be just k = 1 or k ≈1.\\nOn the other hand, if k steps of update (15.12) can be done much faster\\nthan k times a single step of (15.12), then taking additional steps of equa-\\ntion (15.12) in group might be useful. This is what policy iteration is lever-\\naging — the linear system solver can give us the result of Procedure VE with\\nk = ∞much faster than using the Procedure VE for a large k. On the ﬂip\\nside, when such a speeding-up eﬀect no longer exists, e.g.,, when the state\\nspace is large and linear system solver is also not fast, then value iteration is\\nmore preferable.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 204}, page_content='Chapter 16\\nLQR, DDP and LQG\\n16.1\\nFinite-horizon MDPs\\nIn Chapter 15, we deﬁned Markov Decision Processes (MDPs) and covered\\nValue Iteration / Policy Iteration in a simpliﬁed setting. More speciﬁcally we\\nintroduced the optimal Bellman equation that deﬁnes the optimal value\\nfunction V π∗of the optimal policy π∗.\\nV π∗(s) = R(s) + max\\na∈A γ\\nX\\ns′∈S\\nPsa(s′)V π∗(s′)\\nRecall that from the optimal value function, we were able to recover the\\noptimal policy π∗with\\nπ∗(s) = argmaxa∈A\\nX\\ns′∈S\\nPsa(s′)V ∗(s′)\\nIn this chapter, we’ll place ourselves in a more general setting:\\n1. We want to write equations that make sense for both the discrete and\\nthe continuous case. We’ll therefore write\\nEs′∼Psa\\n\\x02\\nV π∗(s′)\\n\\x03\\ninstead of\\nX\\ns′∈S\\nPsa(s′)V π∗(s′)\\nmeaning that we take the expectation of the value function at the next\\nstate. In the ﬁnite case, we can rewrite the expectation as a sum over\\n204'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 205}, page_content='205\\nstates. In the continuous case, we can rewrite the expectation as an\\nintegral. The notation s′ ∼Psa means that the state s′ is sampled from\\nthe distribution Psa.\\n2. We’ll assume that the rewards depend on both states and actions. In\\nother words, R : S ×A →R. This implies that the previous mechanism\\nfor computing the optimal action is changed into\\nπ∗(s) = argmaxa∈A R(s, a) + γEs′∼Psa\\n\\x02\\nV π∗(s′)\\n\\x03\\n3. Instead of considering an inﬁnite horizon MDP, we’ll assume that we\\nhave a ﬁnite horizon MDP that will be deﬁned as a tuple\\n(S, A, Psa, T, R)\\nwith T > 0 the time horizon (for instance T = 100). In this setting,\\nour deﬁnition of payoﬀis going to be (slightly) diﬀerent:\\nR(s0, a0) + R(s1, a1) + · · · + R(sT, aT)\\ninstead of (inﬁnite horizon case)\\nR(s0, a0) + γR(s1, a1) + γ2R(s2, a2) + . . .\\n∞\\nX\\nt=0\\nR(st, at)γt\\nWhat happened to the discount factor γ?\\nRemember that the intro-\\nduction of γ was (partly) justiﬁed by the necessity of making sure that\\nthe inﬁnite sum would be ﬁnite and well-deﬁned. If the rewards are\\nbounded by a constant ¯R, the payoﬀis indeed bounded by\\n|\\n∞\\nX\\nt=0\\nR(st)γt| ≤¯R\\n∞\\nX\\nt=0\\nγt\\nand we recognize a geometric sum! Here, as the payoﬀis a ﬁnite sum,\\nthe discount factor γ is not necessary anymore.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 206}, page_content='206\\nIn this new setting, things behave quite diﬀerently. First, the optimal\\npolicy π∗might be non-stationary, meaning that it changes over time.\\nIn other words, now we have\\nπ(t) : S →A\\nwhere the superscript (t) denotes the policy at time step t. The dynam-\\nics of the ﬁnite horizon MDP following policy π(t) proceeds as follows:\\nwe start in some state s0, take some action a0 := π(0)(s0) according to\\nour policy at time step 0. The MDP transitions to a successor s1, drawn\\naccording to Ps0a0. Then, we get to pick another action a1 := π(1)(s1)\\nfollowing our new policy at time step 1 and so on...\\nWhy does the optimal policy happen to be non-stationary in the ﬁnite-\\nhorizon setting? Intuitively, as we have a ﬁnite numbers of actions to\\ntake, we might want to adopt diﬀerent strategies depending on where\\nwe are in the environment and how much time we have left. Imagine\\na grid with 2 goals with rewards +1 and +10. At the beginning, we\\nmight want to take actions to aim for the +10 goal. But if after some\\nsteps, dynamics somehow pushed us closer to the +1 goal and we don’t\\nhave enough steps left to be able to reach the +10 goal, then a better\\nstrategy would be to aim for the +1 goal...\\n4. This observation allows us to use time dependent dynamics\\nst+1 ∼P (t)\\nst,at\\nmeaning that the transition’s distribution P (t)\\nst,at changes over time. The\\nsame thing can be said about R(t). Note that this setting is a better\\nmodel for real life.\\nIn a car, the gas tank empties, traﬃc changes,\\netc. Combining the previous remarks, we’ll use the following general\\nformulation for our ﬁnite horizon MDP\\n\\x00S, A, P (t)\\nsa , T, R(t)\\x01\\nRemark: notice that the above formulation would be equivalent to\\nadding the time into the state.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 207}, page_content='207\\nThe value function at time t for a policy π is then deﬁned in the same\\nway as before, as an expectation over trajectories generated following\\npolicy π starting in state s.\\nVt(s) = E\\n\\x02\\nR(t)(st, at) + · · · + R(T)(sT, aT)|st = s, π\\n\\x03\\nNow, the question is\\nIn this ﬁnite-horizon setting, how do we ﬁnd the optimal value function\\nV ∗\\nt (s) = max\\nπ\\nV π\\nt (s)\\nIt turns out that Bellman’s equation for Value Iteration is made for Dy-\\nnamic Programming. This may come as no surprise as Bellman is one of\\nthe fathers of dynamic programming and the Bellman equation is strongly\\nrelated to the ﬁeld.\\nTo understand how we can simplify the problem by\\nadopting an iteration-based approach, we make the following observations:\\n1. Notice that at the end of the game (for time step T), the optimal value\\nis obvious\\n∀s ∈S :\\nV ∗\\nT (s) := max\\na∈A R(T)(s, a)\\n(16.1)\\n2. For another time step 0 ≤t < T, if we suppose that we know the\\noptimal value function for the next time step V ∗\\nt+1, then we have\\n∀t < T, s ∈S :\\nV ∗\\nt (s) := max\\na∈A\\nh\\nR(t)(s, a) + Es′∼P (t)\\nsa\\n\\x02\\nV ∗\\nt+1(s′)\\n\\x03i\\n(16.2)\\nWith these observations in mind, we can come up with a clever algorithm\\nto solve for the optimal value function:\\n1. compute V ∗\\nT using equation (16.1).\\n2. for t = T −1, . . . , 0:\\ncompute V ∗\\nt using V ∗\\nt+1 using equation (16.2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 208}, page_content='208\\nSide note We can interpret standard value iteration as a special case\\nof this general case, but without keeping track of time. It turns out that\\nin the standard setting, if we run value iteration for T steps, we get a γT\\napproximation of the optimal value iteration (geometric convergence). See\\nproblem set 4 for a proof of the following result:\\nTheorem Let B denote the Bellman update and ||f(x)||∞:= supx |f(x)|.\\nIf Vt denotes the value function at the t-th step, then\\n||Vt+1 −V ∗||∞= ||B(Vt) −V ∗||∞\\n≤γ||Vt −V ∗||∞\\n≤γt||V1 −V ∗||∞\\nIn other words, the Bellman operator B is a γ-contracting operator.\\n16.2\\nLinear Quadratic Regulation (LQR)\\nIn this section, we’ll cover a special case of the ﬁnite-horizon setting described\\nin Section 16.1, for which the exact solution is (easily) tractable. This\\nmodel is widely used in robotics, and a common technique in many problems\\nis to reduce the formulation to this framework.\\nFirst, let’s describe the model’s assumptions. We place ourselves in the\\ncontinuous setting, with\\nS = Rd,\\nA = Rd\\nand we’ll assume linear transitions (with noise)\\nst+1 = Atst + Btat + wt\\nwhere At ∈Rd×d, Bt ∈Rd×d are matrices and wt ∼N(0, Σt) is some\\ngaussian noise (with zero mean). As we’ll show in the following paragraphs,\\nit turns out that the noise, as long as it has zero mean, does not impact the\\noptimal policy!\\nWe’ll also assume quadratic rewards\\nR(t)(st, at) = −s⊤\\nt Utst −a⊤\\nt Wtat'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 209}, page_content='209\\nwhere Ut ∈Rd×n, Wt ∈Rd×d are positive deﬁnite matrices (meaning that\\nthe reward is always negative).\\nRemark Note that the quadratic formulation of the reward is equivalent\\nto saying that we want our state to be close to the origin (where the reward\\nis higher). For example, if Ut = Id (the identity matrix) and Wt = Id, then\\nRt = −||st||2 −||at||2, meaning that we want to take smooth actions (small\\nnorm of at) to go back to the origin (small norm of st). This could model a\\ncar trying to stay in the middle of lane without making impulsive moves...\\nNow that we have deﬁned the assumptions of our LQR model, let’s cover\\nthe 2 steps of the LQR algorithm\\nstep 1 suppose\\nthat\\nwe\\ndon’t\\nknow\\nthe\\nmatrices\\nA, B, Σ.\\nTo\\nesti-\\nmate them, we can follow the ideas outlined in the Value Ap-\\nproximation section of the RL notes.\\nFirst,\\ncollect transitions\\nfrom an arbitrary policy.\\nThen,\\nuse linear regression to ﬁnd\\nargminA,B\\nPn\\ni=1\\nPT−1\\nt=0\\n\\r\\r\\rs(i)\\nt+1 −\\n\\x10\\nAs(i)\\nt + Ba(i)\\nt\\n\\x11\\r\\r\\r\\n2\\n.\\nFinally, use a tech-\\nnique seen in Gaussian Discriminant Analysis to learn Σ.\\nstep 2 assuming that the parameters of our model are known (given or esti-\\nmated with step 1), we can derive the optimal policy using dynamic\\nprogramming.\\nIn other words, given\\n(\\nst+1\\n= Atst + Btat + wt\\nAt, Bt, Ut, Wt, Σt known\\nR(t)(st, at)\\n= −s⊤\\nt Utst −a⊤\\nt Wtat\\nwe want to compute V ∗\\nt . If we go back to section 16.1, we can apply\\ndynamic programming, which yields\\n1. Initialization step\\nFor the last time step T,\\nV ∗\\nT (sT) = max\\naT ∈A RT(sT, aT)\\n= max\\naT ∈A −s⊤\\nT UTsT −a⊤\\nT WtaT\\n= −s⊤\\nT UtsT\\n(maximized for aT = 0)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 210}, page_content='210\\n2. Recurrence step\\nLet t < T. Suppose we know V ∗\\nt+1.\\nFact 1: It can be shown that if V ∗\\nt+1 is a quadratic function in st, then V ∗\\nt\\nis also a quadratic function. In other words, there exists some matrix Φ\\nand some scalar Ψ such that\\nif V ∗\\nt+1(st+1) = s⊤\\nt+1Φt+1st+1 + Ψt+1\\nthen V ∗\\nt (st) = s⊤\\nt Φtst + Ψt\\nFor time step t = T, we had Φt = −UT and ΨT = 0.\\nFact 2: We can show that the optimal policy is just a linear function of\\nthe state.\\nKnowing V ∗\\nt+1 is equivalent to knowing Φt+1 and Ψt+1, so we just need\\nto explain how we compute Φt and Ψt from Φt+1 and Ψt+1 and the other\\nparameters of the problem.\\nV ∗\\nt (st) = s⊤\\nt Φtst + Ψt\\n= max\\nat\\nh\\nR(t)(st, at) + Est+1∼P (t)\\nst,at[V ∗\\nt+1(st+1)]\\ni\\n= max\\nat\\n\\x02\\n−s⊤\\nt Utst −a⊤\\nt Vtat + Est+1∼N(Atst+Btat,Σt)[s⊤\\nt+1Φt+1st+1 + Ψt+1]\\n\\x03\\nwhere the second line is just the deﬁnition of the optimal value function\\nand the third line is obtained by plugging in the dynamics of our model\\nalong with the quadratic assumption. Notice that the last expression is\\na quadratic function in at and can thus be (easily) optimized1. We get\\nthe optimal action a∗\\nt\\na∗\\nt =\\n\\x02\\n(B⊤\\nt Φt+1Bt −Vt)−1BtΦt+1At\\n\\x03\\n· st\\n= Lt · st\\nwhere\\nLt :=\\n\\x02\\n(B⊤\\nt Φt+1Bt −Wt)−1BtΦt+1At\\n\\x03\\n1Use the identity E\\n\\x02\\nw⊤\\nt Φt+1wt\\n\\x03\\n= Tr(ΣtΦt+1) with wt ∼N (0, Σt)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 211}, page_content='211\\nwhich is an impressive result: our optimal policy is linear in st. Given\\na∗\\nt we can solve for Φt and Ψt. We ﬁnally get the Discrete Ricatti\\nequations\\nΦt = A⊤\\nt\\n\\x10\\nΦt+1 −Φt+1Bt\\n\\x00B⊤\\nt Φt+1Bt −Wt\\n\\x01−1 BtΦt+1\\n\\x11\\nAt −Ut\\nΨt = −tr (ΣtΦt+1) + Ψt+1\\nFact 3: we notice that Φt depends on neither Ψ nor the noise Σt! As Lt\\nis a function of At, Bt and Φt+1, it implies that the optimal policy also\\ndoes not depend on the noise! (But Ψt does depend on Σt, which\\nimplies that V ∗\\nt depends on Σt.)\\nThen, to summarize, the LQR algorithm works as follows\\n1. (if necessary) estimate parameters At, Bt, Σt\\n2. initialize ΦT := −UT and ΨT := 0.\\n3. iterate from t = T −1 . . . 0 to update Φt and Ψt using Φt+1 and Ψt+1\\nusing the discrete Ricatti equations. If there exists a policy that drives\\nthe state towards zero, then convergence is guaranteed!\\nUsing Fact 3, we can be even more clever and make our algorithm run\\n(slightly) faster!\\nAs the optimal policy does not depend on Ψt, and the\\nupdate of Φt only depends on Φt, it is suﬃcient to update only Φt!\\n16.3\\nFrom non-linear dynamics to LQR\\nIt turns out that a lot of problems can be reduced to LQR, even if dynamics\\nare non-linear. While LQR is a nice formulation because we are able to come\\nup with a nice exact solution, it is far from being general. Let’s take for\\ninstance the case of the inverted pendulum. The transitions between states\\nlook like\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nxt+1\\n˙xt+1\\nθt+1\\n˙θt+1\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8= F\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nxt\\n˙xt\\nθt\\n˙θt\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8, at\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\nwhere the function F depends on the cos of the angle etc.\\nNow, the\\nquestion we may ask is\\nCan we linearize this system?'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 212}, page_content='212\\n16.3.1\\nLinearization of dynamics\\nLet’s suppose that at time t, the system spends most of its time in some state\\n¯st and the actions we perform are around ¯at. For the inverted pendulum, if\\nwe reached some kind of optimal, this is true: our actions are small and we\\ndon’t deviate much from the vertical.\\nWe are going to use Taylor expansion to linearize the dynamics. In the\\nsimple case where the state is one-dimensional and the transition function F\\ndoes not depend on the action, we would write something like\\nst+1 = F(st) ≈F(¯st) + F ′(¯st) · (st −¯st)\\nIn the more general setting, the formula looks the same, with gradients\\ninstead of simple derivatives\\nst+1 ≈F(¯st, ¯at) + ∇sF(¯st, ¯at) · (st −¯st) + ∇aF(¯st, ¯at) · (at −¯at)\\n(16.3)\\nand now, st+1 is linear in st and at, because we can rewrite equation (16.3)\\nas\\nst+1 ≈Ast + Bst + κ\\nwhere κ is some constant and A, B are matrices. Now, this writing looks\\nawfully similar to the assumptions made for LQR. We just have to get rid\\nof the constant term κ! It turns out that the constant term can be absorbed\\ninto st by artiﬁcially increasing the dimension by one. This is the same trick\\nthat we used at the beginning of the class for linear regression...\\n16.3.2\\nDiﬀerential Dynamic Programming (DDP)\\nThe previous method works well for cases where the goal is to stay around\\nsome state s∗(think about the inverted pendulum, or a car having to stay\\nin the middle of a lane).\\nHowever, in some cases, the goal can be more\\ncomplicated.\\nWe’ll cover a method that applies when our system has to follow some\\ntrajectory (think about a rocket). This method is going to discretize the\\ntrajectory into discrete time steps, and create intermediary goals around\\nwhich we will be able to use the previous technique! This method is called\\nDiﬀerential Dynamic Programming. The main steps are'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 213}, page_content='213\\nstep 1 come up with a nominal trajectory using a naive controller, that approx-\\nimate the trajectory we want to follow. In other words, our controller\\nis able to approximate the gold trajectory with\\ns∗\\n0, a∗\\n0 →s∗\\n1, a∗\\n1 →. . .\\nstep 2 linearize the dynamics around each trajectory point s∗\\nt, in other words\\nst+1 ≈F(s∗\\nt, a∗\\nt) + ∇sF(s∗\\nt, a∗\\nt)(st −s∗\\nt) + ∇aF(s∗\\nt, a∗\\nt)(at −a∗\\nt)\\nwhere st, at would be our current state and action. Now that we have\\na linear approximation around each of these points, we can use the\\nprevious section and rewrite\\nst+1 = At · st + Bt · at\\n(notice that in that case, we use the non-stationary dynamics setting\\nthat we mentioned at the beginning of these lecture notes)\\nNote We can apply a similar derivation for the reward R(t), with a\\nsecond-order Taylor expansion.\\nR(st, at) ≈R(s∗\\nt, a∗\\nt) + ∇sR(s∗\\nt, a∗\\nt)(st −s∗\\nt) + ∇aR(s∗\\nt, a∗\\nt)(at −a∗\\nt)\\n+ 1\\n2(st −s∗\\nt)⊤Hss(st −s∗\\nt) + (st −s∗\\nt)⊤Hsa(at −a∗\\nt)\\n+ 1\\n2(at −a∗\\nt)⊤Haa(at −a∗\\nt)\\nwhere Hxy refers to the entry of the Hessian of R with respect to x and\\ny evaluated in (s∗\\nt, a∗\\nt) (omitted for readability). This expression can be\\nre-written as\\nRt(st, at) = −s⊤\\nt Utst −a⊤\\nt Wtat\\nfor some matrices Ut, Wt, with the same trick of adding an extra dimen-\\nsion of ones. To convince yourself, notice that\\n\\x001\\nx\\n\\x01\\n·\\n\\x12a\\nb\\nb\\nc\\n\\x13\\n·\\n\\x121\\nx\\n\\x13\\n= a + 2bx + cx2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 214}, page_content='214\\nstep 3 Now, you can convince yourself that our problem is strictly re-written\\nin the LQR framework. Let’s just use LQR to ﬁnd the optimal policy\\nπt. As a result, our new controller will (hopefully) be better!\\nNote: Some problems might arise if the LQR trajectory deviates too\\nmuch from the linearized approximation of the trajectory, but that can\\nbe ﬁxed with reward-shaping...\\nstep 4 Now that we get a new controller (our new policy πt), we use it to\\nproduce a new trajectory\\ns∗\\n0, π0(s∗\\n0) →s∗\\n1, π1(s∗\\n1) →. . . →s∗\\nT\\nnote that when we generate this new trajectory, we use the real F and\\nnot its linear approximation to compute transitions, meaning that\\ns∗\\nt+1 = F(s∗\\nt, a∗\\nt)\\nthen, go back to step 2 and repeat until some stopping criterion.\\n16.4\\nLinear Quadratic Gaussian (LQG)\\nOften, in the real word, we don’t get to observe the full state st. For example,\\nan autonomous car could receive an image from a camera, which is merely\\nan observation, and not the full state of the world. So far, we assumed\\nthat the state was available. As this might not hold true for most of the\\nreal-world problems, we need a new tool to model this situation: Partially\\nObservable MDPs.\\nA POMDP is an MDP with an extra observation layer. In other words,\\nwe introduce a new variable ot, that follows some conditional distribution\\ngiven the current state st\\not|st ∼O(o|s)\\nFormally, a ﬁnite-horizon POMDP is given by a tuple\\n(S, O, A, Psa, T, R)\\nWithin this framework, the general strategy is to maintain a belief state\\n(distribution over states) based on the observation o1, . . . , ot. Then, a policy\\nin a POMDP maps this belief states to actions.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 215}, page_content='215\\nIn this section, we’ll present a extension of LQR to this new setting.\\nAssume that we observe yt ∈Rn with m < n such that\\n(\\nyt\\n= C · st + vt\\nst+1\\n= A · st + B · at + wt\\nwhere C ∈Rn×d is a compression matrix and vt is the sensor noise (also\\ngaussian, like wt). Note that the reward function R(t) is left unchanged, as a\\nfunction of the state (not the observation) and action. Also, as distributions\\nare gaussian, the belief state is also going to be gaussian. In this new frame-\\nwork, let’s give an overview of the strategy we are going to adopt to ﬁnd the\\noptimal policy:\\nstep 1 ﬁrst, compute the distribution on the possible states (the belief state),\\nbased on the observations we have. In other words, we want to compute\\nthe mean st|t and the covariance Σt|t of\\nst|y1, . . . , yt ∼N\\n\\x00st|t, Σt|t\\n\\x01\\nto perform the computation eﬃciently over time, we’ll use the Kalman\\nFilter algorithm (used on-board Apollo Lunar Module!).\\nstep 2 now that we have the distribution, we’ll use the mean st|t as the best\\napproximation for st\\nstep 3 then set the action at := Ltst|t where Lt comes from the regular LQR\\nalgorithm.\\nIntuitively, to understand why this works, notice that st|t is a noisy ap-\\nproximation of st (equivalent to adding more noise to LQR) but we proved\\nthat LQR is independent of the noise!\\nStep 1 needs to be explicated. We’ll cover a simple case where there is\\nno action dependence in our dynamics (but the general case follows the same\\nidea). Suppose that\\n(\\nst+1\\n= A · st + wt,\\nwt ∼N(0, Σs)\\nyt\\n= C · st + vt,\\nvt ∼N(0, Σy)\\nAs noises are Gaussians, we can easily prove that the joint distribution is\\nalso Gaussian'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 216}, page_content='216\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\ns1\\n...\\nst\\ny1\\n...\\nyt\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n∼N (µ, Σ)\\nfor some µ, Σ\\nthen, using the marginal formulas of gaussians (see Factor Analysis notes),\\nwe would get\\nst|y1, . . . , yt ∼N\\n\\x00st|t, Σt|t\\n\\x01\\nHowever, computing the marginal distribution parameters using these\\nformulas would be computationally expensive! It would require manipulating\\nmatrices of shape t × t. Recall that inverting a matrix can be done in O(t3),\\nand it would then have to be repeated over the time steps, yielding a cost in\\nO(t4)!\\nThe Kalman ﬁlter algorithm provides a much better way of computing\\nthe mean and variance, by updating them over time in constant time in\\nt! The kalman ﬁlter is based on two basics steps. Assume that we know the\\ndistribution of st|y1, . . . , yt:\\npredict step compute st+1|y1, . . . , yt\\nupdate step compute st+1|y1, . . . , yt+1\\nand iterate over time steps! The combination of the predict and update\\nsteps updates our belief states. In other words, the process looks like\\n(st|y1, . . . , yt)\\npredict\\n−−−−→(st+1|y1, . . . , yt)\\nupdate\\n−−−−→(st+1|y1, . . . , yt+1)\\npredict\\n−−−−→. . .\\npredict step Suppose that we know the distribution of\\nst|y1, . . . , yt ∼N\\n\\x00st|t, Σt|t\\n\\x01\\nthen, the distribution over the next state is also a gaussian distribution\\nst+1|y1, . . . , yt ∼N\\n\\x00st+1|t, Σt+1|t\\n\\x01\\nwhere'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 217}, page_content='217\\n(\\nst+1|t\\n= A · st|t\\nΣt+1|t\\n= A · Σt|t · A⊤+ Σs\\nupdate step given st+1|t and Σt+1|t such that\\nst+1|y1, . . . , yt ∼N\\n\\x00st+1|t, Σt+1|t\\n\\x01\\nwe can prove that\\nst+1|y1, . . . , yt+1 ∼N\\n\\x00st+1|t+1, Σt+1|t+1\\n\\x01\\nwhere\\n(\\nst+1|t+1\\n= st+1|t + Kt(yt+1 −Cst+1|t)\\nΣt+1|t+1\\n= Σt+1|t −Kt · C · Σt+1|t\\nwith\\nKt := Σt+1|tC⊤(CΣt+1|tC⊤+ Σy)−1\\nThe matrix Kt is called the Kalman gain.\\nNow, if we have a closer look at the formulas, we notice that we don’t\\nneed the observations prior to time step t! The update steps only depends\\non the previous distribution. Putting it all together, the algorithm ﬁrst runs\\na forward pass to compute the Kt, Σt|t and st|t (sometimes referred to as\\nˆs in the literature). Then, it runs a backward pass (the LQR updates) to\\ncompute the quantities Ψt, Ψt and Lt. Finally, we recover the optimal policy\\nwith a∗\\nt = Ltst|t.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 218}, page_content='Chapter 17\\nPolicy Gradient\\n(REINFORCE)\\nWe will present a model-free algorithm called REINFORCE that does not\\nrequire the notion of value functions and Q functions. It turns out to be more\\nconvenient to introduce REINFORCE in the ﬁnite horizon case, which will\\nbe assumed throughout this note: we use τ = (s0, a0, . . . , sT−1, aT−1, sT) to\\ndenote a trajectory, where T < ∞is the length of the trajectory. Moreover,\\nREINFORCE only applies to learning a randomized policy. We use πθ(a|s)\\nto denote the probability of the policy πθ outputting the action a at state s.\\nThe other notations will be the same as in previous lecture notes.\\nThe advantage of applying REINFORCE is that we only need to assume\\nthat we can sample from the transition probabilities {Psa} and can query the\\nreward function R(s, a) at state s and action a,1 but we do not need to know\\nthe analytical form of the transition probabilities or the reward function.\\nWe do not explicitly learn the transition probabilities or the reward function\\neither.\\nLet s0 be sampled from some distribution µ. We consider optimizing the\\nexpected total payoﬀof the policy πθ over the parameter θ deﬁned as.\\nη(θ) ≜E\\n\"T−1\\nX\\nt=0\\nγtR(st, at)\\n#\\n(17.1)\\nRecall that st ∼Pst−1at−1 and at ∼πθ(·|st).\\nAlso note that η(θ) =\\nEs0∼P [V πθ(s0)] if we ignore the diﬀerence between ﬁnite and inﬁnite hori-\\nzon.\\n1In this notes we will work with the general setting where the reward depends on both\\nthe state and the action.\\n218'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 219}, page_content='219\\nWe aim to use gradient ascent to maximize η(θ). The main challenge\\nwe face here is to compute (or estimate) the gradient of η(θ) without the\\nknowledge of the form of the reward function and the transition probabilities.\\nLet Pθ(τ) denote the distribution of τ (generated by the policy πθ), and\\nlet f(τ) = PT−1\\nt=0 γtR(st, at). We can rewrite η(θ) as\\nη(θ) = Eτ∼Pθ [f(τ)]\\n(17.2)\\nWe face a similar situations in the variational auto-encoder (VAE) setting\\ncovered in the previous lectures, where the we need to take the gradient w.r.t\\nto a variable that shows up under the expectation — the distribution Pθ\\ndepends on θ. Recall that in VAE, we used the re-parametrization techniques\\nto address this problem.\\nHowever it does not apply here because we do\\nknow not how to compute the gradient of the function f. (We only have\\nan eﬃcient way to evaluate the function f by taking a weighted sum of the\\nobserved rewards, but we do not necessarily know the reward function itself\\nto compute the gradient.)\\nThe REINFORCE algorithm uses an another approach to estimate the\\ngradient of η(θ). We start with the following derivation:\\n∇θEτ∼Pθ [f(τ)] = ∇θ\\nZ\\nPθ(τ)f(τ)dτ\\n=\\nZ\\n∇θ(Pθ(τ)f(τ))dτ\\n(swap integration with gradient)\\n=\\nZ\\n(∇θPθ(τ))f(τ)dτ\\n(becaue f does not depend on θ)\\n=\\nZ\\nPθ(τ)(∇θ log Pθ(τ))f(τ)dτ\\n(because ∇log Pθ(τ) = ∇Pθ(τ)\\nPθ(τ) )\\n= Eτ∼Pθ [(∇θ log Pθ(τ))f(τ)]\\n(17.3)\\nNow we have a sample-based estimator for ∇θEτ∼Pθ [f(τ)]. Let τ (1), . . . , τ (n)\\nbe n empirical samples from Pθ (which are obtained by running the policy\\nπθ for n times, with T steps for each run). We can estimate the gradient of\\nη(θ) by\\n∇θEτ∼Pθ [f(τ)] = Eτ∼Pθ [(∇θ log Pθ(τ))f(τ)]\\n(17.4)\\n≈1\\nn\\nn\\nX\\ni=1\\n(∇θ log Pθ(τ (i)))f(τ (i))\\n(17.5)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 220}, page_content='220\\nThe next question is how to compute log Pθ(τ).\\nWe derive an analyt-\\nical formula for log Pθ(τ) and compute its gradient w.r.t θ (using auto-\\ndiﬀerentiation). Using the deﬁnition of τ, we have\\nPθ(τ) = µ(s0)πθ(a0|s0)Ps0a0(s1)πθ(a1|s1)Ps1a1(s2) · · · PsT −1aT −1(sT)\\n(17.6)\\nHere recall that µ to used to denote the density of the distribution of s0. It\\nfollows that\\nlog Pθ(τ) = log µ(s0) + log πθ(a0|s0) + log Ps0a0(s1) + log πθ(a1|s1)\\n+ log Ps1a1(s2) + · · · + log PsT −1aT −1(sT)\\n(17.7)\\nTaking gradient w.r.t to θ, we obtain\\n∇θ log Pθ(τ) = ∇θ log πθ(a0|s0) + ∇θ log πθ(a1|s1) + · · · + ∇θ log πθ(aT−1|sT−1)\\nNote that many of the terms disappear because they don’t depend on θ and\\nthus have zero gradients. (This is somewhat important — we don’t know how\\nto evaluate those terms such as log Ps0a0(s1) because we don’t have access to\\nthe transition probabilities, but luckily those terms have zero gradients!)\\nPlugging the equation above into equation (17.4), we conclude that\\n∇θη(θ) = ∇θEτ∼Pθ [f(τ)] = Eτ∼Pθ\\n\" T−1\\nX\\nt=0\\n∇θ log πθ(at|st)\\n!\\n· f(τ)\\n#\\n= Eτ∼Pθ\\n\" T−1\\nX\\nt=0\\n∇θ log πθ(at|st)\\n!\\n·\\n T−1\\nX\\nt=0\\nγtR(st, at)\\n!#\\n(17.8)\\nWe estimate the RHS of the equation above by empirical sample trajectories,\\nand the estimate is unbiased. The vanilla REINFORCE algorithm iteratively\\nupdates the parameter by gradient ascent using the estimated gradients.\\nInterpretation of the policy gradient formula (17.8).\\nThe quantity\\n∇θPθ(τ) = PT−1\\nt=0 ∇θ log πθ(at|st) is intuitively the direction of the change\\nof θ that will make the trajectory τ more likely to occur (or increase the\\nprobability of choosing action a0, . . . , at−1), and f(τ) is the total payoﬀof\\nthis trajectory. Thus, by taking a gradient step, intuitively we are trying to\\nimprove the likelihood of all the trajectories, but with a diﬀerent emphasis\\nor weight for each τ (or for each set of actions a0, a1, . . . , at−1). If τ is very\\nrewarding (that is, f(τ) is large), we try very hard to move in the direction'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 221}, page_content='221\\nthat can increase the probability of the trajectory τ (or the direction that\\nincreases the probability of choosing a0, . . . , at−1), and if τ has low payoﬀ,\\nwe try less hard with a smaller weight.\\nAn interesting fact that follows from formula (17.3) is that\\nEτ∼Pθ\\n\"T−1\\nX\\nt=0\\n∇θ log πθ(at|st)\\n#\\n= 0\\n(17.9)\\nTo see this, we take f(τ) = 1 (that is, the reward is always a constant),\\nthen the LHS of (17.8) is zero because the payoﬀis always a ﬁxed constant\\nPT\\nt=0 γt. Thus the RHS of (17.8) is also zero, which implies (17.9).\\nIn fact, one can verify that Eat∼πθ(·|st)∇θ log πθ(at|st) = 0 for any ﬁxed t\\nand st.2 This fact has two consequences. First, we can simplify formula (17.8)\\nto\\n∇θη(θ) =\\nT−1\\nX\\nt=0\\nEτ∼Pθ\\n\"\\n∇θ log πθ(at|st) ·\\n T−1\\nX\\nj=0\\nγjR(sj, aj)\\n!#\\n=\\nT−1\\nX\\nt=0\\nEτ∼Pθ\\n\"\\n∇θ log πθ(at|st) ·\\n T−1\\nX\\nj≥t\\nγjR(sj, aj)\\n!#\\n(17.10)\\nwhere the second equality follows from\\nEτ∼Pθ\\n\"\\n∇θ log πθ(at|st) ·\\n X\\n0≤j<t\\nγjR(sj, aj)\\n!#\\n= E\\n\"\\nE [∇θ log πθ(at|st)|s0, a0, . . . , st−1, at−1, st] ·\\n X\\n0≤j<t\\nγjR(sj, aj)\\n!#\\n= 0\\n(because E [∇θ log πθ(at|st)|s0, a0, . . . , st−1, at−1, st] = 0)\\nNote that here we used the law of total expectation. The outer expecta-\\ntion in the second line above is over the randomness of s0, a0, . . . , at−1, st,\\nwhereas the inner expectation is over the randomness of at (conditioned on\\ns0, a0, . . . , at−1, st.) We see that we’ve made the estimator slightly simpler.\\nThe second consequence of Eat∼πθ(·|st)∇θ log πθ(at|st) = 0 is the following: for\\nany value B(st) that only depends on st, it holds that\\nEτ∼Pθ [∇θ log πθ(at|st) · B(st)]\\n= E [E [∇θ log πθ(at|st)|s0, a0, . . . , st−1, at−1, st] B(st)]\\n= 0\\n(because E [∇θ log πθ(at|st)|s0, a0, . . . , st−1, at−1, st] = 0)\\n2In general, it’s true that Ex∼pθ[∇log pθ(x)] = 0.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 222}, page_content='222\\nAgain here we used the law of total expectation.\\nThe outer expecta-\\ntion in the second line above is over the randomness of s0, a0, . . . , at−1, st,\\nwhereas the inner expectation is over the randomness of at (conditioned on\\ns0, a0, . . . , at−1, st.) It follows from equation (17.10) and the equation above\\nthat\\n∇θη(θ) =\\nT−1\\nX\\nt=0\\nEτ∼Pθ\\n\"\\n∇θ log πθ(at|st) ·\\n T−1\\nX\\nj≥t\\nγjR(sj, aj) −γtB(st)\\n!#\\n=\\nT−1\\nX\\nt=0\\nEτ∼Pθ\\n\"\\n∇θ log πθ(at|st) · γt\\n T−1\\nX\\nj≥t\\nγj−tR(sj, aj) −B(st)\\n!#\\n(17.11)\\nTherefore, we will get a diﬀerent estimator for estimating the ∇η(θ) with a\\ndiﬀerence choice of B(·). The beneﬁt of introducing a proper B(·) — which\\nis often referred to as a baseline — is that it helps reduce the variance of the\\nestimator.3 It turns out that a near optimal estimator would be the expected\\nfuture payoﬀE\\nhPT−1\\nj≥t γj−tR(sj, aj)|st\\ni\\n, which is pretty much the same as the\\nvalue function V πθ(st) (if we ignore the diﬀerence between ﬁnite and inﬁnite\\nhorizon.) Here one could estimate the value function V πθ(·) in a crude way,\\nbecause its precise value doesn’t inﬂuence the mean of the estimator but only\\nthe variance. This leads to a policy gradient algorithm with baselines stated\\nin Algorithm 7.4\\n3As a heuristic but illustrating example, suppose for a ﬁxed t, the future reward\\nPT −1\\nj≥t γj−tR(sj, aj) randomly takes two values 1000 + 1 and 1000 −2 with equal proba-\\nbility, and the corresponding values for ∇θ log πθ(at|st) are vector z and −z. (Note that\\nbecause E [∇θ log πθ(at|st)] = 0, if ∇θ log πθ(at|st) can only take two values uniformly,\\nthen the two values have to two vectors in an opposite direction.) In this case, without\\nsubtracting the baseline, the estimators take two values (1000 + 1)z and −(1000 −2)z,\\nwhereas after subtracting a baseline of 1000, the estimator has two values z and 2z. The\\nlatter estimator has much lower variance compared to the original estimator.\\n4We note that the estimator of the gradient in the algorithm does not exactly match\\nthe equation 17.11. If we multiply γt in the summand of equation (17.13), then they will\\nexactly match. Removing such discount factors empirically works well because it gives a\\nlarge update.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 223}, page_content='223\\nAlgorithm 7 Vanilla policy gradient with baseline\\nfor i = 1, · · · do\\nCollect a set of trajectories by executing the current policy. Use R≥t\\nas a shorthand for PT−1\\nj≥t γj−tR(sj, aj)\\nFit the baseline by ﬁnding a function B that minimizes\\nX\\nτ\\nX\\nt\\n(R≥t −B(st))2\\n(17.12)\\nUpdate the policy parameter θ with the gradient estimator\\nX\\nτ\\nX\\nt\\n∇θ log πθ(at|st) · (R≥t −B(st))\\n(17.13)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 224}, page_content='Bibliography\\nMikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling\\nmodern machine-learning practice and the classical bias–variance trade-\\noﬀ. Proceedings of the National Academy of Sciences, 116(32):15849–15854,\\n2019.\\nMikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for\\nweak features. SIAM Journal on Mathematics of Data Science, 2(4):1167–\\n1180, 2020.\\nDavid M Blei, Alp Kucukelbir, and Jon D McAuliﬀe. Variational inference:\\nA review for statisticians. Journal of the American Statistical Association,\\n112(518):859–877, 2017.\\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran\\nArora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine\\nBosselut, Emma Brunskill, et al. On the opportunities and risks of foun-\\ndation models. arXiv preprint arXiv:2108.07258, 2021.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Ka-\\nplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sas-\\ntry, Amanda Askell, et al. Language models are few-shot learners. Advances\\nin neural information processing systems, 33:1877–1901, 2020.\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey Hinton.\\nA simple framework for contrastive learning of visual representations. In\\nInternational Conference on Machine Learning, pages 1597–1607. PMLR,\\n2020.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:\\nPre-training of deep bidirectional transformers for language understand-\\ning. In Proceedings of the 2019 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019.\\n224'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 225}, page_content='225\\nJeﬀZ HaoChen, Colin Wei, Jason D Lee, and Tengyu Ma. Shape matters:\\nUnderstanding the implicit bias of the noise covariance. arXiv preprint\\narXiv:2006.08680, 2020.\\nTrevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani.\\nSurprises in high-dimensional ridgeless least squares interpolation. 2019.\\nTrevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani.\\nSurprises in high-dimensional ridgeless least squares interpolation.\\nThe\\nAnnals of Statistics, 50(2):949–986, 2022.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual\\nlearning for image recognition. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pages 770–778, 2016.\\nGareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An\\nintroduction to statistical learning, second edition, volume 112. Springer,\\n2021.\\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic opti-\\nmization. arXiv preprint arXiv:1412.6980, 2014.\\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv\\npreprint arXiv:1312.6114, 2013.\\nYuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and\\nTengyu Ma. Algorithmic framework for model-based deep reinforcement\\nlearning with theoretical guarantees. In International Conference on Learn-\\ning Representations, 2018.\\nSong Mei and Andrea Montanari. The generalization error of random features\\nregression: Precise asymptotics and the double descent curve. Communi-\\ncations on Pure and Applied Mathematics, 75(4):667–766, 2022.\\nPreetum Nakkiran. More data can hurt for linear regression: Sample-wise\\ndouble descent. 2019.\\nPreetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal\\nregularization can mitigate double descent. 2020.\\nManfred Opper. Statistical mechanics of learning: Generalization. The hand-\\nbook of brain theory and neural networks, pages 922–925, 1995.\\nManfred Opper. Learning to generalize. Frontiers of Life, 3(part 2):763–775,\\n2001.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-11T21:44:57-07:00', 'source': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'file_path': 'rag-dataset-main/machine-learning/ML_notes_22.pdf', 'total_pages': 227, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-11T21:44:57-07:00', 'trapped': '', 'modDate': \"D:20230611214457-07'00'\", 'creationDate': \"D:20230611214457-07'00'\", 'page': 226}, page_content='226\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you\\nneed. arXiv preprint arXiv:1706.03762, 2017.\\nBlake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pe-\\ndro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and\\nrich regimes in overparametrized models. arXiv preprint arXiv:2002.09277,\\n2020.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 0}, page_content='S A N J E E V A R O R A , S I M O N PA R K , D E N N I S J A -\\nC O B , D A N Q I C H E N\\nI N T R O D U C T I O N T O\\nM A C H I N E L E A R N I N G\\nL E C T U R E N O T E S F O R C O S 3 2 4 AT P R I N C E T O N U N I V E R S I T Y'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 1}, page_content='Copyright © 2024 Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen\\npublished by\\ntufte-latex.github.io/tufte-latex\\nThis work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 2.0 Generic\\nLicense. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/2.0/ or send a\\nletter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\\nSeptember 8, 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 2}, page_content='Contents\\nI\\nSupervised Learning\\n13\\n1\\nLinear Regression: An Introduction\\n15\\n1.1 A Warm-up Example\\n15\\n1.2 Using Linear Regression for Sentiment Prediction\\n18\\n1.3 Importance of Featurization\\n22\\n1.4 Linear Regression in Python Programming\\n24\\n2\\nStatistical Learning: What It Means to Learn\\n29\\n2.1 A Warm-up Example\\n29\\n2.2 Summary of Statistical Learning\\n31\\n2.3 Implications for Applications of Machine Learning\\n31\\n3\\nOptimization via Gradient Descent\\n33\\n3.1 Gradient Descent\\n33\\n3.2 Implications of the Linearity of a Gradient\\n37\\n3.3 Regularizers\\n38\\n3.4 Gradient Descent in Python Programming\\n42\\n4\\nLinear Classification\\n47\\n4.1 General Form of a Linear Model\\n47\\n4.2 Logistic Regression\\n48'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 3}, page_content='4\\n4.3 Support Vector Machines\\n53\\n4.4 Multi-class Classification (Multinomial Regression)\\n55\\n4.5 Regularization with SVM\\n55\\n4.6 Linear Classification in Python Programming\\n56\\n5\\nExploring “Data Science” via Linear Regression\\n59\\n5.1 Boston Housing: Machine Learning in Economics\\n59\\n5.2 fMRI Analysis: Machine Learning in Neuroscience\\n62\\nII\\nUnsupervised Learning\\n67\\n6\\nClustering\\n69\\n6.1 Unsupervised Learning\\n69\\n6.2 Clustering\\n69\\n6.3 k-Means Clustering\\n71\\n6.4 Clustering in Programming\\n76\\n7\\nLow-Dimensional Representation\\n81\\n7.1 Low-Dimensional Representation with Error\\n82\\n7.2 Application 1: Stylometry\\n84\\n7.3 Application 2: Eigenfaces\\n87\\n8\\nn-Gram Language Models\\n89\\n8.1 Probabilistic Model of Language\\n89\\n8.2 n-Gram Models\\n90\\n8.3 Start and Stop Tokens\\n94\\n8.4 Testing a Language Model\\n97\\n9\\nMatrix Factorization and Recommender Systems\\n105\\n9.1 Recommender Systems\\n105\\n9.2 Recommender Systems via Matrix Factorization\\n107\\n9.3 Implementation of Matrix Factorization\\n110'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 4}, page_content='5\\nIII\\nDeep Learning\\n113\\n10\\nIntroduction to Deep Learning\\n115\\n10.1 A Brief History\\n116\\n10.2 Anatomy of a Neural Network\\n116\\n10.3 Why Deep Learning?\\n119\\n10.4 Multi-class Classification\\n122\\n11\\nFeedforward Neural Network and Backpropagation\\n125\\n11.1 Forward Propagation: An Example\\n125\\n11.2 Forward Propagation: The General Case\\n130\\n11.3 Backpropagation: An Example\\n132\\n11.4 Backpropagation: The General Case\\n136\\n11.5 Feedforward Neural Network in Python Programming\\n142\\n12\\nConvolutional Neural Network\\n145\\n12.1 Introduction to Convolution\\n145\\n12.2 Convolution in Computer Vision\\n146\\n12.3 Backpropagation for Convolutional Nets\\n157\\n12.4 CNN in Python Programming\\n160\\nIV\\nReinforcement Learning\\n165\\n13\\nIntroduction to Reinforcement Learning\\n167\\n13.1 Basic Elements of Reinforcement Learning\\n168\\n13.2 Useful Resource: MuJoCo-based RL Environments\\n172\\n13.3 Illustrative Example: Optimum Cake Eating\\n173\\n14\\nMarkov Decision Process\\n175\\n14.1 Markov Decision Process (MDP)\\n175\\n14.2 Policy and Markov Reward Process\\n177\\n14.3 Optimal Policy\\n181'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 5}, page_content='6\\n15\\nReinforcement Learning in Unknown Environments\\n189\\n15.1 Model-Free Reinforcement Learning\\n190\\n15.2 Atari Pong (1972): A Case Study\\n191\\n15.3 Q-learning\\n195\\n15.4 Applications of Reinforcement Learning\\n199\\n15.5 Deep Reinforcement Learning\\n200\\nV\\nAdvanced Topics\\n205\\n16\\nMachine Learning and Ethics\\n207\\n16.1 Facebook’s Suicide Prevention\\n207\\n16.2 Racial Bias in Machine Learning\\n208\\n16.3 Conceptions of Fairness in Machine Learning\\n209\\n16.4 Limitations of the ML Paradigm\\n210\\n16.5 Final Thoughts\\n213\\n17\\nDeep Learning for Natural Language Processing\\n215\\n17.1 Word Embeddings\\n215\\n17.2 N-gram Model Revisited\\n219\\nVI\\nMathematics for Machine Learning\\n223\\n18\\nProbability and Statistics\\n225\\n18.1 Probability and Event\\n225\\n18.2 Random Variable\\n227\\n18.3 Central Limit Theorem and Confidence Intervals\\n234\\n18.4 Final Remarks\\n238\\n19\\nCalculus\\n239\\n19.1 Calculus in One Variable\\n239\\n19.2 Multivariable Calculus\\n241'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 6}, page_content='7\\n20\\nLinear Algebra\\n245\\n20.1 Vectors\\n245\\n20.2 Matrices\\n249\\n20.3 Advanced: SVD/PCA Procedures\\n253'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 7}, page_content=''),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 8}, page_content='Preface\\nIntroduction\\nThese lecture notes accompany a junior-level machine learning course\\n(COS 324) at Princeton University. This course provides a broad\\nintroduction to machine learning paradigms including supervised,\\nunsupervised, deep learning, and reinforcement learning as a foun-\\ndation for further study or independent work in ML, AI, and data\\nscience. Topics include linear models for classification and regression,\\nclustering, low rank representations (PCA), n-gram language mod-\\nels, matrix factorization, feedforward neural nets and convolutional\\nneural nets, Markov decision process, and reinforcement learning.\\nInteresting applications are presented for all these models.\\nThe course design was shaped by some constraints that may not\\nexist at other universities.\\nBackground assumed: The formal prerequisites are the following\\ncourses, which all our majors have taken through sophomore year:\\nAn introduction to computer science course (COS 126), Data Struc-\\ntures and Algorithms (COS 226), Single-variable Calculus (MAT 103,\\n104) and Linear Algebra (MAT 202/204/217). While many majors\\nalso take multi-variable calculus and a probability course, not\\nall do. Hence we do not include them in the list of prerequisites,\\nalthough we do assume some exposure to elementary probability\\nat high-school level. All our majors take a course on proof-based\\nreasoning (COS 240) but many don’t take it before junior year.\\nHence that course is not a prerequisite and our course doesn’t rely\\non formal proofs per se.\\nA side benefit of assuming minimal math and programming pre-\\nrequisites is that this also makes our course accessible to hundreds\\nof non-majors, many of whom only take introductory CS courses.\\nShould provide a broad introduction to today’s AI and machine learning:\\nSince AI and Machine Learning has been transformed by deep\\nlearning and related methods in the past decade, we wanted\\nto provide a reasonable competence in deep learning as well as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 9}, page_content='10\\nreinforcement learning. Thus students who don’t take another\\nAI/ML course still leave with a skill set appropriate for applying\\nML techniques in their careers. Of course, devoting the second\\nhalf of our course to these “advanced” topics required leaving\\nout several topics that are classically taught in introductory ML.\\nEssentially, the classical topics are condensed into the first half of\\nthe term.\\nProvide a taste of today’s programming environments: Students can start\\nwith zero Python background, and slowly graduate to comfort in\\nPython as well as dipping their toes into deep learning on Google\\nCoLab and RL on OpenAI gym.\\nTaste of interesting applications + discussion of broader issues of societal interest:\\nAn introductory course needs to introduce students to these issues,\\ngiven machine learning’s ubiquitous role in research across dis-\\nciplines, as well as throughout our economy and our society.\\nSometimes we invite guest lecturers to provide such perspectives.\\nFinally, note that all the above material has to fit in Princeton’s\\n12-week term. We do not have 13-15 weeks as at other universities.\\nStructure of the Notes\\nThese notes are divided into six main parts.\\nPart I introduces supervised learning. Topics include linear regres-\\nsion, linear classification, and gradient descent.\\nPart II is about unsupervised learning. Topics include clustering,\\ndimensionality reduction, n-gram language models, and matrix\\nfactorization.\\nPart III covers the basics of deep learning. Topics include feedfor-\\nward neural networks and convolutional neural networks.\\nPart IV presents reinforcement learning. Topics include Markov\\ndecision process and Q-learning.\\nPart V introduces some advanced applications of machine learning.\\nTopics include ethics of machine learning and deep learning for\\nnatural language processing.\\nPart VI provides some mathematical background that is useful for\\nmachine learning. Topics include probability, calculus, and linear\\nalgebra.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 10}, page_content='11\\nBasic Ingredients of Machine Learning\\nMachine learning is the discipline of creating decision-making pro-\\ngrams that improve themselves automatically, based on data or\\nrepeated experience. The entire setup consists of the following ele-\\nments.\\nData: We have a set of data to learn from. The data may have an ad-\\nditional field called a label. If the data is labeled, the goal of the\\nlearning is to predict the label of newly seen data. Such learning\\nis called supervised learning; examples include regression (Chapter\\n1, Chapter 5) or classification (Chapter 4). If the data is unlabeled,\\nthe goal of the learning is to extract the structure of the current\\ndata. Such learning is called unsupervised learning; examples in-\\nclude clustering (Chapter 6) or dimensionality reduction (Chapter\\n7).\\nModel: We have a model that we want to learn. A model is a mapping\\nfrom a data point to a desired answer or output. For supervised\\nlearning, the answer will be the label of the data; for unsupervised\\nlearning, the output will be the structure of the data.\\nModel parameters: Each model is defined by a number of adjustable\\nvalues, or internal parameters. The goal of the learning is to find\\nthe values of these parameters that yield the best model. Through-\\nout the training process, the values of the parameters will be\\nupdated or reset.\\nModel fitting: The process of finding the best (or good enough) values\\nof model parameters, based on the provided data, is called fitting\\nthe model. There are many different ways to assess how “good”\\na model is. In many cases, a loss function is defined to measure\\nhow far the predictions of the model are from the actual output.\\nIn Chapter 4, we see how different loss functions are defined for\\ndifferent models.\\nTesting: In many cases, especially in supervised learning, we want\\nto predict how “good” the model will be for a newly seen data.\\nThis process is called testing the model. For this purpose, it is\\ncustomary to use only a fraction (e.g., 80%) of the data to fit the\\nmodel, and use the rest (e.g., 20%) to test the model. The portion\\nof the data that is used for testing and not for fitting the model is\\ncalled the held-out data and is assumed to be a good sample of the\\npopulation data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 11}, page_content=''),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 12}, page_content='Part I\\nSupervised Learning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 13}, page_content=''),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 14}, page_content='1\\nLinear Regression: An Introduction\\nThis chapter introduces least squares linear regression, one of the sim-\\nplest and most popular model in data science. Several of you may\\nhave seen it in high school. In particular, we focus on understanding\\nlinear regression in the context of machine learning. Using linear\\nregression as an example, we will introduce the terminologies and\\nideas (some of them were mentioned in the Preface) that are widely\\napplicable to more complicated models in ML.\\n1.1\\nA Warm-up Example\\nFigure 1.1: A dataset of heights\\nand weights of some male adults.\\nThe figure on the right shows\\nthe least squares regression line\\nthat fits the data. Data from\\nhttps://gist.github.com/nstokoe/\\n7d4717e96c21b8ad04ec91f361b000cb\\nSuppose we have a dataset of heights and weights of some male\\nindividuals randomly chosen from the population. We wish to\\ndetermine a relationship between heights and weights. One simple\\nrelationship would be a linear relationship; namely:\\nw = a0 + a1h\\n(1.1)\\nwhere w is the weight, h is the height, and a0, a1 are constant coef-\\nficients. We can think of this as a predictor that maps height h to a\\npredicted weight a0 + a1h, and we want this value to be similar to\\nthe actual weight w. Obviously, a linear relationship won’t describe\\nthe data exactly but we hope it is a reasonable fit to data. 1 In an ML\\n1 Similar linear models are used in\\nmany disciplines. For instance, the\\nfamous Philips model in economics\\nsuggests a linear relationship between\\ninflation rate and unemployment rate,\\nat least when the inflation rate is high.\\nsetting, this relationship between h and w is called a model — a linear\\nmodel to be more specific.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 15}, page_content='16\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nBased on the values of a0 and a1, there are infinitely many dif-\\nferent choices of this linear model. Therefore, it is natural that we\\nwant to find the values of a0, a1 that yield the “best” model. In an ML\\nsetting, finding these optimal values of a0, a1 is known as fitting the\\nmodel. One can posit different criteria for defining “goodness” of the\\nmodel.\\nHere we use classic least squares fit, invented by Gauss. Given a\\ndataset {(h1, w1), (h2, w2), . . . , (hn, wn)} of n pairs of heights and\\nweights, the “goodness” of the model in (1.1) is\\n1\\nn\\nn\\n∑\\ni=1\\n(wi −a0 −a1hi)2\\n(1.2)\\nNotice that wi −a0 −a1hi is the difference between the actual weight\\nwi and the predicted weight a0 + a1hi. This difference is called the\\nresidual for the data point (hi, wi), and the full term in (1.2) is called\\nthe average squared residuals, or equivalently the mean squared error\\n(MSE), of the dataset. The smaller the MSE, the closer the model’s\\npredictions are to actual weights, and the more accurate the model\\nis. Therefore, the “best” model according to the least squares method\\nwould be the one defined by the values of a0, a1 that minimize (1.2).\\nIn an ML setting, a mathematical expression like (1.2) that captures\\nthe “badness” of the model is called a loss function. In general, we\\nfind the “best” model by minimizing the loss function.\\nExample 1.1.1. If the data points (h, w) are given as {(65, 130), (58, 120),\\n(73, 160)}, the least squares fit will find the values of a0, a1 that minimize\\n1\\n3((130 −a0 −65a1)2 + (120 −a0 −58a1)2 + (160 −a0 −73a1)2)\\nwhich are a0 = −510\\n13 , a1 = 35\\n13.\\nProblem 1.1.2. Between the two lines in Figure 1.2, which is more preferred\\nby the least squares regression method?\\nFigure 1.2: Two lines that describe the\\nrelationship of the same dataset.\\nProblem 1.1.3. Using calculus, give an exact expression for a0, a1 that\\nminimize (1.2). (Hint: (1.2) is quadratic in both a0 and a1. Fix the value of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 16}, page_content='linear regression: an introduction\\n17\\na1 and minimize for a0. Then minimize for a1. Completing the square may\\nbe useful.) 2\\n2 A more general calculus based ap-\\nproach will be introduced in a later\\nchapter.\\n1.1.1\\nMultivariate Linear Regression\\nOne can generalize the above example to multi-variable settings. In\\ngeneral, we have k predictor variables and one effect variable. 3 The\\n3 In the above example k = 1. The\\npredictor variable was height and effect\\nvariable was weight.\\ndata points consist of k + 1 coordinates, where the last coordinate is\\nthe value y of the effect variable and the first k coordinates contain\\nvalues of the predictor variables x1, x2, . . . , xk. Then the relationship\\nwe are trying to fit has the form\\ny = a0 + a1x1 + a2x2 + · · · + akxk\\n(1.3)\\nand the least squares fit method will find the values of a0, a1, · · · , ak\\nthat minimize\\n1\\nn\\nn\\n∑\\ni=1\\n(yi −a0 −a1xi\\n1 −a2xi\\n2 −· · · −akxi\\nk)2\\n(1.4)\\nwhere (xi\\n1, xi\\n2, . . . xi\\nk, yi) is the i-th data point.\\nWe can simplify the notation by rewriting everything above in\\na vectorized notation. If we set⃗x = (1, x1, x2, · · · , xk) 4 and⃗a =\\n4 The 1 in the first coordinate is a\\ndummy variable to naturally include\\nthe constant term into the vectorized\\nnotation.\\n(a0, a1, · · · , ak), then the relationship we are trying to fit has the form\\ny =⃗a ·⃗x\\n(1.5)\\nand the least squares fit method will find⃗a ∈Rk+1 that minimize\\n1\\nn\\nn\\n∑\\ni=1\\n(yi −⃗a ·⃗xi)2\\n(1.6)\\nwhere (⃗xi, yi) is the i-th data point. We discuss how to find the best\\nvalues of a0, a1, . . . , ak later in Chapter 3; for now just assume that the\\nsolution can be found.\\n1.1.2\\nTesting a model (Held-out Data)\\nA crucial step in machine learning is to test the trained/fitted model\\non newly seen data, or held-out data, that was not used during training.\\nIf we were to test the model in the above example, we would hold\\nout a portion of the data points (say 20%) — i.e., not use them during\\ntraining — and check the average squared residual of the model on\\nthe held-out data points.\\nWe can think of the average squared residual of the held-out data\\nas an estimate of the average squared residual of the fitted model on\\nthe entire population of male adults. 5 The reason is that if the train-\\n5 If later in life you ever write up the\\nresults of a regression study, be sure\\nto report the RMSE error, which is\\nthe square root of the average square\\nresidual on held-out data. Also report\\nthe R2 value, which is closely related.\\ning data points were a random sample of the adult male population,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 17}, page_content='18\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nthen so is the set of held-out data points. This is quite analogous to\\nopinion polls, where the opinions of a few thousand randomly sam-\\npled individuals can be a reasonable estimate for the average opinion\\nacross the US. The math for such sampling estimates is covered in\\nChapter 18.\\n1.1.3\\nMore about Linear Regression\\nIn the above example, we used the least squares method, which uses\\nthe average squared residual to assess the model. The least squares\\nfit is very common but other notions of fit may also be used. For\\ninstance, instead of taking the sum of squares of residuals, one could\\nconsider the sum of absolute values, or expressions using logarithms,\\netc. We see some of these examples in Chapter 4.\\nIt is important to note that the relationship learnt via regression —\\nand machine learning in general — is (a) approximate and (b) only\\nholds for the population that the data was drawn from. Therefore,\\nwe cannot use the relationship to predict the output of a data that is\\nnot from the same distribution. Additionally, if the distribution of\\nthe data is shifted, the relationship no longer holds. We will discuss\\nmore about this in depth in Chapter 2.\\n1.2\\nUsing Linear Regression for Sentiment Prediction\\n1.2.1\\nIntroduction\\nWhile you might have seen linear regression as early as in high\\nschool, you probably did not see this cool application. In sentiment\\nclassification, we are given a piece of text and have to label it with +1\\nif it expresses positive sentiment and −1 otherwise.\\nExample 1.2.1. Consider the following dataset, collected by showing snippets\\nof text to humans and asking them to label them as positive (+1) or negative\\n(−1)\\nThe film’s performances are thrilling.\\n+1\\nIt’s not a great monster movie.\\n−1\\nIt is definitely worth seeing.\\n+1\\nUnflinchingly bleak and desperate.\\n−1\\nTable 1.1: Data from Stanford Sentiment\\nTreebank (SST). https://nlp.stanford.\\nedu/sentiment/treebank.html\\nHow can we train a model to label text snippets with the correct\\nsentiment value, given a dataset of training examples? Here is an\\nidea to try to solve it using a linear regression model. We first enu-\\nmerate all English words and assign a real-valued score to each word,\\nwhere the score for the i-th word is denoted by wi. These scores will'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 18}, page_content='linear regression: an introduction\\n19\\nbe the parameters of the model. The output of the model, given a\\ntraining example, is defined as ∑j∈S wj where S is the multiset of\\nindices of words in the text. 6 Then the least squares method needs\\n6 Unlike in a set, an element can appear\\nmultiple times in a multiset. For\\nexample, if the word good appears twice\\nin a text, then S contains two copies of\\ngood.\\nto solve the following optimization problem for a dataset of (text,\\nsentiment) pairs\\nminimize ∑\\ni\\n\\uf8eb\\n\\uf8edyi −∑\\nj∈Si\\nwj\\n\\uf8f6\\n\\uf8f8\\n2\\n(1.7)\\nwhere Si is the multiset of words in the i-th piece of text. Each of the\\nvalues\\n \\nyi −∑\\nj∈Si wj\\n!2\\nis called a least squares error or more generally\\nthe loss for that particular training example. The full summation is\\ncalled a training loss of the dataset.\\nExample 1.2.2. Assume we are training a sentiment prediction model on\\na dataset. Table 1.1 shows some of the model parameter values. Then the\\noutput of the model on the sentence “I like this movie” from the training\\ndata will be 0.15 + 0.55 + 0.03 −0.07 = 0.66. The output for “I dislike this\\nmovie” from the training data will be 0.15 −0.74 + 0.03 −0.07 = −0.63\\ni\\nword\\nwi\\n1\\nI\\n0.15\\n2\\nlike\\n0.55\\n3\\ndislike\\n−0.74\\n4\\nthis\\n0.03\\n5\\nmovie\\n−0.07\\n6\\na\\n0\\nTable 1.2: Some of the parameter values\\nof a sentiment prediction model.\\nWe can also cast this in the standard formulation of linear regres-\\nsion as follows. The bag of words (BoW) representation of a piece of\\ntext is a vector in RN where N is the number of dictionary words.\\nThe i-th coordinate is the number of times the i-th word appears in\\nthe piece of text. This represents the text as a very long vector, one\\ncoordinate per one English word in the dictionary. The vector usually\\ncontains a lot of zeros, since most words probably do not appear in\\nthis piece of text. If we denote the BoW vector as⃗x, the output of the\\nmodel is seen to be\\n∑\\nj∈S\\nwj = ∑\\ni\\nwixi\\nwhich shows that the linear model we have proposed for sentiment\\nprediction is just a subcase of linear regression (see (1.5)).\\nExample 1.2.3. Consider the same model in Example 1.2.2. The BoW repre-\\nsentation for the sentence “I like this movie” is (1, 1, 0, 1, 1, 0 · · · ). The BoW\\nrepresentation for the sentence “I dislike this movie” is (1, 0, 1, 1, 1, 0 · · · ).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 19}, page_content='20\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n1.2.2\\nTesting the Model\\nHere we use the model from Example 1.2.2 to illustrate the training\\nand testing process of a model. Assume that the following four\\nsentences were a part of the training dataset.\\nI like this movie.\\n+1\\nI dislike this movie.\\n−1\\nI like this.\\n+1\\nI dislike this.\\n−1\\nTable 1.3: A portion of the training data\\nfor a sentiment prediction model.\\nAssuming that the model parameters are the same as reported in\\nTable 1.2, we can calculate the training loss of the sentence “I like this\\nmovie” as (+1 −0.66)2 ≃0.12. Similarly, the squared residual for each\\nof the four training sentences in Table 1.3 can be calculated as\\nI like this movie.\\n0.12\\nI dislike this movie.\\n0.14\\nI like this.\\n0.07\\nI dislike this.\\n0.19\\nTable 1.4: The squared residual for four\\ntraining examples.\\nNow it is time to test the model. Assume that the sentence “I like\\na movie” is provided to the model as a test data. The test loss can be\\ncalculated in a way similar to the training loss as (+1 −0.63)2 ≃0.14.\\nBut to actually test if the model produces the correct sentiment label\\nfor this newly seen data, we now wish the model to output either +1\\nor −1, the only two labels that exist in the population. An easy fix\\nis to change the output of the model at test time to be sign(∑j∈S wj).\\nFor this test data, the model will output sign(0.63) = +1.\\nOn the Stanford Sentiment Treebank, this approach of training a\\nleast squares model yields a success rate of 78% 7. By contrast, the\\n7 To be more exact, this result is from\\na model called ridge regression model,\\nwhich is linear regression model\\naugmented by an ℓ2 regularizer, which\\nwill be explained in Chapter 3\\nstate-of-the-art deep learning methods yield success rates exceeding\\n96%!\\nOne thing to note is that while the training loss is calculated and\\nexplicitly used in the training process, the test loss is only a statistic\\nthat is generated after the training is over. It is a metric to assess if\\nthe model fitted on the training data also performs well for a more\\ngeneral data.\\n1.2.3\\nTest Loss, Generalization, and Test accuracy\\nAs mentioned already, the goal of training a model is that it should\\nmake good predictions on new, previously-unseen data. Most models\\nwill exhibit a low training loss, but not all of them show a low test\\nloss. This observation motivates the following definition:\\nGeneralization Error = |training loss −test loss|'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 20}, page_content='linear regression: an introduction\\n21\\nA trained model is said to generalize well if the generalization error\\nis small. In our case, the loss is the average squared residual. Thus\\ngood generalization means that the average squared residual on test\\ndata points is similar to that on the training data.\\nLet us see what happens on our sentiment model when it is fitted\\nand tested on the SST dataset.\\nTrain MSE\\n0.0727\\nTest MSE\\n0.7523\\nTraining accuracy\\n99.55%\\nTest accuracy\\n78.09%\\nTable 1.5: Accuracy refers to the classi-\\nfication accuracy when we make the\\nmodel to output only ±1 labels.\\nExample 1.2.4. The generalization error above is the difference between\\nMSE on test points and the MSE on training points, namely 0.75 −0.07 =\\n0.68.\\nLet’s try to understand the relationship between low test loss (the\\nsquared residual) and high test accuracy (for what fraction of test\\ndata points the sentiment was correct). Heuristically, the test loss\\n(average squared residual) being 0.75 means that the the absolute\\nvalue of the residual on a typical point is\\n√\\n0.75 ≈0.87. This means\\nthat for a data point with an actual positive sentiment (i.e., label +1),\\nthe output of the model is roughly expected to lie in the interval\\n[1 −0.87, 1 + 0.87], and similarly, for a data point with an actual\\nnegative sentiment (i.e., label −1), the output of the model is roughly\\nexpected to lie in the interval [−1 −0.87, −1 + 0.87]. Once we take\\nthe sign sign(∑j∈S wj) of the output of the model, the output is thus\\nlikely to be rounded off to the correct label. We also note that the\\ntraining accuracy is almost 100%. This usually happens in settings\\nwhere the number of parameters (i.e., number of predictor variables)\\nexceeds the number of training data points (or is close to it). The\\nfollowing problem explores this.\\nProblem 1.2.5. An expert on TV claims to have a formula to predict\\nthe outcome of presidential elections. It uses 31 measurements of various\\neconomic and societal quantities (inflation, divorce rate, etc). The formula\\ncorrectly predicts the winner of all elections 1928-2020. Should you believe\\nthe formula’s prediction for the 2024 election? (Hint: Under fairly general\\nconditions, T + 1 completely nonsense variables — i.e., having nothing\\nto do with presidential politics — can be used to perfectly fit (via linear\\nregression) the outcomes for T past presidential elections. 8)\\n8 If a model does not generalize well,\\nthen it is said to overfit the training\\ndata.\\n1.2.4\\nInterpreting the Model\\nIn many settings (e.g., medicine), an important purpose of regres-\\nsion modeling is to understand the data or the phenomenon a bit'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 21}, page_content='22\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nbetter. In this case, the phenomenon is “sentiment” and we are natu-\\nrally curious about what positive or negative sentiment amounts to.\\nSpecifically, what caused the model’s output to be +1 or −1 given a\\nspecific sentence?\\nFigure 1.3 shows a histogram of the values of wi, the parameters\\nof a sentiment prediction model that was trained on the Stanford\\nSentiment Treebank. Positive values of wi imply that the words\\ncarry a positive sentiment, while negative values of wi imply that the\\nwords carry a negative sentiment. Also, the greater the absolute value\\nof wi is, the stronger the sentiment. Notice that most words have a\\nvalue of wi close to zero, meaning the model views most words as\\nneutral. The model “pays attention” to only a tiny set of words.\\nWords with high positive wi values (i.e., positive words) include\\nenjoyable, fun, and remarkable. Words with high negative values (i.e.,\\nnegative words) include suffers, dull, and worst. Words with wi values\\nclose to 0 (i.e., neutral words) include duty and desire.\\nFigure 1.3: A histogram of the learned\\nparameters wi of a sentiment predic-\\ntion model trained on the Stanford\\nSentiment Treebank.\\n1.3\\nImportance of Featurization\\nIn the sentiment model, we chose a particular method to represent\\na piece of text with a vector. The coordinates of this vector are often\\nreferred to as features and this process of converting data into vectors\\nis called featurization. One can conceive of other choices for featuriz-\\ning text. For example, bigram featurization consists of the following:\\nthe coordinates of the vector correspond to pairs of words and the\\ncoordinate contains the number of times this pair of words appeared\\nconsecutively in the piece of text. In contrast, the choice of featuriza-\\ntion from the earlier example matches each coordinate with a single\\nword, and is called a unigram featuraization.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 22}, page_content='linear regression: an introduction\\n23\\nBigram features allow the model to access information about\\nphrases that were present in the text. For instance, in isolation\\n“pretty” is a positive word and “bad” is a negative word. If they\\nboth occur in text one would imagine that they cancel each other out\\nas far as overall sentiment is concerned. But the phrase “pretty bad”\\nis more negative than “bad.” Thus bigram features can improve the\\nmodel’s ability to capture sentiment.\\nThe required number of dimension for bigram representations can\\nget rather large. If the number of words is N, then the number of\\ncoordinates is N2. Realize that the number of model parameters in\\nlinear regressions is the same as the number of coordinates. Thus if\\nN is 30, 000 then the number of coordinates in bigram feature vector\\n(and hence the number of model parameters) is close to a billion,\\nwhich is a rather large number. In practice one might throw away\\ninformation for all pairs except say the 10, 000 most common ones\\nin the dataset. Usually models that incorporate bigram features do\\nbetter than unigram-only models.\\nIf one is trying to do studies of medical treatment with regression,\\nthere can be many potential featurizations of patient data. Doctors’\\nannotations, test results, X-ray scans, etc. all have to be converted\\nsomehow into real-valued features, and the experimenter uses their\\nprior knowledge and intuitions while featurizing the data.\\nExample 1.3.1. Patients’ raw data might include height and weight.\\nIf we use linear regression, the effect variable can only depend upon a\\nlinear combination of height and weight. But it is known that several\\nhealth outcomes are better modeled using Body Mass Index, defined as\\nweight/(height2). Thus the experimenter may include a separate coordinate\\nfor BMI, even though it duplicates information already present in the other\\ncoordinates.\\nExample 1.3.2. In Example 1.3.1, the weight in pounds may span a range\\nof [90, 350], whereas cholesterol ratio may span a range of [1, 10]. It is often\\na good idea to normalize the coordinate, which means to replace x with\\n(x −µ)/σ where µ is the mean of the coordinate values in the dataset and σ\\nis the standard deviation.\\nThus the same raw dataset can have multiple featurizations, with\\ndifferent number of coordinates. Problem 1.2.5 may make us wary\\nof using featurizations with too many coordinates. We will learn a\\ntechnique called regularization in Chapter 3, which helps mitigate the\\nissue identified in Problem 1.2.5.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 23}, page_content='24\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n1.4\\nLinear Regression in Python Programming\\nIn this section, we briefly discuss how to write the Python code to\\nperform linear regression (e.g., sentiment prediction). Python is often\\nthe language of choice for many machine learning applications due\\nto its relative ease of use and the large variety of external packages\\navailable to automate the process. Here, we introduce a few of these\\npackages:\\n• numpy: This package is ubiquitous throughout the machine learn-\\ning community. It provides access to specialized array data struc-\\ntures which are implemented in highly optimized C code. Linear\\nalgebra computations and array restructuring operations are signif-\\nicantly faster with numpy compared to using Python directly. 9\\n9 Documentation is available at https:\\n//numpy.org/\\n• matplotlib: This package enables Python programmers to create\\nhigh quality plots and graphs. Visualizations are highly config-\\nurable and interoperable with several other Python packages. 10\\n10 Documentation is available at https:\\n//matplotlib.org/\\n• sklearn: This package provides a potpourri of machine learning\\nand data science models through an easy to use object-oriented\\nAPI. In addition to linear regression, sklearn makes it possible to\\nimplement SVMs, clustering, neural networks, and much more;\\nyou will learn about a some of these models later in the course. 11\\n11 Documentation is available at https:\\n//scikit-learn.org/stable/index.\\nhtml\\nThroughout this course, you will be asked to make use of func-\\ntions defined in some of these external packages. You may not always\\nbe familiar with the usage of these functions. It is important to check\\nthe official documentation to learn about the usage and the signature\\nof the functions.\\nThe code snippet below uses the three aforementioned packages to\\nperform linear regression on any given dataset.\\n# import necessary packages\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import mean_squared_error as mse\\nimport matplotlib.pyplot as plt\\n# prepare train, test data\\nX = ...\\ny = ...\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\\n# perform linear regression on train data\\nlinreg = LinearRegression().fit(X_train, y_train)\\npred_train = linreg.predict(X_train)\\npred_test = linreg.predict(X_test)\\n# print train results\\nprint(’Train MSE: ’, ’{0:.4f}’.format(mse(y_train, pred_train)))'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 24}, page_content='linear regression: an introduction\\n25\\nprint(’Test MSE: ’, ’{0:.4f}’.format(mse(y_test, pred_test)))\\nprint(’Train Acc: ’, ’{0:.2f}’.format(100*(np.sign(pred_train)==y_train).\\nmean()))\\nprint(’Test Acc: ’, ’{0:.2f}’.format(100*(np.sign(pred_test)==y_test).mean()\\n))\\n# plot gold (actual) vs predicted value\\nplt.scatter(y_test, pred_test, c=\"red\")\\nplt.xlabel(\"actual y value (y)\")\\nplt.ylabel(\"predicted y value (y hat)\")\\nplt.title(\"y vs y hat\")\\nFor readers who are not familiar with Python, we discuss some\\nkey details. In the first section of the code, we import the relevant\\npackages\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import mean_squared_error as mse\\nimport matplotlib.pyplot as plt\\nAs seen in this example, there are two ways to load a package. The\\nfirst option is to import the full package with the import keyword\\nimport numpy as np\\nNotice that the we can assign the imported package a customized\\nname with the as keyword. In this case, we decided refer to the\\npackage numpy with the name np throughout the rest of the code.\\nThis is indeed the case when we call\\nnp.sign()\\nHere we refer to the method sign() of the numpy package with the\\ncustomized name np. Alternatively, we can selectively import particu-\\nlar methods or classes with the from keyword\\nfrom sklearn.model_selection import train_test_split\\nThe next part of the code is preparing the train, test data.\\nX = ...\\ny = ...\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\\nX will have to be an array of arrays, and y will have to be an array\\nof values, with the same length as X. These arrays can be defined di-\\nrectly by specifying each of their entries, or they could be read from\\nsome external data (most commonly a csv file). Here, we present an\\nexample dataset where⃗x ∈R2:\\nX = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\\ny = [1, 1, 1, -1, -1]\\nThen we call the train_test_split() method to split the dataset into\\ndata for model training and testing. Alternatively, we can split the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 25}, page_content='26\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\ndataset by manually slicing the data arrays. 12 In general, slicing\\n12 In Python, the term slicing refers to\\nthe process of creating a subarray of an\\narray.\\na Python array involves the : operator along with start and end\\nindices. For instance, consider an arbitrary array a. Then, the output\\nof a[i:j] will be a subarray of a from the index i (inclusive) to the\\nindex j (exclusive). In the following code sample, we slice the data by\\nspecifying the number of training data points\\ntrain_size = ...\\nX_train = X[:train_size]\\nX_test = X[train_size:]\\ny_train = y[:train_size]\\ny_test = y[train_size:]\\nNote that we have omitted some of the bounding indices. If the start\\nindex is omitted, Python assumes it to be 0 (so that the subarray is\\nfrom the start of the array); for example, X[:train_size] is the first\\ntrain_size entries of X. If the end index is omitted, Python assumes it\\nto be n, the length of the array (so that the subarray ends at the end\\nof the array); for instance, X[train_size:] is the remaining entries of X,\\nonce we remove the first train_size entries. Another way to slice the\\narrays is by specifying the number of test data points\\ntest_size = ...\\nX_train = X[:-test_size]\\nX_test = X[-test_size:]\\ny_train = y[:-test_size]\\ny_test = y[-test_size:]\\nHere, notice that the index -test_size is a negative number. In this case,\\nPython interprets this as n - test_size, where n is the size of the array.\\nIn other words, it is the index of the test_size-th element from the\\nback of the array.\\nThe third part of the code is fitting the linear regression model.\\nlinreg = LinearRegression().fit(X_train, y_train)\\npred_train = linreg.predict(X_train)\\npred_test = linreg.predict(X_test)\\nThe first line will generate the least squares fit model based on the\\ntrain data. Then we can have the model make predictions on the\\ntrain, test data.\\nNext, we print out the mean squared loss and the accuracy for the\\ntrain, test data.\\nprint(’Train MSE: ’, ’{0:.4f}’.format(mse(y_train, pred_train)))\\nprint(’Test MSE: ’, ’{0:.4f}’.format(mse(y_test, pred_test)))\\nprint(’Train Acc: ’, ’{0:.2f}’.format(100*(np.sign(pred_train)==y_train).\\nmean()))\\nprint(’Test Acc: ’, ’{0:.2f}’.format(100*(np.sign(pred_test)==y_test).mean()\\n))\\nNotice that we use the mse() method that we imported from the\\nsklearn package. Also notice that when computing the accuracy, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 26}, page_content='linear regression: an introduction\\n27\\nchanged the output of the model to be the sign of the predicted\\nvalues, so that we can compare them with the gold values. In many\\ncases, there are packages that perform these elementary operations\\nfor machine learning.\\nFinally, we plot the actual and predicted values using the matplotlib\\npackage.\\nplt.scatter(y_test, pred_test, c=\"red\")\\nplt.xlabel(\"actual y value (y)\")\\nplt.ylabel(\"predicted y value (y hat)\")\\nplt.title(\"y vs y hat\")\\nThe first line draws a scatter plot with the y_test in the x-axis and\\npred_test in the y-axis. Notice that you can specify the color of the\\ndata points by specifying the value of the parameter c. In general,\\nparameters are optional values you can provide to Python functions.\\nIf the values to parameters are omitted, the functions will use their\\ndefault values. The second and third lines specify the labels that will\\nbe written next to the axes. The final line specifies the title of the\\nplot.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 27}, page_content=''),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 28}, page_content='2\\nStatistical Learning: What It Means to Learn\\nStudents often get confused about the meaning and significance of\\na relationship learnt via fitting a model to data. Some of them think\\nsuch relationships are analogous to, say, a law of nature like F = ma,\\nwhich applies every time force is applied to a mass anywhere in the\\nuniverse. The main goal of this chapter is to explain the statistical\\nnature of machine learning — models are fitted on a particular dis-\\ntribution of data points, and its predictions are valid only for data\\npoints from the same distribution. (See Chapter 18.)\\n2.1\\nA Warm-up Example\\nWe work through a concrete example 1 before enunciating the gen-\\n1 This example is purely hypothetical,\\nand all numbers in this section are\\nmade up.\\neral properties of statistical learning. Suppose we are studying the\\nrelationship between the following quantities for the population of\\nPrinceton: height (H), number of exercise hours per week (E), amount of\\ncalories consumed per week (C), and weight (W). After collecting infor-\\nmation from 200 randomly sampled residents, and using a 80/20\\ntrain/test split, we perform a linear regression on the training dataset\\nto come up with the following relationship:\\nW = 50 + H + 0.1C −4E\\n(2.1)\\nLet’s also say that the average squared residual on train and test\\ndata were both 100. This means that the relationship (2.1) holds with\\nan error of 10 lbs on a typical test data point. 2\\n2 Also, the trained model exhibits perfect\\ngeneralization: test loss is the same as\\ntraining loss!\\nQuestion 2.1.1. Alice was one of the Princeton residents in the study, but\\nthe prediction of the model is very off from her actual value (squared residual\\nis 300). Does this prove the model wrong?\\nThe answer is no. The least squares linear regression finds the\\nmodel that minimizes the average squared residual across all training\\ndata points. The residual could be large for a particular individual.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 29}, page_content='30\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nQuestion 2.1.2. There was a follow-up research for every Princeton resident\\nwho is taller than 7 feet. All of them reported squared residual of 500. Does\\nthis prove the model wrong?\\nThe answer is still no. People who are taller than 7 feet make up\\na tiny fraction of the entire population. Their residuals have a very\\nsmall effect on the average squared residual. The residual could be\\nlarge for a small subset of the population.\\nQuestion 2.1.3. There was a follow-up survey that tested the model on every\\nsingle Princeton resident. Is it possible that the average squared residual is\\n200 for the entire population?\\nThe answer is yes, although it is unlikely. Consider the distribution\\nof 4-tuples (H, E, C, W) over the entire Princeton population. This is\\nsome distribution over a finite set of 4-dimensional vectors. 3 The 200\\n3 31, 000 vectors to be more exact. The\\npopulation of Princeton is 31, 000.\\nresidents we surveyed were randomly drawn from this distribution.\\nOut of these 200 data points, 40 were randomly chosen to be held-\\nout as test data, while the remaining 160 were used as training data.\\nWe can also say that these 40 data points were chosen at random\\nfrom the distribution over the entire population of Princeton. Thus\\nwhen we test the model in (2.1) on held-out data, we’re testing this\\nrelationship over a random sample of 40 data points drawn from\\nthe population. 40 is a large enough number to give us some con-\\nfidence that the average squared residual of the test data is a good\\nestimate of the squared residual in the population, but just as polling\\nerrors happen during elections, there is some chance that this esti-\\nmate is off. In this case, we would say that the 40 test samples were\\nunrepresentative of the full population.\\nIt is important to remember that the training and test data are\\nsampled from the same distribution as the population. Therefore,\\nthe average squared residual of the training and test data are only a\\ngood estimate of the squared residual of the distribution they were\\nsampled from. This also means that the relationship found from\\nthe training data only holds (with small residue) for that particular\\ndistribution. If the population is different, or if the distribution shifts\\nwithin the same population, the relationship is not guaranteed to\\nhold. For example, the relationship in (2.1) is not expected to hold for\\npeople from Timbuktu, Mali (a different population), or for residents\\nof Princeton who are taller than 7 feet (a tiny subpopulation that\\nis likely unrepresentative of the population). Now consider the\\nfollowing situation:\\nQuestion 2.1.4. It becomes fashionable in Princeton to try to gain weight.\\nBased on the relationship in (2.1), everyone decides to increase their value\\nof C and reduce their value of E. Does the model predict that many of them\\nwill gain weight?'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 30}, page_content='statistical learning: what it means to learn\\n31\\nThe answer is no. The model was fitted to and tested on the distri-\\nbution obtained before everyone tried to gain weight. It has not been\\nfitted on the distribution of data points from people who changed\\ntheir values of C and E. In particular, note that if everyone reduces\\ntheir E and increases their C, then the distribution has definitely\\nchanged — the average value of the E coordinate in this distribution\\nhas decreased, whereas the average value of the C coordinate has\\nincreased.\\nIn general, a relationship learned from a fitted model illustrates\\ncorrelation and need not imply causation. The values of H, C, E in (2.1)\\ndo not cause W to take a specific value. The equation only shows that\\nthe values are connected via this linear relationship on average (with\\nsome bounded square residuals).\\n2.2\\nSummary of Statistical Learning\\nThe above discussion leads us to summarize properties of Statisti-\\ncal Learning. Note that these apply to most methods of machine\\nlearning, not just linear regression.\\nTraining/test data points are sampled from some distribution D: In the\\nabove example, 200 residents were randomly sampled from the\\nentire population of Princeton residents.\\nThe learnt relationship holds only for the distribution D that the data was sampled from.\\nThe performance of the model on test data is an estimate of the\\nperformance of the model on the full distribution D.\\nThere is a small probability that the estimate using test data is off. This is\\nanalogous to polling errors in opinion polls. The computation of\\n“confidence bounds” is discussed in Chapter 18.\\n2.3\\nImplications for Applications of Machine Learning\\nThe above framework and its limitations have real-life implications.\\n1. Results of medical studies may not apply to minority populations. This\\ncan happen if the minority population is genetically distinct and\\nconstitutes only a small fraction of the population. Then test\\nerror could be large on the minority population even if it is small\\non average. In fact, there have been classic studies about heart\\ndisease in the 1960s whose conclusions and recommendations\\nfail to apply well to even a group that is half of the population:\\nfemales! In those days heart disease was thought to largely strike\\nmales (which subsequently turned out to be quite false) and so'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 31}, page_content='32\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nthe studies were done primarily on males. It turns out that heart\\ndiseases in female patients behave differently. Many practices\\nthat came out of those studies turned out to be harmful to female\\npatients. 4\\n4 See https://www.theatlantic.\\ncom/health/archive/2015/10/\\nheart-disease-women/412495/.\\n2. Classifiers released by tech companies in the recent past were found to\\nhave high error rates on certain minority populations. It was quickly\\nrecognized that relying on test error alone can lead to adverse\\noutcomes on subpopulations. 5\\n5 See https://time.com/5520558/\\nartificial-intelligence-racial-gender-bias/.\\n3. Creating interactive agents is difficult. In an interactive setting (e.\\ng., an online game), a decision-making program is often called\\nan agent. When an agent has to enter an extended number of\\ninteractions 6 with a human (or another agent designed by a\\n6 Later in the book we encounter\\nReinforcement Learning, which deals\\nwith such settings.\\ndifferent group of researchers, as happens in Robocup soccer\\n7), then statistical learning requires that the agent to have been\\n7 See https://www.robocup.org/.\\nexposed to similar situations/interactions during training (i.e.,\\nfrom a fixed distribution). It is quite unclear if this is true.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 32}, page_content='3\\nOptimization via Gradient Descent\\nThis chapter discusses how to train model parameters through op-\\ntimization techniques that help find the best (or fairly good) model\\nthat has low training loss. We assume that you have seen simple\\nroot-finding techniques in high school or in calculus. Optimization\\nin machine learning often uses a procedure called gradient descent.\\nThis chapter assumes your knowledge of basic multivariable calcu-\\nlus. If you have not taken a course in multivariable calculus, read\\nChapter 19 to familiarize yourself with the basic definitions.\\n3.1\\nGradient Descent\\nIn general, an ML model has an associated loss function. The “best”\\nmodel is the one that minimizes the training loss. In most cases, it is\\nimpossible or difficult to find the minimum analytically; instead, we\\nuse a numerical method called the gradient descent algorithm to find\\nthe (approximate) optimum.\\n3.1.1\\nUnivariate Example\\nLet’s start with an univariate example to motivate the topic. Let\\nf (w) = 4w2 −6w −9 be a quadratic function. Figure 3.1 shows the\\ngraph of this function.\\nFigure 3.1: The graph of f (w) =\\n4w2 −6w −9\\nLet’s say that f attains its minimum at some point w = w∗. How'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 33}, page_content='34\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nshould we find the value of w∗? Here is an idea. Let’s start from\\nsome random point on the curve and “walk down” the curve.\\nNotice from the graph that f ′(w∗) = 0. Also, f is decreasing (i.\\ne., f ′(w) < 0) when w < w∗and increasing (i.e., f ′(w) > 0) when\\nw > w∗. So if we examine a point w and find that f ′(w) = 0, then we\\nhave arrived at our minimum. If f ′(w) > 0, then we are currently on\\nthe right side of the minimum, so we need to decrease w. On the other\\nhand, if f ′(w) < 0, then we need to increase w.\\nFor example, we start with the point w = 0. Since f ′(w) = −6 < 0,\\nwe know that we are on the left side of the minimum, so we update\\nw ←1. Since f ′(w) = 2 > 0, we are now on the right side of\\nthe minimum, so we update w ←1\\n2. When we iterate this process,\\nwe hope that we eventually slide down to the bottom of the curve.\\nObserve that the change of the value of w has the opposite sign from\\nf ′(w) at that point. That is, for each step of this iteration, we can\\nalways find a η > 0 which decreases the value of w when\\nw ←w −η f ′(w)\\nThis is not a mere coincidence — a similar result holds for a multi-\\nvariate function.\\n3.1.2\\nGradient Descent (GD)\\nLet f : Rd →R be a multivariate function. If we want to “walk\\ndown” the curve of f as in the univariate case, we need to find a\\ndirection from the current point ⃗w that decreases f.\\nA generalization of the Taylor expansion in the multivariable\\nsetting shows that the value of f in a small neighborhood around\\n⃗x = (x1, x2, . . . , xd) can be approximated as a linear function in terms\\nof the gradient.\\nf (⃗w +⃗h) ≈f (⃗w) + ∇f (⃗w) ·⃗h\\nwhere ⃗h ∈Rd is small enough (i.e.,\\n\\r\\r\\r⃗h\\n\\r\\r\\r ≈0).\\nIf ∇f is nonzero and we choose ⃗h = −η∇f where η is a suffi-\\nciently small positive number, then\\nf (⃗w −η∇f ) ≈f (⃗w) −η∥∇f ∥2\\n2\\nSince ∥∇f ∥2\\n2 is positive, being the squared length of the vector ∇f,\\nwe conclude that the update ⃗w ←⃗w −η∇f causes a decrease in value\\nof f. 1 This discussion motivates the gradient descent algorithm, which\\n1 In fact, the gradient ∇f is known as\\nthe direction of steepest increase of f.\\nHence, the opposite direction −∇f is\\nthe direction of steepest decrease of f.\\niteratively decreases the value of f until ∇f ≈0.\\nDefinition 3.1.1 (Gradient Descent). Gradient descent is an iterative\\nalgorithm that updates the weight vector ⃗w with the following rule:\\n⃗w ←⃗w −η∇f (⃗w)\\n(3.1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 34}, page_content='optimization via gradient descent\\n35\\nwhere η > 0 is a sufficiently small positive constant, called the learning\\nrate or step size.\\nWe illustrate with an example.\\nExample 3.1.2. Let f (w1, w2) = (w2\\n1 + w2\\n2)4 −7(w2\\n1 + w2\\n2)3 + 13(w2\\n1 +\\nw2\\n2)2. From Figure 3.2, we see that it attains a global minimum at (0, 0).\\nThe partial derivatives of f can be calculated as:\\n∂f\\n∂w1\\n= 2w1(w2\\n1 + w2\\n2)(4(w2\\n1 + w2\\n2)2 −21(w2\\n1 + w2\\n2) + 26)\\n∂f\\n∂w2\\n= 2w2(w2\\n1 + w2\\n2)(4(w2\\n1 + w2\\n2)2 −21(w2\\n1 + w2\\n2) + 26)\\nNow imagine initiating the gradient descent algorithm from the point\\n(0.5, 1) where the gradient vector is (7.5, 15). One iteration of gradient\\ndescent with η = 0.01 would move from (0.5, 1) to (0.425, 0.85). The\\ngradient vector at (0.425, 0.85) is (7.90, 15.81) and the next iteration of GD\\nwill move the point from (0.425, 0.85) to (0.35, 0.69). After 200 iterations,\\nthe algorithm moves the point to (0.03, 0.06), which is very close to the\\nglobal minimum of f.\\nFigure 3.2: The graph of f (w1, w2) =\\n(w2\\n1 + w2\\n2)4 −7(w2\\n1 + w2\\n2)3 + 13(w2\\n1 +\\nw2\\n2)2. The function attains a global\\nminimum at (0, 0).\\n3.1.3\\nLearning Rate (LR)\\nChoosing an appropriate learning rate is crucial for GD. Figure 3.3\\nshows the result of two iterations of gradient descent with a different\\nlearning rate. On the left, we see the result when η is too small. The\\nchange of w is too small, and the loss function converges to the\\nminimum very slowly. On the right, we see the result when η is too\\nbig. The change of w is too large, and the algorithm “shoots past” the\\nminimum. If η is even larger, the algorithm may even fail to converge\\nto the minimum.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 35}, page_content='36\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nFigure 3.3: Two iterations of gradient\\ndescent with different learning rates.\\nThe natural question to ask is: what is the appropriate learning\\nrate? There is some theory, and the best setting is known in some\\ncases. But in general, it has to be set by trial and error, especially\\nfor non-convex loss functions. For instance, we start with some\\nlearning rate, say 0.5 and decrease η by 1\\n2 if we do not observe a\\nsteady decrease in the training loss. Such heuristics are called training\\nschedules and they are derived via trial and error on that particular\\ndataset and model. 2\\n2 Constants whose values are decided\\nby trial and error based on dataset\\nand model are called hyperparameters.\\nModern ML models have several\\nhyperparameters. Often optimization\\npackages will suggest a default value\\nand a fine-tuning method.\\n3.1.4\\nNon-convex Functions\\nFor convex functions that are “bowl shaped,” gradient descent with\\na small enough learning rate provably converges to the minimum\\nsolution. But for non-convex functions, the best we can hope for is\\nconverging to a point where ∇f = 0. 3 Finding the global minimum\\n3 Points where the gradient is zero are\\ncalled stationary points, which include\\nlocal minima, local maxima, and\\nsaddle points. It is possible for a GD\\nalgorithm to terminate at a saddle point,\\ninstead of the intended local minimum.\\nThere is advanced theory on how to\\nescape saddle points, which will not be\\ncovered in this course.\\nof a non-convex function is NP-hard in the worst case.\\nIn practice, loss functions are non-convex and have multiple local\\nminima. Then, the gradient descent algorithm may converge to a\\ndifferent local minimum based on the initialization of the parameter\\nvector ⃗w.\\nFigure 3.4: An example of a convex\\nand a non-convex function in two\\nvariables. For non-convex functions,\\nGD will reach a stationary point, where\\nthe gradient is zero. Figure from\\nhttps://www.kdnuggets.com/2016/\\n12/hard-thing-about-deep-learning.\\nhtml.\\nExample 3.1.3. Consider the function f (w) = 1\\n3w4 −1\\n2w3 −w2 + w, which\\nhas two local minima at (−1, −1) and (2, −1). As seen in Figure 3.5, the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 36}, page_content='optimization via gradient descent\\n37\\nlocal minimum that the gradient descent algorithm outputs depends on the\\ninitial point.\\nFigure 3.5: The graph of f (w) =\\n1\\n3 w4 −1\\n2 w3 −w2 + w with two local\\nminima.\\n3.2\\nImplications of the Linearity of a Gradient\\nThe fact that gradient is a linear operator (i.e., ∇( f1 + f2) = ∇f1 +\\n∇f2) has great practical importance in machine learning.\\nJust like in (1.4), the training loss of a machine learning model is\\nusually defined as the average (or the sum) of the loss on individual\\ntraining data points. By the linearity of gradient, the gradient of the\\nentire loss can be found by taking the sum of the gradient of the loss\\non individual data points.\\n3.2.1\\nStochastic Gradient Descent\\nSince computing the gradient of the loss involves some computation\\non each of the data points, the computation can be quite slow for\\ntoday’s large data sets, which can contain millions of data points.\\nA simple workaround is to estimate the gradient at each step by\\nrandomly sampling k data points and averaging the corresponding\\nloss gradients. This is very analogous to opinion polls, which can\\nalso be seen as sampling from a distribution on vectors and using the\\naverage of the sample as a substitute for the population average. This\\nalgorithm is called Stochastic Gradient Descent (SGD). 4 This technique\\n4 Some authors call this the Batch SGD\\nand use the name SGD only for the case\\nwhere k = 1.\\nworks for two reasons: (1) all training data points are assumed to be\\nsampled from the same distribution; (2) the overall training loss is\\njust the sum/average of loss for individual data points.\\n3.2.2\\nMini-batch Stochastic Gradient Descent\\nToday, large scale machine learning is done using special-purpose\\nprocessors called Graphical Processing Units (GPUs). 5 These highly\\n5 As the name suggests, these were\\noriginally developed for computer\\ngraphics operations, which were\\nmost oftenly used in computer games.\\nAround 2012, deep learning experts\\nrealized their usefulness for deep\\nlearning. At that time, writing code\\nfor GPUs was extremely difficult, but\\ntoday’s environments have made this\\nmuch easier.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 37}, page_content='38\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nspecialized architectures have the ability to perform fast parallel\\ncomputations. To exploit these special capabilities, a special vari-\\nant of SGD — Mini-batch SGD — can be used. Here the dataset is\\nrandomly partitioned into mini batches whose size is dictated by\\nthe degree of parallelism available in the GPU, usually a power of 2,\\nsuch as 256. The members of each batch are loaded onto a different\\nprocessor. Together the processors compute the gradient for one mini-\\nbatch in one go, add up the gradients to perform a single iteration for\\nthe gradient descent. Then they move on to the next batch, perform\\nanother update step, and so on.\\n3.2.3\\nFederated Learning\\nThis is a conceptual framework for training an ML model on data be-\\nlonging to different parties, some of whom do not wish to hand the\\ndata over to a central server. Consider the following two examples:\\n1. Hospitals who wish to train an ML model on their pooled data,\\nbut who are forbidden by privacy laws to hand the data to other\\norganizations.\\n2. Owners of Internet of Things (IoT) devices, who wish to benefit\\nfrom training on their data but do not wish to submit the data.\\nIn Federated Learning, the model is trained at a central server,\\nwhereas data remains with the data owners, who actively participate\\nin the training. Users retrieve the current model parameters from\\nthe server and calculate the gradients locally. They send only the\\ngradients, but not the data, to the server, and the overall gradient is\\ncalculated at the server as the weighted sum (or average) of the user\\ngradients.\\n3.3\\nRegularizers\\nThis section describes regularization, a useful idea that often improves\\ngeneralization of the model. The main idea is that instead of min-\\nimizing the training loss function ℓ(⃗w), we minimize the function\\nℓ(⃗w) + λR(⃗w)\\n(3.2)\\nwhere λ > 0 is a constant and R(⃗w) is some non-negative function.\\nR(⃗w) is called a regularizer or sometimes penalty. We refer to (3.2) as\\nthe regularized loss function.\\nThe most commonly used regularizer is the ℓ2 regularizer where\\nthe squared ℓ2 norm R(⃗w) = ∥⃗w∥2\\n2 of the weight vector is used.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 38}, page_content='optimization via gradient descent\\n39\\nExample 3.3.1. Recall the sentiment prediction model using least squares\\nloss. Suppose the training data consists of two data points: (⃗x1, y1) =\\n((1, 0, 1), −1) and (⃗x2, y2) = ((1, 1, 0), +1). Then the least squares loss,\\nwithout any regularizer, can be written as\\n1\\n2((−1 −(w0 + w2))2 + (1 −(w0 + w1))2)\\n(3.3)\\nA little thought suggests that the minimum value of this loss is 0 provided\\nthere exists (w0, w1, w2) such that\\n(−1 −(w0 + w2))2 = 0 = (1 −(w0 + w1))2.\\nYou can verify that infinitely many solutions exist: all ⃗w∗= (w0, w1, w2)\\nthat lie on the line (0, 1, −1) + t(1, −1, −1) where t ∈R. In other words,\\nthe loss has infinitely many minimizers.\\nNow if impose an ℓ2 regularizer, the loss becomes\\n1\\n2((−1 −(w0 + w2))2 + (1 −(w0 + w1))2) + λ(w2\\n0 + w2\\n1 + w2\\n2)\\n(3.4)\\nAny minimizer of this loss must make the gradient zero. In other words, the\\nminimizer will satisfy the following system of linear equations:\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n(2 + 2λ)w0 + w1 + w2 = 0\\nw0 + (1 + 2λ)w1 = 1\\nw0 + (1 + 2λ)w2 = −1\\nYou can verify that ⃗w∗∗=\\n\\x10\\n0,\\n1\\n1+2λ, −\\n1\\n1+2λ\\n\\x11\\nis the unique minimizer for\\nany λ > 0. For a sufficiently small value of λ, the corresponding ⃗w∗∗is\\nclose enough to the line (0, 1, −1) + t(1, −1, −1). That is, it has a non-zero\\ntraining loss, but the value is very close to zero. Combined with the fact it\\nhas a relatively small norm, ⃗w∗∗becomes the minimizer for the regularized\\nloss.\\nFigure 3.6: The graph of the line\\n(0, 1, −1) + t(1, −1, −1) and the point\\n⃗w∗∗= (0,\\n1\\n1+2λ , −\\n1\\n1+2λ ) when λ = 0.01\\nNote that if ⃗w∗is the minimizer of ℓ(⃗w) and ⃗w∗∗the minimizer\\nof the regularized loss, then by definition of a minimizer, it always'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 39}, page_content='40\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nholds that ℓ(⃗w∗) ≤ℓ(⃗w∗∗). In general, regularization ends up lead-\\ning to training models with a higher value of ℓ(⃗w). This is considered\\nacceptable because the models often generalize better. In other words,\\na slightly higher training loss is considered a price worth paying for a\\nsignificantly lower test loss. This is illustrated by the example of sen-\\ntiment prediction from Chapter 1. As hinted there, the results shown\\nused a model trained with an ℓ2 regularizer. The dataset involves 15k\\ndistinct words, so that is the number of model variables. There are\\n8k data points. Recall from Problem 1.2.5 that in such settings, there\\nusually will exist a linear model that perfectly fits the data points.\\nIndeed, we see in Table 3.1 that this is the case when we don’t use a\\nregularizer. However, using a regularizer prevents the model from\\nperfectly fitting the training data. But the test loss drops tenfold with\\nregularization.\\nNo regularizer\\nWith ℓ2-regularizer\\nTrain MSE\\n0.0000\\n0.0727\\nTest MSE\\n7.9469\\n0.7523\\nTraining accuracy\\n100.00%\\n99.55%\\nTest accuracy\\n61.67%\\n78.07%\\nTable 3.1: Training sentiment model on\\nthe SST with and without ℓ2 regularizer.\\n3.3.1\\nEffects of Regularization\\nHere we briefly list some benefits of regularization.\\n1. Regularizers often help improve generalization. Above we saw a\\nconcrete example with the sentiment prediction model.\\n2. Adding a scalar multiple of ∥⃗w∥2\\n2 to a function can speed up\\noptimization by slightly reshaping the optimization landscape.\\nThe mathematical treatment of this is beyond the scope of this\\ncourse.\\n3. Without a regularizer term, models such as logistic regression and\\nsoft-margin SVMs begin to lose their power. This will be explained\\nwhen we discuss these models in Chapter 4.\\n3.3.2\\nWhy Does Regularization Help?\\nThe simplest answer is that we do not fully understand this concept\\nyet. In this section, we present some intuitions derived from simple\\nmodels, but keep in mind that these ideas might be misleading in\\nmore complicated models.\\nThe usual explanation given is that the norm of the parameter\\nvector controls the expressiveness or complexity of the model. Here\\n“complexity” is being used in the sense of “complicatedness”. By'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 40}, page_content='optimization via gradient descent\\n41\\ntrying to minimize loss as well as the norm of the parameter vector,\\nthe learned model tends to stay simple. 6 Whereas this discussion\\n6 Recall the famous Occam’s Razor for\\njudging goodness of scientific theories:\\nThe simpler the theory that explains the\\nknown facts, the more likely it is to be\\ncorrect. An ML model can be seen as a\\n“theory” about relationships in the data,\\nand thus the simplest theory is to be\\npreferred.\\ncan be made fairly rigorous for linear models, it does not seem to\\napply to more complicated models; for instance, regularization often\\nhelps a lot in deep learning, but rigorous explanations appear to be at\\nbest incomplete and at worst incorrect there. 7\\n7 See the blog https://www.offconvex.\\norg for posts about generalization and\\ndeep learning. They also discuss how\\nother ideas such as VC dimension,\\nwhich we did not cover in this course,\\nalso do not apply in deep learning.\\nAnother explanation 8 is that a regularizer serves as a penalty for\\n8 See the online lecture video by An-\\ndrew Ng. https://www.youtube.com/\\nwatch?v=QjOILAQ0EFg\\nlarge weights and forces the model to choose smaller absolute values\\nof parameters. According to this explanation, adding regularizers to\\na model penalizes higher-order terms or unnecessary variables and\\nis able to avoid overfitting. Indeed, Figure 3.7 shows that the weights\\nof the parameters in our sentiment model are significantly smaller\\nwhen trained with a regularizer. But one lingering question with\\nthis explanation is: How come attaching the same penalty to all variables\\nforces the model to identify variables that are needed, and those that are not?\\nWhat causes this disparate treatment of the variables?\\nFigure 3.7: The histogram of weights of\\nthe parameters in the sentiment predic-\\ntion model with (right) or without (left)\\nan ℓ2 regularizer.\\nNow consider this explanation — ℓ2 regularization introduces a\\nnew dynamic to gradient descent, whereby gradient updates have to\\nconstantly battle against a rescaling that is always trying to whittle\\nall variables down to zero. The effort succeeds only for variables\\nwhere gradient updates are pushing hardest to make them nonzero.\\nTherefore, the weights for “necessary” variables survive, while\\n“unnecessary” variables are thrown away. To say this more precisely,\\nconsider the regularized loss ℓ(⃗w) + λ ∥⃗w∥2\\n2 whose gradient is\\n∇ℓ+ 2λ⃗w\\nThus the update rule in gradient descent can be written as\\n⃗wt+1 ←⃗wt −η(∇ℓ+ 2λ⃗wt)\\nwhere ⃗wt denotes the weight vector at the t-th time step. This update\\nrule can be rewritten as\\n⃗wt+1 ←⃗wt(1 −2ηλ) −η∇ℓ\\n(3.5)\\nThe first term is down-scaling: if for example η = λ = 0.1, this\\namounts to multiplying the current vector by 0.98, and this of course\\nwill make ⃗w very small in a few hundred iterations.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 41}, page_content='42\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nThe second term is the gradient update to ⃗w. It can counteract\\nthe down-scaling by making the variables larger. But notice that the\\namount of change is based on how much each of the coordinates con-\\ntribute to reducing the loss. Variables that are not useful will tend not\\nto get increased by the gradient update and thus will keep getting\\ndown-scaled to low values. 9 The choice of λ mediates between these\\n9 It is one of those “use it or lose it”\\nsituations!\\ntwo processes.\\n3.4\\nGradient Descent in Python Programming\\nIn this section, we briefly discuss how to implement the Gradient De-\\nscent algorithm in Python. It is customary to use the numpy package\\nto speed up computation and the matplotlib package for visualization.\\n# import necessary packages\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom matplotlib import cm # colormap\\n# initialize variables\\nnum_iter = ...\\nx = np.zeros(num_iter + 1)\\ny = np.zeros(num_iter + 1)\\nx[0], y[0], eta = ...\\n# define functions to calculate f and grad_f\\ndef f(x, y):\\n...\\nreturn f\\ndef grad_f(x, y):\\n...\\nreturn grad_f\\n# run Gradient Descent\\nfor i in range(num_iter):\\ngrad_x, grad_y = grad_f(x[i], y[i])\\nx[i + 1] = x[i] - eta * grad_x\\ny[i + 1] = y[i] - eta * grad_y\\n# plot the surface\\nxmin, xmax, ymin, ymax, n = ...\\nX, Y = np.meshgrid(np.linspace(xmin, xmax, n),\\nnp.linspace(ymin, ymax, n))\\nZ = f(X, Y)\\nfig = plt.figure(figsize=(12, 10))\\nax = fig.add_subplot(projection=’3d’)\\nax.set_xlabel(’X’)\\nax.set_ylabel(’Y’)\\nax.set_zlabel(’Z’)\\nax.view_init(elev=ax.elev, azim=ax.azim)\\nax.plot_surface(X, Y, Z, cmap=cm.coolwarm, alpha=0.5)\\n# plot the trajectory of Gradient Descent\\nax.plot(x, y, f(x, y), color=’orange’, markerfacecolor=’black’,\\nmarkeredgecolor=’k’, marker=’o’, markersize=5)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 42}, page_content='optimization via gradient descent\\n43\\nWe first start off by importing necessary packages and initializing\\nvariables. The following code initializes numpy arrays of length\\nnum_iter + 1, with all entries initialized to 0\\nx = np.zeros(num_iter + 1)\\ny = np.zeros(num_iter + 1)\\nSometimes, it is useful to make use of np.ones(), which will generate\\narrays filled with entries equal to 1.\\nWe then define functions that will calculate the values of f and ∇f\\ngiven an array of data points (x, y).\\ndef f(x, y):\\n...\\nreturn f\\ndef grad_f(x, y):\\n...\\nreturn grad_f\\nThis allows us to run the Gradient Descent algorithm as in\\nfor i in range(num_iter):\\ngrad_x, grad_y = grad_f(x[i], y[i])\\nx[i + 1] = x[i] - eta * grad_x\\ny[i + 1] = y[i] - eta * grad_y\\nHere we iteratively update the value of (x, y) using ∇f (x, y) and\\nstore each of the points in the array x and y.\\nWe next plot the surface of the function f (x, y). To start, we first\\ncreate a grid of (x, y) points to evaluate f (x, y) at.\\nX, Y = np.meshgrid(np.linspace(xmin, xmax, n),\\nnp.linspace(ymin, ymax, n))\\nZ = f(X, Y)\\nThe function call np.linspace(min, max, n) generates an array of n\\nequally spaced values from min to max. For example, the code\\nnp.linspace(-2, 2, 5)\\nwill create an array [−2, −1, 0, 1, 2]. Then np.meshgrid(x, y) will create\\na grid from the array of x values and the array of y values. We can\\nnow perform the 3D plotting with the following code.\\nfig = plt.figure(figsize=(12, 10))\\nax = fig.add_subplot(projection=’3d’)\\nax.set_xlabel(’X’)\\nax.set_ylabel(’Y’)\\nax.set_zlabel(’Z’)\\nax.view_init(elev=ax.elev, azim=ax.azim)\\nax.plot_surface(X, Y, Z, cmap=cm.coolwarm, alpha=0.5)\\nFeel free to change the values of the optional parameters to under-\\nstand their purpose. Unlike the code for plotting a scatter plot of a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 43}, page_content='44\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nlinear regression in Chapter 1, here we create an object of the Axes\\nclass with the function plt.figure().gca()10. Then we call its instance\\n10 You can read more about the\\ndifferences between these two\\nmatplotlib interfaces at https:\\n//matplotlib.org/matplotblog/posts/\\npyplot-vs-object-oriented-interface/\\nmethods to add features to it (e.g., x-, y-, z-labels).\\nFinally, we can plot the trajectory of the Gradient Descent algo-\\nrithm with the code\\nax.plot(x, y, f(x, y), color=’orange’, markerfacecolor=’black’,\\nmarkeredgecolor=’k’, marker=’o’, markersize=5)\\nYou can alternatively call\\nax.scatter(x, y, f(x, y))\\nbut the names of optional parameters might be slightly different.\\n3.4.1\\nUsing Machine Learning Packages\\nWhen the function f is simple and it is possible to calculate ∇f by\\nhand, we can implement the Gradient Descent algorithm by hand\\nas in the previous subsection. However, in most ML programs, the\\nloss function f is very high-dimensional, and it is difficult to write\\na single function to directly compute the gradient ∇f. Instead, we\\ncan make use of functions defined in popular ML packages. Here, we\\nintroduce one such package called PyTorch:\\n• torch: This is a popular package used for designing and train-\\ning machine learning models. PyTorch uses an object-oriented\\ninterface for user convenience and provides access to optimized\\narray data structures called tensors to make computations faster\\nand more efficient. The package also provides support for GPU\\ntraining. 11\\n11 Documentation is available at https:\\n//pytorch.org/docs/stable/index.\\nhtml\\nUsing PyTorch, Gradient Descent can be implemented in just a few\\nlines:\\nimport torch\\nmodel = ...\\nopt = torch.optim.SGD(model.parameters(), lr=0.1)\\nThe code above will create an instance of the Optimzer class, which\\nhas pre-defined methods that will compute the gradients and auto-\\nmate the Gradient Descent process.\\n3.4.2\\nBeyond Vanilla Gradient Descent\\nIf you visit the documentation for torch.optim, 12 you may notice that\\n12 https://pytorch.org/docs/stable/\\noptim.html\\nthere are other algorithms listed as an alternative to the Stochastic\\nGradient Descent. A lot of these algorithms are extensions of the GD\\nalgorithm we explained throughout this chapter, which have proven\\nto be more effective than the vanilla GD algorithm in certain cases'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 44}, page_content='optimization via gradient descent\\n45\\n(e.g., Adam, Adagrad, Nesterov momentum). For example, these\\nalgorithms may choose to add a momentum to the gradient, so that\\nthe rate of change of f will be accelerated if it has been updating in\\nthe same direction in the recent few steps. These algorithms may also\\nchoose to use a different learning rate for each of the model param-\\neters. In particular, an appropriate learning rate can be computed\\nbased on the mean and the variance of the gradient values from the\\nrecent few steps.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 45}, page_content=''),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 46}, page_content='4\\nLinear Classification\\nMulti-way Classification is a task of learning to predict a label on\\nnewly seen data out of k possible labels. In binary classification, there\\nare only two possible labels, say ±1. Sentiment prediction in Chap-\\nter 1 was an example of a binary classification task. In this chapter,\\nwe introduce two other linear models that perform binary classi-\\nfication: logistic regression and Support Vector Machines (SVMs).\\nFrom these two models, we learn more about the thought process of\\ndesigning loss functions that are appropriate to the task. 1\\n1 All the linear models we will study fall\\nunder an all-encompassing framework\\ncalled Generalized Linear Models. If you\\never are faced with a new situation\\nwhere none of the models below are an\\nexact match, try looking up this general\\nframework.\\nIn this chapter, we are interested in using linear models to perform\\nclassification. In a binary classification problem, the training dataset\\nconsists of (point, label) pairs (⃗x, y) where y can take two values (e.g.,\\n{±1} or {0, 1}). In a more general multi-class classification problem,\\nthe data has one of k labels, drawn from {0, 1, . . . , k −1}.\\n4.1\\nGeneral Form of a Linear Model\\nYou already encountered a linear model in Chapter 1 — the least\\nsquares regression model for sentiment prediction. Given an input⃗x,\\nwe learned a parameter vector ⃗w that minimizes the loss ∑i(yi −⃗w ·\\n⃗xi)2. The model can be seen as mapping an input vector⃗x to a real\\nvalue ⃗w ·⃗x. For sentiment classification, we changed this real-valued\\noutput at test time to ±1 by outputting sign(⃗w ·⃗x).\\nYou probably wondered there: Why don’t we simply use sign(⃗w ·⃗x)\\ndirectly as the output of the model while training? In other words, why\\nnot do training on the following loss:\\n∑\\ni\\n(yi −sign(⃗w ·⃗xi))2\\n(4.1)\\nThe answer is that using the sign(z) function in the loss makes\\ngradient-based optimization ill-behaved. The derivative of sign(z) is\\n0 except at z = 0 (where the derivative is discontinuous) and thus the\\ngradient is uninformative about how to update the weight vector.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 47}, page_content='48\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nSo the work-around in Chapter 1 (primarily for ease of exposition)\\nwas to train the sentiment classification model using the least squares\\nloss ∑i(yi −⃗w ·⃗xi)2, which in practice is used more often in settings\\nwhere the desired output yi is real-valued output as opposed to bi-\\nnary. This gave OK results, but in practice one would use either of\\nthe two linear models 2 introduced in this chapter: Logistic Regression\\n2 They are called linear because they use\\nthe mapping⃗x 7→⃗w ·⃗x.\\nand Support Vector Machines. These are similar in spirit to the linear\\nregression model — (1) given an input⃗x, the models learn a parame-\\nter vector ⃗w that minimizes a loss, defined as a differentiable function\\non ⃗w ·⃗x; (2) at test-time, the model outputs sign(⃗w ·⃗x). 3 The main\\n3 There are other ways to output a\\ndiscrete ±1 label, but using the sign\\nfunction is the most canonical way. We\\nwill discuss the behavior of the models\\nat test-time later in the chapter.\\ndifference, however, is that the loss for the linear models introduced\\nin this chapter is designed specifically for the binary classification\\ntask. Pay close attention to our “story” for why the loss makes sense.\\nThis will prepare you to understand any new loss functions you\\ncome across in your future explorations.\\n4.2\\nLogistic Regression\\nThe logistic regression model arises from thinking of the answer\\nas being probabilistic: the model assigns a “probability” to each of\\nthe two labels, with the sum of the two probabilities being 1. 4 This\\n4 This “probability” is what is called\\nsubjective probability, analogous to what\\nwe mean when say things like “I am\\n99 percent sure my friend X will like\\nmovie Y.” There is only one person\\nX and one movie Y and they are not\\ndrawn from some probability space.\\nInstead we’re expressing a subjective\\nfeeling of near-certainty based upon\\npast observations of person X.\\nparadigm of a probabilistic answer is a popular way to design loss\\nfunctions in a host of ML settings, including deep learning.\\nDefinition 4.2.1 (Logistic model). Given the input⃗x, 5 the model assigns\\n5 As in Chapter 1 we assume⃗x contains\\na dummy coordinate x0 that is 1 at\\nall points: this allows us to include a\\nconstant bias term when we take the\\ndot product ⃗w ·⃗x with the weight vector.\\nthe “Probability that the output is +1” to be\\nσ(⃗w ·⃗x) =\\n1\\n1 + exp(−⃗w ·⃗x)\\n(4.2)\\nwhere σ is the sigmoid function (see Chapter 19). This implies that “Proba-\\nbility that the output is −1” is given by\\n1 −\\n1\\n1 + exp(−⃗w ·⃗x) =\\nexp(−⃗w ·⃗x)\\n1 + exp(−⃗w ·⃗x) =\\n1\\n1 + exp(⃗w ·⃗x)\\n(4.3)\\nSee Figure 4.1. Note that “the probability that the output is +1”\\nis greater than 1\\n2 precisely if ⃗w ·⃗x > 0. Furthermore, increasing the\\nvalue of ⃗w ·⃗x causes the probability to rise towards 1. Conversely, if\\n⃗w ·⃗x < 0, then “the probability of label −1” is greater than 1\\n2. When\\n⃗w ·⃗x = 0, the probability of label +1 and −1 are both equal to 1\\n2. In\\nthis way, the logistic model can be seen as a continuous version of the\\nsign(⃗w ·⃗x).\\nExample 4.2.2. If⃗x = (1, −3) and ⃗w = (0.2, −0.1), then the probability of\\nlabel +1 is\\n1\\n1 + exp(−0.2 −0.3) =\\n1\\n1 + e−0.5 ≃0.62'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 48}, page_content='linear classification\\n49\\nFigure 4.1: The graph of the probability\\nthat the output of a logistic model is +1\\n(red) or -1 (blue) given ⃗w ·⃗x.\\n4.2.1\\nDefining Goodness of Probabilistic Predictions\\nThus far, we explained how the logistic model generates its output\\ngiven an input vector⃗x and the current weight vector ⃗w. But we have\\nnot yet talked about how to train the model. To define a loss function,\\nwe need to decide what are the “good” values for ⃗w. Specifically, we\\nformulate a definition of “quality” of probabilistic predictions.\\nDefinition 4.2.3 (Maximum Likelihood Principle). Given a set of\\nobserved events, the goodness of a probabilistic prediction model 6 is the\\n6 This is a definition of goodness, not the\\nconsequence of some theory.\\nprobability it assigned to the set of observed events.\\nWe illustrate with an example.\\nExample 4.2.4. You often see weather predictions that include an estimate\\nof the probability of rain. Table 4.1 shows the predictions by two models at\\nthe start of each day of the week. After the week is over, we have observed if\\nit actually rained on each of the days. Based on these observations, which\\nmodel made better predictions this week?\\nM\\nT\\nW\\nTh\\nF\\nModel 1\\n60%\\n20%\\n90%\\n50%\\n40%\\nModel 2\\n70%\\n50%\\n80%\\n20%\\n60%\\nRained?\\nY\\nN\\nY\\nN\\nN\\nTable 4.1: Weather predictions by Model\\n1 and Model 2.\\nWe can answer this question by seeing which model assigns higher\\nlikelihood to the events that were actually observed (i.e., whether or not it\\nrained). For instance, the likelihood of the observed sequence according to\\nModel 1 is\\n0.6 × (1 −0.2) × 0.9 × (1 −0.5) × (1 −0.4) = 0.1296'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 49}, page_content='50\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nThe corresponding number for Model 2 is 0.0896 (check this!). So Model 1\\nwas a “better” model for this week.\\n4.2.2\\nLoss Function for Logistic Regression\\nWe employ the Maximum Likelihood Principle from the previous\\npart to define the loss function for the logistic model. Suppose we\\nare provided the labeled dataset {(⃗x1, y1), (⃗x2, y2), . . . , (⃗xN, yN)} for\\ntraining where yi is a ±1 label for the input⃗xi. By the description\\ngiven in Definition 4.2.1, the probability assigned by the model with\\nthe weights ⃗w to the i-th labeled data point is\\n1\\n1 + exp(−yi⃗w ·⃗xi)\\nwhich means that the total probability (“likelihood”) assigned to the\\ndataset is\\nP =\\nN\\n∏\\ni=1\\n1\\n1 + exp(−yi⃗w ·⃗xi)\\n(4.4)\\nWe desire the model ⃗w that maximizes P. Since log(x) is an increas-\\ning function, the best model is also the one that maximizes log P,\\nhence the one that minimizes −log P = log 1\\nP. This leads to the\\nlogistic loss function:\\nlog\\n \\nN\\n∏\\ni=1\\n(1 + exp(−yi⃗w ·⃗xi))\\n!\\n=\\nN\\n∑\\ni=1\\nlog(1 + exp(−yi⃗w ·⃗xi))\\n(4.5)\\nNote that this expression involves a sum over training data points,\\nwhich as discussed in Section 3.2, is a very desirable and practical\\nproperty of loss in machine learning.\\nProblem 4.2.5. Verify that the gradient for the logistic loss function is\\n∇ℓ=\\nN\\n∑\\ni=1\\n−yi⃗xi\\n1 + exp(yi⃗w ·⃗xi)\\n(4.6)\\n4.2.3\\nUsing Logistic Regression for Roommate Matching\\nIn this part, we use the following example to illustrate some of the\\nmaterial covered in the previous parts.\\nExample 4.2.6. Suppose Princeton University decides to pair up newly\\nadmitted undergraduate students as roommates. All students are asked to\\nfill a questionnaire about their sleep schedule and their music taste. The\\nquestionnaire is used to generate a compatibility score in [0, 1] for each of\\nthe two attributes, for each pair of students. Table 4.2 shows the calculated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 50}, page_content='linear classification\\n51\\nSleep (S)\\nMusic (M)\\nCompatible?\\n1\\n0.5\\n+1\\n0.75\\n1\\n+1\\n0.25\\n0\\n−1\\n0\\n1\\n−1\\nTable 4.2: Sample data of compatibility\\nscores for four pairs of students.\\ncompatibility scores for four pairs of roommates from previous years and\\nwhether or not they turned out to be compatible (+1 for compatible, −1 for\\nincompatible).\\nWe wish to train a logistic model to predict if a pair of students\\nwill be compatible based on their sleep and music compatibility\\nscores. To do this, we first convert the data in Table 4.2 into a vector\\nform.\\n⃗x1 = (1, 1, 0.5)\\n⃗x2 = (1, 0.75, 1)\\n⃗x3 = (1, 0.25, 0)\\n⃗x4 = (1, 0, 1)\\ny1 = +1\\ny2 = +1\\ny3 = −1\\ny4 = −1\\n(4.7)\\nwhere the first coordinate xi\\n0 of⃗xi is a dummy variable to introduce a\\nconstant bias term, and the second and third coordinates are respec-\\ntively for sleep and music compatibility scores.\\nFigure 4.2: Graph representing the\\npoints in Table 4.2. The x-, y-axis in\\nthe graph correspond to the Sleep\\nand Music compatibility scores, or the\\nsecond and third coordinates in (4.7).\\nConsider two models — Model 1 with the weight vector ⃗w1 =\\n(0, 1, 0) and Model 2 with the weight vector ⃗w2 = (0, 0, 1). Model 1\\nonly looks at the sleep compatibility score to calculate the probability\\nthat a pair of students will be compatible as roommates, whereas'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 51}, page_content='52\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nModel 2 only uses the music compatibility score. For example, Model\\n1 assigns the probability that the first pair of students are compatible\\nas\\nσ(⃗w1 ·⃗x1) =\\n1\\n1 + exp(−1) ≃0.73\\nWe can calculate the probability for the other pairs and for Model 2\\nand fill out the following Table 4.3:\\nPair 1\\nPair 2\\nPair 3\\nPair 4\\nModel 1\\n0.73\\n0.68\\n0.56\\n0.50\\nModel 2\\n0.62\\n0.73\\n0.50\\n0.73\\nCompatible?\\nY\\nY\\nN\\nN\\nTable 4.3: Roommate compatibility\\npredictions by Model 1 and Model 2.\\nThen the likelihood of the observations (YYNN) according to\\nModel 1 can be calculated as\\n0.73 × 0.68 × (1 −0.56) × (1 −0.50) ≃0.11\\nwhere as the likelihood of the observations according to Model 2 is\\n0.62 × 0.73 × (1 −0.50) × (1 −0.73) ≃0.06\\nTherefore, the Maximum Likelihood Principle tells us that Model 1 is\\na “better” model than Model 2.\\nThe full logistic loss for this training data can be written as\\n4\\n∑\\ni=1\\nlog(1 + exp(−yi⃗w ·⃗xi)) = log(1 + exp(−(w0 · 1 + w1 · 1 + w2 · 0.5))+\\nlog(1 + exp(−(w0 · 1 + w1 · 0.75 + w2 · 1))+\\nlog(1 + exp(w0 · 1 + w1 · 0.25 + w2 · 0)+\\nlog(1 + exp(w0 · 1 + w1 · 0 + w2 · 1)\\nand the values that minimize this loss can be found as w0 ≈−21, w1 ≈\\n32, w2 ≈8.9.\\n4.2.4\\nTesting the Model\\nAfter training the model on the training data, we can use it to define\\nlabel probabilities on any new data point. However, the probabilities\\ndo not explicitly tell us what label to output on a new data point.\\nThere are two options:\\n1. (Probabilistic) If p is the probability of the label +1 according\\nto (4.2), then use a random number generator to output +1 with\\nprobability p and −1 with probability 1 −p.\\n2. (Deterministic) Output the label with a higher probability.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 52}, page_content='linear classification\\n53\\nRecall from an earlier discussion that Pr[+1] ≥Pr[−1] if and\\nonly if ⃗w ·⃗x ≥0. In other words, the second deterministic option is\\nequivalent to the sign(z) function: sign(⃗w ·⃗x)!\\nWe conclude that logistic regression is quite analogous to what we\\ndid in Chapter 1, except instead of least squares loss, we are using\\nlogistic loss to train the model. The logistic loss is explicitly designed\\nwith binary classification in mind. 7\\n7 Using logistic loss (and ℓ2 regularizer)\\ninstead of least squares in our senti-\\nment dataset boosts test accuracy from\\n78.1% to 80.7%.\\n4.3\\nSupport Vector Machines\\nA Support Vector Machine (SVM) 8 is also a linear model. It comes in\\n8 From An optimal algorithm for training\\nmaximum margin classifiers. by Boser,\\nGuyon, and Vapnik in COLT 1992. The\\nname Support Vector Machine comes\\nfrom a theorem that characterizes\\nthe optimum model in terms of “sup-\\nport vectors.” We will not cover that\\ntheorem here.\\nseveral variants, including a more powerful kernel SVM that we will\\nnot study here. But this rich set of variants made it an interesting\\nfamily of models, and it is fair to say that in the 1990s its popularity\\nwas somewhat analogous to the popularity of deep nets today. It\\nremains a very useful model for your toolkit. The version we are\\ndescribing is a so-called soft margin SVM.\\nAs in the least squares regression, the main idea in designing the\\nloss is that the label should be +1 or −1 according to sign(⃗w ·⃗x). But\\nwe want to design a loss with a well-behaved gradient that provides\\na clearer direction of improvement. To be more specific, we want the\\nmodel to have more “confident” answers, and we will penalize the\\nmodel if it comes up with a correct answer but with a low degree of\\n“confidence.”\\nFor z ∈R, let us define\\nHinge(z) = max{0, 1 −z}\\n(4.8)\\nFigure 4.3: The graph of the hinge\\nfunction.\\nNote that this function is always at least zero, and strictly positive\\nfor z < 1. When z decreases to negative infinity, there is no finite\\nupper bound to the value. The derivative is zero for z > 1 and 1 for\\nz < 1. The derivative is currently undefined at z = 1, but we can\\narbitrarily choose between 0 or 1 as the newly defined value.\\nFor a single labeled data point (⃗x, y) where y ∈{−1, 1}, the SVM\\nloss is defined as\\nℓ= Hinge(y⃗w ·⃗x)\\n(4.9)\\nand its gradient is\\n∇ℓ=\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n−y⃗x\\ny⃗w ·⃗x < 1\\n0\\ny⃗w ·⃗x > 1\\nThe SVM loss for the entire training dataset can be defined as\\n∑\\ni\\nHinge(yi⃗w ·⃗xi)\\n(4.10)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 53}, page_content='54\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nthat is, the sum of the SVM loss on each of the training data points.\\nSuppose y = +1. Then this loss is 0 only when ⃗w ·⃗x > 1. In other\\nwords, making loss zero not only requires ⃗w ·⃗x to be positive, but also\\nbe comfortably above 0. If ⃗w ·⃗x dips below 1, the loss is positive and\\nincreases towards +∞as ⃗w ·⃗x →−∞. (Likewise if the label y = −1,\\nthen the loss is 0 only when ⃗w ·⃗x < −1.)\\nRecall that the goal of a gradient-based optimization algorithm is\\nto minimize the loss. Therefore, the loss gives a clear indication of\\nthe direction of improvement until the data point has been classified\\ncorrectly with a comfortable margin away from 0, out of the zone of\\nconfusion.\\nExample 4.3.1. Recall the roommate compatibility data from Table 4.2.\\nConsider the soft-margin SVM with the weight vector ⃗w = (−1.5, 3, 0).\\nThis means the decision boundary — the set of points where ⃗w ·⃗x = 0 — is\\ndrawn at Sleep = 1\\n2, and the margins — the set of points where ⃗w ·⃗x = ±1\\n— are drawn at Sleep = 5\\n6 and Sleep = 1\\n6. Figure 4.4 shows the decision\\nboundary and the two margin lines of the model. The SVM loss is zero\\nfor the point (1, 0.5) because it is labeled +1 and away from the decision\\nboundary with enough margin. Similarly, the loss is zero for the point (0, 1).\\nThe loss for the point (0.75, 1), however, can be calculated as\\nHinge(1 · (−1.5 · 1 + 3 · 0.75)) = 0.25\\nand similarly, the loss for the point (0.25, 0) is 0.25.\\nFigure 4.4: The decision boundary of\\na soft-margin SVM on the roommate\\nmatching example. The region to the\\nleft of the two dotted lines is where\\nthe model confidently classifies as\\n−1; the region to the right is where it\\nconfidently classifies as +1; and the\\nregion between the two dotted lines is\\nthe zone of confusion.\\nThe gradient of the loss at the point (0.75, 1) is\\n−y⃗x = (−1, −0.75, −1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 54}, page_content='linear classification\\n55\\nand the update rule for a gradient descent algorithm will be written as\\n⃗w ←(−1.5, 3, 0) −0.1(−1, −0.75, −1) = (−1.4, 3.075, 0.1)\\nwhere η = 0.1, and the new SVM loss will be\\nHinge(1 · (−1.4, 3.075, 0.1) · (1, 0.75, 1)) = 0\\nwhich is now lower than the SVM loss before the update.\\n4.4\\nMulti-class Classification (Multinomial Regression)\\nSo far, we have only seen problems where the model has to classify\\nusing two labels ±1. In many settings there are k possible labels\\nfor each data point 9 and the model has to assign one of them. The\\n9 This is the case in most settings in\\nmodern machine learning. For instance\\nin the famous ImageNet challenge, each\\nimage belongs to one of 1000 classes.\\nconceptual framework is similar to logistic regression, except the\\nmodel defines a nonzero probability for each label as follows. The\\nnotation assumes data is d-dimensional and the model parameters\\nconsist of k vectors ⃗θ1, ⃗θ2, . . ., ⃗θk ∈Rd. We define a new vector⃗z ∈Rk\\nwhere each coordinate zi satisfies zi = ⃗θi ·⃗x. Then the probability\\nof a particular label is defined through the softmax function (see\\nChapter 19):\\nPr[label i on input⃗x] = so f tmax(⃗z)\\n=\\nexp(⃗θi ·⃗x)\\n∑k\\nj=1 exp(⃗θj ·⃗x)\\n(4.11)\\nThis distribution can be understood as assigning a probability to\\nlabel i such that it is exponentially proportional to the value of ⃗θi ·⃗x.\\nProblem 4.4.1. Using the result of Problem 19.2.4, verify that the definition\\nof logistic regression as in (4.2), (4.3) are equivalent to the definition of\\nmulti-class regression as in (4.11).\\nProblem 4.4.2. Reasoning analogously as in logistic regression, derive a\\ntraining loss for this model using Maximum Likelihood Principle.\\nSince exp(z) > 0 for every real number z the model above assigns\\na nonzero probability to every label. In some settings that may be\\nappropriate. But as in case of logistic regression, at test time we also\\nhave the option of extracting a deterministic answer out of the model:\\nthe i ∈{1, 2, . . . , k} that has the largest value of ⃗θj ·⃗x.\\n4.5\\nRegularization with SVM\\nIt is customary to use a regularizer, typically ℓ2, with logistic regres-\\nsion models and SVMs. When a ℓ2 regularizer is applied, the full'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 55}, page_content='56\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nSVM loss is rewritten as\\n∑\\ni\\nHinge(yi⃗w ·⃗xi) + λ ∥⃗w∥2\\n2\\n(4.12)\\nLet’s see why regularization is sensible for SVMs, and even needed.\\nThe Hinge function (4.8) treats the point z = 1 as special. In terms of\\nthe SVM loss, this translates to the thought that having ⃗w ·⃗x > 1 is\\na more “confident” classification of⃗x than just having the sign(⃗w ·⃗x)\\nbe correct (i.e., ⃗w ·⃗x > 0). But this choice is arbitrary because we\\nhave not specified the scale of ⃗w. If ⃗w ·⃗x = 1/10 then scaling ⃗w by\\na factor 10 ensures ⃗w ·⃗x > 1. Thus the training algorithm has cheap\\nand meaningless ways of reducing the training loss. By applying an\\nℓ2 regularizer, we are able to prevent this easy route for the model,\\nand instead, force the training to find optimal weights ⃗w with a small\\nnorm.\\nProblem 4.5.1. Write a justification for why it makes sense to limit the ℓ2\\nnorm of the classifier during logistic regression. How can large norm lead to\\nfalse confidence (i.e., unrealistically low training loss)?\\n4.6\\nLinear Classification in Python Programming\\nIn this section, we briefly discuss how to implement the logistic re-\\ngression model in Python. It is customary to use the numpy package\\nto speed up computation and the matplotlib package for visualization.\\n# import necessary packages\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n# prepare dataset\\nX = ... # array of shape (n, d), each row is a d-dimensional data point\\ny = ... # array of shape (n), each value = -1 or +1\\nw = ... # array of shape (d), each value is a weight for each dimension\\nX_train, X_test, y_train, y_test, eta = ...\\n# define functions\\ndef loss(X, y, w):\\n# returns the logistic loss\\n# return sum log(1 + exp(-y*w*x))\\n...\\ndef grad_loss(X, y, w):\\n# returns the gradient of the logistic loss\\n# return sum (-y*x)/(1 + exp(y*w*x))\\n...\\ndef gradient_descent(X, y, w0, eta)\\n...\\nreturn w\\n# run Gradient Descent\\nw = gradient_descent(X_train, y_train, w, eta)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 56}, page_content='linear classification\\n57\\n# plot the learned classifier\\n# assuming data is 2-dimensional\\ncolors = {1: ’blue’, -1: ’red’}\\nxmin, xmax, ymin, ymax = ...\\nplt.scatter(X[:,0], X[:,1], c=np.array([colors[y_i] for y_i in y]))\\nplt.plot([xmin, xmax], [ymin, ymax], c=’black’)\\nWe have already discussed how to implement the majority of the\\ncode sample above in previous chapters. The only parts that are new\\nare the functions to calculate the logistic loss and its gradient. This is\\nconsistent with the theme of this chapter — to discuss how to design\\nloss functions that are appropriate for the task. Nevertheless, while\\nthe content of this code sample is familiar, some sections of the code\\nintroduce new Python functionality and syntax. We first consider the\\nlogistic loss and gradient functions:\\ndef loss(X, y, w):\\n# returns the logistic loss\\n# return sum log(1 + exp(-y*w*x))\\n...\\ndef grad_loss(X, y, w):\\n# returns the gradient of the logistic loss\\n# return sum (-y*x)/(1 + exp(y*w*x))\\n...\\nIn Java, the programming language you learned in earlier program-\\nming classes, you would have to rely on a for loop to account for the\\narray inputs in the loss() and grad_loss() functions. However, Python\\nand numpy support many vectorized operations, including matrix mul-\\ntiplication and element-wise multiplication. These operations are\\nfar more concise to read and will also improve the runtime of the\\nprogram by a great margin. Note that the code snippet above does\\nnot contain these operations; it is simply pseudo-code for your intu-\\nition. You will be introduced to these vectorized operations during\\nthe precept, and you will be expected to implement the loss function\\nwith these new tools in your programming assignments.\\nNext, we use a Python dictionary to store information correspond-\\ning to the plot’s coloring scheme:\\ncolors = {1: ’blue’, -1: ’red’}\\nThis is equivalent to a hash table from Java. Here, 1 and −1 are the\\nkeys and “blue” and “red” are respectively their values.\\nWe will now discuss multi-dimensional arrays in Python. There\\nare multiple ways to perform array indexing. For example, if X is a\\n2-dimensional array, both X[i][j] and X[i, j] can be used to extract the\\nentry at the i-th row, j-th column. It is also possible to provide a set\\nof rows or a set of columns to extract. The following code snippet\\ngenerates an array of shape (2, 2), where each entry is from the row 0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 57}, page_content='58\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nor 1 and column 0 or 2:\\nX[[0, 1], [0, 2]]\\nNote that similar to the 1D case, the : operator is used to perform\\narray slicing. Bounding indices can be omitted as shown in the\\nfollowing code snippet:\\nX[:,0]\\nThis extracts the full set of rows and the column 0, or in other words,\\nthe first column of X.\\nFinally, we use a list comprehension to specify the plotting color for\\neach data point:\\n[colors[y_i] for y_i in y]\\nThis is Python syntactic sugar that allows the user to create an array\\nwhile iterating over the elements of an iterator. The code snippet here\\nis equivalent to the following code.\\nlist = []\\nfor y_i in y:\\nlist.append(colors[y_i])'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 58}, page_content='5\\nExploring “Data Science” via Linear Regression\\nSo far, our treatment of machine learning has been from the perspec-\\ntive of a computer scientist. It is important to note, however, that\\nmodels such as linear regression are useful in a variety of other fields\\nincluding the physical sciences, social sciences, etc. In this chapter,\\nwe present case studies from different fields. Here, the inputs xi are\\nconsidered to be explanatory variables, the output y is considered to be\\nthe effect variable, and the weights wi quantify the causal significance\\nof the associated inputs xi on the output y. The interpretation of\\nweights as a type of causality is crucial; often, the ideal method of\\ndetermining causality through a set of rigorous randomized control\\ntrials is too expensive.\\n5.1\\nBoston Housing: Machine Learning in Economics\\nOur first case study comes from the field of economics. In 1978,\\nHarrison and Rubinfeld released a classic study on the willingness to\\npay for clean air in the Boston metropolitan area. Their methodology\\ninvolved an economic model called hedonic pricing, 1 which essentially\\n1 This definition is paraphrased from\\nthe following Wikipedia article: https:\\n//en.wikipedia.org/wiki/Hedonic_\\nregression\\nestimates the value of a good by breaking it down into “constituent\\ncharacteristics.” It turns out we can use linear regression to help\\ndetermine which of these attributes are most important. Specifically,\\nsuppose we have a dataset of house sales where y represents the\\nprice of the house and⃗x ∈R15 represents a set of house attributes. 2\\n2 x0 is a dummy variable, and the\\nremaining 14 coordinates x1, . . . , x14\\neach correspond to an attribute.\\nThen, we aim to find an optimum set of weights ⃗w for the linear\\nmodel:\\ny ≈\\n14\\n∑\\ni=0\\nwixi\\n(5.1)\\nTable 5.1 lists all 14 attributes that were used in the linear regres-\\nsion model. Before fitting the model with these attributes, it is useful\\nto intuitively reason about some of the attributes. For instance, we\\nexpect the weight w5 corresponding to RM, the number of bedrooms,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 59}, page_content='60\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nIndex\\nCode\\nDescription\\n1\\nZN\\nproportion of residential land zoned for\\nlots over 25, 000 ft2\\n2\\nINDUS\\nproportion of non-retail business acres per town\\n3\\nCHAS\\nCharles River indicator variable\\n(1 if tract bounds river; 0 otherwise)\\n4\\nNOX\\nnitric oxides concenteration (parts per 10 million)\\n5\\nRM\\naverage number of rooms per dwelling\\n6\\nAGE\\nproportion of owner-occupied units\\nbuilt prior to 1940\\n7\\nDIS\\nweighted distances to\\nfive Boston employment centres\\n8\\nRAD\\nindex of accessibility to radial highways\\n9\\nTAX\\nfull-value property-tax rate per $10, 000\\n10\\nMEDV\\nMedian value of owner-occupied homes\\n(in $1, 000s)\\n11\\nCRIM\\nper capita crime rate in town\\n12\\nPTRATIO\\npupil-teacher ratio by town\\n13\\nLSTAT\\n% lower status of the population\\n14\\nB\\n1000(Bk −0.63)2 where Bk is the proportion\\nof black population in town\\nTable 5.1: 14 attributes used in the\\nBoston housing regression model. The\\nattributes are presented in a different\\norder from the paper.\\nto be positive because larger houses typically sell for more. Con-\\nversely, we expect the weight w4 corresponding to NOX, the amount\\nof air pollution, to be negative as people would prefer not to live in a\\npolluted environment. After running the regression, it indeed turns\\nout that these intuitions are correct. 3 In general, it can be useful to\\n3 The regression weights can be found\\non page 100 of the original paper.\\nhttps://deepblue.lib.umich.edu/\\nbitstream/handle/2027.42/22636/\\n0000186.pdf?sequence=1&isAllowed=y.\\ndouble-check that the calculated weights align with intuition: if they\\ndo not, it could be a sign that a modeling assumption is incorrect.\\n5.1.1\\nThe Strange Math of Feature B\\nThe headline result of the paper is that the willingness to pay for\\ncleaner air increases both when income level is higher and when\\nthe current pollution level is higher. However, if you read the paper\\nclosely, you may notice the presence of a curious parameter B, which\\nis defined in terms of Bk, the proportion of black population in the\\nneighborhood. This parameter is meant to represent a social segrega-\\ntion effect present within the Boston housing market. The authors of\\nthe paper speculated that (1) at a lower level of Bk, the housing price\\nwill decrease as Bk increases since the white population tend to avoid\\nblack population, but (2) at a very high level of Bk, the housing price\\nwill increase as Bk increases because black population prefer predom-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 60}, page_content='exploring “data science” via linear regression\\n61\\ninantly black neighborhoods. To capture this intuition, they defined\\nthe attribute B in the parabolic expression B = 1000(Bk −0.63)2. It\\nindeed turns out that the weight w14 corresponding to B is positive as\\nshown in Figure 5.1.\\nFigure 5.1: The graph of B =\\n1000(Bk −0.63)2. This is an exam-\\nple of featurization as discussed in\\nChapter 1. It encodes prevailing dis-\\ncrimination of that period. The term\\n“black” is not favored today either.\\n5.1.2\\nEthnic Concerns Behind a Model\\nIt seems strange to have such a sensitive attribute as B have an in-\\nfluence on the model. We might wonder about the social harm that\\ncould arise if the model was used by real-life sellers or buyers (e.g.,\\nthe buyers could demand a house for a lower price based on the pro-\\nportion of black population in the neighborhood). On the other hand,\\nthe fitted model confirms that there is an underlying segregation\\neffect already present in the society. Also, we cannot guarantee that\\nthe model would be race-neutral even if we eliminated the parameter\\nB. For instance, maybe one or more of the other variables (e.g., air\\nquality variables) is highly correlated with B. 4\\n4 We will revisit such issues of bias in\\nChapter 16.\\nUltimately, the primary takeaway from this case study is that\\nimplementing machine learning models in real life is a challenge\\nitself. At a technical level, the model may make sense and make good\\npredictions of house prices. But one has to consider the social effects\\nof an ML model on the phenomenon being studied: in particular,\\nwhether it supports or extends prevailing inequities. The following\\nare some important considerations to keep in mind:\\n1. If the world has a problem, the data will reflect it and so will our\\nmodels\\n2. If a problematic model later gets used in real life, it can exacerbate\\nthe existing problem\\n3. The choices of attributes when making a model might bias the\\noutcome'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 61}, page_content='62\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n4. Carelessly using data can later lead to modeling issues\\n5.2\\nfMRI Analysis: Machine Learning in Neuroscience\\nWe next consider an application of ML in a vastly different field.\\nOne of the most important tools in contemporary neuroscience is\\nFunctional Magnetic Resonance Imaging (fMRI). fMRI has been used\\nsuccessfully to map human functionality (e.g., speech, memory) to\\nbrain regions. In a more active role, it can assist with tumor surgery\\nor “decoding” thoughts and emotions.\\nFigure 5.2: A sample image of an\\nfMRI reading. Source: https://en.\\nwikipedia.org/wiki/Functional_\\nmagnetic_resonance_imaging\\nfMRI experiments often involve presenting a set of stimuli (e.g.,\\nimages of human faces) to the subject in order to elicit a neurological\\nresponse, which is then captured through an fMRI reading. Each\\nreading reveals the concentration of oxygen in the blood stream\\nthroughout the brain, which is used as a proxy for brain activity. 5\\n5 Formally, this is referred to as the\\nblood-oxygen-level-dependent (BOLD)\\nsignal.\\nThrough the result of the reading, we are able to conclude if a par-\\nticular voxel responds to a particular stimulus. The naive way of\\nconducting these experiments is to present one stimulus at a time\\nand wait until we get a reading of the brain response before we move\\non to the next stimulus.\\nBut if you have previously taken a course in neuroscience, you\\nmay recall that fMRI is unfortunately a double-edged sword. It\\nfeatures excellent spatial resolution, with each voxel as small as 1 mm3.\\nHowever, it has poor temporal resolution: often, readings require\\nseveral seconds for blood flow to stabilize! Coupled with the fact that\\nregulations limit the amount of time human subjects can spend in\\nthe scanner, it becomes clear that methodologies based on sequential\\npresentation of stimuli are too inefficient. In this section, we explore'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 62}, page_content='exploring “data science” via linear regression\\n63\\nhow to leverage techniques from linear regression in order to solve\\nthis problem.\\n5.2.1\\nLinear Superposition\\nThe key intuition involves a concept called linear superposition: if a\\nsubject is shown multiple stimuli in quick succession, the strength\\nof the voxel’s response is the sum of the strength of its response to\\neach of the individual stimuli. 6 Instead of waiting until we have\\n6 This is exactly like the linear superpo-\\nsition of wave functions in physics.\\nthe image of one stimulus to move on, considering showing a new\\nstimulus every 1 or 2 seconds. Each fMRI reading will now capture\\nthe composite brain response to the stimuli from the past few seconds.\\nWe will use linear regression to disentangle the information, and\\nextract which voxel responded to which stimulus. 7\\n7 Note that this is a very simplified\\nversion. The actual process is much\\nmore complicated.\\nConsider the following example.\\nExample 5.2.1. See Figure 5.3. The graph on the top left represents a voxel’s\\nresponse when the subject is shown the image of a face. The graph on the\\ntop right represents the response when the subject is shown the image of a\\nflower. The bottom graph represents the response when the subject is shown\\nthe image of a flower 1 second after the image of a face. Notice that the first\\ntwo graphs have been superposed to create the third graph. In practice, we\\nare interested in the problem of extracting the individual graphs when given\\nthe superposed graph.\\nFigure 5.3: Three graphs explaining the\\neffect of linear superposition.\\n5.2.2\\nLinear Regression\\nNow let us describe how to formulate this problem in terms of linear\\nregression. First assume that the subject is shown one of k types\\nof stimuli at each time step t where t ∈{1, 2, . . . , T}. Let yt be the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 63}, page_content='64\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nresponse of a particular voxel at step t. The main assumption is that\\nyt is the linear superposition of the responses to stimuli from the\\nsteps in [t −10, t]. We also define a T × k matrix X with 0/1 entries,\\nwhere Xts = 1 if stimulus type s is shown during [t −10, t] and 0\\notherwise. Then we can set up the following linear regression model:\\nyt ≈\\nk\\n∑\\ns=1\\nwsXts\\nWhen we find the optimal values of ws via least squares, ws = 1\\nmeans that the particular voxel responds to the stimulus type s.\\n5.2.3\\nNeural Correlates of Thought\\nNow we know how to find the values of ws for a specific voxel. That\\nis, we can test if a particular voxel responds to a particular stimu-\\nlus. Combining this method with a spatial smoothing (i.e., applying\\nthe principle that nearby voxels behave similarly), 8 we are able to\\n8 The simplest smoothing method is to\\ntake the ws values for one voxel and\\nreplace them with the average of the\\nneighboring voxels.\\nidentify which region of a brain is associated to which stimulus. So\\nfar, more than 1, 000 regions of the brain have been identified and\\nmapped.\\nFigure 5.4: A detailed map labeling\\nareas of the brain with corresponding\\nstimuli. https://www.nature.com/\\narticles/nature17637\\n5.2.4\\nBrain-Computer Interface (BCI)\\nWe finish off with a tangible example of how our studies can help\\npeople. Patients who are suffering from Locked-in Syndrome (LIS)\\nare aware of their surroundings and have normal reasoning capacities'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 64}, page_content='exploring “data science” via linear regression\\n65\\nbut have no way of communicating with others through speech or\\nfacial movements. Using a combination of a technology called Brain-\\nComputer Interface and a linear regression model, we are able to\\ncommunicate with these patients.\\nBrain-Computer Interface is an electode sensor implanted near the\\nmotor cortex that can detect the electric signal that LIS patients are\\ntrying to send to the motor cortex. We can teach the patients to visu-\\nalize writing with their dominant hand if they want to answer “no”\\nand visualize writing with their non-dominant hand if they want to\\nanswer “yes.” Since the neural correlates of the two movements are\\nvery different, BCI will pick up essentially disjoint signals, and we\\ncan use linear regression model to distinguish between them. 9\\n9 Note: training also requires labeled\\ndata, which can be produced by asking\\nthe patient questions about known\\nfacts (e.g., birth date, marital status,\\netc.). This technique has been used\\nto communicate with patients in\\ndeep coma and presumed to be in a\\nvegetative state. See Science of Mind\\nReading, New Yorker, December 6 2021,\\nwhich also profiles several Princeton\\nresearchers.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 65}, page_content=''),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 66}, page_content='Part II\\nUnsupervised Learning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 67}, page_content=''),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 68}, page_content='6\\nClustering\\nSo far, we have considered ML models which require labeled data\\nin order to learn. However, there is a large class of models which\\ncan learn from unlabeled data. From this chapter, we will begin to\\nintroduce models from this modeling paradigm, called unsupervised\\nlearning. In this chapter, we focus on one application of unsupervised\\nlearning, called clustering algorithm.\\n6.1\\nUnsupervised Learning\\nUnsupervised learning is a branch of machine learning which only uses\\nunlabeled data. Examples of unlabeled data include a text corpus\\ncontaining the works of William Shakespeare (Chapter 8) or a set of\\nunlabeled images (Chapter 7). Some key goals in this setting include:\\n• Learn the structure of data: It is possible to learn if the data consists\\nof clusters, or if it can be represented in a lower dimension.\\n• Learn the probability distribution of data: By learning the probability\\ndistribution where the training data came from, it is possible to\\ngenerate synthetic data which is “similar” to real data.\\n• Learn a representation for data: We can learn a representation that is\\nuseful in solving other tasks later. With this new representation,\\nfor example, we can reduce the need for labeled examples for\\nclassification.\\n6.2\\nClustering\\nClustering is one of the main tasks in unsupervised learning. It is the\\nprocess of detecting clusters in the dataset. Often the membership\\nof a cluster can replace the role of a label in the training dataset. In\\ngeneral, clusters reveal a lot of information about the underlying\\nstructure of the data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 69}, page_content='70\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nFigure 6.1: Height vs weight scatter\\nplot of basketball players. In the plot on\\nthe right, the points in green and blue\\nrespectively correspond to female and\\nmale players.\\nIn Figure 6.1, we see a scatter plot of measurements of height and\\nweight of basketball players. If you look at the plot on the left, it is\\neasy to conclude that there is a usual linear relationship between the\\nheight and the weight of the athletes. However, upon further inspec-\\ntion, it seems like there are two clusters of the data points, separated\\naround the middle of the plot. In fact, this is indeed the case! If we\\nlabel the dataset with the additional information of whether the data\\npoint is from a male or female athlete, the plot on the right shows\\nsomething more than just the linear relationship. In practice, however,\\nwe do not always have access to this additional label. Instead, one\\nuses clustering algorithms to find natural clusterings of the data. This\\nraises the question of what a “clustering” is, in the first place.\\nTechnically, any partition of the dataset D into k subsets C1, C2, . . . , Ck\\ncan be called a clustering. 1 That is,\\n1 Here k, the number of clusters may be\\ngiven as part of the problem, or k may\\nhave to be decided upon after looking\\nat the dataset. We’ll revisit this soon.\\nk[\\ni=1\\nCi = D\\nand\\nk\\\\\\ni=1\\nCi = ∅\\nBut we intuitively understand that not all partitions are a natural\\nclustering of the dataset; our goal therefore will be to define what a\\n“good” clustering is.\\n6.2.1\\nSome Attempts to Define a “Good” Cluster\\nThe sample data in Figure 6.1 suggests that our vision system has\\nevolved to spot natural clusterings in two or three dimensional data.\\nTo do machine learning, however, we need a more precise definition\\nin Rd: specifically, for any partition of the dataset into clusters, we try\\nto quantify the “goodness” of the clusters.\\nDefinition 6.2.1 (Cluster: Attempt 1). A “good” cluster is a subset of\\npoints which are closer to each other than to all other points in the dataset.\\nBut this definition does not apply to the clusters in Figure 6.1. The\\npoints in the middle of the plot are far away from the points on the\\ntop right corner or the bottom left corner. So whichever cluster we\\nassign the middle points to, they will be farther away from some'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 70}, page_content='clustering\\n71\\npoints in their assigned cluster than to some of the points on the\\nother cluster. Ok, so that did not work. Consider the following\\ndefinition.\\nDefinition 6.2.2 (Cluster: Attempt 2). A “good” cluster is a subset of\\npoints which are closer to the mean of their own cluster than to the mean of\\nother clusters.\\nHere Mean and Variance are defined as follows:\\nDefinition 6.2.3 (Mean and Variance of Clusters). Let Ci be one of the\\nclusters for a dataset D. Let mi = |Ci| denote the cluster size. The mean of\\nthe cluster Ci is\\n⃗yi = 1\\nmi ∑\\n⃗x∈Ci\\n⃗x\\nand the variance within the cluster Ci is\\nσ2\\ni = 1\\nmi ∑\\n⃗x∈Ci\\n∥⃗x −⃗yi∥2\\n2\\nYou may notice that Definition 6.2.2 appears to be using circular\\nreasoning: it defines clusters using the mean of the clusters, but the\\nmean can only be calculated once the clusters have been defined. 2\\n2 Such circular reasoning occurs in most\\nnatural formulations of clustering. Look\\nat the Wikipedia page on clustering for\\nsome other formulations.\\n6.3\\nk-Means Clustering\\nIn this section, we present a particular partition of the dataset called\\nthe k-means clustering. Given k, the desired number of clusters, the\\nk-means clustering partitions D into k clusters C1, C2, . . . , Ck so as to\\nminimize the cost function:\\nk\\n∑\\ni=1 ∑\\n⃗x∈Ci\\n∥⃗x −⃗yi∥2\\n2\\n(6.1)\\nThis can be seen as minimizing the average of the individual cost of\\nthe k clusters, where cost of Ci is ∑\\n⃗x∈Ci\\n∥⃗x −⃗yi∥2\\n2. 3 This idea is similar\\n3 Notice that each cluster cost is the\\ncluster size times the variance.\\nin spirit to our earlier attempt in Definition 6.2.2 — we want the\\ndistance of each data point to the mean of the cluster to be small. But\\nthis method is able to circumvent the problem of circular reasoning.\\nThe process of finding the optimal solution for (6.1) is called the\\nk-means clustering problem.\\n6.3.1\\nk-Means Algorithm\\nSomewhat confusingly, the most famous algorithm that is used\\nto solve the k-means clustering problem is also called k-means. It is\\ntechnically a heuristic, meaning it makes intuitive sense but it is not'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 71}, page_content='72\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nguaranteed to find the optimum solution.4 The following is the k-\\n4 There is extensive research on finding\\nnear-optimal solutions to k-means. The\\nproblem is known to be NP-complete,\\nso we believe that an algorithm that is\\nguaranteed to produce the optimum\\nsolution on all instances must require\\nexponential time.\\nmeans algorithm. It is given some initial clustering (we discuss some\\nchoices for initialization below) and we repeat the following iteration\\nuntil we can no longer improve the cost function:\\nMaintain clusters C1, C2, . . . , Ck\\nFor each cluster Ci, find the mean ⃗yi\\nInitialize new clusters C′\\ni ←∅\\nfor⃗x ∈D do\\nix = arg mini ∥⃗x −⃗yi∥2\\nC′\\nix ←C′\\nix ∪{⃗x}\\nend for\\nUpdate clusters Ci ←C′\\ni\\nAt each iteration, we find the mean of each current cluster. Then\\nfor each data point, we assign it to the cluster whose mean is the\\nclosest to the point, without updating the mean of the clusters. In\\ncase there are multiple cluster means that the point is closest to, we\\napply the tie-breaker rule that the point gets assigned to the current\\ncluster if it is among the closest ones; otherwise, it will be randomly\\nassigned to one of them. Once we have assigned all points to the new\\nclusters, we update the current set of clusters, thereby updating the\\nmean of the clusters as well. We repeat this process until there is no\\npoint that is mis-assigned.\\n6.3.2\\nWhy Does k-Means Algorithm Terminate in Finite time?\\nThe k-means algorithm is actually quite akin to Gradient Descent, in\\nthe sense that the iterations are trying to improve the cost.\\nLemma 6.3.1. Given a set of points⃗x1,⃗x2, . . . ,⃗xm, their mean⃗y = 1\\nm\\nm\\n∑\\ni=1\\n⃗xi\\nis the point that minimizes the average squared distance to the points.\\nProof. For any vector⃗z, let C(⃗z) denote the sum of squared distance\\nto the set of points. That is,\\nC(⃗z) =\\nm\\n∑\\ni=1\\n∥⃗z −⃗xi∥2\\n2 =\\nm\\n∑\\ni=1\\n((⃗z −⃗xi) · (⃗z −⃗xi))\\n=\\nm\\n∑\\ni=1\\n(⃗z ·⃗z −2⃗z ·⃗xi +⃗xi ·⃗xi)\\n=\\nm\\n∑\\ni=1\\n(∥⃗z∥2\\n2 −2⃗z ·⃗xi + ∥⃗xi∥2\\n2)\\nTo find the optimal⃗z, we set the gradient ∇C to 0\\n∇C(⃗z) =\\nm\\n∑\\ni=1\\n(2⃗z −2⃗xi) = 0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 72}, page_content='clustering\\n73\\nwhich yields the solution\\n⃗z = 1\\nm\\nm\\n∑\\ni=1\\n⃗xi\\nWe are ready to prove the main result.\\nTheorem 6.3.2. Each iteration of the k-means Algorithm 6.3.1, possibly\\nexcept for the last iteration before termination, strictly decreases the total\\ncluster cost (6.1).\\nProof. We follow the same notation as in Algorithm 6.3.1. The total\\ncost at the end of one iteration is:\\nk\\n∑\\ni=1 ∑\\n⃗x∈C′\\ni\\n\\r\\r⃗x −⃗y′\\ni\\n\\r\\r2\\n2\\nwhere ⃗y′\\ni is the mean of the newly defined cluster C′\\ni. Notice that\\neach of the cluster cost ∑\\n⃗x∈C′\\ni\\n\\r\\r⃗x −⃗y′\\ni\\n\\r\\r2\\n2 is the sum of the squared dis-\\ntance between a set of points⃗x ∈C′\\ni and their mean. By Lemma 6.3.1,\\nthis sum is smaller than the sum of squared distance between the\\nsame set of points to any other point. In particular, we can compare\\nwith ⃗yi, the mean of Ci before the update. That is,\\n∑\\n⃗x∈C′\\ni\\n\\r\\r⃗x −⃗y′\\ni\\n\\r\\r2\\n2 ≤∑\\n⃗x∈C′\\ni\\n∥⃗x −⃗yi∥2\\n2\\nfor any 1 ≤i ≤k. If we sum over all clusters, we see that\\nk\\n∑\\ni=1 ∑\\n⃗x∈C′\\ni\\n\\r\\r⃗x −⃗y′\\ni\\n\\r\\r2\\n2 ≤\\nk\\n∑\\ni=1 ∑\\n⃗x∈C′\\ni\\n∥⃗x −⃗yi∥2\\n2\\nNow notice that the summand ∥⃗x −⃗yi∥2\\n2 in the right hand side of the\\ninequality is the squared distance between the point⃗x and the mean\\n⃗yi (before update) of the cluster C′\\ni that⃗x is newly assigned to. In\\nother words, we can rewrite this term as ∥⃗x −⃗yix∥2\\n2 and instead sum\\nover all points⃗x in the dataset. That is,\\nk\\n∑\\ni=1 ∑\\n⃗x∈C′\\ni\\n∥⃗x −⃗yi∥2\\n2 = ∑\\n⃗x∈D\\n∥⃗x −⃗yix∥2\\n2\\nFinally, recall that the index ix was defined as ix = arg mini ∥⃗x −⃗yi∥2.\\nIn particular, if j was the index of the cluster that a data point⃗x\\noriginally belonged to, then ∥⃗x −⃗yix∥2\\n2 ≤\\n\\r\\r⃗x −⃗yj\\n\\r\\r2\\n2. Therefore, we\\nhave the following inequality,\\n∑\\n⃗x∈D\\n∥⃗x −⃗yix∥2\\n2 ≤\\nk\\n∑\\nj=1 ∑\\n⃗x∈Cj\\n\\r\\r⃗x −⃗yj\\n\\r\\r2\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 73}, page_content='74\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nThe equality holds in the inequality above if and only if when\\n∥⃗x −⃗yix∥2\\n2 =\\n\\r\\r⃗x −⃗yj\\n\\r\\r2\\n2 for each point⃗x, which means that the origi-\\nnal cluster Cj was one of the closest clusters to⃗x. By the tie-breaker\\nrule, ix would have been set to j. This is exactly the case when the\\nalgorithm terminates immediately after this iteration since no point\\nis reassigned to a different cluster. In all other cases, we have a strict\\ninequality:\\n∑\\n⃗x∈D\\n∥⃗x −⃗yix∥2\\n2 <\\nk\\n∑\\nj=1 ∑\\n⃗x∈Cj\\n\\r\\r⃗x −⃗yj\\n\\r\\r2\\n2\\nNotice that the right hand side of the inequality is the total cost at the\\nbeginning of the iteration.\\nNow we are ready to prove that the k-means algorithm is guaran-\\nteed to terminate in finite time. Since each iteration strictly reduces\\nthe cost, we conclude that the current clustering (i.e., partition) will\\nnever be considered again, except at the last iteration when the al-\\ngorithm terminates. Since there is only a finite number of possible\\npartitions of the dataset D, the algorithm must terminate in finite\\ntime.\\n6.3.3\\nk-Means Algorithm and Digit Classification\\nYou might be familiar with the MNIST hand-written digits dataset.\\nHere, each image, which depicts some digit between 0 and 9, is\\nrepresented as a an 8 × 8 matrix of pixels and each pixel can take on a\\ndifferent luminosity value from 0 to 15.\\nWe can apply k-means clustering to differentiate between images\\ndepicting the digit “1” and the digit “0.” After running the model\\nwith k = 2 on 360 images of the two digits, we achieve the clusters in\\nFigure 6.2. 5 Note the presence of two colored regions: a point is col-\\n5 This 2D visualization of the clusters\\nis achieved through a technique called\\nlow dimensional representation, which\\nis covered in Chapter 7.\\nored red if a hypothetical held-out data point at that location would\\nget assigned a “0;” otherwise it is colored blue. This assignment is\\nbased on which cluster center is closer.\\nFigure 6.2: Sample images from the\\nMNIST dataset (left) and 2D visu-\\nalization of the k-means clusters\\ndifferentiating between the digits “1”\\nand “0” (right). Only two images were\\nmisclassified!'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 74}, page_content='clustering\\n75\\nThis example also shows that clustering into two clusters can be\\nturned into a technique for binary classification — use training data\\nto come up with two clusters; at test time, compute a ±1 label for\\neach data point according to which of the two cluster centers it is\\ncloser to.\\n6.3.4\\nImplementation Detail: How to Pick the Initial Clustering\\nThe choice of initial clusters greatly influences the quality of the\\nsolution found by the k-means algorithm. The most naive method is\\nto pick k data points randomly to serve as the initial cluster centers\\nand create k clusters by assigning each data point to the closest\\ncluster center. However, this approach can be problematic. Suppose\\nthere exists some “ground truth” clustering of the dataset. By picking\\nthe initial clusters randomly, we may end up splitting one of these\\nground truth clusters (e.g., two initial centers are drawn from within\\nthe same ground truth cluster), and the final clustering ends up\\nbeing very sub-optimal. Thus one tries to select the initial clustering\\nmore intelligently. For instance the popular k-means++ initialization\\nprocedure 6 is the following:\\n6 It was invented by Arthur and Vassil-\\nvitskii in 2007.\\n1. Choose one center uniformly at random among all data points.\\n2. For each data point⃗x compute D(⃗x), the distance between⃗x and\\nthe nearest center which has already been chosen.\\n3. Choose a new data point at random as a new center, where a\\npoint⃗x is chosen with probability proportional to D(⃗x)2.\\n4. Repeat steps 2 and 3 until k centers have been chosen.\\nIn COS 324, we will not expect you to understand why this is a\\ngood initialization procedure, but you may be expected to be able to\\nimplement this or similar procedures in code.\\n6.3.5\\nImplementation Detail: Choice of k\\nAbove we assumed that the number of clusters k is given, but in\\npractice you have to choose the appropriate number of clusters k.\\nExample 6.3.3. Is there a value of k that guarantees an optimum cost of\\n0? Yes! Just set k = n (i.e., each point is its own cluster). Of course, this is\\nuseless from a modeling standpoint!\\nProblem 6.3.4. Argue that the optimum cost for k + 1 clusters is no more\\nthan the optimum cost for k clusters.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 75}, page_content='76\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nNote that Problem 6.3.4 only concerns the optimum cost, which\\nas we discussed may not be attained by the k-means algorithm.\\nNevertheless, it does suggest that we can try various values of k and\\nsee when the cost is low enough to be acceptable.\\nA frequent heuristic is the elbow method: create a plot of the num-\\nber of clusters vs. the final value of the cost as in Figure 6.3 and look\\nfor an “elbow” where the objective tapers off. Note that if the dataset\\nis too complicated for a simple Euclidean distance cost, the data\\nmight not be easy to cluster “nicely” meaning there is no “elbow”\\nshown on the plot.\\nFigure 6.3: Two graphs of number of\\nclusters vs. final value of cost. There is\\na distinct elbow on the left, but not on\\nthe right.\\n6.4\\nClustering in Programming\\nIn this section, we briefly discuss how to implement k-means algo-\\nrithm for digit classification in Python. As usual, we use the numpy\\npackage to speed up computation and the matplotlib package for vi-\\nsualization. Additionally, we use the sklearn package to help perform\\nthe clustering.\\n# import necessary packages\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.preprocessing import scale\\nfrom sklearn.decomposition import PCA\\n# prepare dataset\\nX, y = load_digits(n_class=2, return_X_y=True)\\nX = scale(X)\\nX_train, X_test = ...\\n# define functions\\ndef initialize_cluster_mean(X, k):\\n# X: array of shape (n, d), each row is a d-dimensional data point\\n# k: number of clusters\\n# returns Y: array of shape (k, d), each row is the center of a cluster\\ndef assign_cluster(X, Y)\\n# X: array of shape (n, d), each row is a d-dimensional data point\\n# Y: array of shape (k, d), each row is the center of a cluster\\n# returns loss, the sum of squared distance from each point to its\\nassigned cluster'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 76}, page_content='clustering\\n77\\n# returns C: array of shape (n), each value is the index of the closest\\ncluster\\ndef update_cluster_mean(X, k, C):\\n# X: array of shape (n, d), each row is a d-dimensional data point\\n# k: number of clusters\\n# C: array of shape (n), each value is the index of the closest cluster\\n# returns Y: array of shape (k, d), each row is the center of a cluster\\ndef k_means(X, k, max_iters=50, eps=1e-5):\\nY = initialize_cluster_mean(X, k)\\nfor i in range(max_iters):\\nloss, C = assign_cluster(X, Y)\\nY = update_cluster_mean(X, k, Y)\\nif loss_change < eps:\\nbreak\\nreturn loss, C, Y\\ndef scatter_plot(X, C):\\nplt.figure(figsize=(12, 10))\\nk = int(C.max()) + 1\\nfrom itertools import cycle\\ncolors = cycle(’bgrcmk’)\\nfor i in range(k):\\nidx = (C == i)\\nplt.scatter(X[idx, 0], X[idx, 1], c=next(colors))\\nplt.show()\\n# run k-means algorithm and plot the result\\nloss, C, Y = k_means(X_train, 2)\\nlow_dim = PCA(n_components=2).fit_transform(X_train)\\nscatter_plot(low_dim, C)\\nWe start by importing outside packages.\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.preprocessing import scale\\nfrom sklearn.decomposition import PCA\\nThe load_digits() method loads the MNIST digits dataset, with around\\n180 data points per digit. The scale() method linearly scales each of\\nthe data points such that the mean is 0 and variance is 1. The PCA()\\nmethod helps visualize the MNIST digits data points, which are 64-\\ndimensional, in the Cartesian plane (i.e., R2). See the next Chapter 7\\nfor details on this process.\\nNext we prepare the dataset by calling the load_digits() method.\\nX, y = load_digits(n_class=2, return_X_y=True)\\nX = scale(X)\\nX_train, X_test = ...\\nNotice that we discard the target array y because we are performing\\nclustering, a type of unsupervised learning. If we were to perform\\nsupervised learning instead, we would need to make use of y.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 77}, page_content='78\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nThen we define the functions necessary for the k-means algorithm.\\ndef initialize_cluster_mean(X, k):\\nreturn Y\\ndef assign_cluster(X, Y)\\nreturn loss, C\\ndef update_cluster_mean(X, k, C):\\nreturn Y\\ndef k_means(X, k, max_iters=50, eps=1e-5):\\nY = initialize_cluster_mean(X, k)\\nfor i in range(max_iters):\\nloss, C = assign_cluster(X, Y)\\nY = update_cluster_mean(X, k, Y)\\nif loss_change < eps:\\nbreak\\nreturn loss, C, Y\\nIn practice, it is common to limit the number of cluster update itera-\\ntions (i.e., the parameter max_iters) and specify the smallest amount\\nof loss change allowed for one iteration (i.e., the constant ϵ). By termi-\\nnating the algorithm once either one of the conditions is reached, we\\ncan get an approximate solution within a reasonable amount of time.\\nNext, take a look at the helper function used to plot the result of\\nthe k-means algorithm.\\ndef scatter_plot(X, C):\\nplt.figure(figsize=(12, 10))\\nk = int(C.max()) + 1\\nfrom itertools import cycle\\ncolors = cycle(’bgrcmk’)\\nfor i in range(k):\\nidx = (C == i)\\nplt.scatter(X[idx, 0], X[idx, 1], c=next(colors))\\nplt.show()\\nThe cycle() method from the itertools package lets you iterate through\\nan array indefinitely, with the index wrapping around back to the\\nstart, once it reaches the end of the array.\\nNow, consider the for loop section in the helper function above. We\\nfirst use Boolean conditions to concisely generate a new array.\\nidx = (C == i)\\nThis generates a Boolean array with the same length as C, where each\\nentry is either True/False based on whether the corresponding entry in\\nC is equal to i. The following code is equivalent.\\nidx = np.zeros(C.size)\\nfor j in range(C.size):'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 78}, page_content='clustering\\n79\\nidx[j] = (C[j] == i)\\nWe then use a technique called Boolean masking to extract a particular\\nsubset of rows of X.\\nX[idx, 0]\\nNotice that in place of a list of indices of rows to extract, we are\\nindexing with the Boolean array we just defined. The code will extract\\nonly the rows where the Boolean value is True. For example, if the\\nvalue of idx is [True, False, True], then the code above is equivalent to\\nX[[0, 2], 0]\\nFinally, we make use of the helper functions we defined earlier to\\nrun the k-means algorithm and plot results.\\n_, C, _ = k_means(X_train, 2)\\nlow_dim = PCA(n_components=2).fit_transform(X_train)\\nscatter_plot(low_dim, C)\\nThe first line of this code snippet shows how we can use the _ symbol\\nto selectively disregard individual return values of a function call.\\nThe second line of code uses the PCA() method to transform the\\n64-dimensional data X_train into 2-dimensional data so that we can\\nvisualize it with the scatter_plot() method. We will learn the details of\\nthis process in the next Chapter 7.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 79}, page_content=''),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 80}, page_content='7\\nLow-Dimensional Representation\\nHigh-dimensional datasets arise in quite a few settings. This chapter\\nconcerns a phenomenon that arises frequently: the data points (i.\\ne., vectors) collectively turn out to be “approximately low rank.” A\\nrunning theme in this chapter is that arrays and matrices, which in\\nintroductory courses like COS 126 and COS 226 were thought of as\\ndata structures (i.e., an abstraction from programming languages),\\nare treated now as objects that we can pass through some remarkable\\n(but simple) mathematical procedures.\\nIf a large dataset of N vectors in Rd has rank k, then we can think\\nof a natural compression method. Let U be the k-dimensional sub-\\nspace spanned by the vectors, and identify k basis vectors for U. For\\neach of the N vectors, find the k coefficients of their representation\\nin terms of the basis vectors. Following this method, instead of spec-\\nifying the N vectors using Nd real numbers, we can represent them\\nusing k(N + d) real numbers, which is a big win if d is much larger\\nthan k.\\nFigure 7.1: ⃗v1,⃗v2,⃗v3 ∈R3 (left) and\\ntheir 2-dimensional representations\\nb⃗v1, b⃗v2, b⃗v3 ∈R2.\\nExample 7.0.1. Figure 7.1 shows three vectors⃗v1 = (3.42, −1.33, 6.94),⃗v2 =\\n(7.30, 8.84, 1.95),⃗v3 = (−7.92, −6.37, −5.66) in R3. The three vectors\\nhave rank 2 — they are all in the 2-dimensional linear subspace generated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 81}, page_content='82\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nby ⃗u1 = (8, 8, 4) and ⃗u2 = (1, −4, 6). Specifically,\\n⃗v1 = 0.31⃗u1 + 0.95⃗u2\\n⃗v2 = 0.95⃗u1 −0.31⃗u2\\n⃗v3 = −0.95⃗u1 −0.31⃗u2\\nTherefore, we can represent these vectors in a 2-dimensional plane, as\\nb⃗v1 = (0.31, 0.95), b⃗v2 = (0.95, −0.31), b⃗v3 = (−0.95, −0.31)\\n7.1\\nLow-Dimensional Representation with Error\\nOf course, in general, high dimensional datasets are not exactly\\nlow rank. We’re interested in datasets which have low-dimension\\nrepresentations once we allow some error.\\nDefinition 7.1.1 (Low-dimensional Representation with Error). We\\nsay a set of vectors⃗v1,⃗v2, . . . ,⃗vN ∈Rd has rank k with mean-squared\\nerror ϵ if there exist some basis vectors ⃗u1,⃗u2, . . . ,⃗uk ∈Rd and N vectors\\nb⃗v1, b⃗v2, . . . , b⃗vN ∈span(⃗u1,⃗u2, . . . ,⃗uk) such that\\n1\\nN ∑\\ni\\n\\r\\r\\r⃗vi −b⃗vi\\n\\r\\r\\r\\n2\\n2 ≤ϵ\\n(7.1)\\nWe say that b⃗v1, . . . , b⃗vN are the low-rank or low-dimensional approxi-\\nmation of⃗v1, . . . ,⃗vN. Typically we will assume without loss of generality\\nthat the basis vectors are orthonormal (i.e., have ℓ2 norm equal to 1 and are\\npairwise orthogonal).\\nDefinition 7.1.1 can be thought of as a lossy compression of the\\ndataset of vectors since the low-dimensional representation of vectors\\nis roughly correct, but with a bound of ϵ on the MSE. This compres-\\nsion view will be used in Section 7.3.\\nFigure 7.2: ⃗v1,⃗v2,⃗v3 ∈R3 (left) and\\ntheir 2-dimensional approxima-\\ntions b⃗v1, b⃗v2, b⃗v3 represented in the\\n2-dimensional subspace spanned by\\n⃗u1,⃗u2.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 82}, page_content='low-dimensional representation\\n83\\nExample 7.1.2. Figure 7.2 shows three vectors⃗v1 = (3.42, −1.33, 6.94),⃗v2 =\\n(7.30, 8.84, 1.95),⃗v3 = (−6.00, −7.69, −6.86) in R3. The three vectors\\nhave rank 2 with mean-squared error 2.5. If you choose the basis vec-\\ntors ⃗u1 = (8, 8, 4),⃗u2 = (1, −4, 6) and the low-rank approximations\\nb⃗v1 = ⃗v1, b⃗v2 = ⃗v2, b⃗v3 = (−7.92, −6.37, −5.66) ∈span(⃗u1,⃗u2) then,\\n1\\n3 ∑\\ni\\n\\r\\r\\r⃗vi −b⃗vi\\n\\r\\r\\r\\n2\\n2 ≃2.28 ≤2.5\\nNote that the basis vectors in this example are only orthogonal and not\\northonormal, but it is easy to set them as orthonormal by normalizing them.\\nProblem 7.1.3. Show that if ⃗u1,⃗u2, . . . ,⃗uk ∈Rd is any set of orthonor-\\nmal vectors and⃗v ∈Rd then the vector b⃗v in span(⃗u1,⃗u2, . . . ,⃗uk) that\\nminimizes\\n\\r\\r\\r⃗v −b⃗v\\n\\r\\r\\r\\n2\\n2 is\\nk\\n∑\\nj=1\\n(⃗v ·⃗uj)⃗uj\\n(7.2)\\n(Hint: If α1, α2, . . . , αk minimize\\n\\r\\r\\r⃗v −∑j αj⃗uj\\n\\r\\r\\r\\n2\\n2 then the gradient of this\\nexpression with respect to α1, α2, . . . , αk must be zero.)\\nProblem 7.1.3 illustrates how to find the low-dimensional represen-\\ntation of the vectors, once we specify the k basis vectors. Notice that\\n(7.2) is the vector projection of ⃗v onto the subspace U spanned by the\\nvectors ⃗u1,⃗u2, . . . ,⃗uk. Therefore, the approximation error\\n\\r\\r\\r⃗v −b⃗v\\n\\r\\r\\r\\n2\\n2 is\\nthe squared norm of the component of ⃗v that is orthogonal to U. 1\\n1 Also known as the vector rejection of ⃗v\\nfrom U.\\nProblem 7.1.4 is only for more advanced students but all students\\nshould read its statement to understand the main point. It highlights\\nhow miraculous it is that real-life datasets have low-rank represen-\\ntations. It shows that generically one would expect ϵ in (7.1) to be\\n1 −k/n, which is almost 1 when k ≪n. And yet in real life ϵ is small\\nfor fairly tiny k.\\nProblem 7.1.4. Suppose the⃗vi’s are unit vectors 2 and the vectors\\n2 Note: the maximum possible value\\nof ϵ when ⃗vi’s are unit vectors is 1.\\nConvince yourself!\\n⃗u1,⃗u2, . . . ,⃗uk were the basis vectors of a random k-dimensional subspace in\\nRd. (That is, chosen without regard to the⃗vi’s.) Heuristically argue that the\\nϵ one would need in (7.1) would be 1 −k/n.\\n7.1.1\\nComputing the Low-Dimensional Representation with Error\\nIn Problem 7.1.3, we have already seen how to find the low-dimension\\nrepresentation with error, once we are given the basis vectors. All\\nthere remains is to identify a suitable value of k and find the corre-\\nsponding basis vectors that will minimize the error.\\nThere is a simple linear algebraic procedure, the Singular Value\\nDecomposition (SVD). Given a set of vectors ⃗vi and a positive integer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 83}, page_content='84\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nk, SVD can output the best orthonormal basis in sense of Defini-\\ntion 7.1.1 that has the lowest possible value of ϵ. In practice, we treat\\nk as a hyperparameter and use trial and error to find the most suit-\\nable k. Problem 7.1.5 shows that the accuracy of the low-dimensional\\nrepresentation will decrease when we choose a smaller number of\\ndimensions. So we are making a choice between the accuracy of the\\nrepresentations against how condensed our compression is.\\nProblem 7.1.5. Show that as we decrease k in Definition 7.1.1, the corre-\\nsponding ϵ can only increase (i.e., cannot decrease).\\nFormally, SVD takes a matrix as its input; the rows of this matrix\\nare the vector ⃗vi’s. The procedure operates on this matrix to output\\na low-rank approximation. We discuss details in Section 20.3. To\\nfollow the rest of this chapter, you do not need to understand details\\nof the procedure. You just need to remember the fact that the best\\nk-dimensional representation is computable for each k. In practice,\\nprogramming languages have packages that will do the calculations\\nfor you. Below is a Python code snippet that will calcuate the SVD.\\nimport sklearn.decomposition.TruncatedSVD\\n# n * n matrix\\ndata = ...\\n# prepare transform on dataset matrix \"data\"\\nsvd = TruncatedSVD(n_components=k)\\nsvd.fit(data)\\n# apply transform to dataset and output an n * k matrix\\ntransformed = svd.transform(data)\\nNow we see some fun applications.\\n7.2\\nApplication 1: Stylometry\\nIn many cases in old literature, the identity of the author is disputed.\\nFor instance, the King James Bible (i.e., the canonical English bible\\nfrom the 17th century) was written by a team whose identities and\\nwork divisions are not completely known. Similarly the Federalist\\nPapers, an important series of papers explicating finer points of the\\nUS government and constitution, were published in the early days of\\nthe republic with the team of authors listed as Alexander Hamilton,\\nJames Madison, and John Jay. But it was not revealed which paper\\nwas written by whom. In such cases, can machine learning help\\nidentify who wrote what?\\nHere we present a fun example about the books in the Wizard of\\nOz series. 3 L. Frank Baum was the author of the original Wonderful\\n3 Original paper at http://dh.\\nobdurodon.org/Binongo-Chance.pdf.\\nA survey paper by Erica Klarreich in\\nScience News Dec 2003: Statistical tests\\nare unraveling knotty literary mysteries\\nat http://web.mit.edu/allanmc/www/\\nstylometrics.pdf\\nWizard of Oz, which was a best-seller in its day and remains highly\\npopular to this day. The publisher saw a money-making opportunity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 84}, page_content='low-dimensional representation\\n85\\nand convinced Baum to also write 15 follow-up books. After Baum’s\\ndeath the publisher managed to pass on the franchise to Ruth Plumly\\nThompson, who wrote many other books.\\nFigure 7.3: Royal Book of Oz\\n(1921). Cover image from https:\\n//en.wikipedia.org/wiki/The_Royal_\\nBook_of_Oz\\nHowever, the last of the Baum books, Royal Book of Oz (RBOO), al-\\nways seemed to Oz readers closer in style to Thompson’s books than\\nto Baum’s. But with all the principals in the story now dead, there\\nseemed to be no way to confirm the suspicion. Now we describe how\\nsimple machine learning showed pretty definitively that this book\\nwas indeed written by Ruth Plumly Thompson. The main idea is to\\nrepresent the books vectors in some way and then find their low-rank\\nrepresentations.\\nThe key idea is that different authors use English words at differ-\\nent frequencies. Surprisingly, the greatest difference lies in frequen-\\ncies of function words such as with, however, upon, rather than\\nfancy vocabulary words (the ones found on your SAT exam).\\nExample 7.2.1. Turns out Alexander Hamilton used upon about 10\\ntimes more frequently than James Madison. We know this from analyzing\\ntheir individual writing outside their collaboration on the Federalist Papers.\\nUsing these kinds of statistics, it has been determined that Hamilton was\\nthe principal author or even the sole author of almost all of the Federalist\\nPapers.\\nThe statistical analysis of the Oz books consisted of looking at the\\nfrequencies of 50 function words. All Oz books except RBOO were\\ndivided into text blocks of 5000 words each. For each text block, the\\nfrequency (i.e., number of occurrences) of each function word was\\ncomputed, which allows us to represent the block as a vector in R50.\\nThere were 223 text blocks total, so we obtain 223 vectors in R50.\\nFigure 7.4: The top 50 most frequently\\nused function words in the Wizard\\nof Oz series. Their occurrences were\\ncounted in 223 text blocks. Figure from\\nBinongo’s paper.\\nThen we compute a rank 2 approximation of these 223 vectors.\\nFigure 7.5 shows the scatter plot in the 2-dimensional visualization.\\nThe two axes correspond to the two basis vectors we found for the\\nrank 2 approximation. It becomes quickly clear that the vectors from\\nthe Baum books are in a different part of the space than those from\\nthe Thompson books. It is also clear that RBOO vectors fall in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 85}, page_content='86\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nFigure 7.5: Rank-2 visualization of the\\n223 text block vectors from books of\\nOz. The dots on the left correspond\\nto vectors from Oz books known to be\\nwritten by Ruth Plumly Thompson. The\\nhearts on the left correspond to vectors\\nfrom RBOO. The ones on the right\\ncorrespond to ones written by L. Frank\\nBaum. Figure from Binongo’s paper.\\nsame place as those from other Thompson books. Conclusion: Ruth\\nPlumly Thompson was the true author of Royal Book of Oz!\\nBy the way, if one takes the non-Oz writings of Baum and Thomp-\\nson and plot their vectors in the 2D-visualization in Figure 7.6, they\\nalso fall on the appropriate side. So the difference in writing style\\ncame across clearly even in non-Oz books!\\nFigure 7.6: Rank-2 visualization of text\\nblock vectors from books written by\\nBaum and Thompson outside of the Oz\\nseries. Figure from Binongo’s paper.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 86}, page_content='low-dimensional representation\\n87\\n7.3\\nApplication 2: Eigenfaces\\nThis section uses the lossy compression viewpoint of low-rank rep-\\nresentations. As you may remember from earlier computer science\\ncourses (e.g., Seam Carver from COS 226), images are vectors of\\npixel values. In this section, let us only consider grayscale (i.e., B&W)\\nimages where each pixel has an integer value in [−127, 127]. −127\\ncorresponds to the pixel being pitch black; 0 corresponds to middle\\ngray; and 127 corresponds to total white. We can also reorganize the\\nentries to form a single vector:\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\na11\\na12\\n· · ·\\na1n\\na21\\na22\\n· · ·\\na2n\\n...\\n...\\n...\\nam1\\nam2\\n· · ·\\namn\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n→(a11, a12, · · · , a1n, a21, · · · , a2n, · · · , amn)\\nOnce we have this vectorized form of an image, it is possible to\\nperform linear algebraic operations on the vectors. For example, we\\ncan take 0.3 times the first image and add it to −0.8 times the second\\nimage, etc. See Figure 7.7 for some of these examples.\\nFigure 7.7: Example of linear algebra on\\nimages.\\nEigenfaces was an idea for face recognition 4. The dataset in this\\n4 L. Sirovich; M. Kirby (1987). Low-\\ndimensional procedure for the character-\\nization of human faces. Journal of the\\nOptical Society of America.\\nlecture is from a classic Olivetti dataset from 1990s. Researchers\\ntook images of people facing the camera in good light, downsized\\nto 64 × 64 pixels. This makes them vectors in R4096. Now we can\\nfind a 64-rank approximation of the vectors using procedures we will\\nexplore in more detail in Section 20.3.\\nFigure 7.8 shows four basis vectors in the low-rank approximation\\nof the images. The first image looks like a generic human with a\\nill-defined nose and lips; the second image looks like having glasses\\nand a wider nose; the third image potentially looks like a female\\nface; the fourth image looks like having glasses, a moustache, and\\na beard. All images in the dataset can be approximated as a linear'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 87}, page_content='88\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nFigure 7.8: Some basis vectors (which\\nturn out to be face-like) in the low-rank\\napproximation of the Olivetti dataset.\\ncombination of 128 of these basis images, and the approximations are\\nsurprisingly accurate. Figure 7.9 shows four original images of the\\ndataset, compared with their 64-rank approximations and 128-rank\\napproximations.\\nFigure 7.9: 4 original images in the\\nOlivetti dataset (left), compared with\\ntheir 64-rank approximations (middle)\\nand 128-rank approximations (right).\\nFrom Figure 7.9, we also see that the approximations are more\\naccurate when the corresponding value of k is larger. In fact, Fig-\\nure 7.10 shows the average value of ∥⃗vi−b⃗vi∥\\n2\\n2\\n∥⃗vi∥2\\n2\\nas a function of the\\nrank of the approximation. Note that this value roughly represents\\nthe fraction of⃗v lost in the approximation. It can be seen that the error\\nis a decreasing function in terms of k. 5 However, note that doing\\n5 This was also explored in Prob-\\nlem 7.1.5\\nmachine learning — specifically face recognition — on low-rank\\nrepresentations is computationally more efficient particularly because\\nthe images are compressed to a lower dimension. With a smaller\\nvalue of k, we can improve the speed of the learning.\\nFigure 7.10: What fraction of norm\\nof the image is not captured in the\\nlow-dimensional representation, plotted\\nversus the rank k.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 88}, page_content='8\\nn-Gram Language Models\\nIn this chapter, we continue our investigation into unsupervised\\nlearning techniques and now turn our attention to language mod-\\nels. You may have heard of natural language processing (NLP) and\\nmodels such as GPT-3 in the news lately. The latter is quite impres-\\nsive, being able to write and publish its own opinion article on a\\nreputable news website! 1 While most of these models are trained\\n1 The full piece can be found at\\nhttps://www.theguardian.com/\\ncommentisfree/2020/sep/08/\\nrobot-wrote-this-article-gpt-3\\nusing state-of-the-art deep learning techniques which we will discuss\\nlater on in this text, this chapter explores a key idea, which is to view\\nlanguage as the output of a probabilistic process, which leads to an\\ninteresting measure of the “goodness” of the model. Specifically, we\\nwill investigate the so-called n-gram language model.\\n8.1\\nProbabilistic Model of Language\\nClassical linguistics focused on the syntax or the formal grammar of\\nlanguages. The linguists believed that a language can be modeled\\nby a set of sentences, constructed from a finite set of vocabularies\\nand a finite set of grammatical rules. But this approach in language\\nmodeling had limited success in machine learning.\\nFigure 8.1: An example of a syntax tree\\nof an English sentence.\\nInstead, the approach of machine learning in language models,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 89}, page_content='90\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\npioneered by Claude Shannon, has been to learn the distribution of\\npieces of text.\\nFigure 8.2: Claude Shannon, inventor\\nof the n-gram language model in\\nhttps://languagelog.ldc.upenn.edu/\\nmyl/Shannon1950.pdf. Picture from\\nhttps://en.wikipedia.org/wiki/\\nClaude_Shannon.\\nIn other words, the model assigns a probability to all conceivable\\nfinite pieces of English text (even those that have not yet been spoken\\nor written). For example, the sentence “how can I help you” will be\\nassigned some probability, most likely larger than the probability\\nassigned to the sentence “can I how you help.” Note that we don’t\\nexpect to find a “correct” model; all models found to date are ap-\\nproximations. But even an approximate probabilistic model can have\\ninteresting uses, such as the following:\\n1. Speech recognition: A machine processes a recording of a human\\nspeech that sounds somewhere between “I ate a cherry” and “eye\\neight a Jerry.” If the model assigns a higher probability score to the\\nformer, speech recognition can still work in this instance.\\n2. Machine translation: “High winds tonight” should be considered a\\nbetter translation than “large winds tonight.”\\n3. Context sensitive spelling correction: We can compare the proba-\\nbilities of sentences that are similar to the following sentence —\\n“Their are problems wit this sentence.” — and output the cor-\\nrected version of the sentence.\\n4. Sentence completion: We can compare the probabilities of sentences\\nthat will complete the following phrase — “Please turn off your ...”\\n— and output the one with the highest probability.\\n8.2\\nn-Gram Models\\nSay we are in the middle of the process of assigning a probability\\ndistribution over all English sentences of length 5. We want to find\\nthe probability of the sentence “I love you so much.” If we let Xi be\\nthe random variable that represents the value of the i-th word, the\\nprobability we are looking for is the joint probability\\nPr[X1 = ”I”, X2 = ”love”, X3 = ”you”, X4 = ”so”, X5 = ”much”] (8.1)\\nBy the Chain Rule, we can split this joint probability into the product\\nof a marginal probability and four conditional probabilities:\\n(8.1) = Pr[X1 = ”I”]\\n(8.2)\\n× Pr[X2 = ”love” | X1 = ”I”]\\n× Pr[X3 = ”you” | X1 = ”I”, X2 = ”love”]\\n× Pr[X4 = ”so” | X1 = ”I”, X2 = ”love”, X3 = ”you”]\\n× Pr[X5 = ”much” | X1 = ”I”, X2 = ”love”, X3 = ”you”, X4 = ”so”]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 90}, page_content='n-gram language models\\n91\\nIf we estimate all components of the product in (8.2), we will be able\\nto estimate the joint probability (8.1).\\nNow consider the bigram model, which has the following two\\nassumptions:\\n1. The probability of a word is only dependent on the immediately\\nprevious word.\\n2. That probability does not depend on the position of the word in\\nthe sentence.\\nThe first assumption says that, for example, the conditional proba-\\nbility\\nPr[X3 = ”you” | X1 = ”I”, X2 = ”love”]\\ncan be simplified as\\nPr[X3 = ”you” | X2 = ”love”]\\nThe second assumption says that\\nPr[X3 = ”you” | X2 = ”love”] = Pr[Xi+1 = ”you” | Xi = ”love”]\\nfor any 1 ≤i ≤4. We abuse notation and denote any of these\\nprobabilities as Pr[”you” | ”love”].\\nApplying these assumptions to (8.2), we can simplify it as\\n(8.1) = Pr[”I”] × Pr[”love” | ”I”] × Pr[”you” | ”love”]\\n× Pr[”so” | ”you”] × Pr[”much” | ”so”]\\n(8.3)\\nNow we are going to estimate each component of (8.3) from a\\nlarge corpus of text. The estimation for the marginal probability of\\nthe word “I” is given as\\nPr[”I”] ≈\\nCount(\"I\")\\ntotal number of words\\n(8.4)\\nwhere Count refers to the number of occurrences of the word in\\nthe text. In other words, this is the proportion of the occurrence of\\nthe word “I” in the entire corpus. Similarly, we can estimate the\\nconditional probability of the word “love” given its previous word is\\n“I” as\\nPr[”love” | ”I”] ≈\\nCount(\"I love\")\\n∑\\nw\\nCount(\"I\" + w)\\n(8.5)\\nwhere in the denominator, we sum over all possible words in the\\ndictionary. This is the proportion of the word “love” occurring im-\\nmediately after the word “I” out of every time some word w in the\\ndictionary occurring immediately after the word “I.” 2 In general, we\\n2 Notice that there is no word occurring\\nimmediately after the word “I” when\\n“I” is at the end of the sentence in\\nthe training corpus. Therefore, the\\ndenominator in (8.5) is equal to the\\nCount of “I” minus the Count of “I”\\nat the end of a sentence. This is not\\nnecessarily the case when we introduce\\nthe sentence stop tokens in Section 8.3.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 91}, page_content='92\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\ncan estimate the following conditional probability as\\nPr[wi+1 | wi] ≈Count(wiwi+1)\\n∑\\nw Count(wiw)\\n(8.6)\\nwhere wi is the i-th word of the sentence. Once we calculate these\\nestimates from the corpus, we are able to define the probability of the\\nsentence \"I love you so much.\"\\n8.2.1\\nDefining n-Gram Probabilities\\nWe can extend the example above to a more general setting. Now we\\nwant to define the probability distribution over all sentences of length\\nk (grammatical or not). Say we want to find the joint probability of\\nthe sentence w1w2 . . . wk where wi is the i-th word of the sentence. We\\nwill employ an n-gram model which has two assumptions:\\n1. The probability of a word is only dependent on the immediately\\nprevious n −1 words. 3\\n3 If n = 1, the model is called a unigram\\nmodel, and the probability is not de-\\npendent on any previous word. When\\nn = 2 and n = 3, the model is respec-\\ntively called a bigram and a trigram\\nmodel.\\n2. That probability does not depend on the position of the word in\\nthe sentence.\\nBy a similar logic from the earlier example, we abuse notation\\nand denote the joint probability of the sentence w1w2 · · · wk as\\nPr[w1w2 . . . wk]; the marginal probability of the first word being\\nw1 as Pr[w1]; and so on. We can apply the Chain Rule again to define\\nthe n-gram model.\\nDefinition 8.2.1 (n-Gram Model). An n-gram model assigns the following\\nprobability to the sentence w1w2 . . . wk if n > 1: 4\\n4 max(1, i −n + 1) in the third line is to\\nensure that we access the correct indices\\nfor the first n −1 words, where there are\\nless than n −1 previous words to look\\nat.\\nPr[w1w2 . . . wk] = Pr[w1] Pr[w2 | w1] · · · Pr[wk | w1w2 . . . wk−1]\\n= Pr[w1] ×\\nk\\n∏\\ni=2\\nPr[wi | w1 . . . wi−1]\\n= Pr[w1] ×\\nk\\n∏\\ni=2\\nPr[wi | wmax(1,i−n+1) . . . wi−1]\\n(8.7)\\nand the following probability if n = 1:\\nPr[w1w2 . . . wk] =\\nk\\n∏\\ni=1\\nPr[wi]\\n(8.8)\\nwhere the n-gram probabilities are estimated from a training corpus as the\\nfollowing\\nPr[wi] ≈\\nCount(wi)\\ntotal number of words\\nPr[wj | wi . . . wj−1] ≈Count(wi . . . wj−1wj)\\n∑\\nw Count(wi . . . wj−1w)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 92}, page_content='n-gram language models\\n93\\nThis defines the “best” possible probabilistic model in terms of the\\nMaximum Likelihood Principle from Subsection 4.2.1. 5 We now turn\\n5 We will prove this for n = 1 later.\\nto the following example.\\nExample 8.2.2. We investigate a cowperson language which has two words\\nin the dictionary: {Yee, Haw}. Suppose the training corpus is given as “Yee\\nHaw Haw Yee Yee Yee Haw Yee.” Then the unigram probabilities can be\\nestimated as\\nPr[”Yee”] = 5\\n8\\nPr[”Haw”] = 3\\n8\\nWe can also create the bigram frequency table as in Table 8.1 and we normal-\\nize the rows of the bigram frequency table to get the bigram probability table\\nin Table 8.2.\\nprevious\\nnext\\n“Yee”\\n“Haw”\\nTotal\\n“Yee”\\n2\\n2\\n4\\n“Haw”\\n2\\n1\\n3\\nTable 8.1: Bigram frequency table of the\\ncowperson language.\\nprevious\\nnext\\n“Yee”\\n“Haw”\\nTotal\\n“Yee”\\n2/4\\n2/4\\n1\\n“Haw”\\n2/3\\n1/3\\n1\\nTable 8.2: Bigram probabilty table of the\\ncowperson language.\\nFrom Table 8.2, we get the following bigram probabilities:\\nPr[”Yee” | ”Yee”] = 2\\n4\\nPr[”Haw” | ”Yee”] = 2\\n4\\nPr[”Yee” | ”Haw”] = 2\\n3\\nPr[”Haw” | ”Haw”] = 1\\n3\\nThen by the bigram model, the probability that we see the sentence “Yee Haw\\nYee” out of all sentences of length 3 can be calculated as\\nPr[”Yee”] × Pr[”Haw” | ”Yee”] × Pr[”Yee” | ”Haw”] = 5\\n8 × 2\\n4 × 2\\n3 ≃0.21\\n8.2.2\\nMaximum Likelihood Principle\\nRecall the Maximum Likelihood Principle introduced in Subsec-\\ntion 4.2.1. It gave a way to measure the “goodness” of a model with\\nprobabilistic outputs.\\nNow we formally prove that the estimation methods given in\\nDefinition 8.2.1 satisfy the Maximum Likelihood Principle for the\\nn = 1 case. A probabilistic model is “better” than another if it assigns\\nmore probability to the actual outcome. Here, the actual outcome is\\nthe training corpus, which also consists of words. So let us denote'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 93}, page_content='94\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nthe training corpus as a string of words w1w2 . . . wT. By definition, a\\nunigram model will assign the probability\\nPr[w1w2 . . . wT] =\\nT\\n∏\\ni=1\\nPr[wi]\\n(8.9)\\nto this string. Remember that each of the wi’s are a member of a\\nfinite set of dictionary words. If we let V be the size of the dictionary,\\nthen the model is defined by the choice of V values, the probabilities\\nwe assign to each of the dictionary words. Let pi be the probability\\nthat we assign to the i-th dictionary word, and let ni be the number\\nof times that the i-th dictionary word appears in the training corpus.\\nThen (8.9) can be rewritten as\\nPr[w1w2 . . . wT] =\\nV\\n∏\\ni=1\\npni\\ni\\n(8.10)\\nWe want to maximize this value under the constraint\\nV\\n∑\\ni=1\\npi = 1. A\\nsolution to this type of a problem can be found via the Lagrange\\nmultiplier method. We will illustrate with an example.\\nExample 8.2.3. We revisit the cowperson language from Example 8.2.2. Here\\nV = 2 and T = 8. Let p1 = Pr[”Yee”] and p2 = Pr[”Haw”]. Then the\\nprobability assigned to the training corpus by the unigram model is\\nPr[”Yee Haw Haw Yee Yee Yee Haw Yee”] = p5\\n1p3\\n2\\nWe want to maximize this value under the constraint p1 + p2 = 1. Therefore,\\nwe want to find the point where the gradient of the following is zero.\\nf (p1, p2) = p5\\n1p3\\n2 + λ(p1 + p2 −1)\\nfor some λ. The gradients are given as\\n∂f\\n∂p1\\n= 5p4\\n1p3\\n2 + λ\\n∂f\\n∂p2\\n= 3p5\\n1p2\\n2 + λ\\nFrom 5p4\\n1p3\\n2 + λ = 3p5\\n1p2\\n2 + λ = 0, we get p1\\np2 = 5\\n3. Combined with the fact\\nthat p1 + p2 = 1, we get the optimal solution p1 = 5\\n8 and p2 = 3\\n8.\\nProblem 8.2.4. Following the same Lagrange multiplier method as in\\nExample 8.2.3, verify that the heuristic solution pi = ni\\nT (the empirical fre-\\nquency) is the optimal solution that maximizes (8.10) under the constraint\\nV\\n∑\\ni=1\\npi = 1.\\n8.3\\nStart and Stop Tokens\\nIn this section, we present a convention that is often useful: start\\ntoken ⟨s⟩and stop token ⟨/s⟩. They signify the start and the end'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 94}, page_content='n-gram language models\\n95\\nof each sentence in the training corpus. They are a special type of\\nvocabulary item that will be augmented to the dictionary, so you\\nwill want to pay close attention to the way they contribute to the\\nvocabulary size, number of words, and the n-gram probabilities.\\nAlso, by introducing these tokens, we are able to define a probability\\ndistribution over all sentences of finite length, not just a given length\\nof k. For the sake of exposition, we will only consider the bigram\\nmodel for most parts of this section.\\n8.3.1\\nRe-estimating Bigram Probabilities\\nConsider the cowperson language again.\\nExample 8.3.1. The training corpus “Yee Haw Haw Yee Yee Yee Haw Yee”\\nactually consists of three different sentences: (1) \"Yee Haw,\" (2) \"Haw Yee\\nYee,\" and (3) \"Yee Haw Yee.\" We can append the start and stop tokens to the\\ncorpus and transform it into\\n⟨s⟩Yee Haw ⟨/s⟩\\n⟨s⟩Haw Yee Yee ⟨/s⟩\\n⟨s⟩Yee Haw Yee ⟨/s⟩\\nWith these start and stop tokens in mind, we slightly relax the As-\\nsumption 2 of the n-gram model and investigate the probability of a\\nword w being the first or the last word of a sentence, separately from\\nother probabilities. We will denote these probabilities respectively as\\nPr[w | ⟨s⟩] and Pr[⟨/s⟩| w]. The former probability will be estimated\\nas\\nPr[w | ⟨s⟩] ≈\\nCount(⟨s⟩w)\\ntotal number of sentences\\n(8.11)\\nwhich is the proportion of sentences that start with the word w in the\\ncorpus. The latter probability is estimated as\\nPr[⟨/s⟩| w] ≈Count(w ⟨/s⟩)\\nCount(w)\\n(8.12)\\nwhich is the proportion of the occurrence of w that is at the end of a\\nsentence in the corpus.\\nAlso, notice that other bigram probabilities are also affected when\\nintroducing the stop tokens. In (8.6), the denominator originally did\\nnot include the occurrence of the substring at the end of the sentence\\nbecause there was no word to follow that substring. However, if we\\nconsider ⟨/s⟩as a word in the dictionary, the denominator can now\\ninclude the case where the substring is at the end of the sentence.\\nTherefore, the denominator is just equivalent to the Count of the\\nsubstring in the corpus. Therefore, the bigram probabilities after\\nintroducing start, stop tokens can be estimated instead as 6\\n6 If we consider ⟨s⟩, ⟨/s⟩as vocabularies\\nof the dictionary, (8.13) can also include\\n(8.11), (8.12).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 95}, page_content='96\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nPr[wj | wj−1] ≈Count(wj−1wj)\\nCount(wj−1)\\n(8.13)\\nExample 8.3.2. We revisit Example 8.2.2. The bigram frequency table and\\nthe bigram probability table can be recalculated as in Table 8.3 and Table 8.4.\\n7\\n7 Note that the values in the Total\\ncolumn now correspond to the unigram\\ncount of that word.\\nprevious\\nnext\\n“Yee”\\n“Haw”\\n⟨/s⟩\\nTotal\\n⟨s⟩\\n2\\n1\\n0\\n3\\n“Yee”\\n1\\n2\\n2\\n5\\n“Haw”\\n2\\n0\\n1\\n3\\nTable 8.3: Bigram frequency table of the\\ncowperson language with start and stop\\ntokens.\\nprevious\\nnext\\n“Yee”\\n“Haw”\\n⟨/s⟩\\nTotal\\n⟨s⟩\\n2/3\\n1/3\\n0/3\\n1\\n“Yee”\\n1/5\\n2/5\\n2/5\\n1\\n“Haw”\\n2/3\\n0/3\\n1/3\\n1\\nTable 8.4: Bigram probabilty table of the\\ncowperson language with start and stop\\ntokens.\\nTherefore, the bigram probabilities of the cowperson language, once we\\nintroduce the start and stop tokens, are given as\\nPr[”Yee” | ⟨s⟩] = 2\\n3\\nPr[”Haw” | ⟨s⟩] = 1\\n3\\nPr[”Yee” | ”Yee”] = 1\\n5\\nPr[”Haw” | ”Yee”] = 2\\n5\\nPr[⟨/s⟩| ”Yee”] = 2\\n5\\nPr[”Yee” | ”Haw”] = 2\\n3\\nPr[”Haw” | ”Haw”] = 0\\n3\\nPr[⟨/s⟩| ”Haw”] = 1\\n3\\n8.3.2\\nRedefining the Probability of a Sentence\\nThe biggest advantage of introducing stop tokens is that now we can\\nassign a probability distribution over all sentences of finite length,\\nnot just a given length k. Say we want to assign a probability to the\\nsentence w1w2 . . . wk (without the start and stop tokens). By introduc-\\ning start and stop tokens, we can interpret this as the probability of\\nw0w1 . . . wk+1 where w0 = ⟨s⟩and wk+1 = ⟨/s⟩. Following the similar\\nlogic from (8.2), we can define this probability by the Chain Rule.\\nDefinition 8.3.3 (Bigram Model with Start, Stop Tokens). A bigram\\nmodel, once augmented with start, stop tokens, assigns the following proba-\\nbility to a sentence w1w2 . . . wk 8\\n8 Notice that we do not have the term\\nPr[w0] in the expansion. A sentence\\nalways starts with a start token, so the\\nmarginal probability that the first word\\nis ⟨s⟩can be understood to be 1.\\nPr[w1w2 . . . wk] =\\nk+1\\n∏\\ni=1\\nPr[wi | wi−1]\\n(8.14)\\nwhere the bigram probabilities are estimated as in (8.13).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 96}, page_content='n-gram language models\\n97\\nExample 8.3.4. The probability that we see the sentence “Yee Haw Yee” in\\nthe cowperson language can be calculated as\\nPr[”Yee” | ⟨s⟩] × Pr[”Haw” | ”Yee”] × Pr[”Yee” | ”Haw”] × Pr[⟨/s⟩| ”Yee”]\\n= 2\\n3 × 2\\n5 × 2\\n3 × 2\\n5 ≃0.07\\nNote that this probability is taken over all sentences of finite length.\\nProblem 8.3.5. Verify that (8.14) defines a probability distribution over all\\nsentences of finite length.\\n8.3.3\\nBeyond Bigram Models\\nIn general, if we have an n-gram model, then we may need to in-\\ntroduce more than 1 start or stop tokens. For example, in a trigram\\nmodel, we will need to define the probability that the word is the first\\nword of the sentence as Pr[w | ⟨s⟩⟨s⟩]. Based on the number of start\\nand stop tokens introduced, the n-gram probabilities will need to be\\nadjusted accordingly.\\n8.4\\nTesting a Language Model\\nSo far, we discussed how to define an n-gram language model given\\na corpus. This is analogous to training a model given a training\\ndataset. Naturally, the next step is to test the model on a newly\\nseen held-out data to ensure that the model generalizes well. In this\\nsection, we discuss how to test a language model.\\n8.4.1\\nShakespeare Text Production\\nFirst consider a bigram text generator — an application of the bi-\\ngram model. The algorithm initiates with the start token ⟨s⟩. It then\\noutputs a random word w1 from the dictionary, according to the\\nprobability Pr[w1 | ⟨s⟩]. It then outputs the second random word\\nw2 from the dictionary, according to the probability Pr[w2 | w1]. It\\nrepeats this process until the newly generated word is the stop to-\\nken ⟨/s⟩. The final output of the algorithm will be the concatenated\\nstring of all outputted words.\\nIt is possible to define a text generator for any n-gram model in\\ngeneral. Figure 8.4 shows the output of the unigram, bigram, trigram,\\nquadrigram text generators when the models were trained on all\\nShakespeare texts.\\nNotice the sentence “I will go seek the traitor Gloucester.” in the\\noutput of the quadrigram text generator. This exact line appears in\\nKing Lear, Act 3 Scene 7. This is not a coincidence. Figure 8.5 presents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 97}, page_content='98\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nFigure 8.3: An example run of the\\nbigram text generator.\\nFigure 8.4: The outputs of unigram,\\nbigram, trigram, quadrigram text\\ngenerators trained on Shakespeare texts.\\nthe snapshot of the bigram, trigram, and quadrigram text generators\\nonce they have outputted the phrase “go seek the.” You can see that\\nbigram models and trigram models assign very small probabilities to\\nthe word “traitor” because there are many more instances of phrases\\n“the” or “seek the” in the corpus than “go seek the.” On the other\\nhand, the quadrigram model assigns a very large probability to the\\nword “traitor” because there is only a limited number of times that\\nthe phrase “go seek the” appears in the corpus.\\nFigure 8.5: The probability of the next\\nword given the previous three words\\nare “go seek the” in the n-gram model,\\nwhere n = 2, 3, 4.\\nOnce the quadrigram model outputs the word “traitor” after the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 98}, page_content='n-gram language models\\n99\\nphrase “go seek the,” the problem is even worse. As can be seen\\nin Figure 8.6, the quadrigram model assigns probability of 1 to the\\nword “Gloucester” meaning that the phrase “seek the traitor” only\\nappears before the word “Gloucester.” So the model has memorized\\none completion of the phrase from the training text. From this exam-\\nple, we can see that text production based on n-grams is sampling\\nand remixing text fragments seen in the training corpus.\\nFigure 8.6: The probability of the next\\nword given the previous three words\\nare “seek the traitor.”\\nThe Shakespeare corpus consists of N = 884, 647 words and V =\\n29, 066 distinct words from the dictionary. There are about V2 ≈845\\nmillion possible combinations of bigrams, but Shakespeare only used\\naround 300, 000 of them in his text. So 99.96% of the possible bigrams\\nwere never seen. The percentage is much higher for quadrigrams!\\nFurthermore, for the quadrigrams that do appear in the corpus,\\nmost do not even repeat. Thus what comes out of the quadrigram\\nmodel looks like Shakespeare because it is a memorized fragment of\\nShakespeare.9\\n9 Do this remind you of overfitting?\\n8.4.2\\nPerplexity\\nHaving described a way to train a simple language model, we now\\nturn our attention to a formal way of testing 10 a language model.\\n10 This method is used even for testing\\nstate of the art models.\\nJust like any other model in ML, a language model will be given\\na corpus w1w2 . . . wT. Then we can assess the performance of the\\nmodel by its perplexity on the corpus.\\nDefinition 8.4.1 (Perplexity). The perplexity of a language model on the\\ncorpus w1w2 . . . wT is defined as\\nPr[w1w2 . . . wT]−1\\nT =\\nT\\ns\\n1\\nPr[w1w2 . . . wT]\\n(8.15)\\nNote that, perplexity is defined for any probabilistic language\\nmodel: the Chain Rule of joint probability applies to every model,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 99}, page_content='100\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nand does not require the n-gram assumptions. That is, 11\\n11 Assume for now that start and stop\\ntokens do not exist in the corpus.\\nPr[w1w2 . . . wT] = Pr[w1] ×\\nT\\n∏\\ni=2\\nPr[wi | w1 . . . wi−1]\\nThen the perplexity of the model can be rewritten as\\nT\\nv\\nu\\nu\\nt\\n1\\nPr[w1] ×\\nT\\n∏\\ni=2\\n1\\nPr[wi | w1 . . . wi−1]\\n(8.16)\\nExample 8.4.2. Consider the uniform (“clueless”) model which assumes that\\nthe probability of all words are equal in any given situation. That is, if V is\\nthe vocabulary size (i.e., size of the dictionary),\\nPr[wi] = Pr[wi | w1 . . . wi−1] = 1\\nV\\nfor any given w1, . . . , wi ∈V. This model assigns\\n\\x10\\n1\\nV\\n\\x11T\\nto every sequence\\nof T words, including the corpus. Therefore, the perplexity of the model is\\n \\x12 1\\nV\\n\\x13T!−1\\nT\\n= V\\nNow we try to understand perplexity at an intuitive level. (8.16) is\\nthe geometric mean 12 of the following T values:\\n12 The geometric mean of T numbers\\na1, a2, . . . , aT is defined as (∏i ai)1/T\\n1\\nPr[w1],\\n1\\nPr[w2 | w1], . . . ,\\n1\\nPr[wT | w1 . . . wT−1]\\nNow note that a probabilistic model splits the total probability of 1\\nto fractions and distributes them to the potential options for the next\\nword. So the inverse of an assigned probability for a word can be\\nthought roughly as the number of choices the model considered for the\\nnext word. With this viewpoint, perplexity as written in (8.16) means:\\nhow much has the model narrowed down the number of choices for the next\\nword on average? The clueless model had not narrowed down the\\npossibilities at all and had the worst-possible perplexity equal to the\\nnumber of vocabulary words.\\nExample 8.4.3. Consider a well-trained language model. At any given place\\nof text, it can identify a set of 20 words and assigns probability 1\\n20 to each of\\nthem to be the next word. It happens that the next word is always one of\\nthe 20 words that the model identifies. The perplexity of the model is\\n \\x12 1\\n20\\n\\x13T!−1\\nT\\n= 20\\nInterestingly enough, the true perplexity of English is believed to\\nbe between 15 and 20. That is, if at an “average” place in text, you\\nask humans to predict the next word, then they are able to narrow\\ndown the list of potential next words to around 15 to 20 words. 13\\n13 The perplexity of state of the art\\nlanguage models is under 20 as well.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 100}, page_content='n-gram language models\\n101\\n8.4.3\\nPerplexity on Test Corpus\\nThe perplexity of a language model is analogous to a loss of an ML\\nmodel. 14 Similar to ML models we have been studying so far, it is\\n14 It is customary to use the logarithm\\nof the perplexity, as we also did for\\nlogistic loss in Chapter 4.\\npossible to define a train perplexity and a test perplexity. The “good-\\nness” of the model will be defined by how low the perplexity was on\\na previously unseen, held-out data.\\nFor example, when n-gram models are trained on 38 million words\\nand tested on 1.5 million words from Wall Street Journal articles,\\nthey show the following test perplexities in Table 8.5. 15 Note that\\n15 To be more exact, the models were\\naugmented with smoothing, which will\\nbe introduced shortly.\\nthe state-of-the-art deep learning models achieve a test perplexity of\\naround 20 on the same corpus.\\nUnigram\\nBigram\\nTrigram\\n962\\n170\\n109\\nTable 8.5: Test perplexities of n-gram\\nmodels on WSJ corpus.\\n8.4.4\\nPerplexity With Start and Stop Tokens\\nWhen start and stop tokens are introduced to a corpus, we also need\\nto redefine how to calculate the perplexity of the model. Again, we\\nwill only focus on a bigram model for the sake of exposition.\\nSay the corpus consists of t sentences:\\n⟨s⟩w1,1w1,2, . . . , w1,T1 ⟨/s⟩\\n⟨s⟩w2,1w2,2, . . . , w2,T2 ⟨/s⟩\\n...\\n⟨s⟩wt,1wt,2, . . . , wt,Tt ⟨/s⟩\\nThe probability of the corpus w1,1w1,2 . . . wt,Tt is redefined as the\\nproduct of the probability of each of the sentences:\\nPr[w1,1w1,2 . . . wt,Tt] =\\nt\\n∏\\ni=1\\nPr[wi,1wi,2 . . . wi,Ti]\\n=\\nt\\n∏\\ni=1\\nTi+1\\n∏\\nj=1\\nPr[wi,j | wi,j−1]\\n(8.17)\\nNow we apply the interpretation of the perplexity that it is the geo-\\nmetric mean of probabilities of each word. Notice that we multiplied\\nt\\n∑\\ni=1\\n(Ti + 1) probabilities to calculate the probability of the corpus.\\nIf we let T =\\nt\\n∑\\ni=1\\nTi denote the total number of words (excluding\\nstart and stop tokens) of the corpus, the number of probabilities we\\nmultiplied can be written as T∗= T + t. 16\\n16 This can also be thought as adding\\nthe number of stop tokens to the\\nnumber of words in the corpus.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 101}, page_content='102\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nDefinition 8.4.4 (Perplexity with Start, Stop Tokens). The perplexity of a\\nbigram model with start, stop tokens can be redefined as\\nT∗\\nv\\nu\\nu\\nu\\nu\\nt\\n1\\nt\\n∏\\ni=1\\nTi+1\\n∏\\nj=1\\nPr[wi,j | wi,j−1]\\n(8.18)\\n8.4.5\\nSmoothing\\nOne big problem with our naive definition of the perplexity of a\\nmodel is that it does not account for a zero denominator. That is,\\nif the model assigns probability exactly 0 to the corpus, then the\\nperplexity of the model will be ∞! 17\\n17 Mathematically, it is undefined, but\\nhere assume that the result is a positive\\ninfinity that is larger than any real\\nnumber.\\nExample 8.4.5. Suppose the phrase “green cream” never appeared in the\\ntraining corpus, but the test corpus contains the sentence “You like green\\ncream.” Then a bigram model will have a perplexity of ∞because it assigns\\nprobability 0 to the bigram “green cream.”\\nTo address this issue, we generally apply smoothing techniques,\\nwhich never allow the model to output a zero probability. By reducing\\nthe naive estimate of seen events and increasing the naive estimate\\nof unseen events, we can always assign nonzero probabilities to\\npreviously unseen events.\\nThe most commonly used smoothing technique is the 1-add\\nsmoothing (a.k.a, Laplace smoothing). We describe how the smooth-\\ning works for a bigram model. The main idea of the 1-add smoothing\\ncan be summarized as “add 1 to all bigram counts in the bigram\\nfrequency table.” Then the bigram probability as defined in Defini-\\ntion 8.2.1 can be redefined as\\nPr[wj | wj−1] ≈\\nCount(wj−1wj) + 1\\n∑\\nw (Count(wj−1w) + 1)\\n=\\nCount(wj−1wj) + 1\\n∑\\nw (Count(wj−1w)) + V\\n(8.19)\\nwhere V is the size of the dictionary. If we had augmented the corpus\\nwith the start and the stop tokens, the denominator in (8.19) is just\\nequal to Count(wj−1) + V∗18 and so the bigram probability can be\\n18 V∗= V + 1 is the size of the dictionary\\nafter adding the start and the stop\\ntokens. It is customary to add only\\none to the vocabulary count. It may\\nhelp to look at the number of rows and\\ncolumns in the bigram frequency table\\n8.3.\\nwritten as\\nPr[wj | wj−1] ≈Count(wj−1wj) + 1\\nCount(wj−1) + V∗\\n(8.20)\\nNotice that the denominator is just V∗, the new vocabulary size,\\nadded to the unigram count of wj−1.\\nExample 8.4.6. Recall the cowperson language with the start and stop\\ntokens from Example 8.3.2. Upon further research, it turns out the language\\nactually consists of three words: {Yee, Haw, Moo}, but the training corpus'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 102}, page_content='n-gram language models\\n103\\n“Yee Haw Haw Yee Yee Yee Haw Yee” left out one of the vocabularies in\\nthe dictionary. By applying add-1 smoothing to the bigram model, we can\\nrecalculate the bigram frequency and the bigram probability table as in\\nTable 8.6 and Table 8.7\\nprevious\\nnext\\n“Yee”\\n“Haw”\\n“Moo”\\n⟨/s⟩\\nTotal\\n⟨s⟩\\n3\\n2\\n1\\n1\\n7\\n“Yee”\\n2\\n3\\n1\\n3\\n9\\n“Haw”\\n3\\n1\\n1\\n2\\n7\\n“Moo”\\n1\\n1\\n1\\n1\\n4\\nTable 8.6: Bigram frequency table of the\\ncowperson language with start and stop\\ntokens with smoothing.\\nprevious\\nnext\\n“Yee”\\n“Haw”\\n“Moo”\\n⟨/s⟩\\nTotal\\n⟨s⟩\\n3/7\\n2/7\\n1/7\\n1/7\\n1\\n“Yee”\\n2/9\\n3/9\\n1/9\\n3/9\\n1\\n“Haw”\\n3/7\\n1/7\\n1/7\\n2/7\\n1\\n“Moo”\\n1/4\\n1/4\\n1/4\\n1/4\\n1\\nTable 8.7: Bigram probability table of\\nthe cowperson language with start and\\nstop tokens with smoothing.\\nThe probability that we see the sentence \"Moo Moo\" in the cowperson\\nlanguage, which would have been 0 before smoothing, is now assigned a\\nnon-zero value:\\nPr[”Moo” | ⟨s⟩] × Pr[”Moo” | ”Moo”] × Pr[⟨/s⟩| ”Moo”]\\n= 1\\n7 × 1\\n4 × 1\\n4 ≃0.01\\nProblem 8.4.7. Verify that (8.19) defines a proper probability distribution\\nover the conditioned event. That is, show that\\n∑\\nw\\nPr[w | w′] = 1\\nfor any w in the dictionary.\\nAnother smoothing technique is called backoff smoothing. The\\nintuition is that n-gram probabilities are less likely to be zero if n is\\nsmaller. So when we run into an n-gram probability that is zero, we\\nreplace it with a linear combination of n-gram probabilities of lower\\nvalues of n.\\nExample 8.4.8. Recall Example 8.4.5. The bigram probability of “green\\ncream” can be approximated instead as\\nPr[“cream′′ | “green′′] ≈Pr[“cream′′]\\nAlso, say we want to calculate the trigram probability of “like green cream,”\\nwhich is also zero in the naive trigram model. We can approximate it instead\\nas\\nPr[“cream′′ | “like green′′] ≈α Pr[“cream′′] + (1−α) Pr[“cream′′ | “green′′]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 103}, page_content='104\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nwhere α is a hyperparameter for the model.\\nThere are other variants of the backoff smoothing, 19 with some\\n19 For instance, Good-Turing and\\nKneser-Ney smoothing.\\ntheory for what the “best” choice is, but we will not cover it in these\\nnotes.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 104}, page_content='9\\nMatrix Factorization and Recommender Systems\\n9.1\\nRecommender Systems\\nCataloging and recommender systems have always been an essential\\nasset for consumers who find it difficult to choose from the vast scale\\nof available goods. As early as 1876, the Dewey decimal system was\\ninvented to organize libraries. In 1892, Sears released their famed\\ncatalog to keep subscribers up to date with the latest products and\\ntrends, which amounted to 322 pages. Shopping assistants at depart-\\nment stores or radio disc jockeys in the 1940s are also examples of\\nrecommndations via human curation. In more contemporary times,\\nbestseller lists at bookstores, or Billboard Hits list aim to capture\\nwhat is popular among people. The modern recommender system\\nparadigm now focuses on recommending products based on what is\\nliked by people “similar” to you. In this long history of recommender\\nsystems, the common theme is that people like to follow trends, and\\nrecommender systems can help catalyze this process.\\n9.1.1\\nMovie Recommendation via Human Curation\\nSuppose we want to design a recommender system for movies. A\\nhuman curator identifies r binary attributes that they think are im-\\nportant for a movie (e.g., is a romance movie, is directed by Steven\\nSpielberg, etc.) Then they assign each movie an r-dimensional at-\\ntribute vector, where each element represents whether the movie has\\nthe corresponding attribute (e.g., coordinate 2 will have value 1 if a\\nmovie is a “thriller” and 0 otherwise).\\nNow, using a list of movies that a particular user likes, the curator\\nassigns an r-dimensional taste vector to a given user in a similar\\nmanner (e.g., coordinate w will have value 1 if a user likes “thrillers”\\nand 0 otherwise). With these concepts in mind, we can start with\\ndefining the affinity of a user for a particular movie:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 105}, page_content='106\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nDefinition 9.1.1 (User Affinity). Given a taste vector Ai = (ai,1, ai,2, . . . , ai,r)\\nfor user i and the genre vector Bj = (b1,j, b2,j, . . . , br,j) for movie j, we de-\\nfine the affinity of user i for movie j as\\nAi · Bj =\\nr\\n∑\\nk=1\\nai,kbk,j\\n(9.1)\\nIntuitively, this metric counts the number of attributes which are\\n1 in both vectors, or equivalently how many of the user’s boxes are\\n“checked off” by the movie. Mathematically, the affinity is defined as\\na dot product, which can be extended to matrix multiplication. Thus\\nif we have a matrix A ∈Rm×r where each of m rows is a taste vector\\nfor a user and a matrix B ∈Rr×n where each of n columns is a genre\\nvector for a movie, the (i, j) entry of the matrix product M = AB\\nrepresents the affinity score of user i for movie j.\\nWe can also define an additional similarity metric:\\nDefinition 9.1.2 (Similarity Metric). Given taste vector Ai for user i and\\ntaste vector Aj for user j, we define the similarity of user i and user j as\\nr\\n∑\\nk=1\\nai,kaj,k\\n(9.2)\\nSimilarly, the similarity of movie i and movie j is defined as\\nr\\n∑\\nk=1\\nbk,ibk,j\\n(9.3)\\nFinally, in practice, each individual is unique and has a different\\naverage level of affinity for movies (for example, some users like\\neverything while others are very critical). This means that directly\\ncomparing the affinity of one user to another might not be helpful.\\nOne way to circumvent this problem is to augment (9.1) in Defini-\\ntion 9.1.1 as\\nr\\n∑\\nk=1\\nai,kbk,j + ai,0\\n(9.4)\\nwith a bias term ai,0.\\nBased on the affinity scores or similarity scores, the human curator\\nwill be able to recommend movies to users. This model design seems\\nlike it does the job as a recommender system. In practice, developing\\nsuch models through human curation comes with a set of pros and\\ncons:\\n• Pros: Using human curation allows domain expertise to be lever-\\naged and this intuition can be critical in the development of a\\ngood model (i.e., which attribute is important). In addition, a\\nhuman curated model will naturally be interpretable.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 106}, page_content='matrix factorization and recommender systems\\n107\\n• Cons: The process is tedious and expensive; thus it is difficult to\\nscale. In addition, it can be difficult to account for niche demo-\\ngraphics and genres and this becomes a problem for companies\\nwith global reach.\\nWe conclude that while human curated models can certainly be\\nuseful, the associated effort is often too great.\\n9.2\\nRecommender Systems via Matrix Factorization\\nIn this section, we provide another technique that can be used for\\nrecommender systems — matrix factorization. This method started to\\nbecome popular since 2005.\\n9.2.1\\nMatrix Factorization\\nMatrix factorizations are a common theme throughout linear alge-\\nbra. Some common techniques include LU and QR decomposition,\\nRank Factorization, Cholesky Decomposition, and Singular Value\\nDecomposition.\\nDefinition 9.2.1 (Matrix Factorization). Suppose we have some matrix\\nM ∈Rm×n. A matrix factorization is the process of finding matrices\\nA ∈Rm×r, B ∈Rr×n such that M = AB for some r < m, n.\\nUnfortunately, these techniques become less directly applicable\\nonce we consider the case where most of the entries of M are missing\\n(i.e., a missing-data setting). As we saw in Section 9.1.1, this is very\\ncommon in real-world applications — for example, if the (m, n) entry\\nof M represents the rating of user m for movie n, most entries in M\\nare missing because not everyone has seen every movie. What can we\\ndo in such a case?\\nIn turns out, if we assume that M is a low-rank matrix (which is\\ntrue for many high-dimensional datasets, as noted in Chapter 7), then\\nwe can consider an approximate factorization M ≈AB on the known\\nentries. We express this as the following optimization problem:\\nDefinition 9.2.2 (Approximate Matrix Factorization). Suppose we have\\nsome matrix M ∈Rm×n where Ω⊂[m] × [n] is the subset of (i, j) where\\nMij is known. An approximate matrix factorization is the process of\\nfinding matrices A ∈Rm×r, B ∈Rr×n for some r < m, n that minimize the\\nloss function:\\nL(A, B) =\\n1\\n|Ω| ∑\\n(i,j)∈Ω\\n(Mij −(AB)ij)2\\n(9.5)\\nWe denote the approximation as M ≈AB.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 107}, page_content='108\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nNotice this form is familiar: we are effectively trying to find op-\\ntimal matrices A, B which will minimize the MSE between known\\nentries of M and corresponding entries in the matrix product AB!\\nOne thing to note is that by calculating the matrix product AB, we\\ncan “predict” entries of M that are unknown.\\nYou can take the following result from linear algebra as granted.\\nTheorem 9.2.3. Given M ∈Rm×n, we can find the matrix factorization\\nM = AB, with A ∈Rm×r and B ∈Rr×n if and only if M has rank at most\\nr. Also, we can find the approximate matrix factorization M ≈AB, with\\nA ∈Rm×r, B ∈Rr×n if and only if M is “close to” rank r.\\n9.2.2\\nMatrix Factorization as Semantic Embeddings\\nRecall the setup in Section 9.1.1. But instead of calculating the affinity\\nmatrix M as the product of the matrices A, B, we will approach\\nfrom the opposite direction. We will start with an affinity matrix\\nM ∈Rm×n (which is only partially known) and find its approximate\\nmatrix factorization M ≈AB. We can understand that A ∈Rm×r\\nrepresents a set of users and that B ∈Rr×n represents a set of\\nmovies.\\nFigure 9.1: Matrix factorization on\\nmovie recommendations. Usually the\\ninner dimension r would be much\\nsmaller than m, n.\\nSpecifically, if we let Ai∗denote the i-th row of A and B∗j denote\\nthe j-th column of B, then Ai∗can be understood as the taste vector\\nof user i and B∗j can be understood as the attribute vector of movie\\nj. One difference to note is that the output of a matrix factorization\\nis real-valued, unlike the the 0/1 valued matrices A, B from Sec-\\ntion 9.1.1. We can then use the vectors Ai∗and B∗j to find similar\\nusers or movies and make recommendations.\\nExample 9.2.4. Assume all columns of B have ℓ2 norm 1. That is,\\n\\r\\rB∗j\\n\\r\\r\\n2 =\\n1 for all j. When the inner product B∗j · B∗j′ of two movie vectors is actually\\n1, the two vectors are exactly the same! They have the same inner product\\nwith every user vector Ai∗— in other words these movies have the same\\nappeal to all users. Now suppose B∗j · B∗j′ is not quite 1 but close to 1, say\\n0.9. This means the movie vectors are quite close but not the same. Still,\\ntheir inner product with typical user vectors will not be too different. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 108}, page_content='matrix factorization and recommender systems\\n109\\nconclude that two movies j, j′ with inner product B∗j · B∗j′ close to 1 tend\\nto get recommended together to users. One can similarly conclude that high\\nvalue of inner product between two user vectors is suggestive that the users\\nhave similar tastes.\\n9.2.3\\nNetflix Prize Competition: A Case Study\\nDuring 2006-09, DVDs were all the rage. Companies like Netflix were\\nquite interested in recommending movies as accurately as possible\\nin order to retain clients. At the time, Netflix was using an algorithm\\nwhich had stagnated around RMSE = 0.95. 1 Seeking fresh ideas,\\n1 RMSE is shorthand for\\n√\\nMSE.\\nNetflix curated an anonymized database of 100M ratings (each rating\\nwas on a 1 −5 scale) of 0.5M users for 18K movies. Adding a cash\\nincentive of $1, 000, 000, Netflix challenged the world to come up\\nwith a model that could achieve a much lower RMSE! 2 It turned out\\n2 This was an influential competi-\\ntion, and is an inspiration for today’s\\nhackathons, Kaggle, etc.\\nthat matrix factorization would be the key to achieving lower scores.\\nIn this example, m = 0.5M, n = 18k, and Ωcorresponds to the 100M\\nratings out of m · n = 10B affinities. 3\\n3 Less than 1% of possible elements are\\naccounted for by Ω.\\nAfter a lengthy competition, 4 the power of matrix factorization is\\n4 Amazingly, a group of Princeton\\nundergraduates managed to achieve the\\nsecond place!\\non full display when we consider the final numbers:\\n• Netflix’s algorithm: RMSE = 0.95\\n• Plain matrix factorization: RMSE = 0.905\\n• Matrix factorization and bias: RMSE = 0.9\\n• Final winner (an ensemble of many methods) : RMSE = 0.856\\nFigure 9.2: 2D visualization of embed-\\ndings of film vectors. Note that you\\nsee clusters of “artsy” films on top\\nright, and romantic films on the bottom.\\nCredit: Koren et al., Matrix Factorization\\nTechniques for Recommender Systems,\\nIEEE Computer 2009.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 109}, page_content='110\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n9.2.4\\nWhy Does Matrix Factorization Work?\\nIn general, we need mn entries to completely describe a m × n matrix\\nM. However, if we find factor M into the product M = AB of m × r\\nmatrix A and r × n matrix B, then we can describe M with essentially\\nonly (m + n)r entries. When r is small enough such that (m + n)r ≪\\nmn, some entries of M (including the missing entries) are not truly\\n“required” to understand M.\\nExample 9.2.5. Consider the matrix\\nM =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n∗\\n2\\n1\\n1\\n∗\\n∗\\n4\\n∗\\n8\\n∗\\n4\\n∗\\n∗\\n∗\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nIs it possible to fill in the missing elements such that the rank of M is 1?\\nSince r = 1, it means that all the rows/columns of M are the same up to\\nscaling. By observing the known entries, the second row should be equal to\\nthe first row, and the third and the fourth row should be equal to the the first\\nrow multiplied by 4. Therefore, we can fill in the missing entries as\\nM =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n2\\n2\\n1\\n1\\n2\\n2\\n4\\n4\\n8\\n8\\n4\\n4\\n8\\n8\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nIt is not hard to infer that M = AB where A = (1, 1, 4, 4)T and B =\\n(1, 1, 2, 2)\\nExample 9.2.6. Consider another matrix\\nM =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n∗\\n∗\\n1\\n7\\n∗\\n∗\\n4\\n∗\\n∗\\n2\\n∗\\n4\\n∗\\n∗\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nIs it possible to fill in the missing elements such that the rank of M is 1?\\nThis time, the answer is no. Following a similar logic from Example 9.2.5,\\nthe second row should be equal to the first row multiplied by a constant.\\nThis is not feasible since M2,1/M1,1 = 1 and M2,2/M1,2 = 7.\\n9.3\\nImplementation of Matrix Factorization\\nIn this section, we look more deeply into implementing matrix fac-\\ntorization in an ML setting. As suggested in Definition 9.2.2, we can\\nconsider the process of approximating a matrix factorization to be an\\noptimization problem. Therefore, we can use gradient descent.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 110}, page_content='matrix factorization and recommender systems\\n111\\n9.3.1\\nCalculating the Gradient of Full Loss\\nRecall that for an approximate matrix factorization of a matrix M, we\\nwant to find matrices A, B that minimize the following loss:\\nL(A, B) =\\n1\\n|Ω| ∑\\n(i,j)∈Ω\\n(Mij −(AB)ij)2\\n(9.5 revisited)\\nHere (AB)ij = Ai∗· B∗j. Now we find the gradient of the loss\\nL(A, B) by first finding the derivatives of L with respect to elements\\nof A (a total of mr derivatives), then finding the derivatives of L with\\nrespect to elements of B (a total of nr derivatives).\\nFirst, consider an arbitrary element Ai′k′:\\n∂\\n∂Ai′k′ L(A, B) =\\n1\\n|Ω| ∑\\n(i,j)∈Ω\\n2(Mij −(AB)ij)\\n∂\\n∂Ai′k′ (−(AB)ij)\\n=\\n1\\n|Ω|\\n∑\\nj: (i′,j)∈Ω\\n2(Mi′j −(AB)i′j)\\n∂\\n∂Ai′k′ (−(AB)i′j)\\n=\\n1\\n|Ω|\\n∑\\nj: (i′,j)∈Ω\\n2(Mi′j −(AB)i′j) · (−Bk′j)\\n=\\n1\\n|Ω|\\n∑\\nj: (i′,j)∈Ω\\n−2Bk′j(Mi′j −(AB)i′j)\\n(9.6)\\nNote that the second step is derived because (AB)ij = ∑k AikBkj and\\nif i ̸= i′, then\\n∂(AB)ij\\n∂Ai′k′\\n= 0. Enumerating (i, j) ∈Ωcan be changed to\\nonly enumerating (i′, j) ∈Ω. Similarly, we can consider an arbitrary\\nelement Bk′j′:\\n∂\\n∂Bk′j′ L(A, B) =\\n1\\n|Ω| ∑\\n(i,j)∈Ω\\n2(Mij −(AB)ij)\\n∂\\n∂Bk′j′ (−(AB)ij)\\n=\\n1\\n|Ω|\\n∑\\ni: (i,j′)∈Ω\\n2(Mij′ −(AB)ij′)\\n∂\\n∂Bk′j′ (−(AB)ij′)\\n=\\n1\\n|Ω|\\n∑\\ni: (i,j′)∈Ω\\n2(Mij′ −(AB)ij′) · (−Aik′)\\n=\\n1\\n|Ω|\\n∑\\ni: (i,j′)∈Ω\\n−2Aik′(Mij′ −(AB)ij′)\\n(9.7)\\nWhew! That’s a lot of derivatives, but we now have ∇L(A, B) at our\\ndisposal.\\n9.3.2\\nStochastic Gradient Descent for Matrix Factorization\\nOf course, we could use ∇L(A, B) for a plain gradient descent as\\nshown in Chapter 3. However, given that each derivative in the\\ngradient involves a sum over a large number of indices, it would be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 111}, page_content='112\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nworthwhile to use stochastic gradient descent in order to estimate the\\noverall gradient via a small random sample (as shown in Section 3.2).\\nIf we take a sample S ⊂Ωof the known entries at each iteration,\\nthe loss becomes\\nbL(A, B) = 1\\n|S| ∑\\ni,j∈S\\n(Mij −(AB)ij)2\\n(9.8)\\nand the gradient becomes\\n∂\\n∂Ai′k′\\nbL(A, B) = 1\\n|S|\\n∑\\nj: (i′,j)∈S\\n−2Bk′j(Mi′j −(AB)i′j)\\n(9.9)\\n∂\\n∂Bk′j′\\nbL(A, B) = 1\\n|S|\\n∑\\ni: (i,j′)∈S\\n−2Aik′(Mij′ −(AB)ij′)\\n(9.10)\\nHowever, if we take a uniform sample S of Ω, the computation will\\nnot become much cheaper, since (i, j) ∈S can spread into many\\ndifferent rows and columns. One clever (and common) way to do\\nso is to select a set of columns C by sampling k out of the overall n\\ncolumns. This method is called column sampling. We then only need\\nto consider entries (i, j) ∈Ωwhere j ∈C and compute gradients only\\nfor the entries Bk,j where j ∈C. We can also perform row sampling\\nin a very similar manner. In practice, whether we should use column\\nsampling or row sampling, or gradient descent of full loss, depends\\non the actual sizes of m and n.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 112}, page_content='Part III\\nDeep Learning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 113}, page_content=''),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 114}, page_content='10\\nIntroduction to Deep Learning\\nDeep learning is currently the most successful machine learning\\napproach, with notable successes in object recognition, speech and\\nlanguage understanding, self-driving cars, automated Go playing,\\netc. It is not easy to give a single definition to such a broad and\\ninfluential field; nevertheless here is a recent definition by Chris\\nManning:1\\n1 Source: https://hai.stanford.\\nedu/sites/default/files/2020-09/\\nAI-Definitions-HAI.pdf.\\nDeep Learning is the use of large multi-layer (artificial) neural networks\\nthat compute with continuous (real number) representations, a little like the\\nhierarchically-organized neurons in human brains. It is currently the most\\nsuccessful ML approach, usable for all types of ML, with better generalization\\nfrom small data and better scaling to big data and compute budgets.\\nDeep learning does not represent a specific model per se, but\\nrather categorizes a group of models called (artificial) neural net-\\nworks (NNs) (or deep nets) which involve several computational\\nlayers. Linear models studied in earlier chapters, such as logistic\\nregression in Section 4.2, can be seen as special sub-cases involving\\nonly a single layer. The main difference, however, is that general deep\\nnets employ nonlinearity in between each layer, which allows a much\\nbroader scale of expressivity. Also, the multiple layers in a neural net\\ncan be viewed as computing “intermediate representations” of the\\ndata, or “high level features” before arriving at its final answer. By\\ncontrast, a linear model works only with the data representation it\\nwas given.\\nDeep nets come in various types, including Feed-Forward NNs\\n(FFNNs), Convolutional NNs (CNNs), Recurrent NNs (RNNs), Resid-\\nual Nets, and Transformers. 2 Training uses a variant of Gradient\\n2 Interestingly, a technique called Neural\\nArchitecture Search uses deep learn-\\ning to design custom deep learning\\narchitectures for a given task.\\nDescent, and the gradient of the loss is computed using an algorithm\\ncalled backpropagation.\\nDue to the immense popularity of deep learning, a variety of\\nsoftware environments such as Tensorflow and PyTorch allow quick\\nimplementation of deep learning models. You will encounter them in\\nthe homework.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 115}, page_content='116\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n10.1\\nA Brief History\\nNeural networks are inspired by the biological processes present\\nwithin the brain. The concept of an artificial neuron was first outlined\\nby Warren MuCulloch and Walter Pitts in the 1940s. 3 The basic\\n3 Paper: https://www.cs.cmu.edu/\\n~./epxing/Class/10715/reading/\\nMcCulloch.and.Pitts.pdf.\\nframeworks for CNNs and modern training soon followed in the\\n1980s. 4 Later in 1986, backpropagation was discovered as a new\\n4 Paper: https://link.springer.com/\\narticle/10.1007/BF00344251.\\nprocedure to efficiently apply gradient-based training methods to\\nthese models. 5 However, by the 21st century deep learning had gone\\n5 Paper: https://www.nature.com/\\narticles/323533a0.\\nout of fashion. This changed in 2012, when Krizhevsky, Sutskever,\\nand Hinton leveraged deep learning techniques through their AlexNet\\nmodel and set new standards for performance on the ImageNet\\ndataset. 6 Deep learning has since begun a resurgence throughout\\n6 Paper: https:\\n//papers.nips.cc/paper/2012/file/\\nc399862d3b9d6b76c8436e924a68c45b-Paper.\\npdf.\\nthe last decade, boosted by some key factors:\\n• Hardware, such as GPU and TPU (Tensor Processing Unit, specifi-\\ncally developed for neural network machine learning) technology\\nhas made training faster.\\n• The development of novel neural network architecutres as well as\\nbetter algorithms for training neural networks.\\n• A vast amount of data collection, boosted by the spread of the\\ninternet, has augmented the performance of NN models.\\n• Popular frameworks, such Tensorflow and PyTorch, have made it\\neasier to prototype and deploy NN architectures.\\n• Commercial payoff has caused tech corporations to invest more\\nfinancial resources.\\nEach of the reasons listed above has interfaced in a positively\\nreinforcing cycle, causing the acceleration of this technology into the\\nforeseeable future.\\n10.2\\nAnatomy of a Neural Network\\n10.2.1\\nArtificial Neuron\\nAn artificial neuron, or a node, is the main component of a neural\\nnetwork. Artificial neurons were inspired by early work on neurons\\nin animal brains, with the analogies in Table 10.1.\\nFormally, a node is a computational unit which receives m scalar\\ninputs and outputs 1 scalar. This scalar output can be used as an\\ninput for a different neuron.\\nConsider the vector⃗x = (x1, x2, . . . , xm) of m inputs. A neuron\\ninternally maintains a trainable weight vector ⃗w = (w1, w2, . . . , wm)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 116}, page_content='introduction to deep learning\\n117\\nBiological neuron\\nArtificial neuron\\nDendrites\\nInput\\nCell Nucleus / Soma\\nNode\\nAxon\\nOutput\\nSynapse\\nInterconnections\\nTable 10.1: A comparison between\\nbiological neurons in the brain and\\nartificial neurons in neural networks\\nFigure 10.1: A comparison between a\\nbrain neuron and an artificial neuron.\\nx1\\nx2\\nx3\\nx4\\nx5\\n⃗w ·⃗x\\nf (⃗w ·⃗x)\\nw1\\nw2\\nw3\\nw4\\nw5\\nf\\nFigure 10.2: A sample artificial neuron.\\nand optionally a nonlinear activation function f : R →R and outputs\\nthe following value: 7\\n7 If no activation function is chosen,\\nwe can assume that f is an identity\\nfunction f (z) = z.\\ny = f (⃗w ·⃗x)\\n(10.1)\\nWe can also add a scalar bias b before applying the activation func-\\ntion f (z) in which case the output will look like the following: 8\\n8 If we introduce a dummy variable for\\nthe constant bias term as in Chapter 1\\nwe can absorb the bias term into the\\nequation in (10.1).\\ny = f (⃗w ·⃗x + b)\\n10.2.2\\nActivation Functions\\nAn artificial neuron can choose its nonlinear activation function f (z)\\nfrom a variety of options. One such choice is the sigmoid function\\nσ(z) =\\n1\\n1 + e−z\\n(10.2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 117}, page_content='118\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nNote that in this case, the neuron represents a logistic regression\\nunit. 9 Another popular activation function is the hyperbolic tangent,\\n9 However, in this context the output\\nis not considered to be a subjective\\nprobability as in the case of standard\\nlogistic regression.\\nwhich is similar to the sigmoid function:\\ntanh(z) = ez −e−z\\nez + e−z\\n(10.3)\\nIn fact, we can rewrite the hyperbolic tangent in terms of sigmoid:\\ntanh(z) = 2σ(2z) −1\\n(10.3 revisited)\\nAccording to this expression, tanh function can be viewed as a\\nrescaled sigmoid function. The key difference is: the range of σ(z) is\\n(0, 1) and the range of tanh(z) is (−1, 1).\\nArguably the most commonly used activation function is the\\nRectified Linear Unit, or ReLU:\\nReLU(z) = [z]+ = max{z, 0}\\n(10.4)\\nThere are several benefits to the ReLU activation function. It is far\\ncheaper to compute than the previous two alternatives and avoids\\nthe “vanishing gradient” problem. 10 With sigmoid and hyperbolic\\n10 The vanishing gradient problem refers\\nto a situation where the derivative of\\na certain step is too close to 0, which\\ncan stall the gradient-based learning\\ntechniques common in deep learning.\\ntangent activation functions, the vanishing gradient problem happens\\nwhen z = ⃗x · ⃗w has high absolute values, but ReLU avoids this\\nproblem because the derivative is exactly 1 even for high values of z.\\nExample 10.2.1. Consider a vector⃗x = (−2, −1, 0, 1, 2) of inputs and a\\nneuron with the weights ⃗w = (1, 1, 1, 1, 1). If the activation function of this\\nneuron is the sigmoid, then the output will be:\\ny = σ(⃗w ·⃗x) = σ(0) = 1\\n2\\nIf the activation is ReLU, it will output:\\n[⃗w ·⃗x]+ = [0]+ = 0\\nProblem 10.2.2. Consider a neuron with the weights ⃗w = (1, 1, 5, 1, 1) and\\nthe ReLU activation function. What will the outputs y1 and y2 be for the\\ninputs⃗x1 = (−2, −2, 0, 1, 2) and⃗x2 = (2, −1, 0, 1, 2) respectively?\\n10.2.3\\nNeural Network\\nA neural network consists of nodes connected with directed edges,\\nwhere each edge has a trainable parameter called its “weight” and\\neach node has an activation function as well as associated parame-\\nter(s). There are designated input nodes and output nodes. The input\\nnodes are given some input values, and the rest of the network then\\ncomputes as follows: each node produces its output by taking the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 118}, page_content='introduction to deep learning\\n119\\nFigure 10.3: A sample neural network\\ndesign. Each circle represents one\\nartificial neuron. Two nodes being\\nconnected by an edge means that the\\noutput of the node on the left is being\\nused as one of the inputs for the node\\non the right.\\nvalues produced by all nodes that have a directed edge to it. If the\\ndirected graph of connections is acyclic — which is the case in most\\npopular architectures — this process of producing the values takes\\nfinite time and we end up with a unique value at each of the output\\nnodes. 11 The term hidden nodes is used for nodes that are not input\\n11 We will not study Recurrent Neural\\nNets (RNNs), where the graph contains\\ncycles. These used to be popular until\\na few years ago, and present special\\ndifficulties due to the presence of\\ndirected loops. For instance, can you\\ncome up with instances where the\\noutput is not well-defined?\\nor output nodes.\\n10.3\\nWhy Deep Learning?\\nNow that we are aware of the basic building blocks of neural net-\\nworks, let’s consider why we prefer these models over techniques\\nexplored in previous chapters. The key understanding is that the\\nmodels previously discussed are fundamentally linear in nature. For\\ninstance, if we do binary classification, where the data point⃗x is\\nmapped to a label based on sign(⃗w ·⃗x), then this corresponds to sep-\\narating the points with label +1 from the points with label −1 via a\\nlinear hyperplane ⃗w ·⃗x = 0. But such models are not a good choice for\\ndatasets which are not linearly separable. Deep learning is inherently\\nnonlinear and is able to do classification in many settings where linear\\nclassification cannot work.\\nFigure 10.4: Some examples of datasets\\nthat are not linearly separable.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 119}, page_content='120\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n10.3.1\\nThe XOR Problem\\nConsider the boolean function XOR with the truth table in Table 10.2.\\nx1\\nx2\\nXOR\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n1\\n1\\n1\\n0\\nTable 10.2: The truth table for the XOR\\nBoolean function.\\nLet us first attempt to represent the XOR function with a single\\nlinear neuron. That is, consider a neuron that takes two inputs x1, x2\\nwith weights w1, w2, a bias term b, and the following Heaviside step\\nactivation function: 12\\n12 This neuron is called a linear per-\\nceptron. It uses a nonlinear activation\\nfunction, but the nonlinearity is strictly\\nfor the binary classification in the final\\nstep. The boundary of the classification\\nis still linear.\\ng(z) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n0\\nif z ≤0\\n1\\nif z > 0\\n(10.5)\\nProposition 10.3.1. There are no values of w1, w2, b such that the linear\\nneuron defined by the values represent the XOR function.\\nProof. Assume to the contrary that there are such values. Let⃗x1 =\\n(0, 0),⃗x2 = (0, 1),⃗x3 = (1, 0),⃗x4 = (1, 1). Then we know that\\ng(⃗w ·⃗x1 + b) = g(⃗w ·⃗x4 + b) = 0\\ng(⃗w ·⃗x2 + b) = g(⃗w ·⃗x3 + b) = 1\\nwhich implies that\\n⃗w ·⃗x1 + b ≤0,\\n⃗w ·⃗x4 + b ≤0\\n⃗w ·⃗x2 + b > 0,\\n⃗w ·⃗x3 + b > 0\\nNow let⃗x =\\n\\x10\\n1\\n2, 1\\n2\\n\\x11\\n. Since we have⃗x = 1\\n2⃗x1 + 1\\n2⃗x4, we should have\\n⃗w ·⃗x + b = 1\\n2 · ((⃗w ·⃗x1 + b) + (⃗w ·⃗x4 + b)) ≤0\\nsince we are taking the average of two non-positive numbers. But at\\nthe same time, since⃗x = 1\\n2⃗x2 + 1\\n2⃗x3, we should have\\n⃗w ·⃗x + b = 1\\n2 · ((⃗w ·⃗x2 + b) + (⃗w ·⃗x3 + b)) > 0\\nsince we are taking the average of two positive numbers. This leads\\nto a contradiction.\\nProblem 10.3.2. Verify that the AND, OR Boolean functions can be\\nrepresented by a single linear node.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 120}, page_content='introduction to deep learning\\n121\\nFigure 10.5 visualizes the truth table for XOR in the 2D plane. The\\ntwo axes represent the two inputs x1 and x2; blue circles denote that\\ny = 1; and white circles denote that y = 0. A single linear neuron can\\nbe understood as drawing a red line that can separate white points\\nfrom blue points. Notice that it is possible to draw such a line for\\nAND and OR functions, but the data points that characterize the\\nXOR function are not linearly separable.\\nFigure 10.5: The data points that\\ncharacterize the XOR function are not\\nlinearly separable.\\nInstead, we will leverage neural networks to solve this problem.\\nLet us design an architecture with inputs x1, x2, a hidden layer with\\ntwo nodes h1, h2, and a final output layer with one node y1. We\\nassign the ReLU activation function to the hidden nodes and define\\nweights and biases as shown in Figure 10.6.\\nx1\\nx2\\nb\\nh1\\nh2\\nb\\ny\\n1\\n1\\n1\\n1\\n0\\n−1\\n1\\n−2\\n0\\nFigure 10.6: A sample neural network\\nwhich computes the XOR of its inputs\\nx1 and x2. The weights for inputs are\\nshown by black arrows, while bias\\nterms are shown by grey arrows.\\nTo be more explicit, the neural network is defined by the following\\nthree neurons:\\nh1 = ReLU(x1 + x2)\\nh2 = ReLU(x1 + x2 −1)\\ny1 = ReLU(h1 −2h2)\\nProblem 10.3.3. Verify that the model in Figure 10.6 represents the XOR\\nfunction by constructing a truth table.\\nThe main difference between the single linear neuron approach\\nand the neural network for the XOR function is that the network now'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 121}, page_content='122\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nhas two layers of neurons. If we only focus on the final layer of the\\nneural network, we expect the boundary of the binary classification\\nto be linear in the values of h1, h2. However, the values of h1, h2 are\\nnot linear in the input values x1, x2 because the hidden nodes utilize a\\nnonlinear activation function. Hence the boundary of the classification\\nis also not linear in the input values x1, x2. The nonlinear activation\\nfunction transforms the input space into a space where the XOR\\noperation is linearly separable. As shown in Figure 10.7, the h space is\\nquite clearly linearly separable in contrast to the original x space.\\nFigure 10.7: Unlike the x space, after\\napplying the nonlinear ReLU activation\\nfunction, the mapped h space is linearly\\nseparable.\\n10.4\\nMulti-class Classification\\nNeural networks, like multi-class regression in Chapter 4, can be\\nused for classification tasks where the number of possible labels is\\nlarger than 2. Real-life scenarios include hand-written digit recog-\\nnition on the MNIST dataset, where the model designer could use\\nten different classes to correspond to each possible digit. Another\\npossible example is a speech recognition language model, where the\\nmodel is trained to distinguish between sounds of |V| vocabularies.\\nIt turns out that such functionality can be added by simply includ-\\ning the same number of output neurons as the desired number of\\nclasses in the output layer. Then, the values of the output neurons\\nwill be converted into a probability distribution Pr[y = k] over the\\nnumber of classes.\\n10.4.1\\nSoftmax Function\\nJust as in Chapter 4, we use the softmax function for the purpose of\\nthe multi-class classification. See Chapter 19 for the definition of the\\nsoftmax function.\\nExample 10.4.1. Say⃗o = (3, 0, 1) are the values of the output neurons\\nof a neural network before applying the activation function. If we decide to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 122}, page_content='introduction to deep learning\\n123\\napply the softmax function as the activation function, the final outputs of the\\nnetwork will be so f tmax(⃗o) ≃(0.84, 0.04, 0.11). If the network was trying\\nto solve a multi-class classification task, we can understand that the given\\ninput is most likely to be of class 1, with probability 0.84 according to the\\nmodel.\\nOne notable property of the softmax function is that the output\\nof the function is the same if all coordinates of the input vector is\\nshifted by the same amount; that is so f tmax(⃗z) = so f tmax(⃗z + c ·⃗1)\\nfor any c ∈R, where⃗1 = (1, 1, . . . , 1) is a vector of all ones.\\nExample 10.4.2. Consider two vectors⃗z1 = (5, 2, 3) and⃗z2 = (3, 0, 1).\\nThen so f tmax(⃗z1) = so f tmax(⃗z2) because⃗z2 =⃗z1 −(2, 2, 2).\\nProblem 10.4.3. Prove the property that so f tmax(⃗z) = so f tmax(⃗z + c ·⃗1)\\nfor any c ∈R. (Hint: multiply both the numerator and the denominator of\\nso f tmax(⃗z)k by exp(c).)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 123}, page_content=''),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 124}, page_content='11\\nFeedforward Neural Network and Backpropagation\\nFeedforward Neural Networks (FFNNs) are perhaps the simplest kind of\\ndeep nets and are characterized by the following properties:\\n• There are nodes connected with no cycles.\\n• Nodes are partitioned into layers numbered 1 to k for some k. The\\nnodes in the first layer receive input of the model and output some\\nvalues. Then the nodes in layer i + 1 receive output of the nodes\\nin layer i as their input and output some values. The output of the\\nmodel can be computed with the output of the nodes in layer k.\\n• No outputs are passed back to lower layers.\\nNow, we only consider fully-connected layers — a special case of a\\nlayer in feedforward neural networks.\\nDefinition 11.0.1 (Fully-Connected Layer). A fully-connected layer\\nis a neural network layer in which all the nodes from one layer are fully\\nconnected to every node of the next layer.\\nNote that not all layers of feedforward neural networks are nec-\\nessarily fully-connected (a typical case is a Convolutional Neural\\nNetwork, which we will explore in Chapter 12). However, feedfor-\\nward neural networks with fully-connected layers are very common\\nand also easy to implement.\\n11.1\\nForward Propagation: An Example\\nForward propagation refers to how the network converts a specific\\ninput to the output, specifically the calculation and storage of inter-\\nmediate variables from the input layer to the output layer. In this\\nsection, we use concrete examples to motivate the topic. We will pro-\\nvide a more general formula in the next section. Readers who have a\\nstronger background in math may feel to skip this section altogether.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 125}, page_content='126\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n11.1.1\\nOne Output Node\\nWe start with the network in Figure 11.1 as an example. The network\\nreceives three inputs x1, x2, x3 and has a first hidden layer with two\\nnodes h(1)\\n1 , h(1)\\n2 , a second hidden layer with two nodes h(2)\\n1 , h(2)\\n2 , and\\na final output layer with one node o. We assign the ReLU activa-\\ntion function to the hidden units, and define weights as shown in\\nFigure 11.1.\\nFigure 11.1: A sample feedforward\\nneural network with two hidden layers\\nand one output node.\\nThe two hidden nodes in the first hidden layer are characterized\\nby the following equations:\\nh(1)\\n1\\n= ReLU(2x1 −3x2)\\nh(1)\\n2\\n= ReLU(−x1 + x2 + 2x3)\\n(11.1)\\nand the two hidden nodes in the second hidden layer are character-\\nized by the following equations:\\nh(2)\\n1\\n= ReLU(h(1)\\n1\\n+ 2h(1)\\n2 )\\nh(2)\\n2\\n= ReLU(2h(1)\\n1\\n−2h(1)\\n2 )\\n(11.2)\\nand the output node is characterized by the following equation:\\no = −h(2)\\n1\\n+ 2h(2)\\n2\\nTherefore, if we know the input values x1, x2, x3, we can first calcu-\\nlate the values h(1)\\n1 , h(1)\\n2 , then using these values, calculate h(2)\\n1 , h(2)\\n2 ,\\nand finally using these values, we can calculate the output o of the\\nnetwork.\\nExample 11.1.1. If the provided input vector to the neural network in\\nFigure 11.1 is⃗x = (1, 1, 1), we can calculate the first hidden layer as\\nh(1)\\n1\\n= ReLU(2 −3) = 0\\nh(1)\\n2\\n= ReLU(−1 + 1 + 2) = 2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 126}, page_content='feedforward neural network and backpropagation\\n127\\nand the second hidden layer as\\nh(2)\\n1\\n= ReLU(0 + 2 · 2) = 4\\nh(2)\\n2\\n= ReLU(0 −2 · 2) = 0\\nand the output as\\no = −4 + 2 · 0 = −4\\n11.1.2\\nMultiple Output Nodes\\nNetworks can have more than one output node. An example is the\\nnetwork in Figure 11.2.\\nFigure 11.2: A sample feedforward\\nneural network with two hidden layers\\nand three output nodes.\\nThe networks in Figure 11.1 and Figure 11.2 are the same except\\nfor the output layer; the former has one output node, while the latter\\nhas three output nodes. Now the output values of the network in\\nFigure 11.2 can be calculated as:\\no1 = −h(2)\\n1\\n+ 2h(2)\\n2\\no2 = 2h(2)\\n1\\n+ h(2)\\n2\\no3 = h(2)\\n1\\n+ 2h(2)\\n2\\n(11.3)\\nRecall from the previous Chapter 10 that a FFNN with multiple\\noutput nodes is used for multi-class classfication. After the naive\\noutput values are calculated, the output nodes will use the softmax\\nactivation function to transform the values into the probabilities for\\neach of the three classes. That is, the probability for predicting each\\nclass will be calculated as:\\nbo1 = so f tmax(o1, o2, o3)1\\nbo2 = so f tmax(o1, o2, o3)2\\nbo3 = so f tmax(o1, o2, o3)3\\n(11.4)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 127}, page_content='128\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nExample 11.1.2. If the provided input vector to the neural network in\\nFigure 11.1 is⃗x = (1, 1, 1), we can calculate the output layer as\\no1 = −4 + 0 = −4\\no2 = 2 · 4 + 0 = 8\\no3 = 4 + 0 = 4\\nand the probabilities of each class as\\nbo1 = so f tmax(−4, 8, 4)1 =\\ne−4\\ne−4 + e8 + e4 ≃0.00\\nbo2 = so f tmax(−4, 8, 4)2 =\\ne8\\ne−4 + e8 + e4 ≃0.98\\nbo3 = so f tmax(−4, 8, 4)3 =\\ne4\\ne−4 + e8 + e4 ≃0.02\\n11.1.3\\nMatrix Notation\\nLet w(1)\\ni,j be the weight between the i-th node h(1)\\ni\\nin the first hidden\\nlayer and the j-th input xj. Then (11.1) can be rewritten as\\nh(1)\\n1\\n= ReLU(w(1)\\n1,1 x1 + w(1)\\n1,2 x2 + w(1)\\n1,3 x3)\\nh(1)\\n2\\n= ReLU(w(1)\\n2,1 x1 + w(1)\\n2,2 x2 + w(1)\\n2,3 x3)\\nNotice that if we set⃗x = (x1, x2, x3) ∈R3 and ⃗h(1) = (h(1)\\n1 , h(1)\\n2 ) ∈R2\\nand define a matrix W(1) ∈R2×3 where its (i, j) entry is w(1)\\ni,j , then we\\ncan further rewrite (11.1) as 1\\n1 Here we interpret the vectors⃗x, ⃗h(1) as\\ncolumn vectors, or equivalently a 3 × 1\\nmatrix and a 2 × 1 matrix respectively.\\nThis will be a convention throughout\\nthis chapter.\\n⃗h(1) = ReLU\\n\\x10\\nW(1)⃗x\\n\\x11\\n(11.5)\\nwhere the ReLU function is applied element-wise.\\nSimilarly, if we let w(2)\\ni,j be the weight between the i-th node h(2)\\ni\\nin the second hidden layer and the j-th node h(1)\\nj\\nin the first hidden\\nlayer, (11.2) can be rewritten as\\nh(2)\\n1\\n= ReLU(w(2)\\n1,1 h(1)\\n1\\n+ w(2)\\n1,2 h(1)\\n2 )\\nh(2)\\n2\\n= ReLU(w(2)\\n2,1 h(1)\\n1\\n+ w(2)\\n2,2 h(1)\\n2 )\\nor in a matrix notation as\\n⃗h(2) = ReLU\\n\\x10\\nW(2)⃗h(1)\\x11\\n(11.6)\\nwhere ⃗h(2) = (h(2)\\n1 , h(2)\\n2 ) ∈R2 and W(2) ∈R2×2 is a matrix whose\\n(i, j) entry is w(2)\\ni,j .'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 128}, page_content='feedforward neural network and backpropagation\\n129\\nNext, if we let w(o)\\ni,j be the weight between the i-th output node oi\\n(before softmax) and the j-th node h(2)\\nj\\nin the second hidden layer,\\n(11.3) can be rewritten as\\no1 = w(o)\\n1,1h(2)\\n1\\n+ w(o)\\n1,2h(2)\\n2\\no2 = w(o)\\n2,1h(2)\\n1\\n+ w(o)\\n2,2h(2)\\n2\\no3 = w(o)\\n3,1h(2)\\n1\\n+ w(o)\\n3,2h(2)\\n2\\nor in a matrix notation as\\n⃗o = W(o)⃗h(2)\\n(11.7)\\nwhere ⃗o = (o1, o2, o3) ∈R3 and W(o) ∈R3×2 is a matrix whose (i, j)\\nentry is w(o)\\ni,j .\\nFinally, if we let ⃗bo = (bo1, bo2, bo3) ∈R3, then (11.4) can be rewritten\\nas\\n⃗bo = so f tmax(⃗o)\\n(11.8)\\nWe summarize the results above into the following matrix equations\\n⃗h(1) = ReLU\\n\\x10\\nW(1)⃗x\\n\\x11\\n⃗h(2) = ReLU\\n\\x10\\nW(2)⃗h(1)\\x11\\n⃗o = W(o)⃗h(2)\\n⃗bo = so f tmax(⃗o)\\n(11.9)\\nExample 11.1.3. If the provided input vector to the neural network in\\nFigure 11.2 is⃗x = (1, 1, 1), we can calculate the first hidden layer as\\n⃗h(1) = W(1)⃗x = ReLU\\n\\uf8eb\\n\\uf8ec\\n\\uf8ed\\n\"\\n2\\n−3\\n0\\n−1\\n1\\n2\\n# \\uf8ee\\n\\uf8ef\\uf8f0\\n1\\n1\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fb\\n\\uf8f6\\n\\uf8f7\\n\\uf8f8=\\n\"\\n0\\n2\\n#\\nand the second hidden layer as\\n⃗h(2) = W(2)⃗h(1) = ReLU\\n \"\\n1\\n2\\n2\\n−2\\n# \"\\n0\\n2\\n#!\\n=\\n\"\\n4\\n0\\n#\\nand the output layer⃗o (before the softmax) as\\n⃗o = W(o)⃗h(2) =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n−1\\n2\\n2\\n1\\n1\\n2\\n\\uf8f9\\n\\uf8fa\\uf8fb\\n\"\\n4\\n0\\n#\\n=\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n−4\\n8\\n4\\n\\uf8f9\\n\\uf8fa\\uf8fb\\nThe probability distribution⃗bo of the three classes can then be calculated as\\n⃗bo = so f tmax(⃗o) =\\n\\x12\\ne−4\\ne−4 + e8 + e4 ,\\ne8\\ne−4 + e8 + e4 ,\\ne4\\ne−4 + e8 + e4\\n\\x13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 129}, page_content='130\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n11.2\\nForward Propagation: The General Case\\nWe now consider an arbitrary feedforward neural network with L ≥1\\nlayers. Let⃗x ∈Rd0 be the vector of d0 inputs to the network. For\\nk = 1, 2, . . . , L, let ⃗h(k) = (h(k)\\n1 , h(k)\\n2 , . . . , h(k)\\ndk ) ∈Rdk represent the dk\\noutputs (values of each of the nodes) of the k-th layer. The L-th layer\\nis also known as the output layer, and we alternatively denote d0 = din\\nand dL = dout to emphasize that they are respectively the number of\\ninputs and the number of output nodes. Each of the k-th layer where\\n1 ≤k ≤L −1 is considered a hidden layer, but for convenience, we may\\nabuse notation and refer to the input/output layers as respectively\\nthe 0-th and L-th hidden layers as well.\\nAdditionally, we consider W(k) ∈Rdk×dk−1 to represent the weights\\nfor the k-th hidden layer. Its (i, j) entry is the weight between the\\ni-th node h(k)\\ni\\nof the k-th hidden layer and the j-th node h(k−1)\\nj\\nof the\\n(k −1)-th hidden layer. We also alternatively denote W(L) = W(o) to\\nemphasize that it represents the weights for the output layer.\\nFinally, let f (k) be the nonlinear activation function for layer k.\\nFor instance, consider the output layer. If dout = 1 (i.e., there is one\\noutput node), we can assume that f (L) is the identity function. On\\nthe other hand, if dout > 1 (i.e., there are multiple output nodes), we\\ncan assume that f (L) is the softmax function. It is also possible to use\\ndifferent activation functions for each layer.\\nWith all these new notations in mind, we can express the nodes of\\nlayer k as:\\n⃗h(k) = f (k)(W(k)⃗h(k−1))\\nfor each k = 1, 2, . . . , L.\\nIf dout = 1, we let o = W(L)⃗hL−1 denote the final output of the\\nmodel. If dout > 1, we let ⃗o = W(L)⃗hL−1 denote the output layer\\nbefore the softmax and ⃗bo = f (L)(⃗o) denote the output layer after the\\nsoftmax.\\n11.2.1\\nNumber of Weights\\nWe now briefly consider the number of weights in a feedforward\\nnetwork. There are din · d1 weights (or variables) for the first hidden\\nlayer. Similarly, there are d1 · d2 weights for the second hidden layer. In\\ntotal, the number of weights is\\nL−1\\n∑\\ni=0\\ndi · di+1.\\nExample 11.2.1. The number of weights in the model in Figure 11.2 can be\\ncalculated as\\n3 × 2 + 2 × 2 + 2 × 3 = 16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 130}, page_content='feedforward neural network and backpropagation\\n131\\n11.2.2\\nWhat If We Remove Nonlinearity?\\nIf we removed the nonlinear activation function ReLU in our model\\nfrom (11.9), we would have the following forward propagation equa-\\ntions:\\n⃗h(1) = W(1)⃗x\\n⃗h(2) = W(2)⃗h(1)\\n⃗o = W(o)⃗h(2)\\n⃗bo = so f tmax(⃗o)\\nNotice that if we set W′ = W(o)W(2)W(1) ∈R3×3, then\\n⃗bo = so f tmax(W′⃗x)\\nWe have thus reduced our neural net to a standard multi-classification\\nlogistic regression model! As we have discussed the limitation of lin-\\near models earlier, this indicates the importance of having nonlinear\\nactivation functions between layers.\\n11.2.3\\nTraining Loss\\nJust like we have defined a loss function for ML models so far, we\\nalso define an appropriate loss function for neural networks, where\\nthe objective of the network becomes finding a set of weights that\\nminimize the loss. While there are many different definitions of loss\\nfunctions, here we present two — one that is more appropriate when\\nthere is a single output node, and another that is more appropriate\\nfor multi-class classification.\\nWhen there is only one scalar node in the output layer (i.e., dout =\\n1), we can use a squared error loss, similar to the least squares loss\\nfrom (1.4):\\n∑\\n(⃗x,y)∈D\\n(y −o)2\\n(Squared Error Loss)\\nwhere⃗x ∈Rdin is the input vector, y is its gold value (i.e., actual value\\nin the training data), and o = W(o)⃗h(L−1) is the final output (i.e.,\\nprediction) of the neural network.\\nExample 11.2.2. If the provided input vector to the neural network in\\nFigure 11.1 is⃗x = (1, 1, 1) and the training output is y = 0, we can\\ncalculate the squared error loss as\\n(y −o)2 = (0 −(−4))2 = 16\\nWhen there are multiple nodes in the output layer (e.g., for multi-\\nclass classification), we can use a loss function that is similar to the\\nlogistic loss. Recall that in logistic regression, we defined the logistic'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 131}, page_content='132\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nloss as a sum of log loss over a set of data points:\\n∑\\n(⃗x,y)∈D\\n−log Pr[label y on input⃗x]\\n(4.5 revisited)\\nwhere y ∈{−1, 1} denotes the gold label. For neural networks, we\\ncan analogously define the cross-entropy loss:\\n∑\\n(⃗x,y)∈D\\n−log boy\\n(Cross-Entropy Loss)\\nwhere y ∈{1, . . . , dout} denotes the gold label, and boy denotes the\\nprobability that the model assigns to class y — that is, the y-th coordi-\\nnate of the output vector ⃗bo = so f tmax(⃗o) after applying the softmax\\nfunction.\\nExample 11.2.3. If the provided input vector to the neural network in\\nFigure 11.2 is⃗x = (1, 1, 1) and the training output is y = 2, we can\\ncalculate the cross-entropy loss for this data point as\\n−log boy = −log\\ne4\\ne−4 + e8 + e4 ≃4.02\\n11.3\\nBackpropagation: An Example\\nJust like in previous ML models we have learned, the process of\\ntraining a neural network model involves three steps:\\n1. Defining an an appropriate loss function.\\n2. Calculating the gradient of the loss with respect to the training\\ndata and the model parameters.\\n3. Iteratively updating the parameters via the gradient descent\\nalgorithm.\\nBut once a neural network grows in size, the second step of cal-\\nculating the gradients starts to become a problem. Naively trying to\\ncalculate each of the gradients separately becomes inefficient. Instead,\\nBackpropagation 2 is an efficient way to calculate the gradients with\\n2 Reference: https://www.nature.com/\\narticles/323533a0\\nrespect to the network parameters such that the number of steps for\\nthe computation is linear in the size of the neural network.\\nThe key idea is to perform the computation in a very specific\\nsequence — from the output layer to the input layer. By the Chain\\nRule, we can use the already computed gradient value of a node in a\\nhigher layer in the computation of the gradient of a node in a lower\\nlayer. This way, the gradient values propagate back from the top layer\\nto the bottom layer.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 132}, page_content='feedforward neural network and backpropagation\\n133\\n11.3.1\\nThe Delta Method: Reasoning from First Principles\\nThe main goal of backpropagation is to compute the partial deriva-\\ntive ∂f /∂w where f is the loss and w is the weight of an edge. This\\nwill allow us to apply the gradient descent algorithm and appro-\\npriately update the weight w. Students often find backpropagation\\na confusing idea, but it is actually just a simple application of the\\nChain Rule in multivariate calculus.\\nIn this subsection, we motivate the topic with the Delta Method,\\nwhich is an intuitive way to compute ∂f /∂w. We perturb a weight w\\nby a small amount ∆and measure how much the output changes. In\\ndoing so, we also measure how the rest of the network changes. As\\nwe will see later, the process of computing the partial derivative of\\nthe form ∂f /∂w requires us to also compute the partial derivative of\\nthe form ∂f /∂h where h is the value at a node.\\nReaders who are familiar with the Chain Rule can quickly browse\\nthrough the rest of this subsection.\\nExample 11.3.1. Consider the model from Figure 11.2 but now with the\\ninputs⃗x = (3, 1, 2). We use the same notation for the nodes and the weights\\nthat we used throughout Section 11.1. The goal of this simple example is to\\nillustrate what the derivatives mean.\\nFigure 11.3: The model from Figure 11.2\\nwith inputs⃗x = (3, 1, 2).\\nSuppose we want to take the partial derivative of first output node o1\\nwith respect to the weight w(1)\\n2,2 (i.e., the weight on the edge between the\\nsecond input x2 and the second node h(1)\\n2\\nof first hidden layer). This is\\ndenoted as ∂o1/∂w(1)\\n2,2. Its definition involves considering how changing\\nw(1)\\n2,2 by an infinitesimal amount ∆changes o1, whose current value is −3.\\nAdding ∆to w(1)\\n2,2 will change the values of the first hidden layer to\\nh(1)\\n1\\n= ReLU(2 · 3 + (−3) · 1 + 0 · 2) = 3\\nh(1)\\n2\\n= ReLU((−1) · 3 + (1 + ∆) · 1 + 2 · 2) = 2 + ∆\\nLetting ∆→0, we see that the rate of change of h(1)\\n1\\nand h(1)\\n2\\nwith respect to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 133}, page_content='134\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nFigure 11.4: The model from Figure 11.2\\nwith inputs⃗x = (3, 1, 2). When w(1)\\n2,2 is\\nchanged by a small number ∆, the first\\nhidden layer is also affected.\\nchange of w(1)\\n2,2 is 0 and 1 respectively. In more formal terms, ∂h(1)\\n1 /∂w(1)\\n2,2 =\\n0 and ∂h(1)\\n2 /∂w(1)\\n2,2 = 1.\\nFigure 11.5: The model from Figure 11.2\\nwith inputs⃗x = (3, 1, 2). When w(1)\\n2,2\\nis changed by a small number ∆, the\\nsecond hidden layer is also affected.\\nUsing the updated values of the first hidden layer, the second hidden layer\\nwill be calculated as\\nh(2)\\n1\\n= ReLU(1 · 3 + 2 · (2 + ∆)) = 7 + 2∆\\nh(2)\\n2\\n= ReLU(2 · 3 + (−2) · (2 + ∆)) = 2 −2∆\\nThis shows that the rate of change of h(2)\\n1\\nand h(2)\\n2\\nwith respect to change of\\nw(1)\\n2,2 is 2 and −2 respectively.\\nFinally the output layer can now be calculated as\\no1 = (−1) · (7 + 2∆) + 2 · (2 −2∆) = −3 −6∆\\no2 = 2 · (7 + 2∆) + 1 · (2 −2∆) = 16 + 2∆\\no3 = 1 · (7 + 2∆) + 2 · (2 −2∆) = 11 −2∆\\nThis shows that the rate of change of o1 with respect to change of w(1)\\n2,2 is −6.\\nExample 11.3.2. Now we consider the meaning of ∂o1/∂h(2)\\n1 : how changing\\nthe value of h(2)\\n1\\nby an infinitesimal ∆affects o1. Note that this is a thought'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 134}, page_content='feedforward neural network and backpropagation\\n135\\nFigure 11.6: The model from Figure 11.2\\nwith inputs⃗x = (3, 1, 2). When w(1)\\n2,2\\nis changed by a small number ∆, the\\noutput layer is also affected.\\nexperiment that does not correspond to a change that is possible if the net\\nwere a physical object constructed of nodes and wires — the value of h(2)\\n1\\nis\\ncompletely decided by the previous layers and cannot be changed in isolation\\nwithout touching the previous layers. However, treating these values as\\nvariables, it is possible to put on a calculus hat and and think about the rate\\nof change of one with respect to the other.\\nIf only the value of h(2)\\n1\\nis changed from 7 to 7 + ∆in Figure 11.3, then o1\\ncan be calculated as\\no1 = (−1) · (7 + ∆) + 2 · 2 = −3 −∆\\nwhich shows that ∂o1/∂h(2)\\n1\\n= −1.\\nProblem 11.3.3. Following the calculations in Example 11.3.1 and Ex-\\nample 11.3.2, calculate ∂o1/∂h(2)\\n2 , ∂h(2)\\n1 /∂w(1)\\n2,2, and ∂h(2)\\n2 /∂w(1)\\n2,2. Verify\\nthat\\n∂o1\\n∂w(1)\\n2,2\\n= ∂o1\\n∂h(2)\\n1\\n· ∂h(2)\\n1\\n∂w(1)\\n2,2\\n+ ∂o1\\n∂h(2)\\n2\\n· ∂h(2)\\n2\\n∂w(1)\\n2,2\\nProblem 11.3.4. Following the calculations in Example 11.3.1 and Exam-\\nple 11.3.2, calculate ∂h(2)\\n1 /∂h(1)\\n2 , ∂h(2)\\n2 /∂h(1)\\n2 , and ∂h(1)\\n2 /∂w(1)\\n2,2. Verify that\\n∂o1\\n∂w(1)\\n2,2\\n= ∂o1\\n∂h(2)\\n1\\n· ∂h(2)\\n1\\n∂h(1)\\n2\\n· ∂h(1)\\n2\\n∂w(1)\\n2,2\\n+ ∂o1\\n∂h(2)\\n2\\n· ∂h(2)\\n2\\n∂h(1)\\n2\\n· ∂h(1)\\n2\\n∂w(1)\\n2,2\\n(11.10)\\nSome readers may notice that (11.10) is just the result of the Chain\\nRule in multivariable calculus. It shows that the effect of w(1)\\n2,2 on o1 is\\nthe sum of its effect through all possible paths that the values prop-\\nagate through the network, and the amount of effect for each path\\ncan be calculated by multiplying the appropriate partial derivative\\nbetween each layer.\\nIn this section, we calculated by hand one partial derivative\\n∂o1/∂w(1)\\n2,2. But in general, to compute the loss gradient, we see below\\nthat we want to calculate the partial derivative of each output node'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 135}, page_content='136\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\noi with respect to each weight in the network. Manually applying the\\nChain Rule for each partial derivative as in (11.10) is too inefficient.3\\n3 Putting on your COS 226 hat, you can\\ncheck that the computational cost of\\nthis naive method scales quadratically\\nin the size of the network.\\nInstead, in the next section, we will learn how to utilize matrix opera-\\ntions to combine the computation for multiple partial derivatives into\\none process.4\\n4 This efficiency holds even without\\ntaking into account the fact that today’s\\nGPUs are optimized for fast matrix\\noperations.\\n11.4\\nBackpropagation: The General Case\\n11.4.1\\nJacobian Matrix\\nSuppose some vector ⃗y = (y1, y2, . . . , ym) ∈Rm is a function of\\n⃗x = (x1, x2, . . . , xn) ∈Rn — that is, there is a mapping f : Rn →Rm\\nsuch that ⃗y = f (⃗x), or equivalently, there are m functions fi : Rn →R\\nfor each i = 1, 2, . . . , m such that yi = fi(⃗x).\\nThen the Jacobian matrix of ⃗y with respect to⃗x, denoted as J(⃗y,⃗x),\\nis an m × n matrix whose (i, j) entry is the partial derivative ∂yi/∂xj.\\nNote that each entry of this matrix is itself a function of⃗x. A bit\\nconfusingly, a Jacobian matrix is also often denoted as ∂⃗y/∂⃗x when it\\nis clear from the context that⃗x,⃗y are vectors and hence this object is\\nnot a partial derivative or gradient. 5\\n5 Note that the i-th row of the Jacobian\\nmatrix contains the gradient of yi, i.e.\\nthe gradient of the i-th coordinate of ⃗y.\\nThe mathematical interpretation of the Jacobian matrix is that if\\nwe change⃗x such that each coordinate xi is updated to xi + δi for an\\ninfinitesimal value δi, then the output ⃗y changes to ⃗y + J(⃗y,⃗x)⃗δ.\\nExample 11.4.1. Suppose⃗y is a linear function of⃗x — that is, there exists\\na matrix A ∈Rm×n such that⃗y = A⃗x. Then notice that yi, the i-th\\ncoordinate of⃗y, can be expressed as\\nyi = Ai,∗⃗x = Ai,1x1 + Ai,2x2 + . . . + Ai,nxn\\nNotice that the partial derivative ∂yi/∂xj is equal to Aij. This means that\\nthe (i, j) entry of the Jacobian matrix is the (i, j) entry of the matrix A, and\\nhence J(⃗y,⃗x) = A.\\nProblem 11.4.2. If⃗y ∈R2 is a function of⃗x ∈R3 such that\\ny1 = 2x1 −x2 + 3x3\\ny2 = −x1 + 2x3\\nthen what is the Jacobian matrix J(⃗y,⃗x)?\\nExample 11.4.3. If⃗x ∈Rn and⃗y = ReLU(⃗x) ∈Rn, then notice that\\n∂yi\\n∂xi\\n=\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n1\\nxi > 0\\n0\\notherwise\\nWe can also denote this with an indicator function 1(xi > 0). Also for any\\nj ̸= i, we see that ∂yi/∂xj = 0. Therefore, the Jacobian matrix J(⃗y,⃗x) is a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 136}, page_content='feedforward neural network and backpropagation\\n137\\ndiagonal matrix whose entry down the diagonal is 1(xi > 0); that is\\nJ(⃗y,⃗x) = diag(1(⃗x > 0))\\nwhere we take the indicator function element-wise to the vector⃗x.\\nDefinition 11.4.4 (Jacobian Chain Rule). Suppose vector⃗z ∈Rk is a\\nfunction of⃗y ∈Rm and⃗y is a function of⃗x ∈Rn, then by the Chain Rule,\\nthe Jacobian matrix J(⃗z,⃗x) ∈Rk×n is represented as the matrix product:\\nJ(⃗z,⃗x) = J(⃗z,⃗y)J(⃗y,⃗x)\\n(11.11)\\nIn context of the feedforward neural network, each hidden layer is\\na function of the previous layer. Specifically, vector of activations of a\\nhidden layer is a function of the vector of activations of the previous\\nlayer as well as of the trainable weights within the layer.\\nExample 11.4.5 (Gradient calculation for a single layer with ReLU’s).\\nIf⃗x ∈Rn, A ∈Rm×n,⃗y = A⃗x ∈Rm and⃗z = ReLU(⃗y) ∈Rm, then the\\nJacobian matrix J(⃗z,⃗x) can be calculated as\\nJ(⃗z,⃗x) = J(⃗z,⃗y)J(⃗y,⃗x) = diag(1(A⃗x > 0))A\\n11.4.2\\nBackpropagation — Efficiency Using Jacobian Viewpoint\\nNow we return to backpropagation, and show how the Jacobian\\nviewpoint allows computing the gradient of the loss (with respect to\\nnetwork parameters) with a number of mathematical operations (i.\\ne., additions and multiplications) proportional to the size of the fully\\nconnected net.\\nRecall that we want to find the weights W(1), W(2), . . . , W(o) that\\nminimize the cross-entropy loss ℓ. To apply the standard/stochastic\\ngradient descent algorithm, we need to find the partial derivative\\n∂ℓ\\n∂W(k)\\ni,j\\nof the loss function with respect to each weight W(k)\\ni,j of each\\nlayer k.\\nTo simplify notations, we introduce a new matrix\\n∂ℓ\\n∂W(k) which has\\nthe same dimensions as W(k) (e.g.,\\n∂ℓ\\n∂W(1) ∈R2×3 in Figure 11.2) and\\nthe (i, j) entry of the matrix is:\\n\\x12\\n∂ℓ\\n∂W(k)\\n\\x13\\ni,j\\n=\\n∂ℓ\\n∂W(k)\\ni,j\\n(11.12)\\nfor any layer k. The matrix\\n∂ℓ\\n∂W(k) will be called the gradient with\\nrespect to the weights of the k-th layer. 6 Now the update rule for the\\n6 Alternatively, you can think of flat-\\ntening W(k) into a single vector, then\\nfinding the Jacobian matrix ∂ℓ/∂W(k),\\nand later converting it back to a matrix\\nform.\\ngradient descent algorithm can be written as the following:\\nW(k) →W(k) −η ·\\n∂ℓ\\n∂W(k)\\n(11.13)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 137}, page_content='138\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nwhere η is the learning rate. Now the question remains as how to\\ncalculate these gradients. As the name “backpropagation” suggests,\\nwe will first compute the gradient of the loss ℓwith respect to the\\noutput nodes; we then inductively compute the gradient for the\\nprevious layers, until we reach the input layer.\\n1. Output Layer:\\nFirst recall that the cross-entropy loss due to one\\ndata point is\\nℓ= −log\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\neoy\\ndout\\n∑\\ni=1\\neoi\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n= −log(eoy) + log\\n dout\\n∑\\ni=1\\neoi\\n!\\n= −oy + log\\n dout\\n∑\\ni=1\\neoi\\n!\\nwhere y ∈{1, 2, . . . , dout} is the ground truth value. Therefore, the\\ngradient with respect to the output layer is\\n∂ℓ\\n∂oi 1≤i≤dout\\n=\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n−1 + boi\\ny = i\\nboi\\ny ̸= i\\nTo simplify notations, we introduce a one-hot encoding vector⃗ey, which\\nhas 1 only at the y-th coordinate and 0 everywhere else. Then, we can\\nrewrite the equation above as: 7\\n7 Note that ∂ℓ/∂⃗o, the term on the\\nleft hand side, is a Jacobian matrix in\\nR1×dout. But ⃗bo and⃗ey, the terms on\\nthe right hand side, are both column\\nvectors, or equivalently a dout × 1 matrix.\\nWe resolve the problem by taking the\\ntranspose.\\n∂ℓ\\n∂⃗o =\\n\\x10\\n⃗bo −⃗ey\\n\\x11⊺\\n∈R1×dout\\n(11.14)\\nThis is the Jacobian matrix of the loss ℓwith respect to the output\\nlayer ⃗o.\\n2. Jacobian With Respect To Hidden Layer:\\nWe first compute ∂ℓ/∂⃗h(L−1),\\nthe Jacobian matrix with respect to the last hidden layer before the\\noutput layer. Since ⃗o = W(o)⃗h(L−1), we can apply the result from\\nExample 11.4.1 and get\\n∂ℓ\\n∂⃗h(L−1) = ∂ℓ\\n∂⃗o J(⃗o,⃗h(L−1))\\n= ∂ℓ\\n∂⃗oW(o)\\n(11.15)\\nNow as an inductive hypothesis, assume that we have already com-\\nputed the gradient (or Jacobian matrix) ∂ℓ/∂⃗h(k+1). We now compute'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 138}, page_content='feedforward neural network and backpropagation\\n139\\n∂ℓ/∂⃗h(k) using the result from Example 11.4.5.\\n∂ℓ\\n∂⃗h(k) =\\n∂ℓ\\n∂⃗h(k+1) J(⃗h(k+1),⃗h(k))\\n=\\n∂ℓ\\n∂⃗h(k+1) diag(1(W(k+1)⃗h(k) > 0))W(k+1)\\n(11.16)\\n3. Gradient With Respect to Weights:\\nWe first compute ∂ℓ/∂W(o), the\\ngradients with respect to the weights of the output layer. Notice that\\na particular weight w(o)\\ni,j is only used in computing oi out of all output\\nnodes:\\noi = w(o)\\ni,1 h(L−1)\\n1\\n+ . . . + w(o)\\ni,j h(L−1)\\nj\\n+ . . . + w(o)\\ni,dL−1h(L−1)\\ndL−1\\nTherefore, the gradient with respect to w(o)\\ni,j can be calculated as\\n∂ℓ\\n∂w(o)\\ni,j\\n= ∂ℓ\\n∂oi\\n·\\n∂oi\\n∂w(o)\\ni,j\\n= ∂ℓ\\n∂oi\\n· h(L−1)\\nj\\nWe can combine these results into the following matrix form\\n∂ℓ\\n∂W(o) =\\n\\x12 ∂ℓ\\n∂⃗o\\n\\x13⊺\\x10\\n⃗h(L−1)\\x11⊺\\n(11.17)\\nNow as an inductive hypothesis, assume that we have already\\ncomputed the gradient (or Jacobian) ∂ℓ/∂⃗h(k). We now compute\\n∂ℓ/∂W(k).\\nTo do this, we introduce an intermediate variable⃗z(k) = W(k)⃗h(k−1)\\nsuch that ⃗h(k) = ReLU(⃗z(k)). Then the gradient with respect to a par-\\nticular weight w(k)\\ni,j can be calculated as\\n∂ℓ\\n∂w(k)\\ni,j\\n=\\n∂ℓ\\n∂z(k)\\ni\\n· ∂z(k)\\ni\\n∂w(k)\\ni,j\\n=\\n∂ℓ\\n∂z(k)\\ni\\n· h(k−1)\\nj\\nWe can combine these results into the following matrix form\\n∂ℓ\\n∂W(k) =\\n\\x12 ∂ℓ\\n∂⃗z(k)\\n\\x13⊺\\x10\\n⃗h(k−1)\\x11⊺\\n=\\n\\x12 ∂ℓ\\n∂⃗h(k) J(⃗h(k),⃗z(k))\\n\\x13⊺\\x10\\n⃗h(k−1)\\x11⊺\\n=\\n\\x10\\nJ(⃗h(k),⃗z(k))\\n\\x11⊺\\x12 ∂ℓ\\n∂⃗h(k)\\n\\x13⊺\\x10\\n⃗h(k−1)\\x11⊺\\n= diag(1(W(k)⃗h(k−1) > 0))\\n\\x12 ∂ℓ\\n∂⃗h(k)\\n\\x13⊺\\x10\\n⃗h(k−1)\\x11⊺\\n(11.18)\\n4. Full Backpropagation Process\\nWe summarize the results above into\\nthe following four steps:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 139}, page_content='140\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n1. Compute the Jacobian matrix with respect to the output layer,\\n∂ℓ\\n∂⃗o ∈R1×dout:\\n∂ℓ\\n∂⃗o =\\n\\x10\\n⃗bo −⃗ey\\n\\x11⊺\\n((11.14) revisited)\\n2. Compute the Jacobian matrix with respect to the last hidden layer,\\n∂ℓ\\n∂⃗h(L−1) ∈R1×dL−1:\\n∂ℓ\\n∂⃗h(L−1) = ∂ℓ\\n∂⃗oW(o)\\n((11.15) revisited)\\nThen, compute the gradient with respect to the output weights,\\n∂ℓ\\n∂W(o) ∈Rdout×dL−1:\\n∂ℓ\\n∂W(o) =\\n\\x12 ∂ℓ\\n∂⃗o\\n\\x13⊺\\x10\\n⃗h(L−1)\\x11⊺\\n((11.17) revisited)\\n3. For each successive layer k, calculate the Jacobian matrix with\\nrespect to the k-th hidden layer\\n∂ℓ\\n∂⃗h(k) ∈R1×dk:\\n∂ℓ\\n∂⃗h(k) =\\n∂ℓ\\n∂⃗h(k+1) diag(1(W(k+1)⃗h(k) > 0))W(k+1) ((11.16) revisited)\\nNext, compute the gradient with respect to the (k + 1)-th hidden\\nlayer weights\\n∂ℓ\\n∂W(k+1) ∈Rdk+1×dk:\\n∂ℓ\\n∂W(k+1) = diag(1(W(k+1)⃗h(k) > 0))\\n\\x12\\n∂ℓ\\n∂⃗h(k+1)\\n\\x13⊺\\x10\\n⃗h(k)\\x11⊺\\n((11.18) revisited)\\n4. Repeat Step 3 until we reach the input layer.\\nNote that these instructions are based on a model that adopts the\\ncross-entropy loss and the ReLU activation function. Using alterna-\\ntive losses and/or activation functions would result in a similar form,\\nalthough the actual derivatives may be slightly different.\\nProblem 11.4.6. (i) Show that if A is an m × n matrix and ⃗h ∈Rn then\\ncomputing A⃗h requires mn multiplications and m vector additions. (ii)\\nUsing the previous part, argue that the number of arithmetic operations\\n(additions or multiplications) in backpropagation algorithm on a fully\\nconnected net with ReLU activations is proportional to the number of\\nparameters in the net.\\nWhile the above calculation is in line with your basic algorithmic\\ntraining, it doesn’t exactly describe running time in modern ML\\nenvironments with GPUs, since certain operations are parallelized,\\nand compilers are optimized to run backpropagation as fast as\\npossible.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 140}, page_content='feedforward neural network and backpropagation\\n141\\n11.4.3\\nUsing a Different Activation Function\\nWe briefly consider what happens if we choose a different activation\\nfunction for the hidden layers. Consider the sigmoid activation\\nfunction σ(z) =\\n1\\n1+e−z . Its derivative is given by:\\nσ′(z) =\\n\\x12\\n1\\n1 + e−z\\n\\x13′\\n=\\ne−z\\n(1 + e−z)2\\n= σ(z) ·\\n\\x12\\ne−z\\n1 + e−z\\n\\x13\\n= σ(z) · (1 −σ(z))\\n(11.19)\\nThere is also the hyperbolic tangent function tanh(z) = e2z−1\\ne2z+1.\\nProblem 11.4.7. Compute f ′(z) for f (z) = tanh(z); show how f ′(z) can\\nbe written in terms of f (z).\\nProblem 11.4.8. Say a neural network uses an activation function f (z) at\\nlayer k + 1 such that f ′(z) is a function of f (z). That is, f ′(z) = g( f (z))\\nfor some function g. Then verify that (11.16, 11.18) can be rewritten as:\\n∂ℓ\\n∂⃗h(k) =\\n∂ℓ\\n∂⃗h(k+1) diag(g(W(k+1)⃗h(k)))W(k+1)\\n∂ℓ\\n∂W(k+1) = diag(g(W(k+1)⃗h(k)))\\n\\x12\\n∂ℓ\\n∂⃗h(k+1)\\n\\x13⊺\\x10\\n⃗h(k)\\x11⊺\\n11.4.4\\nFinal Remark\\nDirectly following the steps of backpropagation is complicated and\\ninvolves a lot of calculations. But remember that backpropagation\\nis simply computing gradients by the Chain Rule. At a high level, we\\ncan think of the loss as a function of inputs and all the weights and\\nnote that backpropagation simply entails calculating derivatives\\nwith respect to each variable. The good news is that modern deep\\nlearning software does all the gradient calculations for users. All\\nthe model designer needs to do is to determine the neural network\\narchitecture (e.g., choose number of layers, number of hidden units,\\nand the activation functions).\\nOne note of caution is that the loss function for deep neural nets\\nis highly non-convex with respect to the parameters of the network.\\nJust as we discussed in Chapter 3, the gradient descent algorithm is\\nnot guaranteed to find the actual minimizer in such situation, and the\\nchoice of the initial values of the parameters matter a lot.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 141}, page_content='142\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n11.5\\nFeedforward Neural Network in Python Programming\\nIn this section, we discuss how to write Python code to build neural\\nnetwork models and perform forward propagation and backpropaga-\\ntion. As usual, we use the numpy package to speed up computation.\\nAdditionally, we use the torch package to easily design and train the\\nneural network.\\n# import necessary packages\\nimport numpy as np\\nimport torch\\nimport torch.nn as nn\\n# define the neural network\\nclass Net(nn.Module):\\ndef __init__(self, input_size=2, hidden_dim1=2, hidden_dim2=3,\\nhidden_dim3=2):\\nsuper(Net, self).__init__()\\nself.hidden1 = nn.Linear(input_size, hidden_dim1, bias=False)\\nself.hidden1.weight = nn.Parameter(torch.tensor([[-2., 1.], [3., -1.\\n]]))\\nself.hidden2 = nn.Linear(hidden_dim1, hidden_dim2, bias=False)\\nself.hidden2.weight = nn.Parameter(torch.tensor([[0., 1.], [2., -1.]\\n, [1., 2.]]))\\nself.hidden3 = nn.Linear(hidden_dim2, hidden_dim3, bias=False)\\nself.hidden3.weight = nn.Parameter(torch.tensor([[-1., 2., 1.], [3.,\\n0., 0.]]))\\nself.activation = nn.ReLU()\\n# single step of forward propagation\\ndef forward(self, x):\\nh1 = self.hidden1(x)\\nh1 = self.activation(h1)\\nh2 = self.hidden2(h1)\\nh2 = self.activation(h2)\\nh3 = self.hidden3(h2)\\nreturn h3\\nnet = Net()\\n# forward propagation with sample input\\nx = torch.tensor([3., 1.])\\ny_pred = net.forward(x)\\nprint(’Predicted value:’, y_pred)\\n# backpropagation with sample input\\nloss = nn.functional.cross_entropy(y_pred.unsqueeze(0), torch.LongTensor([1]\\n))\\nloss.backward()\\nprint(loss)\\nprint(net.hidden1.weight.grad)\\nWe start the code by importing all necessary packages.\\nimport numpy as np\\nimport torch'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 142}, page_content='feedforward neural network and backpropagation\\n143\\nimport torch.nn as nn\\nWith PyTorch, we can design the architecture of any neural net-\\nwork by defining the corresponding class.\\nclass Net(nn.Module):\\ndef __init__(self, input_size=2, hidden_dim1=2, hidden_dim2=3,\\nhidden_dim3=2):\\nsuper(Net, self).__init__()\\nself.hidden1 = nn.Linear(input_size, hidden_dim1, bias=False)\\nself.hidden1.weight = nn.Parameter(torch.tensor([[-2., 1.], [3., -1.\\n]]))\\nself.hidden2 = nn.Linear(hidden_dim1, hidden_dim2, bias=False)\\nself.hidden2.weight = nn.Parameter(torch.tensor([[0., 1.], [2., -1.]\\n, [1., 2.]]))\\nself.hidden3 = nn.Linear(hidden_dim2, hidden_dim3, bias=False)\\nself.hidden3.weight = nn.Parameter(torch.tensor([[-1., 2., 1.], [3.,\\n0., 0.]]))\\nself.activation = nn.ReLU()\\ndef forward(self, x):\\nh1 = self.hidden1(x)\\nh1 = self.activation(h1)\\nh2 = self.hidden2(h1)\\nh2 = self.activation(h2)\\nh3 = self.hidden3(h2)\\nreturn h3\\nIn the constructor, we define all the layers and activation functions\\nwe are going to use in the network. In particular, we specify that we\\nneed fully-connected layers by making instances of the nn.Linear class\\nand that we need ReLU activation function by making an instance of\\nthe nn.Relu class. Then in the forward() function, we specify the order\\nin which to apply the layers and activations. See Figure 11.7 for a\\nvisualization of this neural network architecture.\\nFigure 11.7: A sample feedforward\\nneural network with two hidden layers\\nand two output nodes.\\nWe can simulate one step of forward propagation by calling the\\nforward() function of the class Net we defined.\\nx = torch.tensor([3., 1.])\\ny_pred = net.forward(x)\\nprint(’Predicted value:’, y_pred)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 143}, page_content='144\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nSimilarly, we can implement backpropagation by specifying which\\nloss function we want to use, and calling its backward() function.\\nloss = nn.functional.cross_entropy(y_pred.unsqueeze(0), torch.LongTensor([1]\\n))\\nloss.backward()\\nprint(loss)\\nEach function call of backward() will evaluate the gradients of loss\\nwith respect to every parameter in the network. Gradients can be\\nmanually accessed through the following code.\\nprint(net.hidden1.weight.grad)\\nNote that calling backward() multiple times will cause gradients to\\naccumulate. While we do not update model weights in this code\\nsample, it is important to periodically clear the gradient buffer when\\ndoing so to prevent unintended training. 8 We will discuss how to do\\n8 For more information, see https://\\npytorch.org/docs/stable/generated/\\ntorch.Tensor.backward.html\\nthis in the next chapter.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 144}, page_content='12\\nConvolutional Neural Network\\nIn Chapter 11, we focused on a type of a neural network called feed-\\nforward neural networks. But different data has different structure\\n(e.g., image, text, audio, etc.) and we need better ways of exploiting\\nthem. This can help reduce the number of parameters needed in the\\nnetwork, which may allow easier or more data-efficient training. In\\nthis chapter, we present a type of a neural network common in image\\nprocessing called Convolutional Neural Network (CNN); these models\\nuse a mathematical technique called convolution in order to extract\\nimportant visual features from input images.\\n12.1\\nIntroduction to Convolution\\nRoughly speaking, convolution refers to a mathematical operation\\nwhere two functions are “mixed” to output a new function. In ma-\\nchine learning, the main idea of convolution is to reuse the same set\\nof parameters on different portions of input. This is particularly effec-\\ntive at exploiting the structure of images. It was originally motivated\\nby studies of the structure of cortical cells in the V1 visual cortex of\\nthe human brain (Hubel and Wiesel won the Nobel Prize in 1981 for\\nthis breakthrough discovery). 1 Let’s first consider an example of a\\n1 Paper: https://www.jstor.org/\\nstable/24965293.\\n1D convolution.\\nFigure 12.1: The effects of 1D convo-\\nlution on graph of COVID-19 positive\\ncases.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 145}, page_content='146\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nExample 12.1.1. Consider the 3-day moving average of daily COVID-19\\ncases as shown in Figure 12.1. Let xt denote the number of daily cases on\\nday t. We can then take three consecutive values, compute their average and\\ncreate a new output sequence from averages: yt = 1\\n3 (xt−1 + xt + xt+1).\\nThen if we set w1 = w2 = w3 = 1\\n3 and denote ⃗w = (w1, w2, w3), we can\\nwrite: 2\\n2 If we set the weights to a different\\nvalue, we can find a weighted moving\\naverage.\\nyt = w1xt−1 + w2xt + w3xt+1 = ⃗w · (xt−1, xt, xt+1)\\nyt+1 = w1xt + w2xt+1 + w3xt+2 = ⃗w · (xt, xt+1, xt+2)\\nyt+2 = w1xt+1 + w2xt+2 + w3xt+3 = ⃗w · (xt+1, xt+2, xt+3)\\nNotice that we are reusing the same weights and applying them to multiple\\ndifferent values of xt to calculate yt. It is almost like sliding a filter down the\\narray of xt’s and applying it to every set of 3 consecutive inputs. For this\\nreason, we call ⃗w the convolution filter weight of length 3.\\nExample 12.1.2. Consider an input sequence⃗x = (2, 1, 1, 7, −1, 2, 3, 1) and\\na convolution filter ⃗w = (3, 2, −1). The first two output values will be:\\ny1 = 2 × 3 + 1 × 2 + 1 × (−1) = 7\\ny2 = 1 × 3 + 1 × 2 + 7 × (−1) = −2\\nFollowing a similar calculation for the other values, we see that the full\\noutput sequence is⃗y = (7, −2, 18, 17, −2, 11). Note that the length of⃗y\\nshould be |⃗x| −|⃗w| + 1 = 8 −3 + 1 = 6.\\nProblem 12.1.3. If yt = 2xt−1 −xt+1, yt+1 = 2xt −xt+2, and yt+2 =\\n2xt+1 −xt+3, what is the convolution filter weight?\\n12.2\\nConvolution in Computer Vision\\nIn this section, we now focus on the application of convolution in\\ncomputer vision. By the nature of image data, we will be primarily\\ndealing with 2D convolution. Generally, 2D convolution filters are\\ncalled kernels.\\nFigure 12.2: The effect of local smooth-\\ning on a sample image. (The person\\ndepicted here is Admiral Grace Murray\\nHopper, a computing pioneer.)\\nExample 12.2.1 (Local Smoothing (Blurring)). An image can be blurred\\nby constructing a filter that replaces each pixel by the average of neighboring\\npixels:\\nyi,j = 1\\n9\\n∑\\nb1,b2∈{−1,0,1}\\nxi+b1,j+b2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 146}, page_content='convolutional neural network\\n147\\nAn example is shown in Figure 12.2.\\nFigure 12.3: The effect of local sharpen-\\ning on a sample image\\nExample 12.2.2 (Local Sharpening (Edge Detection)). The edge of objects\\nin an image can be detected by constructing a filter that replaces each pixel\\nby its difference with the average of neighboring pixels:\\nyi,j = xi,j −1\\n9\\n∑\\nb1,b2∈{−1,0,1}\\nxi+b1,j+b2\\nAn example is shown in Figure 12.3.\\n12.2.1\\nConvolution Filters for Images\\nComputationally, we perform 2D convolution on an image by \"slid-\\ning\" the filter around every possible location in the image and taking\\nthe inner product:\\nyi,j =\\n∑\\n−k≤r,s≤k\\nwr,sxi+r,j+s\\n(12.1)\\nThe result is a new image and we can view each filter as a transfor-\\nmation which takes an image and returns an image. In the above\\nequation, the filter size is (2k + 1) × (2k + 1). For example, if k = 1, we\\ncan consider the convolution weight filter to be\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nw−1,−1\\nw−1,0\\nw−1,+1\\nw0,−1\\nw0,0\\nw0,+1\\nw+1,−1\\nw+1,0\\nw+1,+1\\n\\uf8f9\\n\\uf8fa\\uf8fb\\nThe filter can only be applied to an image of size m × n at a location\\nwhere the filter completely fits inside the image. Therefore, the\\nlocations in the input image where the center of the filter can be\\nplaced are k < i ≤m −k,\\nk < j ≤n −k and the size of the output\\nimage is (m −2k) × (n −2k).\\nExample 12.2.3. If the input and convolution filter are given as follows:\\nX =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n1\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n0\\n1\\n1\\n0\\n0\\n1\\n1\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nand\\nW =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fb'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 147}, page_content='148\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nthen the pixel at (1, 1) of the resulting image can be calculated by applying\\nthe filter at the top left corner of the input image. That is, we take the inner\\nproduct of the following parts (in red) of the two matrices\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n1\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n0\\n1\\n1\\n0\\n0\\n1\\n1\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nand\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fb\\nwhich is\\n1 × 1 + 1 × 0 + 1 × 1 + 0 × 0 + 1 × 1 + 1 × 0 + 0 × 1 + 0 × 0 + 1 × 1 = 4\\nTherefore, the (1, 1) entry of the resulting image is 4. Similarly, the remain-\\ning pixels of the resulting image can be calculated by moving around the\\nfilter as in Figure 12.4. The output image is given as:\\nY =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n4\\n3\\n4\\n2\\n4\\n3\\n2\\n3\\n4\\n\\uf8f9\\n\\uf8fa\\uf8fb\\nIn this example, X ∈R5×5, W ∈R3×3, k = 1 and Y ∈R3×3.\\nFigure 12.4: Visual representation of\\napplying a 3 × 3 convolutional filter to a\\n5 × 5 image.\\nProblem 12.2.4. Suppose we have a 10 × 10 image and a 5 × 5 filter. What\\nis the size of the output image?\\nFigure 12.5 shows some common filters used in image processing.\\nNote that all these filters are hand-crafted and require domain-\\nspecific knowledge. However, in convolutional neural networks, we\\ndon’t set these weights by hand and we learn all the filter weights\\nfrom the data!\\n12.2.2\\nPadding\\nIn standard 2D convolution, the size of the output image is not equal\\nto the size of the input image because we only consider locations\\nwhere the filter fits completely in the image. However, sometimes\\nwe may want their sizes to be the same. In such a case, we apply a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 148}, page_content='convolutional neural network\\n149\\nFigure 12.5: Some common filters\\nand corresponding weights used in\\nimage processing. Source: https:\\n//en.wikipedia.org/wiki/Kernel_\\n(image_processing)\\ntechnique called padding. The idea is to pad pixels to all four edges\\nof the input image (left, right, up, and down) so that the number of\\nvalid locations for the filter is the same as the number of pixels in the\\noriginal image. In particular, if the filter size is (2k + 1) × (2k + 1), we\\nneed to pad k pixels on each side.\\nThere are multiple ways to implement padding. Zero padding is\\nwhen the values at all padded pixels are set to 0. “Same” padding is\\nwhen the values at padded pixels are set to the value of the nearest\\npixel at the edge of the input image. In practice, zero padding is a\\nmore common form of padding (it is equivalent to adding a black\\nframe around the image).\\n12.2.3\\nDownsampling Input with Stride\\nAnother common operation in convolutional neural networks is\\ncalled stride. Stride controls how the filter convolves around the\\ninput image. Instead of moving the filter by 1 pixel every time, we\\ncan also move the filter every 2 (or in general, s) pixels. Essentially,\\nwe are applying each of the filter weights at fewer locations of the\\nimage than before. This can be viewed as a downsampling strategy,\\nwhich gives a smaller output image while greatly preserving the\\ninformation from the original input.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 149}, page_content='150\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nFigure 12.6: A visual comparison\\nbetween two common types of padding:\\nzero padding and “same” padding\\nSuppose we have an input image of size m × n and a filter of size\\n(2k + 1) × (2k + 1). If padding is applied to the image, the output\\nimage size is ⌊(m + s −1)/s⌋× ⌊(n + s −1)/s⌋. 3 If padding is not\\n3 As a sanity check, you can verify that\\nin the special case of s = 1 the output\\nimage size will be the same as the input\\nimage size\\napplied to the image, the output image size is ⌊(m + s −2k −1)/s⌋×\\n⌊(n + s −2k −1)/s⌋; this is because convolution with stride is per-\\nformed directly on the input image itself, making the effective input\\nimage size (m −2k) × (n −2k).\\nExample 12.2.5. Suppose we have an input image of size 5 × 5 and a filter of\\nsize 3 × 3. If we apply padding and take stride size s = 2, then output size is\\n3 × 3.\\nFigure 12.7: Visual representation of\\napplying a 3 × 3 convolutional filter to\\na 5 × 5 image with padding and stride\\nsize 2.\\n12.2.4\\nNonlinear Convolution\\nFor each location in the image (original or padded) and a single\\nconvolution filter, we can apply a nonlinear activation function after\\nthe convolution\\nyi,j = g\\n \\n∑\\n−k≤r,s≤k\\nwr,sxi+r,j+s\\n!\\n(12.2)\\nwhere g is some function like ReLU, sigmoid, tanh. The intuition is\\nsimilar to what we had earlier in feedforward neural networks — if\\nwe don’t add non-linear activation functions, a multi-layered convolu-\\ntional neural network can be easily reduced to a linear model!'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 150}, page_content='convolutional neural network\\n151\\n12.2.5\\nChannels\\nIn general, we do not only use one convolution filter. We construct\\na network of multiple layers, and for each of the layers, we apply\\nmultiple convolutional filters. Different filters will be able to detect\\ndifferent features of the image (e.g., one filter detects edges and one\\nfilter detects dots), and we want to apply different filters indepen-\\ndently on the input image. The result of applying a given filter on a\\nsingle input image is called a channel. We can stack the channels to\\ncreate a 3D structure, as shown in Figure 12.8.\\nFigure 12.8: Each filter creates one\\nchannel. The output of a convolutional\\nlayer has multiple output channels.\\nNext, let’s imagine that we want to build a deep neural network\\nwith multiple convolutional layers (state-of-the-art CNNs have 100 or\\neven 1000 layers!). A typical convolutional layer in the middle of the\\nnetwork will have several input channels (equivalent to the number\\nof output channels from the previous layer) and multiple output\\nchannels. How can we determine the number of filters needed?\\nFigure 12.9: A convolutional layer\\nwhich has multiple input and multiple\\noutput channels.\\nIn this case, we want to define a filter for every possible pair of\\ninput and output channels. The output image of a particular output\\nchannel will be the summation of the output images from each of\\nthe input channels, after applying the corresponding filter. We can\\nalso add a nonlinear activation function g after taking the summation\\nof the output images. That is, (12.2) can be rewritten for the output'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 151}, page_content='152\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nimage in the v-th output channel as:\\ny(v)\\ni,j = g\\n nin\\n∑\\nu=1\\n∑\\n−k≤r,s≤k\\nw(u,v)\\nr,s\\nx(u)\\ni+r,j+s\\n!\\n(12.3)\\nwhere nin is the number of input channels, X(u) is the image at the\\nu-th input channel, Y(v) is the image at the v-th output channel, and\\nW(u,v) is the filter between the u-th input channel and the v-th output\\nchannel.\\nExample 12.2.6. Assume there are 6 input channels and 3 output channels,\\nand the filter size is 5 × 5. Then for every 6 × 3 pair of input and output\\nchannel, we have a kernel of weights of size 5 × 5, so there are a total of\\n6 × 3 × 5 × 5 = 450 weights.\\n12.2.6\\nPooling\\nPooling is another popular way to reduce the size of the output of a\\nconvolutional layer. In contrast to stride, which applies convolution\\noperation every s pixels, pooling partitions each image (channel) to\\npatches of size ∆× ∆and performs a reduction operation on each\\npatch. You can think of this as similar to what happens when you\\nlower the resolution of an image. The reduction operation can either\\ninvolve taking the max of all the values in the patch (“max-pooling”):\\nyi,j =\\nmax\\n1≤r,s≤∆X(i−1)·∆+r,(j−1)·∆+s\\nor taking the average of all the values in the patch (“mean-pooling”):\\nyi,j = 1\\n∆2\\n∆\\n∑\\nr,s=1\\nX(i−1)·∆+r,(j−1)·∆+s\\nThe pooling operation can reduce the image size by a factor of ∆2.\\nIf the input image is of size m × n, the size of the image after\\npooling will be ⌊m/∆⌋× ⌊n/∆⌋.\\nExample 12.2.7. If the size of an input image to a pooling layer is 6 × 6 and\\n∆= 2, then the output is of size 3 × 3.\\n12.2.7\\nA Full Convolutional Neural Network\\nLet’s put everything together and consider a full convolutional neural\\nnetwork. Figure 12.11 shows a typical example of a convolutional\\nneural network. A convolutional neural network typically begins\\nby stacking multiple convolutional layers and pooling layers. Each\\nconvolutional layer has its own kernel size and number of output\\nchannels; similarly, each pooling layer has its own kernel size. This is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 152}, page_content='convolutional neural network\\n153\\nFigure 12.10: Max-pooling vs mean-\\npooling.\\nFigure 12.11: A illustration of a full\\nconvolutional neural network.\\nfollowed by several fully-connected layers at the end. Since the out-\\nput images of convolutional layers are 2-dimensional, it is customary\\nto “flatten” the images into a 1D vector (i.e., append one row after an-\\nother) before applying the fully connected layers. Intuitively, we can\\nthink of the convolutional and pooling layers as learning interesting\\nimage features (e.g., stripes or dots) while the fully-connected layers\\nmap these features to output classes (e.g., zebras have a lot of stripes).\\nAll the weights in a convolutional neural network (including\\nweights in kernels, fully-connected layers) can be learned via the\\nbackpropagation algorithm in Chapter 11. Again, modern deep\\nlearning libraries (e.g., PyTorch, TensorFlow) have all the convolu-\\ntional and pooling layers implemented and can compute gradients'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 153}, page_content='154\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nautomatically!\\nFinally, the above convolutional neural network is still a simple\\nnetwork, compared to modern convolutional neural networks. Inter-\\nested students can look up architectures such as AlexNet, Inception,\\nVGG, ResNet and DenseNet.\\n12.2.8\\nDesigning a Convolutional Network\\nWhile we described convolutional nets above, we did not give a\\ngood explanation of why they are well-suited to solve vision tasks.\\nWorking through the next few examples will help you understand\\ntheir power. The idea is that convolution is a parameter-efficient 4\\n4 Which usually goes with sample-\\nefficiency!\\narchitecture that can “search for patterns” anywhere in the image.\\nFor example, suppose the net has to decide if the input image has\\na triangle anywhere in it. If the net were fully connected, it would\\nhave to learn how to detect triangles centered at every possible pixel\\nlocation (i, j). By contrast, if a simple convolutional filter can detect\\na triangle, we can just replicate this filter in all patches to detect a\\ntriangle anywhere in the image.\\nNow consider the CNN architecture in Figure 12.12. The architec-\\nture has two convolutional layers, the first with a ReLU activation\\nfunction, and the second with a sigmoid activation function. 5 We\\n5 In both Example 12.2.8 and Exam-\\nple 12.2.9, the second convolutional\\nlayer can be considered a fully con-\\nnected layer if we flatten image Y\\nwill choose an appropriate convolutional weight and bias such that\\nthe architecture can detect a particular simple visual pattern.\\nInput X\\nImage Y\\nOutput o\\nOutput bo\\nConv 1\\nReLU\\nConv 2\\nσ\\nFigure 12.12: A sample CNN archi-\\ntecture that can be used to detect the\\npatterns as aligned in Example 12.2.8\\nand Example 12.2.9.\\nExample 12.2.8. The input to the network is a gray-scale image of size\\n8 × 8 (1 channel), and each pixel takes an integer value between 0 and 255,\\ninclusive. If at least one pixel of the image has value exactly 255, the output\\nof the CNN should have a value close to 1 and otherwise the output should\\nhave a value close to 0.\\nWe will now solve Example 12.2.8 by individually configuring\\nthe parameters for each convolutional layer in Figure 12.12. The first'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 154}, page_content='convolutional neural network\\n155\\nconvolutional layer will have a 1 × 1 filter of weight 1, a bias of −254,\\nand a ReLU activation function. The convolution will be applied with\\nno padding, and with stride 1. That is, the (i, j)-th entry of the output\\nimage of the first convolutional layer will be\\nyi,j = ReLU(xi,j −254)\\n1 ≤i, j ≤8\\nwhere xi,j is the (i, j)-th entry of the input image. Notice that this\\nvalue is zero everywhere, except if xi,j = 255, in which case yi,j takes\\nthe value 1. That is,\\nyi,j =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n1\\nxi,j = 255\\n0\\notherwise\\nSee Figure 12.13 to see the effect of this choice of convolutional layer\\non a sample image. We see that we have now successfully identified\\nthe pixels in the input image that take the value 255.\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n0\\n100\\n200\\n50\\n150\\n250\\n55\\n155\\n255\\n\\uf8f9\\n\\uf8fa\\uf8fb\\nConv 1\\n−→\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n−254\\n−154\\n−54\\n−204\\n−104\\n−4\\n−199\\n−99\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fb\\nReLU\\n−→\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fb\\nFigure 12.13: The effect of the choice\\nof the first convoluational layer for\\nExample 12.2.8 on a sample image.\\nOnly a portion of the image is shown.\\nNext, consider the second convolutional layer with a 8 × 8 filter\\nof all weights equal to 10, a bias of −5, and a sigmoid activation\\nfunction. The convolution will be applied with no padding, and with\\nstride 1. 6 The output, before the sigmoid, will be\\n6 Once the output image of the first\\nconvolutional layer is flattened to a\\nvector of length 64, this can also be\\nthought of as a fully-connected layer\\nwith input size 64 and output size 1.\\no =\\n \\n8\\n∑\\ni,j=1\\n10yi,j\\n!\\n−5\\nNotice that this value is −5 if and only if there is no pixel such that\\nyi,j = 1 (i.e., xi,j = 255). If there is one such pixel, the output is 5; if\\nthere are two, the output is 15. The important thing is, the output is\\nat least 5 if there is at least one pixel in the input image whose value\\nis 255, and otherwise the output is ≤0. That is\\no =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n≥5\\n∃(i, j) : xi,j = 255\\n−5\\notherwise\\nFinally, when we apply the sigmoid function, the final output of the\\nmodel will be\\nbo = σ(o) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n≥0.99\\n∃(i, j) : xi,j = 255\\n0.01\\notherwise\\nThis is exactly what we wanted in Example 12.2.8.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 155}, page_content='156\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nExample 12.2.9. The input to the network is a gray-scale image of size\\n8 × 8 (1 channel), and each pixel takes an integer value between 0 and 255,\\ninclusive. If any part of the input image contains the following pattern:\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n∗\\n255\\n∗\\n255\\n255\\n255\\n∗\\n255\\n∗\\n\\uf8f9\\n\\uf8fa\\uf8fb\\n(12.4)\\nthe output of the CNN should have a value close to 1 and otherwise the\\noutput should have a value close to 0.\\nWe use the same architecture as in Figure 12.12, but now with a\\ndifferent choice of parameters for the convolutional layers. The first\\nconvolutional layer will have a 3 × 3 filter with the following weights:\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n0\\n1\\n0\\n1\\n1\\n1\\n0\\n1\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fb\\na bias of −1274, and a ReLU activation function. The convolution will\\nbe applied with no padding, and with stride 1. That is, the (i, j)-th\\nentry of the output image of the first convolutional layer will be\\nyi,j = ReLU\\n\\x00xi−1,j + xi,j−1 + xi,j + xi,j+1 + xi+1,j −1274\\n\\x01\\n2 ≤i, j ≤7\\nwhere xi,j is the (i, j)-th entry of the input image. 7 Notice that this\\n7 Since there is no padding, the values\\ny1,1, y1,8, y8,1, y8,8 are not defined.\\nvalue is zero everywhere, except if xi,j + xi−1,j + xi,j−1 + xi,j+1 +\\nxi+1,j = 1275, in which case yi,j takes the value 1. This can only\\nhappen if xi−1,j = xi,j−1 = xi,j = xi,j+1 = xi+1,j = 255. That is, if the\\ninput image has the pattern in (12.4) centered around (i, j).\\nyi,j =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n1\\nPattern in (12.4) exists at (i, j)\\n0\\notherwise\\nSee Figure 12.14 to see the effect of this choice of convolutional layer\\non two sample images.\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n0\\n255\\n0\\n255\\n250\\n255\\n0\\n255\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fb\\nConv 1\\n−→\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n∗\\n∗\\n∗\\n∗\\n−4\\n∗\\n∗\\n∗\\n∗\\n\\uf8f9\\n\\uf8fa\\uf8fb\\nReLU\\n−→\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n∗\\n∗\\n∗\\n∗\\n0\\n∗\\n∗\\n∗\\n∗\\n\\uf8f9\\n\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n0\\n255\\n0\\n255\\n255\\n255\\n0\\n255\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fb\\nConv 1\\n−→\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n∗\\n∗\\n∗\\n∗\\n+1\\n∗\\n∗\\n∗\\n∗\\n\\uf8f9\\n\\uf8fa\\uf8fb\\nReLU\\n−→\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n∗\\n∗\\n∗\\n∗\\n1\\n∗\\n∗\\n∗\\n∗\\n\\uf8f9\\n\\uf8fa\\uf8fb\\nFigure 12.14: The effect of the choice\\nof first convoluational layer for Exam-\\nple 12.2.9 on two sample images. Only\\na portion of the images is shown.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 156}, page_content='convolutional neural network\\n157\\nNext, consider the second convolutional layer with a 6 × 6 filter\\nof all weights equal to 10, a bias of −5, and a sigmoid activation\\nfunction. The convolution will be applied with no padding, and with\\nstride 1. The output, before the sigmoid, will be\\no =\\n \\n7\\n∑\\ni,j=2\\n10yi,j\\n!\\n−5\\nNotice that this value is −5 if and only if there is no pixel such that\\nyi,j = 1 (i.e., the pattern exists at (i, j)). If there is one such pixel, the\\noutput is 5; if there are two, the output is 15. The important thing\\nis, the output is at least 5 if there is at least one copy of the given\\npattern, and otherwise the output is ≤0. That is\\no =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n≥5\\n∃(i, j) : Pattern in (12.4) exists at (i, j)\\n−5\\notherwise\\nFinally, when we apply the sigmoid function, the final output of the\\nmodel will be\\nbo = σ(o) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n≥0.99\\n∃(i, j) : Pattern in (12.4) exists at (i, j)\\n0.01\\notherwise\\nThis is exactly what we wanted in Example 12.2.9.\\n12.3\\nBackpropagation for Convolutional Nets\\nA convolutional neural network is a special case of a feedforward\\nneural network where we use convolutional layers, instead of fully-\\nconnected layers as in Chapter 11. Therefore, we can apply the\\nsame basic idea of backpropagation so that we can run the gradient\\ndescent algorithm, although the details of the calculation are slightly\\ndifferent.\\nThe biggest difference is that in a fully-connected layer, each\\nweight is used exactly once, while in a convolutional layer, each\\nweight is applied multiple times throughout the input image. 8 This\\n8 This phenomenon is also known as\\nweight sharing.\\nmakes the computation for the gradient slightly more convoluted.\\nBut the basic idea is the same — identify all paths through which the\\ncorresponding weight affects the output of the model and add up the\\namount of effect for each path.\\nFigure 12.15 shows a portion of a sample neural network where\\nweight sharing occurs. That is, the same weight w is used between\\nthe following four pairs of nodes: (x1, y1), (x2, y2), (x2, y3), (x3, y4).\\nIf we wanted to find the gradient ∂o/∂w, we need to consider the\\nfour paths that the weight w affects the output: w →yi →o where\\n1 ≤i ≤4.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 157}, page_content='158\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nx1\\nx2\\nx3\\ny1\\ny2\\ny3\\ny4\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\no\\nw(1)\\nw(2)\\nw(3)\\nw(4)\\nFigure 12.15: A sample neural net-\\nwork where weight sharing occurs.\\nw(1), w(2), w(3), w(4) are the copies of the\\nsame weight w.\\nWhat we will do is consider the four copies of the weight w as\\nseparate weights that will be denoted as w(i) where 1 ≤i ≤4. Since\\nthese weights are only used in one place in the layer, we are already\\nfamiliar with computing the gradients ∂o/∂w(i). Then we will add\\n(or pool) these values to get the gradient ∂o/∂w. This works because\\nwe can think of each w(i) as a function of w where w(i) = w. Then by\\nChain Rule, we have\\n∂o\\n∂w =\\n4\\n∑\\ni=1\\n∂o\\n∂w(i) · ∂w(i)\\n∂w\\n=\\n4\\n∑\\ni=1\\n∂o\\n∂w(i)\\n12.3.1\\nDeriving Backpropagation Formula for Convolutional Layers\\nIn this subsection, we derive the backpropagation formula for a\\nconvolutional layer. (As in many other places, if your instructor did\\nnot teach it in COS 324, consider this to be advanced reading.)\\nRecall that in a fully-connected layer (without an activation func-\\ntion), which computes ⃗hk = W(k)⃗h(k−1), the gradient with respect to a\\nparticular weight w(k)\\ni,j can be simply computed as\\n∂ℓ\\n∂w(k)\\ni,j\\n=\\n∂ℓ\\n∂h(k)\\ni\\n· ∂h(k)\\ni\\n∂w(k)\\ni,j\\n=\\n∂ℓ\\n∂h(k)\\ni\\n· h(k−1)\\nj\\nThis is because the weight w(k)\\ni,j is only used to compute h(k)\\ni\\nout\\nof all nodes in the next hidden layer. In comparison, consider a\\nconvolutional layer, which computes an output image Y ∈Rn×n from\\nan input image X ∈Rm×m and filter W ∈R(2k+1)×(2k+1). Notice\\nthat the weight wi,j is used to compute all of the pixels in the output\\nimage. Therefore, we just need to add (or pool) the gradient flow from\\neach of these paths. The gradient with respect to a particular weight'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 158}, page_content='convolutional neural network\\n159\\nwi,j will be\\n∂ℓ\\n∂wi,j\\n=\\n \\n∂ℓ\\n∂y1,1\\n· ∂y1,1\\n∂wi,j\\n+\\n∂ℓ\\n∂y1,2\\n· ∂y1,2\\n∂wi,j\\n+ . . . +\\n∂ℓ\\n∂y1,n\\n· ∂y1,n\\n∂wi,j\\n!\\n+\\n \\n∂ℓ\\n∂y2,1\\n· ∂y2,1\\n∂wi,j\\n+\\n∂ℓ\\n∂y2,2\\n· ∂y2,2\\n∂wi,j\\n+ . . . +\\n∂ℓ\\n∂y2,n\\n· ∂y2,n\\n∂wi,j\\n!\\n+ . . .\\n+\\n \\n∂ℓ\\n∂yn,1\\n· ∂yn,1\\n∂wi,j\\n+\\n∂ℓ\\n∂yn,2\\n· ∂yn,2\\n∂wi,j\\n+ . . . +\\n∂ℓ\\n∂yn,n\\n· ∂yn,n\\n∂wi,j\\n!\\nAssuming there is zero padding, this can be calculated as\\n∂ℓ\\n∂wi,j\\n=\\n\\x12 ∂ℓ\\n∂y1,1\\n· xi+1,j+1 +\\n∂ℓ\\n∂y1,2\\n· xi+1,j+2 + . . . +\\n∂ℓ\\n∂y1,n\\n· xi+1,j+n\\n\\x13\\n+\\n\\x12 ∂ℓ\\n∂y2,1\\n· xi+2,j+1 +\\n∂ℓ\\n∂y2,2\\n· xi+2,j+2 + . . . +\\n∂ℓ\\n∂y2,n\\n· xi+2,j+n\\n\\x13\\n+ . . .\\n+\\n\\x12 ∂ℓ\\n∂yn,1\\n· xi+n,j+1 +\\n∂ℓ\\n∂yn,2\\n· xi+n,j+2 + . . . +\\n∂ℓ\\n∂yn,n\\n· xi+n,j+n\\n\\x13\\nNotice that the equation above can be rewritten as\\n∂ℓ\\n∂wi,j\\n=\\n∑\\n1≤r,s≤n\\n∂ℓ\\n∂yr,s\\n· xi+r,j+s\\n(12.5)\\nThat is, the Jacobian matrix ∂ℓ/∂W is the output when applying a\\nconvolution filter ∂ℓ/∂Y to the input matrix X.\\nSimilarly, we can try to calculate the Jacobian matrix with respect\\nto the input matrix X. Each input pixel xi,j is used to calculate the\\noutput pixels yi+r,j+s where −k ≤r, s ≤k. The gradient with respect\\nto a particular input pixel will be\\n∂ℓ\\n∂xi,j\\n=\\n \\n∂ℓ\\n∂yi−k,j−k\\n·\\n∂yi−k,j−k\\n∂xi,j\\n+ . . . +\\n∂ℓ\\n∂yi−k,j+k\\n·\\n∂yi−k,j+k\\n∂xi,j\\n!\\n+\\n \\n∂ℓ\\n∂yi−k+1,j−k\\n·\\n∂yi−k+1,j−k\\n∂xi,j\\n+ . . . +\\n∂ℓ\\n∂yi−k+1,j+k\\n·\\n∂yi−k+1,j+k\\n∂xi,j\\n!\\n+ . . .\\n+\\n \\n∂ℓ\\n∂yi+k,j−k\\n·\\n∂yi+k,j−k\\n∂xi,j\\n+ . . . +\\n∂ℓ\\n∂yi+k,j+k\\n·\\n∂yi+k,j+k\\n∂xi,j\\n!'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 159}, page_content='160\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nAssuming zero padding, this is calculated as\\n∂ℓ\\n∂xi,j\\n=\\n \\n∂ℓ\\n∂yi−k,j−k\\n· wk,k + . . . +\\n∂ℓ\\n∂yi−k,j+k\\n· wk,−k\\n!\\n+\\n \\n∂ℓ\\n∂yi−k+1,j−k\\n· wk−1,k + . . . +\\n∂ℓ\\n∂yi−k+1,j+k\\n· wk−1,−k\\n!\\n+ . . .\\n+\\n \\n∂ℓ\\n∂yi+k,j−k\\n· w−k,k + . . . +\\n∂ℓ\\n∂yi+k,j+k\\n· w−k,−k\\n!\\nwhich can be rewritten as\\n∂ℓ\\n∂xi,j\\n=\\n∑\\n−k≤r,s≤k\\n∂ℓ\\n∂yi+r,j+s\\n· w−r,−s\\n(12.6)\\nThat is, the Jacobian matrix ∂ℓ/∂X is the output when applying the\\nhorizontally and vertically inverted image of W as the convolutional\\nfilter to the input matrix ∂ℓ/∂Y.\\n12.4\\nCNN in Python Programming\\nIn this section, we discuss how to write Python code to implement\\nConvolutional Neural Networks (CNN). As usual, we use the numpy\\npackage to speed up computation and the torch package to easily\\ndesign and train the neural network. We also introduce the torchvision\\npackage:\\n• torchvision: This package focuses on computer vision applications\\nand is integrated with the broader PyTorch framework. It provides\\naccess to pre-built models, popular datasets, and a variety of\\nimage transform capabilities. 9\\n9 Documentation is available at https:\\n//pytorch.org/vision/stable/index.\\nhtml\\nThe following code sample implements a CNN and trains it on a\\nsingle image.\\n# import necessary packages\\nimport random\\nimport numpy as np\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torchvision\\nimport torchvision.transforms as transforms\\nfrom torch.utils.data import DataLoader\\n# set random seeds to ensure reproducibility\\ntorch.manual_seed(0)\\nnp.random.seed(0)\\nrandom.seed(0)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 160}, page_content='convolutional neural network\\n161\\n# load CIFAR10 data\\ntransform = transforms.Compose(\\n[transforms.ToTensor(),\\ntransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\\ntrain_data = torchvision.datasets.CIFAR10(root=’./data’, train=True,\\ndownload=True, transform=transform)\\ntest_data = torchvision.datasets.CIFAR10(root=’./data’, train=False,\\ntransform=transform)\\n# helps iterate through the train/test data in batches\\ntrain_loader = DataLoader(dataset=train_data, batch_size=8, shuffle=True,\\nnum_workers=0)\\ntest_loader = DataLoader(dataset=test_data, batch_size=8, shuffle=False,\\nnum_workers=0)\\n# define the CNN architecture\\nclass ConvNet(nn.Module):\\ndef __init__(self):\\nsuper(ConvNet, self).__init__()\\n# Conv2d takes # input channels, # output channels, kernel size\\nself.conv1 = nn.Conv2d(3, 3, 5)\\nself.pool1 = nn.MaxPool2d(2, 2)\\nself.conv2 = nn.Conv2d(3, 16, 5)\\nself.pool2 = nn.AvgPool2d(2, 2)\\nself.fc1 = nn.Linear(16*5*5, 120)\\nself.fc2 = nn.Linear(120, 10)\\ndef forward(self, x):\\nx = F.relu(self.conv1(x))\\nx = self.pool1(x)\\nx = F.relu(self.conv2(x))\\nx = self.pool2(x)\\nx = x.view(-1, 16*5*5)\\nx = F.relu(self.fc1(x))\\nx = self.fc2(x)\\nreturn x\\n# extract one image from the dataset\\nimages, labels = next(iter(train_loader))\\nimage = images[0].unsqueeze(0)\\n# forward propagation\\nnet = ConvNet()\\noutput = net(image)\\n# choose the optimization technique to invoke\\noptimizer = torch.optim.SGD(net.parameters(), lr=0.01)\\n# backpropagation\\nloss = torch.norm(output - torch.ones(output.shape[1]))**2\\nloss.backward()\\noptimizer.step()\\noptimizer.zero_grad()\\nAs usual, we start by importing packages.\\nimport random\\nimport numpy as np\\nimport torch\\nimport torch.nn as nn'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 161}, page_content='162\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nimport torch.nn.functional as F\\nimport torchvision\\nimport torchvision.transforms as transforms\\nfrom torch.utils.data import DataLoader\\nThe DataLoader class helps iterate through a dataset in batches.\\nNext, we fix all random seeds to ensure reproducibility.\\ntorch.manual_seed(0)\\nnp.random.seed(0)\\nrandom.seed(0)\\nRecall that programming languages on a classical computer can only\\nimplement pseudorandom methods, which always produce the same\\nresult for a given seed.\\nThen we load the CIFAR-10 dataset.\\ntransform = transforms.Compose(\\n[transforms.ToTensor(),\\ntransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\\ntrain_data = torchvision.datasets.CIFAR10(root=’./data’, train=True,\\ndownload=True, transform=transform)\\ntest_data = torchvision.datasets.CIFAR10(root=’./data’, train=False,\\ntransform=transform)\\nThe CIFAR-10 dataset contains simple images of a single object, and\\nthe images are labeled with the category of the objects they contain.\\nNote that we normalize the dataset with a mean of 0.5 and standard\\ndeviation of 0.5 per color channel. Figure 12.16 shows a sampling of\\nimages from the dataset after the normalization.\\nFigure 12.16: Sample images from the\\nCIFAR10 dataset.\\nNext we create DataLoader objects to help iterate through the\\ndataset in batches. Each batch will consist of 8 images and 8 labels.\\ntrain_loader = DataLoader(dataset=train_data, batch_size=8, shuffle=True,\\nnum_workers=0)\\ntest_loader = DataLoader(dataset=test_data, batch_size=8, shuffle=False,\\nnum_workers=0)\\nThen we define our CNN architecture in the ConvNet class.\\nclass ConvNet(nn.Module):\\ndef __init__(self):\\nsuper(ConvNet, self).__init__()\\n# conv2d takes # input channels, # output channels, kernel size\\nself.conv1 = nn.Conv2d(3, 3, 5)\\nself.pool1 = nn.MaxPool2d(2, 2)\\nself.conv2 = nn.Conv2d(3, 16, 5)\\nself.pool2 = nn.AvgPool2d(2, 2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 162}, page_content='convolutional neural network\\n163\\nself.fc1 = nn.Linear(16*5*5, 120)\\nself.fc2 = nn.Linear(120, 10)\\ndef forward(self, x):\\nx = F.relu(self.conv1(x))\\nx = self.pool1(x)\\nx = F.relu(self.conv2(x))\\nx = self.pool2(x)\\nx = x.view(-1, 16*5*5)\\nx = F.relu(self.fc1(x))\\nx = self.fc2(x)\\nreturn x\\nJust like the FFNN code from the previous chapter, we define all the\\nlayers and activations we are going to use in the constructor. Note\\nthat in addition to instances of the nn.Linear class and the nn.ReLU\\nclass, we also make use of classes like nn.Conv2d and nn.MaxPool2d\\nwhich are specifically designed for CNNs.\\nWe extract one training image with the following code.\\nimages, labels = next(iter(train_loader))\\nimage = images[0].unsqueeze(0)\\nThe unsqueeze() function adds one dimension to the training data.\\nThis is called a batch dimension. Normally, we would run the training\\nin batches, and the size of the data along the batch dimension will be\\nequal to the number of images in each batch. Here, we only use one\\nimage for the sake of exposition.\\nWe can now run forward propagation on a sample image with the\\ncode below.\\nnet = ConvNet()\\noutput = net(image)\\nWe then implement the squared error loss. Alternatively, we could\\nhave chosen the cross-entropy loss or any other valid loss function.\\nloss = torch.norm(output - torch.ones(output.shape[1]))**2\\nNext, we calculate the gradients of the loss with the following line of\\ncode\\nloss.backward()\\nand update each of the parameters according to the Gradient Descent\\nalgorithm with the following line.\\noptimizer.step()\\nFinally, we reset the values of the gradients to zero with the following\\ncode.\\noptimizer.zero_grad()'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 163}, page_content='164\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nRecall as discussed in the previous chapter that failing to do so\\nwill cause unintended training during subsequent iterations of\\nbackpropagation. Here, we called the zero_grad() function at the end\\nof one iteration of backpropagation, but it may be a good idea to\\ncall this function right before calling backward(), just in case there\\nare already gradients in the buffer before program execution (e.g., if\\nsomeone was working with the model beforehand in the interpreter).\\nIn this section, we only showed how to run forward propagation\\nand backpropagation on a single data point. In general, we train the\\nmodel on the entire dataset multiple times. A single pass over the\\nentire dataset is called an epoch.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 164}, page_content='Part IV\\nReinforcement Learning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 165}, page_content=''),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 166}, page_content='13\\nIntroduction to Reinforcement Learning\\nThis part of the course concerns Reinforcement Learning (RL), the\\nconceptual underpinning of several modern technologies such as\\nself-driving technologies in new cars. It is the third major category of\\nmachine learning, in addition to the two previously seen categories\\nof supervised and unsupervised learning. In class we saw a video of\\nrobots (made by Boston Dynamics) doing parkour, dancing, and over-\\nall doing a pretty good job of imitating the peak human physique.\\nThat is also achieved via RL.\\nThe basic idea of RL involves the concept of an agent learning to\\nmake a sequence of actions in a dynamic environment. At each discrete\\ntime step, the agent is able to take one of a menu of actions. Each\\nchoice of action leads to changes in the state of the world (i.e., the\\nagent and its surroundings). The agent has an internal representation\\nof the current and potential states of the world (e.g., using vision\\nor other sensing modules). Under this setting, the agent takes a\\nsequence of actions towards a certain goal.\\nThe world contains uncertainty due to a variety of factors. For\\ninstance, there may be other agents in the environment that also\\ntake actions to their own benefit, or the sensing modules may be\\nimperfect. Thus taking the same action from the same state of the\\nworld may lead to different evolution of state in the future — that is,\\nRL is non-deterministic.\\nIn this chapter, we introduce the basic elements of RL using real-\\nworld examples, and what it means for the agent to act optimally.\\nChapter 14 focuses on the setting where the underlying environ-\\nment (e.g., the number of states, the current state, the probability\\ndistribution) is completely known to the agent. 1 In Chapter 15, we\\n1 Think of playing a game where you\\nknow the complete set of rules.\\nwill present the case where the environment is not fully available\\nto the agent, and the agent learns about the environment while also\\nlearning to act in it. 2\\n2 Think of playing a Role-playing Game\\n(RPG) where you need to unlock parts\\nof the map by advancing the story.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 167}, page_content='168\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n13.1\\nBasic Elements of Reinforcement Learning\\nNow we formalize several of the basic elements of reinforcement\\nlearning that were sketched above.\\n13.1.1\\nStates and Actions\\nThere is a finite set S of states, and the entire system agent + envi-\\nronment exists in one of these states at any time. At each state s ∈S,\\nthe agent makes an action a ∈As, where As is the set of allowed\\nactions at state s. We denote A = S\\ns∈S\\nAs to be the set of all possible\\nactions in the whole RL environment.\\nExample 13.1.1. Consider a game of chess. Each state s can be represented\\nas a pair (C, p) where C denotes the current configuration of pieces and p\\ndenotes the player to play next. For example, “white king at e1, black king\\nat e8, and it is white turn to move” would be a possible state s of the game.\\nAn action a is a valid movement of a piece, given a state of the game. For\\nexample, “white king to e2” (i.e., Ke2) would be a possible action of the\\nagent playing white in state s.\\nExample 13.1.2. Self-driving cars, like those built by Tesla, are becoming\\nincreasingly popular. Let’s imagine how we could construct a state diagram\\nfor the task of driving autonomously. Each state can be represented by the\\ncurrent configuration of a number of factors (e.g., the car speed, distance\\nfrom lane boundaries, distance to nearest vehicle, etc.) Possible actions\\ninclude increasing/decreasing speed, changing gear, changing direction,\\nchanging lane, etc.\\n13.1.2\\nModeling Uncertainty via Transition Probabilities\\nAs mentioned, the agent has many sources of uncertainty in its\\nknowledge about the environment, and we can use concepts from\\nprobability to model uncertainty.\\nSuppose S = {s1, s2, . . . , sn} contains n states. When the agent\\ntakes action a while in state s, it will transition into another (poten-\\ntially the same) state s′. The catch is: the agent does not know exactly\\nwhich state it will end up in. Instead, there is a probability pi of end-\\ning up in state si for each si ∈S. Here ∑i pi = 1, meaning each (state,\\naction) pair is associated with a probability distribution over the next\\nstate that the agent will enter. Formally, we define it as follows:\\nDefinition 13.1.3 (Transition Probabilities). Given a state s ∈S and an\\naction a ∈As, there is an associated transition probability p(∗| s, a)\\ndistributed over S such that state s′ ∈S happens with probability p(s′ | s, a)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 168}, page_content='introduction to reinforcement learning\\n169\\nwhen action a is taken at state s and ∑\\ns′∈S\\np(s′ | s, a) = 1. If p(s′ | s, a) > 0,\\nwe say that the state s′ is reachable from s when action a is taken.\\nIn general, not all states are reachable, given a state s and an action\\na. That is, some transition probability p(s′ | s, a) is zero. For these\\nstates, it is conventional to leave out the corresponding transitions\\nwhen representing the RL environment as a state diagram as in\\nFigure 13.1 or Figure 13.2.\\nExample 13.1.4. Consider the state diagram shown in Figure 13.1. This is\\na special case where there is only one action a in the set A. In other words,\\nthe agent is not making any choices; instead, it is just following probabilistic\\ntransition over time steps. To calculate the probability of reaching state s3\\nfrom s0, we note there are two different paths. The first path is s0 −s1 −s3\\nand the second path is s0 −s2 −s3. We thus calculate the probabilities of each\\nof these paths and note that the overall probability of reaching s3 will be the\\nsum of both: 0.2 · 0.7 + 0.8 · 0.4 = 0.46.\\nFigure 13.1: An example diagram\\nwhere |A| = 1. The agent simply\\nfollows probabilistic transitions.\\nNow we consider an example where there is more than one action\\nto make. In this case, each action induces a different probability\\ndistribution on the set of states, so we need to draw a diagram for\\neach option.\\nExample 13.1.5. We can model a baby learning to walk through RL. As\\nshown in Figure 13.2, we can define the state s0 = standing but feeling\\nunsteady, s1 = standing and feeling secure, and s2 = on ground. The\\nbaby has two actions to take: a = not grab onto nearest support and\\na′ = grab onto nearest support. The state diagram on the top represents\\nthe transition probabilities when the baby takes the action a. See that the\\nbaby has a high chance of entering state s2 — falling to the ground. On\\nthe other hand, the state diagram on the bottom represents the transition\\nprobabilities when the baby takes the action a′. The baby now has a high\\nchance of entering state s1 — standing securely on the ground. The two\\nactions have different probability distributions associated with the relevant\\ntransitions.\\nExample 13.1.6. Mechanical ventilators are used to stabilize breathing for\\npatients. Suppose we wished to construct a state diagram. We could define'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 169}, page_content='170\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nFigure 13.2: An example diagram\\nshowing how an action determines the\\nprobability of outgoing transitions.\\nstates that consider the pressure and CO2 level in the patient’s level for the\\npast k seconds. Actions might include adjusting the flow rate of oxygen\\nvia valve settings as needed. Possible transitions might include the typical\\nmechanical response of the lungs, or unexpected spasms. Finally, we can\\ndefine the goal to be maintaining steady pressure in the patient without\\n“overshooting” and causing damage.\\n13.1.3\\nAgent’s Motivation/Goals\\nIn general, an agent is a participant in RL models driven by the\\nneed to maximize “rewards.” In a probabilistic setting, the agent\\nwishes to maximize their expected rewards. In a natural setting, the\\n“rewards” could be innate satisfaction, such as getting to eat food,\\nbeing entertained, etc. But in the usual artificial settings such as\\nrobots and self-driving cars, rewards are sprinkled by the system\\ndesigner into the framework. Some examples appear later.3\\n3 While reward/punishment as a way\\nto shape human or animal behavior is a\\nvery old idea, mathematical modeling\\nof agents as reward-maximisers appears\\nin several disciplines that flowered\\naround the middle of the 20th century\\n(e.g., behaviorism in psychology, profit-\\nmaximization in economics, and of\\ncourse RL).\\nAt each step, the agent takes an action, and is given a reward\\n(which could be negative, i.e., is a punishment) based on the action,\\ncurrent state, and next state.\\nDefinition 13.1.7 (Reward). For each valid 3-tuple (a, s, s′) where s′ ∈S\\nis a state reachable from state s ∈S by taking action a ∈As, we define a\\ncorresponding reward r(a | s, s′) ∈R.\\nExample 13.1.8 (Example 13.1.5 revisited). When the baby stands and\\nfeels secure after grabbing onto something, the parents applaud the baby,\\nand the baby receives a positive reward: r(a′ | s0, s1) = 5. When the baby\\nfeels secure without grabbing onto the nearest support, the parents feel even\\nprouder and the baby gets a more positive reward: r(a | s0, s1) = 10. When\\nthe baby falls to the ground, the baby feels pain and receives negative reward:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 170}, page_content='introduction to reinforcement learning\\n171\\nr(a | s0, s2) = r(a′ | s0, s2) = −5.\\nTypically, the designer of an RL model gets to define the rewards\\nthroughout the framework based on the designer’s judgment. For\\ninstance, in Example 13.1.2, we might design an RL model such that\\nif the car drifts into an adjacent lane, we assign a negative reward.\\nIf another vehicle is in the lane, we might assign an even larger\\nnegative reward. This will induce an RL model to “learn” the proper\\nway to driving — staying in lane.\\n13.1.4\\nComparison with NFA\\nRecall the Non-deterministic Finite Automata (NFA) you learned in\\nCOS 126. In an NFA, there is a finite number of states, and for each\\nstate, we know the set of next possible states, based on the next input\\ncharacter.\\nFigure 13.3: A sample Non-\\ndeterministic Finite Automata. Source:\\nIntroduction to the Theory of Computation\\nby Michael Sipser.\\nWe can consider the following analogy between RL and NFA —\\nthere is someone behind an NFA, who can observe its current state\\nand type in the next input character. This person will be called an\\nagent, and the choice of input character that is typed in will be called\\nan action. Each action can lead to a finite set of next possible states,\\nbut because of some uncertainty in the world, the agent cannot\\nspecify which particular state will be the next one. This is similar to\\nan NFA in the sense that the actions are non-deterministic. Also, just\\nlike in an NFA, the change in the current state is also referred to as a\\ntransition. One major difference between RL and NFA is that while an\\nNFA only cares about the final state of the automata (i.e., whether it\\nis an accept state or a reject state), in RL, the agent is given a reward\\nafter each transition. The goal of the agent will be to take a sequence'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 171}, page_content='172\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nof actions so as to maximize the sum of the reward throughout the\\nsequence of actions.\\n13.2\\nUseful Resource: MuJoCo-based RL Environments\\nReal-life robots with precise and reliable hardware can get very\\nexpensive to buy, let alone train. An easier playground for students\\n(especially those trying to work with a single GPU on CoLab) is\\ndoing RL in a virtual environment.\\nMuJoCo is a famous physics engine that allows creating virtual\\nobjects with somewhat realistic “joints” that can be commanded to\\nmove similar to real-life robots. OpenAI and DeepMind have open-\\nsource environments that allow experimentation in the MuJoCo\\nenvironment. The official website gives a pretty good overview of the\\nsoftware:\\nFigure 13.4: An example of a MuJoCo\\nWalker.\\nMuJoCo is a physics engine that aims to facilitate research and development\\nin robotics, biomechanics, graphics and animation, and other areas where fast\\nand accurate simulation is needed. MuJoCo offers a unique combination of\\nspeed, accuracy and modeling power, yet it is not merely a better simulator.\\nInstead it is the first full-featured simulator designed from the ground up\\nfor the purpose of model-based optimization, and in particular optimization\\nthrough contacts. 4\\n4 Source: https://mujoco.org.\\nOne aspect of MuJoCo simulation involves a representation of a\\nhumanoid figure (i.e., the agent) learning how to navigate an obstacle\\ncourse (i.e., the environment). Training videos are readily available\\nonline and show how the agent learns over time (sometimes, to\\ncomedic effect).\\nExample 13.2.1. Let’s analyze the example of an agent navigating an\\nobstacle course through an RL framework. The states can be considered to be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 172}, page_content='introduction to reinforcement learning\\n173\\nthe set of coordinates, velocity, and acceleration for each limb, the velocity\\nand acceleration for the motors in each joint, and the environment itself\\nstraight ahead. The actions can include the agent increasing or decreasing\\nmotor speed in their joints. Finally, the final goal is to stay upright, run\\nforward at a reasonable pace, and avoid obstacles.\\n13.3\\nIllustrative Example: Optimum Cake Eating\\nLet’s consider an extended example which ties together the elements\\nof RL discussed previously. Suppose you buy a small cake with three\\nslices. The reward of eating one slice at one sitting is 1, but eating\\ntwo or three slices at one sitting is 1.5 and 1.8 respectively. 5\\n5 This sense of diminishing rewards is\\nknown as the satiation effect.\\nProblem 13.3.1. Suppose you plan to eat the cake over a period of three days.\\nWhat eating schedule will maximize the internal reward?\\nNow let’s introduce your roommate, who is oblivious to basic\\nunderstandings of ownership and adheres to the “finders keepers”\\nfaith. We define the probability Pr[sneakily eats a slice overnight] =\\n1\\n2. To account for this uncertainty, we can create a look-ahead tree for\\ndifferent initial actions. We first consider the action where you decide\\nto eat two out of the three slices on the first night. Successive states\\nand associated probabilities are shown in the Figure 13.5.\\nFigure 13.5: The diagram (look-ahead\\ntree) of the cake problem where you\\ndecide to eat two slices on the first\\nnight. Each state represents the number\\nof slices remaining, and each action\\nrepresents the number of slices eaten on\\none night.\\nEven though you eat only two out of the three slices during the\\nfirst night, there is a 1\\n2 chance that your roommate eats the remaining\\nslice overnight. Therefore, the action of “eating 2 slices” can lead\\nto two possible states — “1 slice left” or “0 slices left” — each with\\nprobability 1\\n2.\\nExample 13.3.2. We can calculate the expected reward associated with eating\\ntwo slices on the first night by analyzing the Figure 13.5. You first gain'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 173}, page_content='174\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nreward of 1.5 by eating the two slices on the first night. Then with proba-\\nbility 1\\n2 (where the roommate does not eat the remaining slice overnight),\\nyou get to eat the last slice on the second night and gain additional re-\\nward of 1. With probability of 1\\n2 (where the roommate eats the remaining\\nslice), you cannot gain anymore reward. That is, the expected reward is\\n1.5 + 0.5 · 1 + 0.5 · 0 = 2.\\nProblem 13.3.3. Consider the result of Example 13.3.2. Would you prefer to\\ntake two slices on the first night or three slices?\\nWe next consider the action where you decide to eat one out of\\nthe three slices on the first night. Successive states and associated\\nprobabilities are shown in Figure 13.6.\\nFigure 13.6: The diagram (look-ahead\\ntree) of the cake problem where you\\ndecide to eat one slice on the first night.\\nThe difficulty in this example in contrast to the Figure 13.5 is that\\nif the roommate does not eat a slice after the first night, you have two\\nslices at your disposal on the second night. You have two actions you\\ncan take in this “2 slices left” state — “eat 1 slice” (and hope the third\\nslice is still there on the third night) or “eat 2 slices” — and it is not\\nimmediately obvious which one is more optimal. It turns out that\\nthe expected reward you can get from the remaining 2 slices is 1.5 for\\nboth options.\\nProblem 13.3.4. Verify the previous claim that both options on the second\\nnight have the same expected reward.\\nExample 13.3.5. Given the previous analysis and the look-ahead tree in the\\nFigure 13.6, we note that the total expected reward is 1 + 0.5 · 1 + 0.5 · 1.5 =\\n2.25. You first receive a reward of 1 by eating 1 slice on the first night. Then\\nwith probability 1\\n2, the roommate eats one slice over night, and you gain\\nreward of 1 by eating the last slice on the second night. With the remaining\\nprobability 1\\n2, the roommate does not eat a slice, and you are expected to gain\\nreward of 1.5 from the remaining 2 slices, regardless of the action you choose\\nto take on the second night.\\nProblem 13.3.6. Consider the result of Example 13.3.5. Would you prefer to\\ntake two slices on the first night or one slice?'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 174}, page_content='14\\nMarkov Decision Process\\nIn this chapter, we formally introduce the Markov Decision Process\\n(MDP), a way to formulate an RL environment. We then present\\nways to find the optimal strategy of an agent, provided that the agent\\nknows the full details of the MDP — that is, knows everything about\\nthe environment.\\n14.1\\nMarkov Decision Process (MDP)\\nLet’s review the key ingredients of RL. We have the agent, who senses\\nthe environment and captures it as the current state. There is a finite\\nnumber of actions available at any given state, and taking an action a\\nin state s will cause a transition to s′ with probability p(s′ | s, a). Each\\ntransition is accompanied by a reward r(a | s, si) ∈R. Finally, the\\ngoal of the agent is to maximize the expected reward via a sequence\\nof actions.\\nA Markov Decision Process (MDP) is a formalization of these con-\\ncepts. It is a directed graph which consists of four key features:\\n• A set S which contains all possible states\\n• A set A which contains all possible actions\\n• For each valid tuple of action a and states s1, s2, there is an as-\\nsigned probability p(s2 | s1, a) of transition to s2 if action a is taken\\nin s1\\n• For each valid tuple of action a and states s1, s2, there is an as-\\nsigned reward r(a | s1, s2), which is obtained if action a is taken to\\ntransition from s1 to s2\\nIf a designed MDP has M actions and N states, we can specify the\\nMDP by a table of transition probabilities (with MN2 numbers) and a\\ntable for rewards (with MN2 numbers).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 175}, page_content='176\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n14.1.1\\nRevisiting the Cake Eating Example\\nLet’s return to the case study on eating cake from Subsection 13.3,\\nand formally express it through a MDP. The set of states is given\\nas S = {0, 1, 2, 3}, where each state represents the number of slices\\nleft. The set of actions is given as A = {1, 2, 3}, where each action\\nrepresents the number of slices you choose to eat on a given night.\\nNotice that reward only depends on how many slices you take, not\\nhow many slices are left after your roommate goes through the fridge.\\nThat is, we can define the reward r(a | s, ∗) for each a ∈A to be the\\nsame for every s ∈S where a is feasible. 1\\n1 We still need to include the previous\\nstate s because not all actions are\\nfeasible at each state. For example, you\\ncan’t eat 2 slices when there is only 1\\nslice left.\\nExample 14.1.1. Let’s revisit Example 13.3.2 as a motivating example.\\nIf we let a = 2, s1 = 3, and s2 = 0, then the probability of the specified\\ntransition is p(s2 | s1, a) = 0.5. The associated reward is r(a | s1, s2) = 1.5\\nas discussed earlier.\\nWe are now ready to generalize to the a more complete MDP,\\nwhich is shown in Figure 14.1. Note that every transition is labeled\\nwith its probability, associated action, and associated reward.\\nFigure 14.1: A more complete diagram\\nof the cake problem when described as\\na MDP.\\n14.1.2\\nDiscounting the Future\\nThe MDP describing cake eating in the previous subsection was\\nacyclic. 2 However, in general, MDPs can have directed cycles, and\\n2 This is also known as an Episodic\\nMDP.\\nthe agent’s actions can allow it to continuously collect rewards along\\nthat cycle. For instance, continuing our cake theme, we may have a\\nscenario in which you receive a fresh cake every 3 days. But now we\\nrun into a problem: how can we calculate the expected reward when\\nthere is an unbounded number of steps?\\nThe solution lies in the concept of future discounting. The basic\\nidea is to reduce, or discount, the amount of reward we get from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 176}, page_content='markov decision process\\n177\\nfuture steps. In an MDP, we represent this through a discount factor\\n0 < γ ≤1 and an associated infinite sum. 3\\n3 This is related to notions of discount-\\ning commonly considered in economics.\\nDefinition 14.1.2 (Future Discounting). If a reward rt is received at\\ntime t = 0, 1, 2, . . . , then the perceived value of these rewards rd, or the\\ndiscounted reward, at t = 0 is:\\nrd = r0 + γr1 + γ2r2 + γ3r3 + · · ·\\nExample 14.1.3. Consider the cake eating problem again and let rt denote the\\nreward we get on night t. If the reward is discounted by a factor of γ every\\nnight, the total expected discounted reward E[total] can be rewritten as\\nE[total] = E[r1] + γ · E[r2] + γ2 · E[r3]\\nConsider taking the action a = 2 on the first night. If γ = 0.9, then the\\nexpected discounted reward is\\n1.5 + 0.9 · (0.5 · 1 + 0.5 · 0) = 1.95\\nThis is the same as in Example 13.3.2 except the reward taken from the\\nsecond night is discounted by a factor of 0.9. Now consider taking the action\\na = 1 on the first night and on the second night. If γ = 0.9, the expected\\ndiscounted reward is\\n1 + 0.9 · (0.5 · 1 + 0.5 · 1) + 0.92 · (0.52 · 1) = 2.1025\\nHere, we first take the reward of 1 on the first night without any discount\\nfactor. Then, we calculate the expected reward from the second night — 1\\nwhether or not the roommate eats a slice — and discount it by a factor of 0.9.\\nFinally, we calculate the expected reward from the third night — 1 only if\\nthe roommate did not eat any slice on the first two nights — and discount it\\nby a factor of 0.92.\\nNote that in Definition 14.1.2, if each rt ∈[−R, R] and if γ < 1,\\nthen the magnitude of discounted reward of the infinite sequence has\\nthe following upper bound:\\n|rd| ≤R(1 + γ + γ2 + · · · ) =\\nR\\n1 −γ\\n(14.1)\\n(14.1) is derived by considering the formula for the sum of an infinite\\ngeometric series, which we can invoke if γ < 1. In general, γ is up\\nto the system designer. A lower γ would imply that the agent places\\nlittle importance on future rewards, whereas γ = 1 would imply that\\nthere is effectively no discounting.\\n14.2\\nPolicy and Markov Reward Process\\nNow that we have discussed what an action is and what it does in\\nan MDP, we want to specify what action an agent has to take in each\\nstate. This is known as a policy.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 177}, page_content='178\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nExample 14.2.1. Consider again the cake eating MDP example without\\na discount factor. We already established through Example 13.3.2 and\\nExample 13.3.5 that to maximize the expected reward, you need to eat one\\nslice per day until all slices are gone. That is, in any state j where j = 1, 2, 3,\\nyou need to take action 1.\\nIn general, if S is the set of states, and A is the set of actions, then\\na policy (not necessarily the optimum) π can be defined as a function\\nπ : S →A\\nDefinition 14.2.2 (Policy). If S is the set of states, and A is the set of\\nactions, any function π : S →A is called a policy that describes which\\naction to take at each state. In particular, each state s should only be mapped\\nto a valid action a ∈As at that state.\\nRecall that if there are M actions and N states, there are at most\\nMN2 transitions in the graph of the MDP. Because a policy specifies\\none action per state, there are at most N2 transitions that remain\\nwhen we choose a specific policy. Therefore, it can be understood\\nthat a policy trims out the MDP.\\n14.2.1\\nMarkov Reward Process (MRP)\\nWhen we have an MDP and a fixed policy, we have what is called a\\nMarkov Reward Process (MRP). There are no more decisions to make;\\ninstead, all we need to do is take the action specified by the policy;\\nprobabilistically follow a transition into a new state; and collect the\\nassociated reward.\\nExample 14.2.3. Let’s revisit Figure 14.1. If we fix the policy to be π(s) = 1\\nfor any s ∈S, we can focus our attention to the action a = 1. Then there\\nare three trajectories that will lead from state 3 to state 0, based on what\\nthe roommate does overnight. The first trajectory is 3 →1 →0 with\\nprobability 0.5 × 1 and reward 1 + 1. The second trajectory is 3 →2 →0\\nwith probability 0.5 × 0.5 and reward 1 + 1. The last trajectory is 3 →2 →\\n1 →0 with probability 0.5 × 0.5 × 1 and reward 1 + 1 + 1.\\nIn general, when we fix a policy π and an initial state s, we can\\nredraw the transition diagram of an MDP into a tree diagram for\\nthe MRP, where each node corresponds to a state, and each edge\\ncorresponds to a probabilistic transition. The top node represents the\\ninitial state, and each subsequent row of the tree represents the set of\\npossible states after taking an action from their parent node.\\nExample 14.2.4. We revisit Example 14.2.3. We now transform Figure 14.1\\ninto a tree diagram for the MRP as shown in Figure 14.2. The top node is\\nthe initial state 3. The second row of the tree is all states that can be achieved\\nby taking the action 1 at state 3, and so on.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 178}, page_content='markov decision process\\n179\\nFigure 14.2: A tree representing the\\nMRP in Example 14.2.3.\\nNote that in an MRP tree, the same state can appear multiple\\ntimes, but each copy of the same state is identical — that is, the\\nsubtree rooted at each copy must be identical. In Figure 14.2, the\\nstate 1 appears twice in the tree. Every time it appears, it can only\\nlead to state 0 with probability 1. This is simply the result of fixing\\na policy π — once we know the state we are in, we only have one\\nchoice for the action to take.\\nThe policy also induces a value function on this tree. The value\\nfunction assigns a value to each node of the tree, and each value\\nintuitively measures how much reward the agent should expect to\\ncollect once the agent knows they have arrived at that node. By the\\nobservation from the previous paragraph, this expected reward is the\\nsame for two nodes if they are copies of the same state. Therefore,\\nwe can equivalently define the value function for each state s instead.\\nFormally, we define the value function as the following.\\nDefinition 14.2.5 (Value Function). vπ(s), the value of state s under the\\npolicy π, is the expected discounted reward of a random trajectory starting\\nfrom s. We can define this value by using the following recursive formula:\\nvπ(s) = ∑\\ns′\\np(s′ | s, π(s)) ·\\n\\x00r(π(s) | s, s′) + γvπ(s′)\\n\\x01\\n(14.2)\\nComputing the value function as in (14.2) is also known as the Bellman\\nequation.\\nLet us unpack the intuition behind (14.2). Once we take action\\nπ(s) at state s, it will bring us to state s′ with probability p(s′ | s, a),\\nimmediately giving us a reward r(a | s, s′). Then, the expected reward\\nfrom that point on is already captured by the value vπ(s′). We just\\nneed to apply the discount factor γ because we already took one time\\nstep to reach s′ from s.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 179}, page_content='180\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nOn the other hand, if we pick any random trajectory starting\\nfrom s, its next node will be some state s′ that is reachable from s.\\nTherefore, the contribution of this particular trajectory to vπ(s) is\\naccounted for when we sum over that particular s′.\\n14.2.2\\nConnection with Dynamic Programming\\nIn COS 226, you may have seen an implementation of a bottom-up\\ndynamic programming.\\nFigure 14.3: A Dynamic Programming\\nimplementation of a coin changing\\nproblem that uses the bottom-up\\napproach.\\nIn such implementations, the algorithm divides the problem into\\nsubproblems arranged as directed acyclic graphs and computes\\n“bottom-up.” The MDP from the cake eating problem is acylic and\\nour method using a look-ahead tree is similar to the dynamic pro-\\ngramming algorithms. Therefore, it seems like we can apply a similar\\nalgorithm to the cake eating problem.\\nExample 14.2.6. Consider Example 14.2.3 again, but now with a discount\\nfactor of 0.9. We will find the value vπ(s) of each state s by going bottom-up\\nfrom the tree in Figure 14.2. We start by noticing that vπ(0) = 0 as can be\\nseen from the bottom row. Then from the third node of the third row, we can\\ncalculate\\nvπ(1) = 1 · (1 + 0.9 · 0) = 1\\nFrom the second node of the second row, we can calculate\\nvπ(2) = 0.5 · (1 + 0.9 · 0) + 0.5 · (1 + 0.9 · 1) = 1.45\\nFinally, from the top node, we can calculate\\nvπ(3) = 0.5 · (1 + 0.9 · 1) + 0.5 · (1 + 0.9 · 1.45) = 2.1025\\nBut in general, the dynamic programming approach does not\\ncompletely apply to MDP. The biggest assumption for dynamic\\nprogramming algorithms is that the graph is acyclic, but MDPs are\\ngenerally allowed to have directed cycles if we can return to the same\\nstate after a sequence of actions. Therefore, computing the expected\\nreward for even a single policy π involves solving a system of linear\\nequations.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 180}, page_content='markov decision process\\n181\\nExample 14.2.7. Assume that we have three states s1, s2, s3 and transitions\\nas in Figure 14.4 with a discount factor of γ = 0.7. Then the value at each\\nstate is given as\\nvπ(s1) = 0.2 × (1 + 0.7vπ(s1)) + 0.8 × (2 + 0.7vπ(s2))\\nvπ(s2) = 0.5 × (2 + 0.7vπ(s1)) + 0.5 × (2 + 0.7vπ(s3))\\nvπ(s3) = 1 × (0 + 0.7vπ(s2))\\nUnlike in Example 14.2.6, we cannot compute any of these values one by\\none because the values are interdependent in a cyclic manner. Instead, we\\nneed to solve the linear equation as a whole, which gives us the solution:\\nvπ(s1) ≈5.47, vπ(s2) ≈5.18, vπ(s3) ≈3.63.\\nFigure 14.4: Visual representation of the\\nMDP in Example 14.2.7.\\n14.3\\nOptimal Policy\\nOut of all choices for a policy, we are interested in the optimal policy,\\nthe one that maximizes the expected (discounted) reward. Surpris-\\ningly, it is known that there always exists a policy π∗that obtains the\\nmaximum expected reward from all initial states simultaneously; that\\nis π∗= arg max\\nπ\\nvπ(s) for every state s. 4. The value function of the\\n4 If there are multiple such policies, we\\ndenote any one of them by π∗.\\noptimal policy is called the optimal value function and is often denoted\\nas v∗(s). Then we can express the optimal value function using (14.2)\\nas:\\nv∗(s) = max\\nπ ∑\\ns′\\np(s′ | s, π(s))(r(π(s) | s, s′) + γvπ(s′))\\nThis is just restating the fact that the optimal value of state s is the\\nmaximum of all possible values vπ(s) of s under a policy π — i.e.,\\nthe Bellman equation evaluated with the values vπ(s′) of each child\\nnode s′ under that specific policy π.\\nBut we can even go further than this result. It is known that the\\noptimal value also satisfies the following:\\nv∗(s) = max\\nπ ∑\\ns′\\np(s′ | s, π(s))(r(π(s) | s, s′) + γv∗(s′))\\n(14.3)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 181}, page_content='182\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nNotice that vπ(s′) in the summation has now been replaced with\\nv∗(s′). This property, known as the Bellman Optimality condition,\\nstates that the optimal value is even the maximum when the Bellman\\nequation is evaluated with the values v∗(s′), regardless of the choice\\nof the policy π.\\nNotice that the right-hand side of (14.3) only depends on the\\nchoice of the action a of the given state s, not any other states. There-\\nfore, we can rewrite (14.3) as:\\nv∗(s) = max\\na∈As ∑\\ns′\\np(s′ | s, a)(r(a | s, s′) + γv∗(s′))\\n(14.4)\\nwhich also suggests that the optimal action at state s can be ex-\\npressed as:\\nπ∗(s) = arg max\\na∈As\\n∑\\ns′\\np(s′ | s, a)(r(a | s, s′) + γv∗(s′))\\n(14.5)\\nBut the problem is: it is unclear how to turn this into an efficient\\nalgorithm. Computing the value v∗(s) depends on the value v∗(s′),\\nwhich can also depend on v∗(s), which becomes recursive.\\nIn this section, we present an iterative algorithm called the value\\niteration method which will be used to compute the optimal policy.\\nBefore we describe the algorithm, we unpack the underlying ideas.\\n14.3.1\\nDeveloping Intuition about Optimality: Gridworld\\nTo develop intuition about how to find an optimum policy, let’s\\nconsider a classic example called Gridworld. 5\\n5 Source: Sutton and Barton 2020, https:\\n//web.stanford.edu/class/psych209/\\nReadings/SuttonBartoIPRLBook2ndEd.\\npdf\\nExample 14.3.1 (Gridworld). Consider a 5 × 5 grid. The set of states\\nis given as the cells of this grid. At each state except for A = (1, 2) and\\nB = (1, 4), there are four available actions: move left/right/up/down, each\\nwith reward 0, except in the following setting: if the action will make you\\nmove off the grid. then the reward is −1, and you are made to stay at the\\nsame state instead.\\nAt A, there is only one action: move to A′ = (5, 2) with reward 10 and\\nsimilarly at B, there is one action: move to B′ = (3, 4) with reward 5. 6 The\\n6 The outgoing transition from A and B\\ncan be thought of as “wormholes.”\\ndiscount factor is given as 0.9.\\nHow can we compute the reward for a policy in the example\\nabove? When beginners try to calculate the exact value using the\\nabove definitions, they quickly get bogged down in keeping track of\\ntoo many variables, equations, and recurrences.\\nInstead, let’s try to think intuitively about what an optimal policy\\nshould be trying to do. Since the wormholes are the only source of\\nrewards, an optimal policy should be trying to utilize the wormholes\\nas much as possible. Using this kind of intuition, we can design a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 182}, page_content='markov decision process\\n183\\nFigure 14.5: Visual representation of the\\nGridworld.\\npolicy that looks at least near-optimal, and use its value as a lower\\nbound for the optimal policy.\\nFirst, let v∗(s) denote the value vπ∗(s) of state s for an optimal\\npolicy π∗. Since there is only one action to choose from at state A, we\\nknow that\\nv∗(A) = 10 + γv∗(A′)\\n(14.6)\\nNow, at the state A′, one possible trajectory you can follow is “go\\nup four steps” (each with reward 0) back to A. We know that the\\noptimal value has to be at least as great as this value. That is\\nv∗(A′) ≥γ4v∗(A)\\n(14.7)\\nCombining (14.6) and (14.7), we get\\nv∗(A) ≥10 + γ5v∗(A)\\nIf we solve for v∗(A), we get\\nv∗(A) ≥\\n10\\n1 −γ5 ≈24.4\\nThe value iteration method discussed below is based on this\\nintuition — we can provide a lower bound for the optimal policy\\nby suggesting some potential policy. If we repeat this process, the\\nlower bound for the optimal policy can only go up. At the end of\\nthe section, we will prove that this process converges to the actual\\noptimal value.\\n14.3.2\\nValue Iteration Method\\nValue Iteration is a method guaranteed to find the optimal policy. At\\neach step of the iteration, we are given a lower bound on the optimal\\nvalues of each state s. Using the values of the immediate children\\nnodes in the tree, we can compute an improved lower bound on\\nv∗(s).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 183}, page_content='184\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nExample 14.3.2. See Figure 14.6. Suppose there are two actions to take at\\nstate s. The first action, labeled as blue, will lead to state s1 with reward −1\\nwith probability 0.5 and s3 with reward −1 with probability 0.5. The second\\naction, labeled as red, will lead to state s2 with reward 2 with probability\\n1. The discount factor is given as 0.6. Now assume that someone tells us\\nthat they know a way to get an expected reward of 12 starting from s1, 1\\nfrom s2, and 4 from s3, regardless of the choice of initial action at s. In\\nother words, the optimal values for these three states are lower bounded by:\\nv∗(s1) ≥12, v∗(s2) ≥1 and v∗(s3) ≥4. Using this fact, we consider two\\nstrategies 7 — (1) first take action blue at state s and play optimally thereon\\n7 This is not necessarily a policy because\\nthe second part of playing optimally\\nmay require you to return to state s and\\ntake an action that is inconsistent with\\nyour initial choice of action.\\nbased on the other person’s knowledge; (2) first take action red at state s and\\nplay optimally thereon. The lower bound for the expected reward for each of\\nthe two strategies can be computed as:\\nvblue(s) ≥0.5 × (−1 + 0.6 × 12) + 0.5 × (−1 + 0.6 × 4) = 3.8\\nvred(s) ≥1.0 × (2 + 0.6 × 1) = 2.6\\nThe Bellman Optimality condition in (14.4) guarantees that the optimal\\npolicy is at least as good as either of these strategies. Therefore v∗(s) has to\\nbe larger than both vblue, vred; that is, v∗(s) ≥3.8.\\nFigure 14.6: There are two actions you\\ncan take at state s, and you will end up\\nin one of the three states: s1, s2, s3.\\nIn general, the value iteration algorithm looks like:\\n1. Initialize some values v0(s) for each state s such that we are guar-\\nanteed v0(s) ≤v∗(s)\\n2. For each time step k = 1, 2, . . ., and for each state s, use the values\\nvk(s′) of the immediate children s′ to compute an updated value\\nvk+1(s) such that vk+1(s) ≤v∗(s). 8\\n8 These values vk(s) maintained by the\\nalgorithm is not necessarily associated\\nwith a specific policy. They are just a\\nlower bound for the optimal value v∗(s)\\nthat will be improved over time.\\n3. When k →∞, each vk(s) will converge to the optimal value v∗(s).\\nRecall from (14.1) that if all transition rewards are within [−R, R],\\nthen the expected rewards at any state for any policy lies in\\nh\\n−\\nR\\n1−γ,\\nR\\n1−γ\\ni\\n.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 184}, page_content='markov decision process\\n185\\nTherefore, we can set the initial value v0(s) = −\\nR\\n1−γ to be the lower\\nbound for each state s. 9\\n9 Our proof assumes this special\\ninitialization where all v0(s) = −\\nR\\n1−γ\\nfor all states s. It turns out the value\\niteration method converges to the\\noptimal value for arbitrary initialization,\\nbut the proof is more complicated.\\nAfter the k-th iteration of the algorithm, we will maintain a value\\nvk(s) for state s, where the condition vk(s) ≤v∗(s) is maintained as\\nan invariant. Now at the (k + 1)-th iteration, the algorithm will update\\nthe values at each state s as the following:\\nvk+1(s) = max\\nπ ∑\\ns′\\np(s′ | s, π(s)) ·\\n\\x00r(π(s) | s, s′) + γvk(s′)\\n\\x01\\n(14.8)\\nThis is just the Bellman equation evaluated with the values vk(s′) of\\neach children node.\\nExample 14.3.3 (Example 14.3.1 revisited). Say we start the value\\niteration on the gridworld with all values equal to zero. Now let us compute\\nv1(A), the value of A after the first iteration. Recall that A has only one\\naction to choose from: moving to A′. Denote this action by a. Therefore,\\nv1(A) = p(A′ | A, a) ·\\n\\x00r(a | A, A′) + γv0(A′)\\n\\x01\\n= 1.0 · (10 + 0.9 · 0) = 10\\nProblem 14.3.4 (Example 14.3.1 revisited). Start value iteration with\\nall values equal to zero. What is v2((1, 3)), the value of (1, 3) after second\\niteration?\\n14.3.3\\nWhy Does Value Iteration Find an Optimum Policy?\\nAssume γ < 1. We prove that the values vk(s) maintained by the\\nvalue iteration method converge to the optimal values vπ(s). We\\nbreak this proof down into two parts. We first prove that the invari-\\nant vk(s) ≤v∗(s) holds throughout the algorithm. Then we prove\\nthat in general, vk+1(s) is a tighter lower bound for v∗(s) than vk(s).\\nProposition 14.3.5. For each time step k = 1, 2, . . ., and for each state s, the\\ninvariant vk(s) ≤v∗(s) holds.\\nProof. Proof by mathematical induction. As discussed earlier, our\\nchoice of initial values v0(s) = −\\nR\\n1−γ satisfies the invariant. Now\\nassume that the invariant holds for some k. Now consider the update\\nrule of the value iteration algorithm:\\nvk+1(s) = max\\nπ ∑\\ns′\\np(s′ | s, π(s)) ·\\n\\x00r(π(s) | s, s′) + γvk(s′)\\n\\x01\\nNotice that for any specific policy π and for any next state s′, we\\nhave\\np(s′ | s, π(s)) ·\\n\\x00r(π(s) | s, s′) + γvk(s′)\\n\\x01\\n≤p(s′ | s, π(s)) ·\\n\\x00r(π(s) | s, s′) + γv∗(s′)\\n\\x01'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 185}, page_content='186\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nbecause of the inductive hypothesis that vk(s′) ≤v∗(s′). Therefore, if\\nwe sum over all state s′, we have\\n∑\\ns′\\np(s′ | s, π(s)) ·\\n\\x00r(π(s) | s, s′) + γvk(s′)\\n\\x01\\n≤∑\\ns′\\np(s′ | s, π(s)) ·\\n\\x00r(π(s) | s, s′) + γv∗(s′)\\n\\x01\\nSince this inequality holds for every policy π, we have the following:\\nvk+1(s) = max\\nπ ∑\\ns′\\np(s′ | s, π(s)) ·\\n\\x00r(π(s) | s, s′) + γvk(s′)\\n\\x01\\n≤max\\nπ ∑\\ns′\\np(s′ | s, π(s)) ·\\n\\x00r(π(s) | s, s′) + γv∗(s′)\\n\\x01\\n= v∗(s)\\nwhere we apply the Bellman Optimality condition (14.4) in the last\\nequality. This concludes the inductive step, and it suffices for the\\nproof.\\nNow to prove that these values vk(s) eventually converge to v∗(s),\\nwe introduce the following definition:\\nDefinition 14.3.6. The residual at s at the k-th iteration is defined as\\nδs,k = v∗(s) −vk(s) ≥0.\\nNotice that as long as the residuals at the k-th iteration converge\\nto 0, the values vk(s) also converge to v∗(s). Since the residuals take\\nfinite values when the algorithm is initiated, it suffices to prove that\\nthe residuals decrease non-trivially in every iteration. 10\\n10 Our exposition of Value Iteration with\\nour particular initialization is new. The\\nusual textbook description requires a\\nslightly more complicated argument.\\nProposition 14.3.7. If the largest residual at iteration k is denoted as\\nδk = maxs δs,k, then the largest residual δk+1 at iteration k + 1 satisfies\\nδk+1 ≤γδk\\nProof. Let a∗be the action at s under the optimum policy π∗. Then\\nby (14.2),\\nv∗(s) = ∑\\ns′\\np(s′ | s, a∗)(r(a∗| s, s′) + γv∗(s′))\\n(14.9)\\nNote that taking the action a∗is always an option at the (k + 1)-\\nth iteration, so vk+1(s), the maximum value across all policies (in\\nparticular, across all actions available at s), has to be greater than or\\nequal to the value computed with the action a∗; that is,\\nvk+1(s) = max\\nπ ∑\\ns′\\np(s′ | s, π(s))(r(π(s) | s, s′) + γvk(s′))\\n≥∑\\ns′\\np(s′ | s, a∗)(r(a∗| s, s′) + γvk(s′))\\n(14.10)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 186}, page_content='markov decision process\\n187\\nSubtracting (14.10) from (14.9), we get\\nv∗(s) −vk+1(s) ≤γ\\n \\n∑\\ns′\\np(s′ | s, a∗)(v∗(s′) −vk(s′))\\n!\\nBy the definition of δk, each of v∗(s′) −vk(s′) = δs′,k ≤δk. Therefore,\\nδs,k+1 = v∗(s) −vk+1(s) ≤γδk∑\\ns′\\np(s′ | s, a∗) = γδk\\nwhere the last equality uses the fact that ∑\\ns′ p(s′ | s, a∗) = 1 because p\\nis a probability distribution. Since this inequality holds for any state\\ns, we conclude that\\nδk+1 = max\\ns\\nδs,k+1 ≤γδk\\nTheorem 14.3.8. For each s ∈S, vk(s) converges to v∗(s) when k →∞.\\nProof. By Proposition 14.3.5 and Proposition 14.3.7,\\n|v∗(s) −vk(s)| = v∗(s) −vk(s) ≤δk ≤γkδ0\\nwhich converges to 0 when k goes to infinity.\\nTheoretically, the value iteration method may not converge in a\\nfinite number of steps, and the values maintained by the algorithm\\nvk(s) may only asymptotically approach the optimal values v∗(s).\\nHowever, in practice, the value iteration method will always termi-\\nnate, albeit sometimes not at convergence. The current design of\\ncomputers uses a discrete set of floating point numbers to approx-\\nimate the set of real numbers R. Once the theoretical difference\\nbetween vk(s) and vk+1(s) becomes smaller than what the computers\\ncan process as different, no changes will be made to the values, and\\nthe algorithm is guaranteed to terminate. However, the values when\\nthe algorithm terminates may be slightly off from the optimal values.\\n14.3.4\\nRetrieving Optimal Policy from the v∗’s\\nOne important thing to note is that the value iteration method finds\\nthe optimal value of each state, not the optimal policy. So we need an\\nextra step to retrieve the optimal policy from the output of the value\\niteration algorithm. This can be done by considering the Bellman\\nOptimality condition. For each state s, define π∗(s) = a∗such that\\na∗= arg max\\na∈As\\n∑\\ns′\\np(s′ | s, a)(r(a | s, s′) + γv∗(s′))\\n(14.5 revisited)\\nwhere v∗(s) is the value that the value iteration algorithm converges\\nto. If there are multiple actions a that satisfy the equation above,\\narbitrarily choose an action.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 187}, page_content='188\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nExample 14.3.9 (Example 14.3.1 revisited). Say we ran the value iteration\\nalgorithm on the Gridworld. The output of the algorithm (the optimal values\\nof each state) is given in Table 14.1.\\n22.0\\n24.4\\n22.0\\n19.4\\n17.5\\n19.8\\n22.0\\n19.8\\n17.8\\n16.0\\n17.8\\n19.8\\n17.8\\n16.0\\n14.4\\n16.0\\n17.8\\n16.0\\n14.4\\n13.0\\n14.4\\n16.0\\n14.4\\n13.0\\n11.7\\nTable 14.1: Optimal values v∗(s) of the\\nGridworld.\\nConsider the state A′ = (5, 2). There are four actions to take: left-\\n/right/up/down. Each action would yield the following values when evaluat-\\ning the Bellman equation:\\nvle f t(A′) = 0 + 0.9 × 14.4 = 13.0\\nvright(A′) = 0 + 0.9 × 14.4 = 13.0\\nvup(A′) = 0 + 0.9 × 17.8 = 16.0\\nvdown(A′) = −1 + 0.9 × 16.0 = 13.4\\nThe only action that maximizes the value is the action “go up.” Therefore,\\nwe can conclude that the optimal policy π∗will adopt the action “go up” for\\nthe state A′.\\nProblem 14.3.10 (Example 14.3.1 revisited). Verify that an optimal policy\\ncan assign either the action “go up” or the action “go left” for the state\\n(5, 3).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 188}, page_content='15\\nReinforcement Learning in Unknown Environments\\nIn the previous Chapter 14, we established the principles of reinforce-\\nment learning using a Markov Decision Process (MDP) with set of\\nstates S, set of actions A, transition probabilities p(s′ | a, s), and the\\nrewards r(a | s, s′). We saw a method (value iteration) to find the\\noptimal policy that will maximize the expected reward for every state.\\nThe main assumption of the chapter was that the agent has access to\\nthe full description of the MDP — the set of states, the set of actions,\\nthe transition probabilities, rewards, etc.\\nBut what can we do when some of the parameters of the MDP are\\nnot available to the agent in advance — specifically, the transition\\nprobabilities and the rewards? Instead, the agent makes actions and\\nobserves the new state and the reward it just received. Using such\\nexperiences it begins to learns the reward and transition structure,\\nand then to translate this incremental knowledge into improved\\nactions.\\nThe above scenario describes most real-life agents: the system\\ndesigner does not know a full description of the probabilities and\\ntransitions. For instance, think of the sets of possible states and\\ntransitions in the MuJoCo animals and walkers that we saw. Even\\nwith a small number of joints, the total set of scenarios is too vast.\\nThus the designer can set up an intuitive reward structure and let the\\nlearner figure out from experience (which is tractable since it involves\\na simulation).\\nSettings where agent must determine (or “figure out”) the MDP\\nthrough experience, specifically by taking actions and observing the\\neffects, is called the “model-free” setting of RL. This chapter will\\nintroduce basic concepts, including the famous Q-learning algorithm.\\nIn many settings today, the underlying MDP is too large for the\\nagent to reconstruct completely, and the agent uses deep neural\\nnetworks to represent its knowledge of the environment and its own\\npolicy.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 189}, page_content='190\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n15.1\\nModel-Free Reinforcement Learning\\nIn model-free RL, we know the set of states S and the set of actions A,\\nbut the transition probabilities and rewards are unknown. The agent\\nnow needs to explore the environment to estimate the transition\\nprobabilities and rewards. Suppose the agent is originally in state\\ns1, chooses to take an action a, and ends up in state s2. The agent\\nimmediately observes some reward r(a | s1, s2), but we need more\\ninformation to figure out p(s2, |s1, a).\\nOne way we can estimate the transition probabilities is through the\\nMaximum Likelihood Principle. This concept has been used before\\nwhen considering estimating unigram probabilities in Chapter 8. In\\nmodel-free RL, an agent can keep track of the number of times they\\ntook action a at state s1 and ended up in state s2 — denote this as\\n#(s1, a, s2). Then the estimate of the transition probability p(s′|s, a) is:\\np(s2|s1, a) =\\n#(s1, a, s2)\\n∑\\ns′ #(s1, a, s′)\\n(15.1)\\nThe Central Limit Theorem (see Chapter 18) guarantees that esti-\\nmates will improve with more observations and quickly converge to\\nunderlying state-action transition probabilities and rewards.\\n15.1.1\\nGroundhog Day\\nGroundhog Day is an early movie about a “time loop” and the title\\nhas even become an everyday term. The film tracks cynical TV weath-\\nerman Phil Connors (Bill Murray) who is tasked with going to the\\nsmall town of Punxsutawney and filming its annual Groundhog Day\\ncelebration. He ends up reliving the same day over and over again,\\nand becomes temporarily trapped. Along the way, he attempts to\\ncourt his producer Rita Hanson (Andie MacDowell), and is only\\nreleased from the time loop after a concerted effort to improve his\\ncharacter.\\nSounds philosophically deep! On the internet you can find various\\ninterpretations of the movie: Buddhist interpretation (“many reincar-\\nnations ending in Nirvana”) and psychoanalysis (“revisiting of the\\nsame events over and over again to reach closure”). The RL interpre-\\ntation is that Phil is in an model-free RL environment, 1 revisiting\\n1 Specifically a model-free RL environ-\\nment with an ability to reset to an initial\\nstate. This happens for example with a\\nrobot vacuum that periodically returns\\nto its charging station. After charging,\\nit starts exploring the MDP from the\\ninitial state again.\\nthe same events of the day over and over again and figuring out his\\noptimal actions.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 190}, page_content='reinforcement learning in unknown environments\\n191\\n15.2\\nAtari Pong (1972): A Case Study\\nIn 1972, the classic game of Pong was released by Atari. This was the\\nfirst commercially successful video game, and had a major cultural\\nimpact on the perception of video games by the general public. The\\nrules of the game are simple: each player controls a virtual paddle\\nwhich can move vertically in order to rally a ball back and forth\\n(one participant may be an AI agent). If a player misses the ball, the\\nother player wins a point. We can consider the total number of points\\naccumulated by a player to be their reward so far. While technology\\nand video games have become far more advanced in the present, it\\nis still useful to analyze Pong today. This is because it is a simple\\nexample of a physics-based system, similar to (but far less advanced\\nthan) the MuJoCo stick figure simulations discussed in Chapter 13. It\\nthus provides a useful case study to demonstrate how an agent can\\nlearn basic principles of physics through random exploration and\\nestimation of transition probabilities.\\nLet’s apply some simplifications in the interest of brevity. We\\ndefine the pong table to be 5 × 5 pixels in size, the ball to have a size\\nof 1 pixel, and the paddles to be 2 pixels in height. We define the\\nstate at a time t as the locations of the two paddles at time t, and the\\nlocations of the ball at time t and time t −1. 2\\n2 Storing the location of the ball at time\\nt −1 and time t allows us to calculate\\nthe difference between the two locations\\nand thus gives an estimate for the\\nvelocity.\\nWe additionally restrict attention to the problem of tracking and\\nreturning the ball, also known as “Pico-Pong.” Thus, we define the\\ngame to begin when the opponent hits the ball. The agent gets a\\nreward of +1 if they manage to hit the ball, −1 if they miss, and 0\\nif the ball is still in play. As soon as the agent either hits the ball or\\nmisses, we define that the game ends. Of course, these additional\\nrules of the game are not available to the agent playing the game.\\nThe agent needs to “learn” these rules by observing the possible\\nstates, transitions, and corresponding rewards.\\nIn general, these simplifications remove complications of modeling\\nthe opponent and makes the MDP acyclic; an explanatory diagram\\nis shown in Figure 15.1. Throughout this section, we will build\\nintuition about different aspects of our Pico-Pong model through\\nsome examples.\\n15.2.1\\nPico-Pong Modeling: States\\nSuppose the agent is playing random paddle movements. Consider\\nthe possible states of the game shown in Figure 15.2. We note that\\nout of the three, the third option is never seen. By the definition of\\nthe game, the ball can never move away from the agent. Of course,\\nthe agent is oblivious to this fact at first, but once the game proceeds,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 191}, page_content='192\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nFigure 15.1: The simplified Pico-Pong\\nsetup which will be considered in this\\ncase study.\\nthe agent will be able to implicitly “learn” that the ball can never\\nmove away from them.\\nFigure 15.2: Out of these possible states,\\nthe third option is never seen.\\n15.2.2\\nPico-Pong Modeling: Transitions\\nLet us now add another restriction to the game that the ball always\\nmoves at a speed of 1 pixel every time step (i.e., moves to one of the\\n8 adjacent pixels) and in a straight linear path unless being bounced\\nagainst the top/bottom wall. Consider the possible transitions shown\\nin Figure 15.3. We note that out of the three, the third option is never\\nseen. By the restriction of the game, the ball cannot move 2 pixels in\\none time step. The agent thus implicitly “learns” that the ball moves\\nat a constant speed of 1 pixel per time step.\\nFigure 15.3: Out of these possible\\ntransitions, the third option is never\\nseen.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 192}, page_content='reinforcement learning in unknown environments\\n193\\nProblem 15.2.1. Suppose the agent is playing randomly and the ball is trav-\\neling at a speed of 1 pixel per step. Which of the transitions in Figure 15.4 is\\nnever seen, and why?\\nFigure 15.4: Out of these possible\\ntransitions, one option is never seen.\\n15.2.3\\nPico-Pong Modeling: Rewards\\nSuppose the agent is playing randomly and the ball is traveling at a\\nspeed of 1 pixel per step. Consider the action in Figure 15.5. We note\\nthat the associated reward will be +1 because in the resulting state\\nthe agent has “hit” the ball. The agent thus implicitly learns that if\\nthe ball is 1 pixel away horizontally, it should move to intercept it to\\nobtain a positive reward.\\nFigure 15.5: Taking action ↓results in a\\nreward of +1.\\nProblem 15.2.2. Suppose the agent is playing randomly and the ball is\\ntraveling at a speed of 1 pixel per step. What reward is achieved given the\\ncurrent state and chosen action in Figure 15.6, and why?\\nFigure 15.6: What reward will result\\nwhen taking action ↓?\\n15.2.4\\nPlaying Optimally in the Learned MDP\\nAfter allowing the agent to explore enough, the agent has “learned”\\nsome information about the underlying MDP of the Pico-Pong model.\\nThe first thing the agent can learn is that, out of all possible states,\\nthere is a subset of states that never appear in the game (e.g., ball'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 193}, page_content='194\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nmoving away from the agent or ball moving too fast). The agent will\\nbe able to ignore these states, while learning how to play optimally in\\nstates that did occur while exploring.\\nFigure 15.7: An example look-ahead\\ntree for the Pico-Pong model.\\nAlso, the agent has now “learnt” the transition probabilities and\\nrewards of the MDP. Using these estimates, the agent is able to build\\nup a representation of the MDP. Since the underlying MDP for the\\nsimplified Pico-Pong model is acyclic, the optimal policy can be\\ndetermined using a simple look-ahead tree. An example diagram is\\nshown in Figure 15.7.\\nWe provide a specific example to aid the exposition. Suppose\\nan agent finds themselves in the state shown in Figure 15.8. Since\\nthe path of the ball is already determined, the next possible state is\\nuniquely determined by the choice of the action — “go down” or\\n“stay in place” or “go up.” If the agent chooses to “go down,” the\\ngame will end with a reward of +1. If the agent chooses to “stay in\\nplace” or “go up,” the game continues for another time step, but no\\nmatter the choice of action on that step, the game will end with a\\nreward of −1. Therefore, the agent will learn that the optimal policy\\nwill assign the action of “go down” in the state shown in Figure 15.8.\\nProblem 15.2.3. Draw out the look-ahead tree from the state shown in\\nFigure 15.8.\\nFigure 15.8: A sample state in the game\\nplay of Pico-Pong.\\nProblem 15.2.4. Suppose we start from the state shown in Figure 15.9.\\nAssuming optimal play, what is the expected reward for the agent? (Hint:\\nconsider if the agent will be able to reach the ball in time.)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 194}, page_content='reinforcement learning in unknown environments\\n195\\nFigure 15.9: A sample state in the game\\nplay of Pico-Pong.\\nImpressive! The agent has learnt how to return the ball in Pico-\\nPong by first building up the MDP and its transitions/rewards\\nthrough repeated observations, and then computing the optimum\\npolicy for the constructed MDP through a look-ahead tree. 3\\n3 How would you extend these ideas to\\ndesign a rudimentary ping pong robot\\nwhich can track and return the ball?\\n15.3\\nQ-learning\\n15.3.1\\nExploration vs. Exploitation\\nLet us analyze the case study with Pico-Pong more deeply. We\\ncan separate the process of learning into two different stages —\\nexploration and exploitation:\\n• Exploration: This pertains to what the agent did in the first phase.\\nRandom paddle movements were used to help build up previously\\nunknown knowledge of the MDP — transition probabilities and\\nrewards.\\n• Exploitation: This pertains to what the agent did in the second\\nphase. Specifically, the agent used the learnt MDP to play opti-\\nmally.\\nIn general, an RL environment is more complicated than Pico-\\nPong, and there is no clear-cut boundary of when an agent has\\nexplored “sufficiently.” It is best to combine the two stages (i.e.,\\nexploration and exploitation) into one and “learn as you go.” Also,\\nit is difficult to balance between these two processes, and how to\\nfind the correct trade-off between exploration and exploitation is a\\nrecurring topic in RL.\\n15.3.2\\nQ-function\\nWe now introduce the Q-function, an important concept that helps tie\\ntogether concepts of exploration and exploitation when considering\\ngeneral MDPs with discounted rewards.\\nDefinition 15.3.1 (Q-function). We define the Q-function Q : S × A →R\\nas a table which assigns a real value Q(s, a) to each pair (s, a) where s ∈S\\nand a ∈A.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 195}, page_content='196\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nIntuitively, the value Q(s, a) is the current estimate of the expected\\ndiscounted reward when we take action a from state s. In other words, it\\nis the estimate of the value vπ(s) if π is any policy that will assign\\nthe action a to state s. Using the currently stored values of the Q-\\nfunction, we can define a canonical policy πQ. For each state s, the\\npolicy will assign the action a that maximizes the Q(s, a) value; that\\nis,\\nπQ(s) = arg max\\na\\nQ(s, a)\\nSince the agent only has access to the estimate values Q(s, a), but\\nnot the actual value function v, this is the most optimal policy to\\nthe agent’s knowledge. Therefore, if the agent chooses to take an\\nexploitation step, they will take an action prescribed by the policy πQ\\nwith respect to the currently maintained Q-function.\\nInstead of relying on the currently stored Q-function, we can also\\nchoose to take an exploration step. Every time we take an exploration\\nstep and receive additional information about the RL environment,\\nwe update the values of the Q-function accordingly. The goal of the\\nQ-learning is to learn the optimal Q-function, which approximates the\\noptimal policy π∗and the optimal value function v∗as closely as\\npossible. We formalize the notion as follows:\\nDefinition 15.3.2 (Optimal Q-function). The optimal Q-function is a\\nQ-function that satisfies the following two conditions:\\n• The corresponding canonical policy πQ is an optimal policy for the MDP.\\n• The Q-function satisfies the following condition:\\nQ(s, a) = ∑\\ns′; a\\np(s′ | s, a)(r(a | s, s′) + γ max\\nb\\nQ(s′, b))\\n(15.2)\\nThe first condition of Definition 15.3.2 states that for a fixed state s,\\nthe action a that maximizes Q(s, a) is a = π∗(s). This condition only\\ncares about the relative ordering of the values of Q(s, a) — as long\\nas Q(s, π∗(s)) is the maximum value among all Q(s, a), then it is fine.\\nThis condition guarantees that the action we take in the exploitation\\nstep is an optimal action.\\nThe second condition is formally stating that the values of the\\nQ-function are estimates of the expected reward when we take action\\na from state s. It also suggests that Q-function needs to “behave\\nlike” a value function vπ for some policy π. However, whereas a\\nsimilar condition for a value function vπ only needs to hold for one\\nparticular action (i.e., a = π(s)) given a state s, this condition for a\\nQ-function should hold for any arbitrary action a. Note that for an\\noptimal Q-function, the term maxb Q(s′, b) in (15.2) is equivalent to\\nvπQ(s′).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 196}, page_content='reinforcement learning in unknown environments\\n197\\n15.3.3\\nQ-learning\\nNow that we have defined the Q-function and the optimal Q-function,\\nit is time for us to study how to learn the optimal Q-function. This\\nprocess is called Q-learning. The basic idea is to probabilistically\\nchoose between exploration or exploitation: we define some probabil-\\nity ϵ ∈[0, 1] such that we choose a random action a with probability ϵ\\n(exploration) or choose the action a according to the current canonical\\npolicy πQ with probability 1 −ϵ (exploitation). If we choose the explo-\\nration option, we use its outcome to update the Q(s, a) table. But how\\nshould we define the update rule?\\nLet’s take a step back and consider a (plausibly?) real life scenario.\\nYou are a reporter for the Daily Princetonian at Princeton, and want\\nto estimate the average wealth of alumni at a Princeton Reunions\\nevent. The alumni, understandably vexed by such a request, strike\\na compromise that you are only allowed to ask one alum about their\\nnet worth. Can you get an estimate of the average? Well, you could\\npick an alum at random and ask them their net worth! 4\\n4 The expectation gives the right aver-\\nage. But typically the answer would be\\nfar from the true average; especially if\\nJeff Bezos happens to be attending the\\nreunion.\\nWith this intuition, we return to the world of Q-learning. Suppose\\nyou start at some state st, take an action at, receive a reward of rt, and\\narrive at state st+1. We call this process an experience. Now, when we\\nupdate the current estimate of Q(st, at), we ideally want to mimic the\\nbehavior of the optimal Q-function in (15.2) and update it to:\\nQ(st, at) = ∑\\ns′; at\\np(s′ | st, at) · (r(at | st, s′) + γ max\\nb\\nQ(s′, b))\\n(15.3)\\nNotice that this is the weighted average of the expected reward\\nr(at | st, s′) + γ maxb Q(s′, b) over all possible next states s′ given the\\naction at. But in practice, the agent only has the ability to take a single\\nexperience; they lack the ability to “reset” and retake the step to try\\nall states s′ according to the transition probability p(s′ | st, at). We\\nthus must consider an alternative idea — we define the estimate for\\nQ(st, at) according to the experience at time step t as\\nQ′\\nt = rt + γ max\\nb\\nQ(st+1, b)\\nThis estimate can be calculated using the observed reward rt and\\nlooking up the Q values of the state st+1 on the Q-function table.\\nNote that the expectation of Q′\\nt is exactly the right hand side of (15.3).\\nThat is,\\nE[Q′\\nt] = ∑\\ns′; at\\np(s′ | st, at) · (r(at | st, s′) + γ max\\nb\\nQ(s′, b))\\nThis is because the agent took a transition to state st+1 with probabil-\\nity p(st+1 | st, at) (of course, the agent does not know this value). This'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 197}, page_content='198\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nis thus analogous to the single-sample estimate of average alumni\\nwealth at the Princeton Reunions event. We can now define the\\nfollowing update rule of the Q-learning process:\\nQ(st, at) ←Q(st, at) + η(Q′\\nt −Q(st, at))\\n= (1 −η)Q(st, at) + ηQ′\\nt\\n(15.4)\\nfor some learning rate η > 0. You can understand this update rule in\\ntwo different ways. First, we are gently nudging the value of Q(st, at)\\ntowards the estimate Q′\\nt from the most recent experience. We can\\nalternatively think of the updated value of Q(st, at) as the weighted\\naverage of the previous value of Q(st, at) and the estimate Q′\\nt. In\\neither approach, the most important thing to note is that we combine\\nboth the previous Q value and the new estimate to compute the\\nupdated Q value. This is because the new estimate is just a single\\nsample that can be far off from the actual expectation, and also\\nbecause after enough iterations, we can assume the previous Q value\\nto contain information from past experience.\\nExample 15.3.3. Let’s return to our adventures in Pico-Pong and consider\\nthe situation in Figure 15.10. Denote the state in the left diagram as st and\\nthe state in the right as st+1. Suppose the current value of Q(st, a) = 0.4\\nwith a =↑. Assuming that Q(st+1, a) = 0 for all a, we can compute the\\nestimate Q′\\nt from this experience as\\nQ′\\nt = rt + γ max\\nb\\nQ(st+1, b) = 1\\nThen the Q value will be increased to 0.4 + 0.6η.\\nFigure 15.10: The diagram representing\\ntwo states in a game of Pico-Pong.\\n15.3.4\\nDeep Q-learning\\nNote that the update rule in (15.4) looks similar to the Gradient De-\\nscent algorithm. They are both iterative processes which incorporate\\na learning rate η. In fact, you can consider the Q-learning update rule\\nto be trying to minimize the squared difference between Q(st, at) and\\nQ′\\nt. The similarity between the Q-learning update rule and the Gra-\\ndient Descent algorithm allows us to utilize a deep neural network'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 198}, page_content='reinforcement learning in unknown environments\\n199\\nto learn the optimal Q-function. Such a network is called the Deep Q\\nNetwork (DQN).\\nIn a DQN, the Q-function can be represented by the parameters\\nW of the network. We emphasize this by denoting the Q-function as\\nQW(s, a). Now instead of directly updating the Q-function as in the\\nupdate rule\\nQ(st, at) ←Q(st, at) + η(Q′\\nt −Q(st, at))\\n(15.4 revisited)\\nwe instead update the parameters W such that the Q-function is\\nupdated accordingly.\\nFirst consider the case that Q′\\nt > Q(st, at). That is, the estimated\\nQ-value is larger than the currently stored value. Then the update\\nrule (15.4) will increase the value of Q(st, at). To mimic this behavior,\\nwe want to find an update rule for W that will increase the Q-value.\\nThis is given as:\\nW ←W + β · ∇WQW(st, sa)\\nfor some learning rate β > 0.\\nProblem 15.3.4. Suppose Q′\\nt < Q(st, at). How should we design the weight\\nupdates?\\nOne final thing to note is a technique called experience replay. Ex-\\nperiencing the environment can be expensive (i.e., computation time,\\nmachine wear, etc.). Therefore, it is customary to keep a history of\\nold experiences and their rewards, and periodically take a random\\nsample out of the old experiences to update the Q values. In partic-\\nular, experience replay ensures that DQNs are efficient and avoid\\n“catastrophic forgetting.” 5\\n5 Catastrophic forgetting is a phe-\\nnomenon where a neural network, after\\nbeing exposed to new information,\\n“forgets” information it had learned\\nearlier.\\n15.4\\nApplications of Reinforcement Learning\\n15.4.1\\nQ-learning for Breakout (1978)\\nWe previously considered using reinforcement learning for Pong. We\\ncan also use it for another famous Atari game called Breakout. One\\nparticular design uses a CNN to process the screen and uses the\\n\"score\" as a reward. As shown in Figure 15.11, the model becomes\\nquite successful after several epochs.\\n15.4.2\\nSelf-help Apps\\nSelf-help apps are designed to aid in recovery of the user from ad-\\ndiction, trauma, heart disease, etc. A typical design involves an RL\\nalgorithm which determines the next advice/suggestion based upon\\nreversals, achieved milestones, etc. so far. These can be a helpful\\nsupplement to expensive therapy/consultation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 199}, page_content='200\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nFigure 15.11: An application of Q-\\nlearning to the famous Atari game\\nBreakout.\\n15.4.3\\nContent Recommendation\\nAt reputable websites, we might imagine that there exists a page cre-\\nation system designed to capture the “reward” of user engagement.\\nWe can use MDP techniques to model this situation. Specifically,\\nwe can define s0 as the outside link which brought the user to the\\nlanding page and/or the past history of the user on the site. If the\\nuser clicks on a link, a new page is created and we can define s1\\nas a concatenation of s0 and the new link. If the user again clicks\\non a link, another new page is created and we can define s2 as the\\nconcatenation of s1 and the new link.\\n15.5\\nDeep Reinforcement Learning\\nDeep Reinforcement Learning is a subfield of machine learning that\\ncombines the methods of Deep Learning and Reinforcement Learning\\nthat we have discussed earlier. 6 The goal of it is to create an artificial\\n6 Source: https://www.youtube.com/\\nwatch?v=x5Q79XCxMVc\\nagent with human-level intelligence (Artifical General Intelligence,\\nAGI). In general, Reinforcement Learning defines the objective and\\nDeep Learning gives the mechanism for optimizing that objective.\\nDeep RL combines the problem given by the RL specification with\\nthe solution given by the DL technique. In the cited source video,\\nRL expert David Silver made three broad conjectures related to this\\ntopic.\\n1. RL is enough to formalize the problem of intelligence\\n2. Deep neural networks can represent and learn any computable\\nfunction\\n3. Deep RL can solve the problem of intelligence\\nMany Deep RL models are trained to play games (e.g., chess, Go) be-\\ncause it is easy to evaluate progress. By letting them compete against'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 200}, page_content='reinforcement learning in unknown environments\\n201\\nhumans, we can easily compare them to human-level intelligence. As\\nan example, Google Deepmind trained a Deep RL model called DQN\\nto play 49 arcade games. 7 The computer is not given the explicit set\\n7 For the full paper, visit https:\\n//storage.googleapis.com/\\ndeepmind-media/dqn/DQNNaturePaper.\\npdf\\nof rules; instead, given only the pixels and game score as input, it\\nlearns by using deep reinforcement learning to maximize its score.\\nAmazingly, on about half of the games, the model played at least at a\\nhuman level of intelligence!\\n15.5.1\\nChess: A Case Study\\nFounders of AI considered chess to be the epitome of human in-\\ntelligence. In principle, the best next move can be calculated via a\\nlook-ahead tree (similar to Figure 13.5 from the cake-eating example).\\nSince chess is a two-player game, we can use an algorithm called the\\nmin-max search on the look-ahead game tree. 8\\n8 Source: https://www.youtube.com/\\nwatch?v=l-hh51ncgDI\\nUsually, RL agents are playing against the nature that causes\\nthem to take random transitions according to the MDP’s transition\\nprobabilities. But in chess, the agent plays against an opponent that\\nis trying to make the agent take the largest possible loss (the largest\\npossible gain for the opponent). That is why we need a min-max\\nevaluation of the look-ahead tree.\\nFigure 15.12: An example look-ahead\\ngame tree for chess with depth 3. White\\nwill choose the right option.\\nIn Figure 15.12, the numbers at the leaf nodes represent a static\\nevaluation of how good the game configuration is for white. This\\nis an approximation for the actual value of the node. An example\\nmetric in chess would be the difference in the number of pieces (#\\nwhite −# black). These numbers are evaluated either when the game\\nterminates or when the algorithm has reached the specified number\\nof steps to look ahead. If the game ever reaches the specified node,\\nthe white has two options to choose from: if white chooses the left\\nchild node, it will end up with reward of −1; whereas if it chooses\\nthe right child node, the reward will be 3. Then to maximize reward,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 201}, page_content='202\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nthe best move of white will be to choose 3. 9\\n9 For those who are familiar with chess\\nor game theory in general, this is\\nknown as the best response.\\nFigure 15.13: Black will choose the left\\noption.\\nIn Figure 15.13, it is now black’s turn to choose. Note that the\\nreward for black is the opposite of the reward for white, so black\\nwants to minimize the value on the tree. Therefore, black will want to\\nchoose the left child node.\\nSo whenever we are at a configuration, we can create a look-\\nahead tree for a reasonable number of steps and try to calculate\\nthe best move. But the size of a game tree is astronomical, so it is\\ncomputationally infeasible to search all levels of the tree. 10\\n10 There is an optimization method\\ncalled alpha-beta pruning. Consult\\nthe video referenced above for an\\nimplementation on the game of chess.\\n15.5.2\\nAlphaGo: A Case Study\\nGo is a game invented in China around 500 BC. It is played by 2\\nplayers on a 19 × 19 grid. Players take turns placing stones on the grid,\\nand if any set of stones is entirely surrounded by opponent stones,\\nthe enclosed stones are taken away from the board and awarded to\\nthe opponent as points. Even though the rules are very simple, no\\ncomputer could beat a good human amateur at Go until 2015. 11\\n11 In comparison, IBM’s Deep Blue\\nmodel beat the world chess champion\\nKasparov in 1997.\\nHow can we utilize RL concepts to play this game? In general, we\\ncan create a Deep Policy Net (DPN) to learn W, which is a function\\nthat takes state s as an input and outputs a probability distribution\\npW(a | s) over the next possible actions from s. AlphaGo is an\\nexample of a DPN engineered by the Google Deepmind lab. It takes\\nthe current board position as the input and uses ConvNet to learn the\\ninternal weights, and outputs the value given by a softmax function.\\nIn its initial setup, the DPN was trained using a big dataset of past\\ngames. 12\\n12 Source: https://www.youtube.com/\\nwatch?v=Wujy7OzvdJk\\nTo be more specific, AlphaGo used supervised learning from\\nhuman data to learn the optimal policy (action to take at each game\\nsetting). In other words, it used convolutional layers to replicate the\\nmoves of professional players as closely as possible. Since the CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 202}, page_content='reinforcement learning in unknown environments\\n203\\nFigure 15.14: The diagram representing\\nthe process of training AlphaGo.\\nis just mimicking human players, it cannot beat human champions.\\nHowever, it can be used to search the full game tree more efficiently\\nthan the alpha-beta search. Formally, this method is called the Monte\\nCarlo Tree Search, where the CNN is used to decide the order in\\nwhich to explore the tree. After the policy network was sufficiently\\ntrained, reinforcement learning was used to train the value network\\nfor position evaluation. Given a board setting, the network was\\ntrained to estimate the value (i.e., likelihood of winning) of that\\nsetting.\\nAlphaGo Zero is a newer version of the model that does not\\ndepend on any human data or features. In this model, policy and\\nvalue networks are combined into one neural network, and the model\\ndoes not use any randomized Monte-Carlo simulations. It learns\\nsolely by self-play reinforcement learning and uses neural network\\n(ResNet) to evaluate its performance. Within 3 days of training,\\nAlphaGo Zero surpassed an earlier version of AlphaGo that beat Lee\\nSe Dol, the holder of 8 world titles; within 21 days, it surpassed the\\nversion that beat Ke Jie, the world champion. Interestingly enough,\\nAlphaGo Zero adopted some opening patterns commonly played by\\nhuman players, but it also discarded some common human patterns\\nand it also discovered patterns unknown to humans.\\nThe newest version of AlphaGo is called AlphaZero. It is a model\\nthat can be trained to play not just Go but simultaneously Chess and\\nShogi (Japanese chess). After just a few hours of training, AlphaZero\\nsurpassed the previous computer world champions (Stockfish in\\nChess, Elmo in Shogi, and AlphaGo Zero in Go). Just as AlphaGo Zero\\ndid, AlphaZero was able to dynamically adopt or discard known\\nopenings in chess.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 203}, page_content=''),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 204}, page_content='Part V\\nAdvanced Topics'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 205}, page_content=''),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 206}, page_content='16\\nMachine Learning and Ethics\\nThroughout this course, we have discussed the technical aspects of\\nmodel design, training, and testing in depth. However, we have not\\nyet discussed some of the social implications of this technology. What\\nare some ethical and legal issues in deployment of ML techniques in\\nsociety? What are the caveats and limitations to temper our exuber-\\nance about the possibilities of ML? This brief chapter addresses these\\nissues, and we hope as technologists you will continue to investigate\\nand consider such issues throughout your career.\\n16.1\\nFacebook’s Suicide Prevention\\nFigure 16.1: A visualization of the\\nFacebook model to predict suicides.\\nIn 2017, Facebook launched a program to use a machine learning\\nalgorithm to predict suicide risk amongst its user population. It has\\ncontinued with various iterations over the years. Figure 16.1 gives a\\nvisualization of the four-step process:\\n1. ML algorithm automatically analyzes a post by processing its text\\ncontent and comments'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 207}, page_content='208\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n2. Algorithm additionally uses spatial-temporal context of the post to\\nperform a risk prediction\\n3. A human behind the algorithm performs a personal review to\\nfinally verify if a threshold is reached\\n4. If the post poses a serious risk, Facebook performs a wellness\\ncheck through the person’s contacts, community organizations, etc.\\nAt first sight, this may appear to be very good idea: even if it saves\\njust one life, surely the project is worth it? But the announcement of\\nthe project cause a lot of controversy among people. The following\\nare some of the potential problems that people identified:\\n1. False positives may result in stigmatization.\\n2. Many people who contemplate suicide do not end up going\\nthrough with it. Facebook’s reporting could lead to criminal\\npenalties (in regions where suicide is a crime), involuntary hospi-\\ntalization, stigmatization, etc.\\n3. Involvement of authorities (e.g., law enforcement) raises risk of\\nillegal seizures.\\n4. Should Facebook be liable for any problem caused by mis-\\ndetection?\\nBeyond these points, there are deep philosophical questions as-\\nsociated with the concept of suicide as well. For instance, is suicide\\nactually immoral? Even if it is immoral, is it the responsibility of\\nFacebook to get involved? Is it moral for Facebook to use personal\\ninformation to assess suicide risk? Opinions differ.\\n16.2\\nRacial Bias in Machine Learning\\nSuppose we are designing a machine learning approach for loan\\napproval. The general approach will be to take a dataset of (⃗x, y),\\nwhere⃗x is a vector of the individual’s attributes (e.g., age, education,\\nalma mater, address, etc.) who got a loan and y ∈{−1, 1} indicates\\nwhether they actually paid off the loan or not. Using the approaches\\nwe learned in Section 4.2, we could train a binary classifier through\\nlogistic regression. Civil rights legislation forbids using the individ-\\nual’s race in many of these decisions, so while training we could\\nsimply mask out any coordinates which identify race. However, this\\ndoes not guarantee that the classifier will be entirely “race-neutral.” 1\\n1 The reason is that race happens\\nto be correlated with many other\\nattributes. Thus if a classifier uses any\\nof the correlated attributes, it may be\\nimplicitly using racial information in\\nthe decision making process.\\nIn 2016, a study 2 found that COMPAS, a leading software for\\n2 Machine Bias, by Angwin et al., in Pro\\nPublica 2016.\\nassessing the probability that a prison inmate would commit another'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 208}, page_content='machine learning and ethics\\n209\\nserious crime, disproportionately tags African-American as being\\nlikely to commit crimes — in the sense that African-Americans who\\nwere tagged as likely to commit another crime were only half as\\nlikely to actually commit a crime than a similarly-tagged person of\\nanother race.\\nWhite\\nAfrican-American\\nLabeled Higher Risk\\n23.5%\\n44.9%\\n& Did Not Re-offend\\nLabeled Lower Risk\\n47.7%\\n28.0%\\n& Did Re-offend\\nTable 16.1: COMPAS correctly predicts\\nrecidivism 61 percent on average. But\\nAfrican-Americans are almost twice\\nas likely as whites to be labeled a\\nhigher risk but not actually re-offend.\\nConversely, whites are twice as likely as\\nAfrican-Americans to be labeled lower\\nrisk but go on to commit other crimes.\\n16.3\\nConceptions of Fairness in Machine Learning\\nWe will briefly consider possible ways to formulate fairness in ma-\\nchine learning. Keep in mind that this task is intrinsically difficult,\\nas we are attempting to assign a quantifiable objective to a funda-\\nmentally normative problem. The first property we might want an\\nML classifier to have is called demographic parity, which effectively\\nenforces that the output of classifier does not depend on a protected\\nattribute (e.g., race, ethnicity, gender).\\nDefinition 16.3.1 (Demographic Parity). We say that a binary classifier\\nthat outputs y ∈{−1, 1} satisfies demographic parity if Pr[y | xi = a] =\\nPr[y | xi = b] where a, b are any two values that a protected attribute xi can\\ntake.\\nFigure 16.2: A hypothetical application\\nof ML to a loan approval application.\\nRace has been made a protected at-\\ntribute in an attempt to prevent bias\\nduring training.\\nA visualization of how a protected attribute could be specified in a\\ndataset is shown in Figure 16.2. Consider the loan approval example'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 209}, page_content='210\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nfrom the previous section. If the binary classification model for the\\nloan approval satisfies the demographic parity property, then the\\nmodel approves loans for different races at similar rates. One way\\nto achieve this condition is to use a regularizer term λ(Pr[y | xi =\\na] −Pr[y | xi = b])2) during training. 3\\n3 Does this seem like a good formula-\\ntion of fairness?\\nAnother property we might want a “fair” model to satisfy is called\\nthe predictive parity. This is the property that the model in Table 16.1\\nfailed to satisfy.\\nDefinition 16.3.2 (Predictive Parity). We say that a binary classifier that\\noutputs y ∈{−1, 1} satisfies predictive parity if the true negative, false\\nnegative, false positive, true positive rates are the same for any values of a\\nprotected attribute.\\nFigure 16.3: A table of all possible\\noutcomes based on the model output\\nand the ground truth outcome. This is\\nalso known as a confusion matrix.\\nIdeally, we want an ML model to satisfy both the demographic\\nparity and predictive parity. However, it turns out that these two\\nnotions are incompatible!\\nTheorem 16.3.3 (Fairness Impossibility Theorem). 4 Under fairly general\\n4 See Inherent Trade-Offs in the Fair\\nDetermination of Risk Scores, Kleinberg,\\nMullainathan, and Raghavan, ITCS 2017.\\nThe paper actually considered three\\npossible definitions of “fairness” and\\nshowed every pair of them are mutually\\nincompatible.\\nconditions, demographic parity and predictive parity are incompatible.\\nThere are other formulations of fairness, but it is difficult to find a\\ncombination of these notions that are compatible with each other. So\\none way or another, we need to sacrifice some notions of “fairness.”\\n16.4\\nLimitations of the ML Paradigm\\nThe predictive power of ML seems immense, but is it true that if we\\nhave enough data and the right algorithm, then everything becomes\\npredictable? If yes, then one could imagine societal programs leverag-\\ning this to precisely target help to where it would be more effective.\\nWe first consider a famous — and somewhat amusing — example of\\na study 5 that turned out to be false.\\n5 Extraneous factors in judicial decisions,\\nDanziger et al., PNAS 2011.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 210}, page_content='machine learning and ethics\\n211\\n16.4.1\\nHungry Judge Effect\\nThe study analyzed the parole decisions made by 8 Israeli judges in\\nover 1, 100 cases. The data in Figure 16.4 shows that prisoners were\\nmuch more likely to be granted parole after the judge took a lunch\\nbreak or a coffee break. The study therefore suggested that judges\\ntend to be stricter before a break (maybe because they are “hangry”)\\nbut more lenient when they return from the break.\\nFigure 16.4: Data from the study\\nshows an uptick in favorable decisions\\nfollowing a lunch break or a coffee\\nbreak.\\nNevertheless, it turns out that this “hungry judge effect” can be\\nexplained by a completely different reason. A followup study 6\\n6 Overlooked factors in the analysis of\\nparole decisions, Weinshall-Margel and\\nShepard, PNAS, 2012.\\nfound that the ordering of cases presented to the judge was not\\nrandom: prisoners with attorneys were scheduled at the beginning\\nof each session, while prisoners without an attorney were scheduled\\nat the end of a session. The former group were let on parole with a\\nrate of 67%, while the rate was just 39% for those without attorneys.\\nAnother important observation was that attorneys tended to present\\ntheir cases in decreasing order of strength of case, with the average\\nattorney having 4.1 clients. Computer simulations of hunger-immune\\njudges faced with cases presented according to these percentages\\nshowed the same see-saw effect of Figure 16.4.\\n16.4.2\\nFragile Families Challenge\\nThe Fragile Families Challenge is a collaborative project initiated by\\nthe Center for Research on Child Wellbeing at Princeton University.\\nA brief description of the initiative’s motivation is provided on the\\nwebsite: 7\\n7 Source: http://www.\\nfragilefamilieschallenge.org.\\nThe Fragile Families Challenge is a mass collaboration that combines pre-\\ndictive modeling, causal inference, and in-depth interviews to yield insights\\nthat can improve the lives of disadvantaged children in the United States.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 211}, page_content='212\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nBy working together, we can discover things that none of us can discover\\nindividually.\\nThe Fragile Families Challenge is based on the Fragile Families and Child\\nWellbeing Study, which has followed thousands of American families for\\nmore than 15 years. During this time, the Fragile Families study collected\\ninformation about the children, their parents, their schools, and their larger\\nenvironments.\\nFigure 16.5: Diagram illustrating\\nthe dataset of the Fragile Families\\nChallenge. After training a model on\\nthe training data, participants made\\npredictions on held-out data and\\nsubmitted the results to a leaderboard.\\nThe initiative has collected immense data on multiple families,\\nincluding interviews with mothers, fathers, and/or primary care-\\ngivers at several ages. Interviewees were inquired as to attitudes,\\nrelationships, parenting behavior, economic and employment status,\\netc. Additionally, in-home assessments of children and their home\\nenvironments were performed to assess cognitive and emotional\\ndevelopment, health, and home environment. The goal was to predict\\nsix key outcomes at age 15 (e.g., whether or not the child is attending\\nschool) given background data from birth to age 9 as shown in 16.5.\\nHowever, up to this point no method has done better than random\\nguessing.\\nThis is food for thought: what is going on?\\n16.4.3\\nGeneral Limits to Prediction\\nMatt Salganik and Arvind Narayanan, professors at Princeton Uni-\\nversity, recently started a course 8 which aims to explore the extent\\n8 The course, COS 597E/SOC 555 is a\\nseminar first offered in Fall 2020.\\nto which interdisciplinary problems in social science and computer\\nscience can be predictable. In general, the following are some major\\nthemes that can make prediction difficult:\\n1. The distribution associated with data can shift over time\\n2. The relationship between input data and desired outputs can\\nchange over time'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 212}, page_content='machine learning and ethics\\n213\\n3. There is a possibility for undiscovered coordinates to be uninten-\\ntionally ignored (i.e., as in the hungry judge effect)\\n4. The “8 billion problem,” which describes how data available in the\\nreal world is fundamentally finite and limited\\n16.5\\nFinal Thoughts\\nAs described in the preceding sections, users and designers of ma-\\nchine learning will often face ethical dilemmas. Designers may have\\nto operate without moral clarity or easy technical fixes. In fact, techni-\\ncal solutions may even be impossible. To appropriately acknowledge\\nthese limitations, it is important to embrace a culture of measuring\\nand openly discussing the impact of the system being built. Indeed, a\\ngeneral principle to follow is to avoid harm when trying to do good.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 213}, page_content=''),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 214}, page_content='17\\nDeep Learning for Natural Language Processing\\n17.1\\nWord Embeddings\\nIn traditional NLP, each word is regarded as a distinct symbol each\\nwith a single value of weight. For example, in Chapter 1, we learned\\nhow to use linear regression on sentiment prediction. But with this\\napproach, it is hard for the computer to learn the meaning of the\\nword; instead, each of the words remains as some abstract symbol\\nwith numeric weights.\\nBut how do computers know the meaning of words? We can easily\\nthink of one solution: we can look up words in a dictionary. For\\nexample, WordNet is a project that codes the meaning of the words\\nand the relationship between the words, so that the data can be used\\nfor computers to parse. 1 But resources like WordNet require human\\n1 For more information, check http:\\n//wordnetweb.princeton.edu.\\nlabor to create and adapt, and it is impractical to keep up-to-date\\n(because new words are coined and new meanings appear out of\\nexisting words).\\nAn alternative approach is to represent words as short (50 - 300\\ndimensions 2), real-valued vectors. These vectors encode the meaning\\n2 The dimension of word vectors is\\na hyperparameter that needs to be\\ndecided first.\\nand other properties of words. In this representation, the distance\\nbetween vectors represents the similarity between words. This vector-\\nized form of the words is much easier to be used as input in modern\\nML systems (especially neural networks). These vector forms of\\nwords are known as word embedding. In this section, we explore the\\nprocess of how to learn a good word embedding.\\n17.1.1\\nDistributional Hypothesis\\nWord embedding is based on a concept called the distributional hypoth-\\nesis, a theory developed by John Rupert Firth. The hypothesis, one of\\nthe most successful ideas of modern statistical NLP, says that words\\nthat occur in similar contexts tend to have similar meaning.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 215}, page_content='216\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nDefinition 17.1.1 (Context). When a word w appears in a text, its context\\nis the set of words that appear nearby (within a fixed-size window).\\nExample 17.1.2. Assume that you first heard the word tejuino and have no\\nidea what the word means. But you learn that the word may appear in the\\nfollowing four contexts.\\n• C1: A bottle of\\nis on the table.\\n• C2: Everybody likes\\n.\\n• C3: Don’t have\\nbefore you drive.\\n• C4: We make\\nout of corn.\\nBased on these contexts, it is reasonable to conclude that the word “tejuino”\\nrefers to some form of alcoholic drink made from corn.\\nProblem 17.1.3. To find words with similar meanings as “tejuino,” we\\ntried filling out the contexts from Example 17.1.2 with 5 other words. The\\nresults are given in Table 17.1, where 1 means that a native speaker deemed\\nthe word was appropriate to be used in that context, and 0 means that it was\\ninappropriate.\\nC1\\nC2\\nC3\\nC4\\ntejuino\\n1\\n1\\n1\\n1\\nloud\\n0\\n0\\n0\\n0\\nmotor-oil\\n1\\n0\\n0\\n0\\ntortillas\\n0\\n1\\n0\\n1\\nchoices\\n0\\n1\\n0\\n0\\nwine\\n1\\n1\\n1\\n0\\nTable 17.1: Data showing if 6 words\\nare appropriate for the four contexts in\\nExample 17.1.2.\\nWhich word is closest to “tejuino”?\\n17.1.2\\nWord-word Co-occurrence Matrix\\nGiven a very large collection of documents with words from a dic-\\ntionary V, we construct a |V| × |V| matrix X, where the entry at the\\ni-th row, j-th column denotes the number of times (i.e., frequency)\\nthat wj appears in the context window of wi. This matrix is called the\\nword-word co-occurrence matrix.\\nExample 17.1.4. Table 17.2 shows a portion of a word-word co-occurrence\\nmatrix. Each row corresponds to the center word wi, and each column\\ncorresponds to the context word wj. The value Xij at the (i, j) entry means\\nthat the context word wj appeared Xij times in the context (of length 4) of\\nwi in total.\\nAlthough the portion shown in Table 17.2 mostly has non-zero entries, in\\ngeneral, the entries of the matrix are mostly zero.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 216}, page_content='deep learning for natural language processing\\n217\\n· · ·\\ncomputer\\ndata\\nresult\\npie\\nsugar\\n· · ·\\ncherry\\n· · ·\\n2\\n8\\n9\\n442\\n25\\n· · ·\\nstrawberry\\n· · ·\\n0\\n0\\n1\\n60\\n19\\n· · ·\\ndigital\\n· · ·\\n1670\\n1683\\n85\\n5\\n4\\n· · ·\\ninformation\\n· · ·\\n3325\\n3982\\n378\\n5\\n13\\n· · ·\\nTable 17.2: A portion of a word-word\\nco-occurrence matrix for a corpus\\nof Wikipedia articles. Source: https:\\n//www.english-corpora.org/wiki/.\\n17.1.3\\nFactorization of Word-word Co-occurrence Matrix\\nRecall the example of movie recommendation through matrix factor-\\nization in Chapter 9. In that example m × n matrix M was factored\\ninto M ≈AB where the i-th row of A was a d-dimensional vector\\nthat represented user i and the j-th column of B was a d-dimensional\\nvector that represented movie j.\\nWe can imagine a similar factorization on the word-word co-\\noccurrence matrix. That is, we can represent each center word and\\neach context word as a d-dimensional vector such that Xij ≈Ai∗·\\nB∗j. But this particular idea does not work on the word-word co-\\noccurrence matrix. The key difference is that X is a complete matrix\\nwith no missing entries (although most entries are zero). Therefore\\nwe instead use other standard matrix factorization techniques.\\nOne popular choice of factorization is running the Singular Value\\nDecomposition (SVD) on a weighted co-occurrence matrix. 3 This\\n3 The particular weighting scheme is\\ncalled PPMI. We will not get into that\\ndetail here.\\nidea originates from a concept called Latent Semantic Anlysis. 4 If the\\n4 From Indexing by Latent Semantic\\nAnalysis by Deerwester et al., 1990.\\nSVD returns the following decomposition,\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nX\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nW\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nσ1\\n0\\n· · ·\\n0\\n0\\nσ2\\n· · ·\\n0\\n...\\n...\\n...\\n...\\n0\\n0\\n· · ·\\nσd\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nV⊺\\n\\uf8f9\\n\\uf8fa\\uf8fb\\nwhere X is a |V| × |V| matrix, W is a |V| × d matrix, and V is a\\nd × |V| matrix, then the i-th row of matrix W can be regarded as the\\nembedding for word wi.\\nOther modern approaches tend to treat word vectors as parame-\\nters to be optimized for some objective function and apply the gra-\\ndient descent algorithm. But the principle is the same: “words that\\noccur in similar contexts tend to have similar meanings.” Some of the\\npopular algorithms with this approach include: word2vec (Mikolov et\\nal., 2013), GloVe (Pennington et al., 2014), and fastText (Bojanowski et\\nal., 2017).\\nHere we briefly explain the GloVe algorithm. Given the co-\\noccurrence table X, we will construct a center word vector ⃗ui ∈Rd\\nand a context word vector ⃗vj ∈Rd such that they optimize the follow-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 217}, page_content='218\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\ning objective:\\nJ(θ) = ∑\\ni,j∈V\\nf (Xij)\\n\\x10\\nui · vj + bi + ebj −log Xij\\n\\x112\\n(17.1)\\nwhere f is some non-linear function and bi, ebj are bias terms, and θ is\\nthe set of all entries in ui, vj and bi, ebj. This is within the same line of\\nlogic as optimizing\\nL(A, B) =\\n1\\n|Ω| ∑\\ni,j∈Ω\\n(Mij −(AB)ij)2\\n((9.5) revisited)\\n17.1.4\\nProperties of Word Embeddings\\nA good word embedding should represent the meaning of the words\\nand their relationship with other words as accurately as possible.\\nTherefore there are some properties that we would like a word\\nembedding to preserve. We will discuss three such properties and\\nsee how the current algorithms for word embedding perform on\\npreserving those properties.\\n1. Similar words should have similar word vectors:\\nThis is the most\\nimportant property we can think of.\\nExample 17.1.5. In a certain word embedding, the following is the list of 9\\nmost nearest words to the word “sweden.”\\nWord\\nCosine distance\\nnorway\\n0.760124\\ndenmark\\n0.715460\\nfinland\\n0.620022\\nswitzerland\\n0.588132\\nbelgium\\n0.585835\\nnetherlands\\n0.574631\\niceland\\n0.562368\\nestonia\\n0.547621\\nslovenia\\n0.531408\\nNotice Scandanavian countries are the top 3 entries on the list, and the rest\\nare also European country names.\\n2. Vector difference should encode the relationship between words:\\nIf\\nthere are two or more pairs of words where each pair of words are\\ndistinguishable by the same attribute, you can imagine that the vector\\ndifference within each pair is nearly the same.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 218}, page_content='deep learning for natural language processing\\n219\\nExample 17.1.6. In Figure 17.1, notice that vman −vwoman ≈vking −vqueen.\\nThe vector difference in common can be understood as representing the male-\\nfemale relationship. Similarly, there seems to be a common vector difference\\nfor representing the difference in verb tense.\\nFigure 17.1: Pairs of words that differ\\nin the same attribute show a similar\\ndifference in their word embeddings.\\n3. The embeddings should be translated between different languages:\\nWhen we independently find the word embedding in different lan-\\nguages, we can expect to have a bijective mapping that preserves the\\nstructure of the words in each language. 5\\n5 From Exploiting Similarities among\\nLanguages for Machine Translation by\\nMikolov et at., 2013.\\nExample 17.1.7. In Figure 17.2, notice that if we let W to be the mapping\\nfrom English to Spanish word embeddings, vcuatro ≈W ◦v f our\\nFigure 17.2: Word embeddings are\\ntranslated into the embeddings of other\\nlanguages.\\n17.2\\nN-gram Model Revisited\\nRecall the n-gram model from Chapter 8. It assigned a probability\\nPr[w1w2 . . . wn] to every word sequence w1w2 . . . wn. We discussed\\nthe concept of perplexity of the model to compare the performance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 219}, page_content='220\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nof unigram, bigram, and trigram models. While the n-gram model is\\nimpressive, it has obvious limitations.\\nProblem 17.2.1. “The students opened their\\n.” Can you guess the\\nnext word?\\nProblem 17.2.2. “As the proctor started the clock, the students opened their\\n.” Can you guess the next word?\\nIn a lot of cases, words in a sentence are closely related to other\\nwords and phrases that are far away. But the n-gram model cannot\\nlook beyond the specified frame.\\nExample 17.2.3. The following is a text generated by a 4-gram model\\nToday the price of gold per tan, while production of shoe\\nlasts and shoe industry, the bank intervened just after it\\nconsidered and rejected an imf demand to rebuild depleted\\neuropean stocks, sept 30 and primary 76 cts a share.\\nThe generated text is surprisingly grammatical, but incoherent.\\nExample 17.2.3 shows that we need to consider more than three\\nwords at a time if we want to model language well. But if we use\\na larger value of n for the n-gram model, the data will become too\\nsparse to estimate the probabilities. But even when we restrict our-\\nselves to words that appear in the dictionary, there are 1021 distinct\\nsequences of 4 words.\\n17.2.1\\nFeedforward Neural Language Model\\nThe idea of the feedforward neural language model was proposed by\\nBengio et al. in 2003 in a paper called A Neural Probabilistic Language\\nModel. The intuition is to use a neural network to learn the probabilis-\\ntic distribution of language, instead of estimating raw probabilities.\\nThe key ingredient in this model is the word embeddings we dis-\\ncussed earlier.\\nExample 17.2.4. Assume we are given two contexts “You like green\\n” and “You like yellow\\n” to fill the blanks in. A n-gram\\nmodel will try to calculate the raw probabilities Pr[w | You like green] and\\nPr[w | You like yellow]. However, if the word embeddings showed that\\nvgreen ≈vyellow, then we can imagine that the two contexts are similar\\nenough. Then we may be able to estimate the probabilities better.\\nNow we show how to use the feedforward neural language model\\non a n-gram model. Assume we want to estimate the probability\\nPr[wn+1 | w1 . . . wn]. Then the first step is to find a word embedding\\nv1, v2, . . . , vn ∈Rd'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 220}, page_content='deep learning for natural language processing\\n221\\nof each word w1, w2, . . . , wn. Then we concatenate the word embed-\\ndings into 6\\n6 the order of the input vectors cannot\\nchange\\n⃗x = (v1, . . . , vn) ∈Rnd\\nThis will be the input layer. Then we define the fully connected\\nhidden layer as\\n⃗h = tanh(W⃗x +⃗b) ∈Rh\\nwhere W ∈Rh×nd and ⃗b ∈Rh. Then we define the output layer as\\n⃗z = U⃗h ∈R|V|\\nwhere U ∈R|V|×h. Then finally, the probability will be calculated\\nwith the softmax function:\\nPr[wn+1 = i | w1 . . . wn] = softmaxi(⃗z) =\\nezi\\n∑\\nk∈V\\nezk\\nSo the total number of parameters to train in this network is\\nd |V| + ndh + h + h |V|\\nwhere the terms are respectively for the input embeddings, W,⃗b, U.\\nWhen d = h, sometimes we tie the input and output embeddings.\\nThat is, we can consider U to be the parameters required for the\\noutput embeddings. At this point, the language model reduces\\nto a |V|-way classification, and we can create lots of training ex-\\nample by sliding the input-output indices. That is, when given\\na huge text, we can create lots of input-output tuple as follows:\\n((w1, . . . , wn), wn+1), ((w2, . . . , wn+1), wn+2), . . ..\\n17.2.2\\nBeyond Feedforward Neural Language Model\\nBut the feedforward language model still has its limitations. The\\nmain reason is that W ∈Rh×nd scales linearly with the window\\nsize. Of course, this is better than the traditional n-gram model\\nwhich scales exponentially with n. Another limitation of the neu-\\nral LM is that the model learns separate patterns for the same item.\\nThat is, a substring wkwk+1, for example, will correspond to differ-\\nent parameters in W when trained on (wkwk+1 . . . wk+n−1) or on\\n(wk−1wk . . . wk+n−2).\\nTo mitigate these limitations, we can choose to use similar model-\\ning ideas but use better and bigger neural network architectures like\\nrecurrent neural networks (RNN) or transformers.\\nHere we briefly explain the core ideas of a RNN. RNNs are a fam-\\nily of neural networks that handle variable length inputs. Whereas\\nfeedforward NNs map a fixed-length input to a fixed-length output,\\nrecurrent NNs map a sequence of inputs to a sequence of outputs. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 221}, page_content='222\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nFigure 17.3: A visual representation of\\nan RNN architecture.\\nsequence length can vary and the key is to reuse the weight matrices at\\ndifferent time steps. When the inputs are given as⃗x1,⃗x2, . . .⃗xT ∈Rd\\nand we want to find outputs ⃗h1,⃗h2, . . .⃗hT ∈Rh, we train the parame-\\nters\\nW ∈Rh×h, U ∈Rh×d,⃗b ∈Rh\\nsuch that\\n⃗ht = g(W⃗ht−1 + U⃗xt +⃗b) ∈R\\nwhere g is some non-linear function (e.g., ReLU, tanh, sigmoid). We\\ncan also set ⃗h0 =⃗0 for simplicity.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 222}, page_content='Part VI\\nMathematics for Machine\\nLearning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 223}, page_content=''),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 224}, page_content='18\\nProbability and Statistics\\n18.1\\nProbability and Event\\n18.1.1\\nSample Space and Event\\nProbability is related to the uncertainty and randomness of the world.\\nIt measures the likelihood of some outcome or event happening. To\\nformalize this concept, we introduce the following definition:\\nDefinition 18.1.1 (Sample Space and Event). A set S of all possible\\noutcomes of a random phenomenon in the world is called a sample space.\\nEach element x ∈S is called an outcome. A subset A ⊂S is called an\\nevent.\\nExample 18.1.2. The sample space of “the outcome of tossing two dice” is the\\nset S = {(1, 1), (1, 2), . . . , (6, 6)} of 36 elements. The event “the sum of the\\nnumbers on the two dice is 5” is the subset A = {(1, 4), (2, 3), (3, 2), (4, 1)}\\nof 4 elements.\\n18.1.2\\nProbability\\nGiven a sample space S, we define a probability for each event A of\\nthat space. This probability measures the likelihood that the outcome\\nof the random phenomenon belongs to the set A.\\nDefinition 18.1.3 (Probability). A probability Pr : S →R≥0 is a\\nmapping from each event A ⊂S to a non-negative real number Pr[A] ≥0\\nsuch that the following properties are satisfied:\\n1. 0 ≤Pr[A] ≤1 for any A ⊂S\\n2. Pr[S] = 1\\n3. For any countable collection {A1, A2, . . .} of events that are pairwise\\ndisjoint (i.e., Ai ∩Aj = ∅for any i ̸= j),\\nPr\\n\" ∞\\n[\\ni=1\\nAi\\n#\\n=\\n∞\\n∑\\ni=1\\nPr[Ai]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 225}, page_content='226\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nWhen the sample space is finite or countably infinite, 1 the properties above\\n1 A countably infinite set refers to a\\nset whose elements can be numbered\\nwith integer indices. The set N of\\nnatural numbers or the set Q of rational\\nnumbers are examples of countably\\ninfinite sets.\\ncan be simplified into the following condition:\\n∑\\nx∈S\\nPr[{x}] = 1\\nExample 18.1.4. Consider the sample space of “the outcome of tossing two\\ndice” again. Assuming the two dice are fair, the probability of each outcome\\ncan be defined as 1/36. Then the probability of the event “the sum of the\\nnumbers on the two dice is 5” is 4/36.\\nExample 18.1.5. We are picking a point uniformly at random from the sam-\\nple space [0, 2] × [0, 2] in the Cartesian coordinate system. The probability of\\nthe event that the point is drawn from the bottom left quarter [0, 1] × [0, 1] is\\n1/4.\\n18.1.3\\nJoint and Conditional Probability\\nIn many cases, we are interested in not just one event, but multiple\\nevents, possibly happening in a sequence.\\nDefinition 18.1.6 (Joint Probability). For any set of events A =\\n{A1, . . . , An} of a sample space S, the joint probability of A is the proba-\\nbility Pr[A1 ∩. . . ∩An] of the intersection of all of the events. The probability\\nPr[Ai] of each of the events is also known as the marginal probability.\\nExample 18.1.7. Consider the sample space of “the outcome of tossing two\\ndice” again. Let A1 be the event “the number on the first die is 1” and let\\nA2 be the event “the number on the second die is 4.” The joint probability of\\nA1, A2 is 1/36. The marginal probability of each of the events is 1/6.\\nIt is also useful to define the probability of an event A, based on\\nthe knowledge that other events A1, . . . , An have occurred.\\nDefinition 18.1.8 (Conditional Probability). For any event A and any\\nset of events A = {A1, . . . , An} of a sample space S, where Pr[A1 ∩. . . ∩\\nAn] > 0, the conditional probability of A given A is\\nPr[A | A1, . . . , An] = Pr[A ∩A1 ∩. . . ∩An]\\nPr[A1 ∩. . . ∩An]\\nExample 18.1.9. Consider the sample space of “the outcome of tossing\\ntwo dice” again. Let A1 be the event “the number on the first die is 1” and\\nlet A2 be the event “the sum of the numbers on the two dice is 5.” The\\nconditional probability of A1 given A2 is 1/4. The conditional probability of\\nA2 given A1 is 1/6.\\nUsing the definition of a conditional probability, we can define a\\nformula to find the joint probability of a set A of events of a sample\\nspace.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 226}, page_content='probability and statistics\\n227\\nProposition 18.1.10 (Chain Rule for Conditional Probability). Given a\\nset A = {A1, . . . , An} of events of a sample space S, where all appropriate\\nconditional probabilities are defined, we have the following\\nPr[A1 ∩. . . ∩An] = Pr[A1 | A2 ∩. . . ∩An] · Pr[A2 ∩. . . ∩An]\\n= Pr[A1 | A2 ∩. . . ∩An] · Pr[A2 | A3 ∩. . . ∩An] · Pr[A3 ∩. . . ∩An]\\n...\\n= Pr[A1 | A2 ∩. . . ∩An] · Pr[A2 | A3 ∩. . . ∩An] · · · Pr[An]\\nFinally, from the definition of a conditional probability, we see that\\nPr[B | A] Pr[A] = Pr[A ∩B] = Pr[A | B] Pr[B]\\nThis shows that\\nPr[B | A] = Pr[A | B] Pr[B]\\nPr[A]\\nThis is known as the Bayes’s Rule.\\n18.1.4\\nIndependent Events\\nDefinition 18.1.11 (Independent Events). Two events A, B are indepen-\\ndent if Pr[A], Pr[B] > 0 and\\nPr[A] = Pr[A | B]\\nor equivalently\\nPr[B] = Pr[B | A]\\nor equivalently\\nPr[A ∩B] = Pr[A] · Pr[B]\\nExample 18.1.12. Consider the sample space of “the outcome of tossing\\ntwo dice” again. Let A1 be the event “the number on the first die is 1” and\\nlet A2 be the event “the number on the second die is 4.” A1 and A2 are\\nindependent.\\nExample 18.1.13. Consider the sample space of “the outcome of tossing two\\ndice” again. Let A1 be the event “the number on the first die is 1” and let\\nA2 be the event “the sum of the numbers on the two dice is 5.” A1 and A2\\nare not independent.\\n18.2\\nRandom Variable\\nIn the previous section, we only learned how to assign a probability\\nto an event, a subset of the sample space. But in general, we can\\nassign a probability to a broader concept called a random variable,\\nassociated to the sample space.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 227}, page_content='228\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nDefinition 18.2.1 (Random Variable). Given a sample space S, a mapping\\nX : S →R that maps each outcome x ∈S to a value r ∈R is called a\\nrandom variable.\\nExample 18.2.2. Consider the sample space of “the outcome of tossing two\\ndice” again. Then the random variable X = “sum of the numbers on the two\\ndice” maps the outcome (1, 4) to the value 5.\\nDefinition 18.2.3 (Sum and Product of Random Variables). If\\nX, X1, . . . , Xn are random variables defined on the same sample space S\\nsuch that X(x) = X1(x) + . . . + Xn(x) for every outcome x ∈S, then we\\nsay that X is the sum of the random variables X1, . . . , Xn and denote\\nX = X1 + . . . + Xn\\nIf X(x) = X1(x) × . . . × Xn(x) for every outcome x ∈S, then we say that\\nX is the product of the random variables X1, . . . , Xn and denote\\nX = X1 · · · Xn\\nExample 18.2.4. Consider the sample space of “the outcome of tossing two\\ndice” again. Then the random variable X = “sum of the numbers on the two\\ndice” is the sum of the two random variables X1 = “the number on the first\\ndie” and X2 = “the number on the second die.”\\n18.2.1\\nProbability of Random Variable\\nThere is a natural relationship between the definition of an event\\nand a random variable. Given a sample space S and random variable\\nX : S →R, the “event that X takes a value in B” is denoted Pr[X ∈B].\\nIt is the total probability of all outcomes x ∈S such that X(x) ∈B. In\\nparticular, the event that X takes a particular value r ∈R is denoted\\nas X = r and the event that X takes a value in the interval [a, b] is\\ndenoted as a ≤X ≤b and so on.\\nExample 18.2.5. Consider the sample space of “the outcome of tossing two\\ndice” and the random variable X = “sum of the numbers on the two dice”\\nagain. Then\\nPr[X = 5] = Pr[{(1, 4), (2, 3), (3, 2), (4, 1)}] = 4/36\\nOften we are interested in the probability of the events of the form\\nX ≤x. Plotting the values of Pr[X ≤x] with respect to x completely\\nidentifies the distribution of the values of X.\\nDefinition 18.2.6 (Cumulative Distribution Function). Given a random\\nvariable X, there is an associated cumulative distribution function (cdf)\\nFX : R →[0, 1] defined as\\nFX(x) = Pr[X ≤x]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 228}, page_content='probability and statistics\\n229\\nProposition 18.2.7. The following properties hold for a cumulative distribu-\\ntion function FX:\\n1. FX is increasing\\n2.\\nlim\\nx→−∞FX(x) = 0 and lim\\nx→∞FX(x) = 1\\n18.2.2\\nDiscrete Random Variable\\nIf the set of possible values of a random variable X is finite or count-\\nably infinite, we call it a discrete random variable. For a discrete ran-\\ndom variable, the probability Pr[X = i] for each value i that the\\nrandom variable can take completely identifies the distribution of X.\\nIn view of this fact, we denote the probability mass function (pmf) by\\npX(i) = Pr[X = i]\\nProposition 18.2.8. The following properties hold for a probability mass\\nfunction pX:\\n1. ∑\\ni\\npX(i) = 1\\n2. FX(x) = ∑\\ni≤x\\npX(i)\\n18.2.3\\nContinuous Random Variable\\nWe now consider the case where the set of all possible values of\\na random variable X is an interval or a disjoint union of intervals\\nin R. We call such X a continuous random variable. In this case, the\\nprobability of the event X = i is zero for any i ∈R. Instead, we care\\nabout the probability of the events of the form a ≤X ≤b.\\nDefinition 18.2.9 (Probability Density Function). Given a continuous\\nrandom variable X, there is an associated probability density function\\n(pdf) fX : R →R≥0 such that\\nPr[a ≤X ≤b] =\\nZ b\\na fX(x)dx\\nfor any a, b ∈R.\\nProposition 18.2.10. The following properties hold for a probability density\\nfunction fX:\\n1. R ∞\\n−∞fX(x)dx = 1\\n2. FX(x) = R x\\n−∞fX(y)dy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 229}, page_content='230\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n18.2.4\\nExpectation and Variance\\nDefinition 18.2.11 (Expectation). The expectation or the expected value\\nof a discrete random variable X is defined as\\nE[X] = ∑\\ni\\ni · pX(i) = ∑\\ni\\ni · Pr[X = i]\\nwhere pX is its associated probability mass function. Similarly, the expecta-\\ntion for a continuous random variable X is defined as\\nE[X] =\\nZ ∞\\n−∞x · fX(x)dx\\nwhere fX is the associated probability density function. In either case, it is\\ncustomary to denote the expected value of X as µX or just µ if there is no\\nsource of confusion.\\nExample 18.2.12. Consider the sample space of “the outcome of tossing one\\ndie.” Then the expected value of the random variable X = “the number on\\nthe first die” can be computed as\\nE[X] = 1 · 6\\n36 + 2 · 6\\n36 + 3 · 6\\n36 + 4 · 6\\n36 + 5 · 6\\n36 + 6 · 6\\n36 = 3.5\\nProposition 18.2.13 (Linearity of Expectation). If X is the sum of the\\nrandom variables X1, . . . , Xn, then the following holds:\\nE[X] = E[X1] + . . . + E[Xn]\\nAlso, if a, b ∈R and X is a random variable, then\\nE[aX + b] = aE[X] + b\\nExample 18.2.14. Consider the sample space of “the outcome of tossing two\\ndice.” Then the expected value of the random variable X = “the sum of the\\nnumbers of the two dice” can be computed as\\nE[X] = 3.5 + 3.5 = 7\\nsince the expected value of the number on each die is 3.5.\\nDefinition 18.2.15 (Variance). The variance of a random variable X, whose\\nexpected value is µ, is defined as\\nVar[X] = E[(X −µ)2]\\nIts standard deviation is defined as\\nσX =\\nq\\nVar[X]\\nIt is customary to denote the variance of X as σ2\\nX.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 230}, page_content='probability and statistics\\n231\\nProposition 18.2.16. If a ∈R and X is a random variable, then\\nVar[aX] = a2Var[X]\\nσaX = |a| σX\\nProblem 18.2.17. Prove Chebyshev’s inequality:\\nPr[|X −µ| ≥kσ] ≤1\\nk2\\nfor any k > 0. (Hint: Suppose the probability was greater than 1/k2. What\\ncould you conclude about E[(X −µ)2]? )\\n18.2.5\\nJoint and Conditional Distribution of Random Variables\\nJust as in events, we are interested in multiple random variables\\ndefined on the sample space.\\nDefinition 18.2.18 (Joint Distribution). If X, Y are discrete random\\nvariables defined on the same sample space S, the joint probability mass\\nfunction pX,Y is defined as\\npX,Y(i, j) = Pr[X = i, Y = j]\\nwhere the event X = i, Y = j refers to the intersection (X = i) ∩(Y = j).\\nIf X, Y are continuous random variables defined on S, there is an as-\\nsociated joint probability density function fX,Y : R →R≥0 such\\nthat\\nPr[a ≤X ≤b, c ≤Y ≤d] =\\nZ d\\nc\\nZ b\\na fX,Y(x, y)dxdy\\nThe joint probability mass/density function defines the joint distribution of\\nthe two random variables.\\nDefinition 18.2.19 (Marginal Distribution). Given a joint distribution\\npX,Y or fX,Y of two random variables X, Y, the marginal distribution of X\\ncan be found as\\npX(i) = ∑\\nj\\npX,Y(i, j)\\nif X, Y are discrete and\\nfX(x) =\\nZ ∞\\n−∞fX,Y(x, y)dy\\nif continuous. We can equivalently define the marginal distribution of Y.\\nDefinition 18.2.20 (Conditional Distribution). Given a joint distribution\\npX,Y or fX,Y of two random variables X, Y, we define the conditional\\ndistribution of X given Y as\\npX | Y(i | j) = pX,Y(i, j)\\npY(j)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 231}, page_content='232\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nwhenever pY(j) > 0 if X, Y are discrete and\\nfX | Y(x | y) = fX,Y(x, y)\\nfY(y)\\nwhenever fY(y) > 0 if continuous. We can equivalently define the condi-\\ntional distribution of Y given X.\\n18.2.6\\nBayes’ Rule for Random Variables\\nSometimes it is easy to calculate the conditional distribution of X\\ngiven Y, but not the other way around. In this case, we can apply\\nthe Bayes’ Rule to compute the conditional distribution of Y given X.\\nHere, we assume that X, Y are discrete random variables. By a simple\\napplication of Bayes’ Rule, we have\\nPr[Y = j | X = i] = Pr[X = i | Y = j] Pr[Y = j]\\nPr[X = i]\\nNow by the definition of a marginal distribution, we have\\nPr[X = i] = ∑\\nj′\\nPr[X = i, Y = j] = ∑\\nj′\\nPr[X = i | Y = j′] Pr[Y = j′]\\nfor all possible values j′ that Y can take. If we plug this into the\\ndenominator above,\\nPr[Y = j | X = i] =\\nPr[X = i | Y = j] Pr[Y = j]\\n∑\\nj′ Pr[X = i | Y = j′] Pr[Y = j′]\\nExample 18.2.21. There is a coin, where the probability of Heads is\\nunknown and is denoted as θ. You are told that there is a 50% chance that it\\nis a fair coin (i.e., θ = 0.5) and 50% chance that it is biased to be θ = 0.7.\\nTo find out if the coin is biased, you decide to flip the coin. Let D be the\\nresult of a coin flip. Then it is easy to calculate the conditional distribution\\nof D given θ. For example,\\nPr[D = H | θ = 0.5] = 0.5\\nBut we are more interested in the probability that the coin is fair/biased\\nbased on the observation of the coin flip. Therefore, we can apply the Bayes’\\nRule.\\nPr[θ = 0.7 | D = H] = Pr[D = H | θ = 0.7] Pr[θ = 0.7]\\nPr[D = H]\\nwhich can be calculated as\\nPr[D = H | θ = 0.7] Pr[θ = 0.7]\\nPr[D = H | θ = 0.7] Pr[θ = 0.7] + Pr[D = H | θ = 0.5] Pr[θ = 0.5]\\n=\\n0.7 · 0.5\\n0.7 · 0.5 + 0.5 · 0.5 ≃0.58'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 232}, page_content='probability and statistics\\n233\\nSo if we observe one Heads, there is a 58% chance that the coin was biased\\nand a 42% chance that it was fair.\\nProblem 18.2.22. Consider Example 18.2.21 again. This time, we decide to\\nthrow the coin 10 times in a row. Let N be the number of observed Heads.\\nWhat is the probability that the coin is biased if N = 7?\\n18.2.7\\nIndependent Random Variables\\nAnalogous to events, we can define the independence of two random\\nvariables.\\nDefinition 18.2.23 (Independent Random Variables). Two discrete\\nrandom variables X, Y are independent if for every i, j, we have\\npX(i) = pX | Y(i | j)\\nor equivalently,\\npY(j) = pY | X(j | i)\\nor equivalently\\npX,Y(x, y) = pX(x) · pY(y)\\nTwo continuous random variables X, Y are independent if the analogous\\nconditions hold for the probability density functions.\\nDefinition 18.2.24 (Mutually Independent Random Variables). If any\\npair of n random variables X1, X2, . . . , Xn are independent of each other,\\nthen the random variables are mutually independent.\\nProposition 18.2.25. If X1, . . . , Xn are mutually independent random\\nvariables, the following properties are satisfied:\\n1. E[X1 · · · Xn] = E[X1] · · · E[Xn]\\n2. Var[X1 + . . . + Xn] = Var(X1) + . . . + Var(Xn)\\nWe are particularly interested in independent random variables\\nthat have the same probability distribution. This is because if we\\nrepeat the same random process multiple times and define a random\\nvariable for each iteration, the random variables will be independent\\nand identically distributed.\\nDefinition 18.2.26. If X1, . . . , Xn are mutually independent random\\nvariables that have the same probability distribution, we call them indepen-\\ndent, identically distributed random variables, which is more commonly\\ndenoted as iid or i.i.d. random variables.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 233}, page_content='234\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n18.3\\nCentral Limit Theorem and Confidence Intervals\\nNow we turn our attention to two very important topics in statistics:\\nCentral Limit Theorem and confidence intervals.\\nYou may have seen confidence intervals or margin of error in the\\ncontext of election polls. The pollster usually attaches a caveat to the\\nprediction, saying that there is some probability that the true opinion\\nof the public is ±ϵ of the pollster’s estimate, where ϵ is typically a\\nfew percent. This section is about the most basic form of confidence\\nintervals, calculated using the famous Gaussian distribution. It\\nalso explains why the Gaussian pops up unexpectedly in so many\\nsettings.\\nA running example in this chapter is estimating the bias of a coin\\nwe have been given. Specifically, Pr[Heads] = p where p is unknown\\nand may not be 1/2. We wish to estimate p by repeatedly tossing the\\ncoin. If we toss the coin n times, we expect to see around np Heads.\\nConfidence intervals ask the converse question: after having seen the\\nnumber of heads in n tosses, how “confidently” can we estimate p?\\n18.3.1\\nCoin Tossing\\nSuppose we toss the same coin n times. For each i = 1, 2, . . . , n, define\\nthe random variable Xi as an indicator random variable such that\\nXi =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n1\\ni-th toss was Heads\\n0\\notherwise\\nIt is easily checked that X1, . . . , Xn are iid random variables, each\\nwith E[Xi] = p and Var[Xi] = p(1 −p). Also if we have another\\nrandom variable X = “number of heads,” notice that X is the sum of\\nX1, . . . , Xn. Therefore, E[X] = np and Var[X] = np(1 −p).\\nProblem 18.3.1. Show that if Pr[Heads] = p then E[X] = np and\\nVar[X] = np(1 −p). (Hint: use linearity of expectation and the fact that\\nXi’s are mutually independent.)\\nSuppose p = 0.8. What is the distribution of X? Figure 18.1 gives\\nthe distribution of X for different n’s.\\nLet’s make some observations about Figure 18.1.\\nExpected value may not happen too often. For n = 10, the expected\\nnumber of Heads is 8, but that is seen only with probability 0.3. In\\nother words, with probability 0.7, the number of Heads is different\\nfrom the expectation. 2\\n2 In such cases, expectation can be a\\nmisleading term. It may in fact be never\\nseen. For instance, the expected number\\nof eyes in an individual drawn from\\nthe human population is somewhere\\nbetween 1 and 2 but no individual has a\\nnon-integral number of eyes. Thus mean\\nvalue is a more intuitive term.\\nThe highly likely values fall in a smaller and smaller band around the\\nexpected value, as n increases.\\nFor n = 10, there is a good chance that the number of Heads is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 234}, page_content='probability and statistics\\n235\\nFigure 18.1: Distribution of X when we\\ntoss a coin n times, and p = 0.8. The\\nplots were generated using a calculator.\\nquite far from the the expectation. For n = 100, the number of\\nHeads lies in [68, 90] with quite high probability. For n = 1000 it\\nlies in [770, 830] with high probability.\\nThe probability curve becomes more symmetrical around the mean. Contrast\\nbetween the case where n = 10 and the case where n = 100.\\nProbability curve starts resembling the famous Gaussian distribution .\\nAlso called Normal Distribution and in popular math, the Bell curve,\\ndue to its bell-like shape.\\n18.3.2\\nGaussian Distribution\\nWe say that a real-valued random variable X is distributed according\\nto N (µ, σ2), the Gaussian distribution with mean µ and variance σ2,\\nif\\nfX(x) =\\n1\\nσ\\n√\\n2π\\ne−(x−µ)2\\n2σ2\\n(18.1)\\nIt is hard to make an intuitive sense of this expression. The following\\nfigure gives us a better handle.\\nFigure 18.2: Cheatsheet for the Gaus-\\nsian distribution with mean µ and\\nvariance σ2. It is tightly concentrated\\nin the interval [µ −kσ, µ + kσ] for even\\nk = 1 and certainly for k = 2, 3. Source:\\nhttps://en.wikipedia.org/wiki/\\nNormal_distribution\\nFigure 18.2 shows that X concentrates very strongly around the\\nmean µ. The probability that X lies in various intervals around µ of\\nthe type [µ −kσ, µ + kσ] are as follows: (i) For k = 1 it is 68.3%; (ii) For\\nk = 2 it is 95.5%; (iii) For k = 3 it is 99.7%.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 235}, page_content='236\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n18.3.3\\nCentral Limit Theorem (CLT)\\nThis fundamental result explains our observations in Subsection 18.3.1.\\nTheorem 18.3.2 (Central Limit Theorem, informal statement). Suppose\\nX1, X2, . . . , is a sequence of random variables that are mutually independent\\nand each of whose variance is upper bounded by some constant C. Then\\nas n →∞, the sum X1 + X2 + . . . + Xn tends to N (µ, σ2) where µ =\\n∑i E[Xi] and σ2 = ∑i Var(Xi).\\nWe won’t prove this theorem. We will use it primarily via the\\n“cheatsheet” of Figure 18.2.\\n18.3.4\\nConfidence Intervals\\nWe return to the problem of estimating the bias of a coin, namely\\np = Pr[Heads]. Suppose we toss it n times and observe X heads.\\nThen X = ∑i Xi where Xi is the indicator random variable that\\nsignifies if the i-th toss is Heads.\\nSince the Xi’s are mutually independent, we can apply the CLT\\nand conclude that X will approximately follow a Gaussian distribu-\\ntion as n grows. This is clear from Figure 18.1, where the probability\\nhistogram (which is a discrete approximation to the probability\\ndensity) looks quite Gaussian-like for n = 1000. In this course we\\nwill assume for simplicity that CLT applies exactly. Using the mean\\nand variance calculations from Problem 18.3.1, X is distributed like\\nN (µ, σ2) where µ = np, σ2 = np(1 −p). Using the cheatsheet of\\nFigure 18.2, we can conclude that\\nPr[X ̸∈[np −2σ, np + 2σ]] ≤4.6%\\nSince X ∈[np −2σ, np + 2σ] if and only if np ∈[X −2σ, X + 2σ], some\\nstudents have the following misconception:\\nGiven the observation of X heads in n coin tosses, the probability that np ̸∈\\n[X −2σ, X + 2σ] is at most 4.6%.\\nBut there is no a priori distribution on p. It is simply some (unknown)\\nconstant of nature that we’re trying to estimate. So the correct infer-\\nence should be:\\nIf np ̸∈[X −2σ, X + 2σ], then the probability (over the n coin tosses) that we\\nwould have seen X heads is at most 4.6%.\\nThe above is an example of confidence bounds. Of course, you may\\nnote that σ also depends on p, so the above conclusion doesn’t give\\nus a clean confidence interval. In this course we use a simplifying\\nassumption: to do the calculation we estimate σ2 as np′(1 −p′) where\\np′ = X/n. (The intuitive justification is that we expect p to be close to\\nX/n.)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 236}, page_content='probability and statistics\\n237\\nExample 18.3.3. Suppose X = 0.8n. Using our simplified calculation,\\nσ2 ≈n(0.8)(0.2), implying σ = 0.4√n. Thus we conclude that if p ̸∈\\n[0.8 −0.4/√n, 0.8 + 0.4/√n], then the probability of observing this many\\nHeads in n tosses would have been less than 100 −68.2%, that is, less than\\n31.8%.\\nThe concept of confidence intervals is also relevant to ML models.\\nExample 18.3.4. A deep neural network model was trained to predict cancer\\npatients’ chances of staying in remission a year after chemotherapy, and\\nwe are interested in finding out its accuracy p. When the model is tested\\non n = 1000 held-out data points, this problem is equivalent to the coin\\nflipping problem. For each of the held-out data point, the probability that the\\nmodel makes the correct prediction is p. By observing the number of correct\\npredictions on the held-out data, we can construct a confidence interval for\\np. Say the test accuracy was p′ = 70%. Then the 68% confidence interval\\ncan be written as\\nnp ∈[np′ −σ, np′ + σ]\\nSubstituting p′ = 0.7, σ ≈\\np\\nnp′(1 −p′), n = 1000, we get\\n1000p ∈[685.5, 714.5]\\nor equivalently,\\np ∈[0.6855, 0.7145]\\n18.3.5\\nConfidence Intervals for Vectors\\nIn the above settings, sampling was being used to estimate a real\\nnumber, namely, Pr[Heads] for a coin. How about estimating a vec-\\ntor? For instance, in an opinion poll, respondents are being asked\\nfor opinions on multiple questions. Similarly, in stochastic gradient\\ndescent (Chapter 3), the gradient vector is being estimated by sam-\\npling a small number of data points. How can we develop confidence\\nbounds for estimating a vector in Rk from n samples?\\nThe confidence intervals for the coin toss setting can be easily\\nextended to this case using the so called Union Bound:\\nPr[A1 ∪A2 ∪· · · ∪Ak] ≤Pr[A1] + Pr[A2] + · · · + Pr[Ak]\\n(18.2)\\nThis leads to the simplest confidence bound for estimating a vector in\\nRk. Suppose the probability of the estimate being off by δi in the i-th\\ncoordinate is at most qi. Then\\nPr[estimate is off by ⃗δ] ≤q1 + q2 + · · · + qk\\nwhere ⃗δ = (δ1, δ2, . . . , δk)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 237}, page_content='238\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n18.4\\nFinal Remarks\\nThe CLT applies to many settings, but it doesn’t apply everywhere.\\nIt is useful to clear up a couple of frequent misconceptions that\\nstudents have:\\n1. Not every distribution involving a large number of samples is\\nGaussian. For example, scores on the final exam are usually not\\ndistributed like a Gaussian. Similarly, human heights are not really\\ndistributed like Gaussians.\\n2. Not everything that looks Gaussian-like is a result of the Central\\nLimit Theorem. For instance, we saw that the distribution of\\nweights in the sentiment model in Chapter 1 looked vaguely\\nGaussian-like, but they are not the sum of independent random\\nvariables as far as we can tell.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 238}, page_content='19\\nCalculus\\n19.1\\nCalculus in One Variable\\nIn this section, we briefly review calculus in one variable.\\n19.1.1\\nExponential and Logarithmic Functions\\nWhen we multiply the same number a by itself n times, we denote it\\nas an. The exponential function is a natural extension of this concept.\\nDefinition 19.1.1 (Exponential Function). There is a unique function\\nf : R →R such that f (n) = en for any n ∈N and f (x + y) = f (x) f (y)\\nfor any x, y ∈R. This function is called the exponential function and is\\ndenoted as ex or exp(x).\\nFigure 19.1: The graph of the exponen-\\ntial function.\\nProposition 19.1.2. The following properties hold for the exponential\\nfunction:\\n1. exp(x) > 0 for any x ∈R\\n2. exp(x) is increasing\\n3.\\nlim\\nx→−∞exp(x) = 0\\n4.\\nlim\\nx→∞exp(x) = ∞\\n5. exp(−x) =\\n1\\nexp(x)\\nWe are also interested in the inverse function of the exponential\\nfunction.\\nDefinition 19.1.3 (Logarithmic Function). The logarithmic function\\nlog : (0, ∞) →R is defined as the inverse function of the exponential\\nfunction. That is, log(x) = y where x = ey.\\nFigure 19.2: The graph of the logarith-\\nmic function.\\nProposition 19.1.4. The following properties hold for the logarithmic\\nfunction:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 239}, page_content='240\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n1. log(x) is increasing\\n2.\\nlim\\nx→0+ log(x) = −∞\\n3.\\nlim\\nx→∞log(x) = ∞\\n4. log(xy) = log(x) + log(y)\\n19.1.2\\nSigmoid Function\\nIn Machine Learning, a slight variant of the exponential function,\\nknown as the sigmoid function is widely used.\\nDefinition 19.1.5 (Sigmoid Function). The sigmoid function denoted as\\nσ : R →R is defined as\\nσ(x) =\\n1\\n1 + exp(−x)\\nFigure 19.3: The graph of the sigmoid\\nfunction.\\nProposition 19.1.6. The following properties hold for the sigmoid function:\\n1. 0 < σ(x) < 1 for any x ∈R\\n2. σ(x) is increasing\\n3.\\nlim\\nx→−∞σ(x) = 0\\n4.\\nlim\\nx→∞σ(x) = 1\\n5. The graph of σ is symmetrical to the point\\n\\x10\\n0, 1\\n2\\n\\x11\\n. In particular,\\nσ(x) + σ(−x) = 1\\nBecause of the last property in Proposition 19.1.6, the sigmoid\\nfunction is well suited for binary classification (e.g., in logistic re-\\ngression in Chapter 1). Given some output value x of a classification\\nmodel, we interpret it as the measure of confidence that the input is\\nof label 1, where we implicitly assume that the measure of confidence\\nthat the input is of label 2 is −x. Then we apply the sigmoid function\\nto translate this into a probability distribution over the two labels.\\n19.1.3\\nDifferentiation\\nDefinition 19.1.7 (Derivative). Given a function f : R →R, its\\nderivative f ′ is defined as\\nf ′(x) = lim\\nh→0\\nf (x + h) −f (x)\\nh\\nWe alternatively denote f ′(x) as\\nd\\ndx f (x).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 240}, page_content='calculus\\n241\\nExample 19.1.8. The derivative of the exponential function is itself:\\nexp′(x) = exp(x)\\nand the derivative of the logarithmic function is:\\nlog′(x) = 1\\nx\\nIn general, there are more than two variables, that are related to\\neach other through a composite function. The chain rule helps us find\\nthe derivative of the composite function.\\nDefinition 19.1.9 (Chain Rule). If there are functions f, g : R →R such\\nthat y = f (x) and z = g(y), then\\n(g ◦f )′(x) = g′( f (x)) f ′(x) = d\\ndy g( f (x)) · d\\ndx f (x)\\nor equivalently\\ndz\\ndx = dz\\ndy · dy\\ndx\\n19.2\\nMultivariable Calculus\\nIn this section, we introduce the basics of multivariable calculus,\\nwhich is widely used in Machine Learning. Since this is a general-\\nization of the calculus in one variable, it will be useful to pay close\\nattention to the similarity with the results from the previous section.\\n19.2.1\\nMappings of Several Variables\\nSo far, we only considered functions of the form f : R →R that\\nmap a real value x to a real value y. But now we are interested in\\nmappings f : Rn →Rm that map a vector⃗x = (x1, . . . , xn) with\\nn coordinates to a vector ⃗y = (y1, . . . , ym) with m coordinates. In\\ngeneral, a function is a special case of a mapping where the range is R.\\nIf the mappings are of the form f : Rn →R (i.e., m = 1), it can still be\\ncalled a function of several variables.\\nFirst consider an example where m = 1.\\nExample 19.2.1. Let f (x1, x2) = x2\\n1 + x2\\n2 be a function in two variables.\\nThis can be understood as mapping a point⃗x = (x1, x2) in the Cartesian\\ncoordinate system to its squared distance from the origin. For example,\\nf (3, 4) = 25 shows that the squared distance between the point (3, 4) and\\nthe origin (0, 0) is 25.\\nWhen m > 1, we notice that each coordinate y1, . . . , ym is a func-\\ntion of x1, . . . , xn. Therefore, we can decompose f into m functions\\nf1, . . . , fm : Rn →R such that\\nf (⃗x) = ( f1(⃗x), . . . , fm(⃗x))'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 241}, page_content='242\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nExample 19.2.2. Let f (x1, x2) = (x2\\n1x2, x1x2\\n2) be a mapping from R2 to R2.\\nThen we can decompose f into two functions f1, f2 in two variables where\\nf1(x1, x2) = x2\\n1x2\\nf2(x1, x2) = x1x2\\n2\\n19.2.2\\nSoftmax Function\\nThe softmax function is a multivariable function widely used in Ma-\\nchine Learning, especially for multi-class classification (see Chapter 4,\\nChapter 10). It takes in a vector of k values, each corresponding to\\na particular class, and outputs a probability distribution over the k\\nclasses — that is, a vector of k non-negative values that sum up to\\n1. The resulting probability is exponentially proportional to the input\\nvalue of that class. We formally write this as:\\nDefinition 19.2.3 (Softmax Function). Given a vector⃗z = (z1, z2, . . . , zk) ∈\\nRk, we define the softmax function as a probability function so f tmax :\\nRk →[0, 1]k where the “probability of predicting class i” is:\\nso f tmax(⃗z)i =\\nezi\\n∑k\\nj=1 ezj\\n(19.1)\\nProblem 19.2.4. Show that for k = 2, the definition of the softmax function\\nis equivalent to the sigmoid function (after slight rearrangement/renaming of\\nterms).\\nThe sigmoid function is used for binary classification, where it\\ntakes in a single real value and converts it to a probability of one\\nclass (and the probability of the other class can be inferred as its com-\\nplement). The softmax function is used for multi-class classification,\\nwhere it takes in k real values and converts them to k probabilities,\\none for each class.\\n19.2.3\\nDifferentiation\\nJust like with functions in one variable, we can define differentiation\\nfor mappings in several variables. The key point is that now we will\\ndefine a partial derivative for each pair (xi, yj) of coordinate xi of the\\ndomain and coordinate yj of the range.\\nDefinition 19.2.5 (Partial Derivative). Given a function f : Rn →Rm, the\\npartial derivative of yj with respect to xi at the point⃗x is defined as\\n∂yj\\n∂xi\\n\\x0c\\x0c\\x0c\\x0c\\n⃗x\\n= lim\\nh→0\\nfj(x1, . . . , xj + h, . . . , xn) −fj(x1, . . . , xj, . . . , xn)\\nh'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 242}, page_content='calculus\\n243\\nDefinition 19.2.6 (Gradient). If f : Rn →R is a function of several\\nvariables, the gradient of f is defined as a mapping ∇f : Rn →Rn that\\nmaps each input vector to the vector of partial derivatives at that point:\\n∇f (⃗x) =\\n\\x12 ∂f\\n∂x1\\n, . . . , ∂f\\n∂xn\\n\\x13\\x0c\\x0c\\x0c\\x0c\\n⃗x\\nSimilarly to the chain rule in one variable, we can define a chain\\nrule for multivariable settings. The key point is that there are mul-\\ntiple ways that a coordinate xj can affect the value of zi. Defini-\\ntion 19.2.7 can be thought as applying the chain rule for one variable\\nin each of the paths, and adding up the results.\\nx1\\nx2\\ny1\\ny2\\ny3\\nz1\\nz2\\nFigure 19.4: A visualization of the chain\\nrule in multivariable settings. Notice\\nthat x2 can affect the value of z1 in\\nthree different paths. The amount of\\neffect from each path will respectively\\nbe calculated as (∂z1/∂y1)(∂y1/∂x2)\\n(red), (∂z1/∂y2)(∂y2/∂x2) (blue), and\\n(∂z1/∂y3)(∂y3/∂x2) (cyan).\\nDefinition 19.2.7 (Chain Rule). If f : Rn →Rm and g : Rm →Rℓare\\nmappings of several variables, where⃗y = f (⃗x) and⃗z = g(⃗y), the following\\nchain rule holds for each 1 ≤i ≤ℓand 1 ≤j ≤n:\\n∂zi\\n∂xj\\n=\\nm\\n∑\\nk=1\\n∂zi\\n∂yk\\n· ∂yk\\n∂xj\\nExample 19.2.8. Suppose we define the functions h = s + t2, s = 3x, and\\nt = x −2. Then, we can find the partial derivative ∂h\\n∂x using the chain rule:\\n∂h\\n∂x = ∂s\\n∂x + ∂(t2)\\n∂x\\n= ∂s\\n∂x + ∂(t2)\\n∂t\\n· ∂t\\n∂x\\n= 3 + 2t · 1\\n= 2x −1\\nProblem 19.2.9. Suppose we define the functions h = s + t2, s = xy, and\\nt = x −2y. Compute the partial derivative ∂h/∂x.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 243}, page_content=''),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 244}, page_content='20\\nLinear Algebra\\n20.1\\nVectors\\nx\\ny\\n⃗v = (2, 1)\\nFigure 20.1: A visualization of a vector\\n⃗v = (2, 1) in R2.\\nVectors are a collection of entries (here, we focus only on real num-\\nbers). For example, the pair (1, 2) is a real vector of size 2, and the\\n3-tuple (1, 0, 2) is a real vector of size 3. We primarily categorize vec-\\ntors by their size. For example, the set of all real vectors of size n is\\ndenoted as Rn. Any element of Rn can be thought of as representing\\na point (or equivalently, the direction from the origin to the point) in\\nthe n-dimensional Cartesian space. A real number in R is also known\\nas a scalar, as opposed to vectors in Rn where n > 1.\\n20.1.1\\nVector Space\\nx\\ny\\n⃗x = (2, 1)\\n⃗y = (1, 2)\\n⃗x +⃗y = (3, 3)\\nFigure 20.2: A visualization of⃗x +⃗y\\nwhere⃗x = (2, 1) and ⃗y = (1, 2).\\nWe are interested in two operations defined on vectors — vector\\naddition and scalar multiplication. Given vectors⃗x = (x1, x2, . . . , xn)\\nand ⃗y = (y1, y2, . . . , yn) and a scalar c ∈R, the vector addition is\\ndefined as\\n⃗x +⃗y = (x1 + y1, x2 + y2, . . . , xn + yn) ∈Rn\\nwhere we add each of the coordinates element-wise. As shown in\\nFigure 20.2, vector addition is the process of finding the diagonal\\nof the parallelogram made by the two vectors⃗x and ⃗y. The scalar\\nmultiplication is similarly defined as\\nc⃗x = (cx1, cx2, . . . , cxn) ∈Rn\\nx\\ny\\n⃗x = (4, 2)\\n0.5⃗x = (2, 1)\\nFigure 20.3: A visualization of 0.5⃗x\\nwhere⃗x = (4, 2).\\nAs shown in Figure 20.3, scalar multiplication is the process of\\nscaling one vector up or down.\\nRn is closed under these two operations — i.e., the resulting\\nvector of either operation is still in Rn. Any subset S of Rn that is\\nclosed under vector addition and scalar multiplication is known as a\\nsubspace of Rn.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 245}, page_content='246\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\n20.1.2\\nInner Product\\nThe inner product is defined as\\n⃗x ·⃗y = x1y1 + x2y2 + . . . + xnyn =\\nn\\n∑\\ni=1\\nxiyi ∈R\\nClosely related to the inner product is the norm of a vector, which\\nmeasures the length of it. It is defined as ∥⃗x∥=\\n√\\n⃗x ·⃗x. 1\\n1 There are many other definitions of a\\nnorm. This particular one is called an ℓ2\\nnorm.\\nProposition 20.1.1. The inner product satisfies the following properties:\\n• Symmetry: ⃗x ·⃗y = ⃗y ·⃗x\\n• Linearity: (a1⃗x1 + a2⃗x2) ·⃗y = a1(⃗x1 ·⃗y) + a2(⃗x2 ·⃗y)\\nand the norm satisfies the following property:\\n• Absolute Homogeneity: ∥a⃗x∥= |a| ∥⃗x∥\\n20.1.3\\nLinear Independence\\nAny vector of the form\\na1⃗x1 + a2⃗x2 + . . . + ak⃗xk\\nwhere ai’s are scalars and⃗xi’s are vectors is called a linear combination\\nof the vectors⃗xi’s. Notice that the zero vector⃗0 (i.e., the vector with\\nall zero entries) can always be represented as a linear combination of\\nan arbitrary collection of vectors, if all ai’s are chosen as zero. This\\nis known as a trivial linear combination, and any other choice of ai’s is\\nknown as a non-trivial linear combination.\\nDefinition 20.1.2. k vectors⃗x1,⃗x2, . . . ,⃗xk ∈Rn are called linearly\\ndependent if⃗0 can be represented as a non-trivial linear combination of the\\nvectors⃗x1, . . . ,⃗xk; or equivalently, if one of the vectors can be represented as\\na linear combination of the remaining k −1 vectors. The vectors that are not\\nlinearly dependent with each other are called linearly independent.\\nConsider the following analogy. Imagine trying to have a family\\nstyle dinner at a fast food restaurant, where the first person orders a\\nburger, the second person orders a chilli cheese fries, and the third\\nperson orders a set menu with a burger and a chili cheese fries. The\\nthird person’s order did not contribute to the diversity of the food\\non the dinner table. Similarly, if some set of vectors are linearly\\ndependent, it means that at least one of the vectors is redundant.\\nExample 20.1.3. The set {(−1, 2), (3, 0), (1, 4)} of three vectors is linearly\\ndependent because\\n(1, 4) = 2 · (−1, 2) + (3, 0)\\ncan be represented as the linear combination of the remaining two vectors.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 246}, page_content='linear algebra\\n247\\nExample 20.1.4. The set {(−1, 2, 1), (3, 0, 0), (1, 4, 1)} of three vectors is\\nlinearly independent because there is no way to write one vector as a linear\\ncombination of the remaining two vectors.\\n20.1.4\\nSpan\\nDefinition 20.1.5. The span of a set of vectors⃗x1, . . . ,⃗xk is the set of all\\nvectors that can be represented as a linear combination of⃗xi’s.\\nExample 20.1.6. (1, 4) is in the span of {(−1, 2), (3, 0)} because\\n(1, 4) = 2 · (−1, 2) + (3, 0)\\nExample 20.1.7. (1, 4, 1) is not in the span of {(−1, 2, 1), (3, 0, 0)} because\\nthere is no way to choose a1, a2 ∈R such that\\n(1, 4, 1) = a1(−1, 2, 1) + a2(3, 0, 0)\\nThe span is also known as the subspace generated by the vectors\\n⃗x1, . . . ,⃗xk. This is because if you add any two vectors in the span, or\\nmultiply one by a scalar, it is still in the span (i.e., the span is closed\\nunder vector addition and scalar multiplication).\\nExample 20.1.8. In the R3, the two vectors (1, 0, 0) and (0, 1, 0) span the\\n2-dimensional XY-plane. Similarly, the vectors (1, 0, 1) and (0, 2, 1) span\\nthe 2-dimensional plane 2x + y −2z = 0. 2\\n2 The term dimension will be formally\\ndefined soon. Here, we rely on your\\nintuition.\\nIn Example 20.1.8, we see examples where 2 vectors span a 2-\\ndimensional subspace. In general, the dimension of the subspace\\nspanned by k vectors can go up to k, but it can also be strictly smaller\\nthan k. This is related to the linear independence of the vectors.\\nProposition 20.1.9. Given k vectors,⃗x1, . . . ,⃗xk ∈Rn, there is a maximum\\nnumber d ≥1 such that there is some subcollection⃗xi1, . . . ,⃗xid of these\\nvectors that are linearly independent. Then\\nspan(⃗x1, . . . ,⃗xk) = span(⃗xi1, . . . ,⃗xid)\\n(20.1)\\nis a d-dimensional subspace of Rn.\\nConversely, if we know that the span of the k vectors is a d-dimensional\\nsubspace, then the maximum number of vectors that are linearly indepen-\\ndent with each other is d, and any subcollection of linearly independent d\\nvectors satisfies (20.1).\\nProposition 20.1.9 states that the dimension of the span of some\\nset of k vectors is equivalent to the maximum number d of linearly\\nindependent vectors. It also states that the span of the k vectors is\\nequal to the span of the linearly independent d vectors, meaning all\\nof the information is captured by the d vectors; the remaining k −d'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 247}, page_content='248\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nvectors are just redundancies. But trying to directly compute the\\nmaximum number of linearly independent vectors is inefficient —\\nit may require checking the linear independence of an exponential\\nnumber of subsets of the vectors. In the next section, we discuss a\\nconcept called matrix rank that is very closely related to this topic.\\n20.1.5\\nOrthogonal Vectors\\nDefinition 20.1.10. If vectors⃗x1, . . . ,⃗xk ∈Rn satisfy⃗xi ·⃗xj = 0 for\\nany i ̸= j, then they are called orthogonal vectors. In particular, if they\\nalso satisfy the condition that ∥⃗xi∥= 1 for each i, then they are also\\northonormal.\\nIn Rn, orthogonal vectors form a 90 degree angle with each other.\\nExample 20.1.11. The two vectors (1, 0), (0, 2) are orthogonal. So are the\\nvectors (1, 2), (−2, 1).\\nx\\ny\\n⃗x = (1, 2)\\n⃗y = (−2, 1)\\nFigure 20.4: A visualization of orthogo-\\nnal vectors⃗x = (1, 2) and ⃗y = (−2, 1).\\nGiven any set of orthogonal vectors, it is possible to transform it\\ninto a set of orthonormal vectors, by normalizing each vector (i.e.,\\nscale it such that the norm is 1).\\n20.1.6\\nBasis\\nDefinition 20.1.12. A collection {⃗x1, . . . ,⃗xk} of linearly independent\\nvectors in Rn that span a set S is known as a basis of S. In particular, if\\nthe vectors of the basis are orthogonal/orthonormal, the basis is called an\\northogonal/orthonormal basis of S.\\nThe set S in Definition 20.1.12 can be the entire vector space Rn,\\nbut it can also be some subspace of Rn with a lower dimension.\\nExample 20.1.13. The set {(1, 0, 0), (0, 1, 0), (0, 0, 1)} of three vec-\\ntors is a basis for R3. When we exclude the last vector (0, 0, 1), the set\\n{(1, 0, 0), (0, 1, 0)} is a basis of the 2-dimensional XY-plane in R3.\\nGiven some subspace S, the basis of S is not unique. However,\\nevery basis of S must have the same size — this size is called the\\ndimension of S. For a finite dimensional space S, it is known that\\nthere exists an orthogonal basis of S. There is a well-known algorithm\\n— Gram-Schmidt process — that can transform an arbitrary basis\\ninto an orthogonal basis (and eventually an orthonormal basis via\\nnormalization).\\n20.1.7\\nProjection\\nVector projection is the key concept used in the Gram-Schmidt process\\nthat computes an orthogonal basis. Given a fixed vector⃗a, it decom-\\nposes any given vector⃗x into a sum of two components — one that is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 248}, page_content='linear algebra\\n249\\northogonal to⃗a (“distinct information”) and the other that is parallel\\nto⃗a (“redundant information”).\\nDefinition 20.1.14 (Vector Projection). Fix a vector⃗a ∈Rn. Given\\nanother vector⃗x, the projection of⃗x on⃗a is defined as\\nproj⃗a(⃗x) = ⃗x ·⃗a\\n⃗a ·⃗a⃗a\\nand is parallel to the fixed vector⃗a. The remaining component\\n⃗x −proj⃗a(⃗x)\\nis called the rejection of⃗x from⃗a and is orthogonal to⃗a.\\nProposition 20.1.15 (Pythagorean Theorem). If⃗x,⃗y are orthogonal, then\\n∥⃗x +⃗y∥2 = ∥⃗x∥2 + ∥⃗y∥2\\nIn particular, given two vectors⃗a,⃗x, we have\\n∥⃗x −proj⃗a(⃗x)∥2 = ∥⃗x∥2 −∥proj⃗a(⃗x)∥2\\nNow assume we are given a space S and a subspace T ⊂S. Then\\na vector⃗x ∈S in the larger space does not necessarily belong in T.\\nInstead, we can find a vector⃗x′ ∈T that is “closest” to⃗x using vector\\nprojection. 3\\n3 We ask you to prove this in Prob-\\nlem 7.1.3.\\nDefinition 20.1.16 (Vector Projection on Subspace). Given a space S, its\\nsubspace T with an orthogonal basis {⃗t1, . . . ,⃗tk}, and a vector⃗x ∈S, the\\nprojection of⃗x on T is defined as\\nprojT(⃗x) =\\nk\\n∑\\ni=1\\nproj⃗ti(⃗x) =\\nk\\n∑\\ni=1\\n⃗x ·⃗ti\\n⃗ti ·⃗ti\\n⃗ti\\nthe sum of projection of⃗x on each of the basis vectors of T.\\n20.2\\nMatrices\\nMatrices are a generalization of vectors in 2-dimension — a m × n\\nmatrix is a collection of numbers assembled in a rectangular shape\\nof m rows and n columns. The set of all real matrices of size m × n\\nis denoted as Rm×n. A vector of size n is customarily understood as\\na column vector — that is, a n × 1 matrix. Also, if m = n, then the\\nmatrix is known as a square matrix.\\n20.2.1\\nMatrix Operation\\nSimilarly to vector operations, we are interested in four matrix opera-\\ntions — matrix addition, scalar multiplication, matrix multiplication,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 249}, page_content='250\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nand transpose. Given a scalar c ∈R and matrices X, Y ∈Rm×n such\\nthat\\nX =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx1,1\\nx1,2\\n· · ·\\nx1,n\\nx2,1\\nx2,2\\n· · ·\\nx2,n\\n...\\n...\\n...\\n...\\nxm,1\\nxm,2\\n· · ·\\nxm,n\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nand\\nY =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\ny1,1\\ny1,2\\n· · ·\\ny1,n\\ny2,1\\ny2,2\\n· · ·\\ny2,n\\n...\\n...\\n...\\n...\\nym,1\\nym,2\\n· · ·\\nym,n\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nthe matrix addition is defined as\\nX + Y =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx1,1 + y1,1\\nx1,2 + y1,2\\n· · ·\\nx1,n + y1,n\\nx2,1 + y2,1\\nx2,2 + y2,2\\n· · ·\\nx2,n + y2,n\\n...\\n...\\n...\\n...\\nxm,1 + ym,1\\nxm,2 + ym,2\\n· · ·\\nxm,n + ym,n\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nwhere we add each of the coordinates element-wise. The scalar\\nmultiplication is similarly defined as\\ncX =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\ncx1,1\\ncx1,2\\n· · ·\\ncx1,n\\ncx2,1\\ncx2,2\\n· · ·\\ncx2,n\\n...\\n...\\n...\\n...\\ncxm,1\\ncxm,2\\n· · ·\\ncxm,n\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nThe matrix multiplication XY is defined for a matrix X ∈Rℓ×m and\\na matrix Y ∈Rm×n; that is, when the number of columns of the\\nfirst matrix is equal to the number of rows of the second matrix. The\\noutput XY of the matrix multiplication will be a ℓ× n matrix. The (i, j)\\nentry of the matrix XY is defined as\\n(XY)i,j =\\nm\\n∑\\nk=1\\nxi,kyk,j\\nThat is, it is defined as the inner product of the i-th row of X and the\\nj-th column of Y.\\nProposition 20.2.1. The above matrix operations satisfy the following\\nproperties:\\n• c(XY) = (cX)Y = X(cY)\\n• (X1 + X2)Y = X1Y + X2Y\\n• X(Y1 + Y2) = XY1 + XY2\\nFinally, the transpose X⊺∈Rn×m of a matrix X ∈Rm×n is the re-\\nsulting matrix when the entries of X are reflected down the diagonal.\\nThat is,\\n(X⊺)i,j = Xj,i'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 250}, page_content='linear algebra\\n251\\nProposition 20.2.2. The transpose of a matrix satisfies the following\\nproperties:\\n• (X + Y)⊺= X⊺+ Y⊺\\n• (cX)⊺= c(X⊺)\\n• (XY)⊺= Y⊺X⊺\\n20.2.2\\nMatrix and Linear Transformation\\nRecall that a vector of size n is often considered a n × 1 matrix.\\nTherefore, given a matrix A ∈Rm×n and a vector⃗x ∈Rn, we\\ncan define the following operation\\n⃗y = A⃗x ∈Rm\\nthrough matrix multiplication. This shows that A can be understood\\nas a mapping from Rn to Rm. We see that ai,j (the (i, j) entry of the\\nmatrix A) is the coefficient of xj (the j-th coordinate of the input\\nvector) when computing yi (the i-th coordinate of the output vector).\\nSince each yi is linear in terms of each xj, we say that A is a linear\\ntransformation.\\n20.2.3\\nMatrix Rank\\nMatrix rank is one of the most important concepts in basic linear\\nalgebra.\\nDefinition 20.2.3. Given a matrix A ∈Rm×n of m rows and n columns,\\nthe number of linearly independent rows is known to be always equal to the\\nnumber of linearly independent columns. This common number is known as\\nthe rank of A and is denoted as rank(A).\\nThe following property of rank is implied in the definition, but we\\nstate it explicitly as follows.\\nProposition 20.2.4. The rank of a matrix is invariant to reordering rows/-\\ncolumns.\\nExample 20.2.5. Consider the matrix M =\\n\"\\n1\\n1\\n−2\\n0\\n−1\\n−1\\n2\\n0\\n#\\n, we notice\\nthat the second row is simply the first row negated, and thus the rank of M\\nis 1.\\nExample 20.2.6. Consider the matrix M =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n1\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fb, the rank of M is 3\\nbecause all the row (or column) vectors are linearly independent (they form\\nbasis vectors of R3).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 251}, page_content='252\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nExample 20.2.7. Consider the matrix M =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n1\\n0\\n1\\n−2\\n−3\\n1\\n3\\n3\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fb, the rank of\\nM is 2 because the third row can be expressed as the second row subtracted\\nfrom the first row.\\nWhen we interpret a matrix as a linear transformation, the rank\\nmeasures the dimension of the output space.\\nProposition 20.2.8. A ∈Rm×n has rank k if and only if the image of the\\nlinear transformation, i.e., the subspace\\n{A⃗x |⃗x ∈Rn}\\nof Rm, has dimension k.\\nThere are many known algorithms to compute the rank of a\\nmatrix. Examples include Gaussian elimination or certain decom-\\npositions (expressing a matrix as the product of other matrices with\\ncertain properties). Given m vectors in Rn, we can find the maximum\\nnumber of linearly independent vectors by constructing a matrix with\\neach row equal to each vector 4 and finding the rank of that matrix.\\n4 By Proposition 20.2.4, the order of the\\nrows can be arbitrary.\\n20.2.4\\nEigenvalues and Eigenvectors\\nSay we have a square matrix A ∈Rn×n. This means that the linear\\ntransformation expressed by A is a mapping from Rn to itself. For\\nmost vectors⃗x ∈Rn,⃗x is mapped to a very “different” vector A⃗x\\nunder this mapping. However, some vectors are “special” and they\\nare mapped to another vector with the same direction.\\nDefinition 20.2.9 (Eigenvalue/Eigenvector). Given a square matrix\\nA ∈Rn×n, if a vector⃗v ∈Rn satisfies\\nA⃗v = λ⃗v\\nfor some scalar λ ∈R, then⃗v is known as an eigenvector of A, and λ is its\\ncorresponding eigenvalue.\\nEach eigenvector can only be associated with one eigenvalue, but\\neach eigenvalue may be associated with multiple eigenvectors.\\nProposition 20.2.10. If⃗x,⃗y are both eigenvectors of A for the same eigen-\\nvalue λ, then any linear combination of them is also an eigenvector for A\\nwith the same eigenvalue λ.\\nProposition 20.2.10 shows that the set of eigenvectors for a par-\\nticular eigenvalue forms a subspace, known as the eigenspace of that\\neigenvalue. The dimension of this subspace is known as the geometric\\nmultiplicity of the eigenvalue. The following result ties together some\\nof the concepts we discussed so far.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 252}, page_content='linear algebra\\n253\\nProposition 20.2.11 (Rank-Nullity Theorem). Given a square matrix\\nA ∈Rn×n, the eigenspace of 0 is the set of all vectors that get mapped to\\nzero vector⃗0 under the linear transformation A. This subspace is known as\\nthe null space of A and its dimension (i.e., the geometric multiplicity of 0)\\nis known as the nullity of A and is denoted as nullity(A). Then\\nrank(A) + nullity(A) = n\\n20.3\\nAdvanced: SVD/PCA Procedures\\nNow we briefly introduce a procedure called Principal Component\\nAnalysis (PCA), which is commonly used in low-dimensional repre-\\nsentation as in Chapter 7.\\nWe are given vectors ⃗v1,⃗v2, . . . ,⃗vN ∈Rd and a positive integer k\\nand wish to obtain the low-dimensional representation in the sense\\nof Definition 7.1.1 that minimizes ϵ. This is what we mean by “best”\\nrepresentation.\\nTheorem 20.3.1. The best low-dimensional representation consists of\\nk eigenvectors corresponding to the top k eigenvalues (largest numerical\\nvalues) of the matrix AA⊺where the columns of A are⃗v1,⃗v2, . . . ,⃗vN.\\nTheorem 20.3.1 shows what the best low-dimensional represen-\\ntation is, but it does not show how to compute it. It turns out some-\\nthing called the Singular Value Decomposition (SVD) of the matrix A\\nis useful. It is known that any matrix A can be decomposed into the\\nfollowing product\\nA = UΣV⊺\\nwhere Σ is a diagonal matrix with entries equal to the square root\\nof the nonzero eigenvalues of AA⊺and the columns of U are the\\northonormal eigenvectors of AA⊺, where the i-th column is the\\neigenvector that corresponds to the eigenvalue at the i-th diagonal\\nentry of Σ. There are known computationally efficient algorithms that\\nwill perform the SVD of a matrix.\\nIn this section, we will prove Theorem 20.3.1 for the case where\\nk = 1. To do this, we need to introduce some preliminary results.\\nTheorem 20.3.2. If a square matrix A ∈Rn×n is symmetric (i.e., A = A⊺),\\nthen there is an orthonormal basis of Rn consisting of n eigenvectors of A. 5\\n5 This is known as the Spectral Theo-\\nrem.\\nProof. A real symmetric matrix is known to be diagonalizable, and\\ndiagonalizable matrices are known to have n eigenvectors that form a\\nbasis for Rn. In particular, the eigenvectors are linearly independent,\\nmeaning the eigenvectors corresponding to a particular eigenvalue\\nλ will form a basis for the corresponding eigenspace. Through the\\nGram-Schmidt process, we can replace some of these eigenvectors'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 253}, page_content='254\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nsuch that the eigenvectors for λ are orthogonal to each other. That\\nis, if ⃗u,⃗v are eigenvectors for the same eigenvalue λ, then ⃗u ·⃗v = 0.\\nNow assume ⃗u,⃗v are two eigenvectors with distinct eigenvalues λ, µ\\nrespectively. Then\\nλ⃗u ·⃗v = (λ⃗u) ·⃗v = (A⃗u) ·⃗v =\\nn\\n∑\\ni,j=1\\nai,jujvi\\n= ⃗u · (A⊺⃗v) = ⃗u · (A⃗v) = ⃗u · (µ⃗v) = µ⃗u ·⃗v\\nwhere the third and the fourth equality can be verified by direct\\ncomputation. Since λ ̸= µ, we conclude ⃗u ·⃗v = 0. We have now\\nshowed that ⃗u ·⃗v = 0 for any pair of eigenvectors ⃗u,⃗v — this means\\nthat the basis of eigenvectors is also orthogonal. After normalization,\\nthe basis can be made orthonormal.\\nThe following result is not necessarily needed for the proof of\\nTheorem 20.3.1, but the proofs are similar.\\nTheorem 20.3.3. If A ∈Rn×n is symmetric, then the unit vector⃗x that\\nmaximizes ∥A⃗x∥is an eigenvector of A with an eigenvalue whose absolute\\nvalues is the largest out of all eigenvalues.\\nProof. By Theorem 20.3.2, there is an orthonormal basis {⃗u1, . . . ,⃗un}\\nof Rn consisting of eigenvectors of A. Then any vector⃗x is in the\\nspan of the eigenvectors and can be represented as the linear combi-\\nnation\\n⃗x = α1⃗u1 + α2⃗u2 + . . . + αn⃗un\\nfor some scalars αi’s. Then\\n∥⃗x∥2 =⃗x ·⃗x\\n= (α1⃗u1 + α2⃗u2 + . . . + αn⃗un) · (α1⃗u1 + α2⃗u2 + . . . + αn⃗un)\\n=\\nn\\n∑\\ni,j=1\\nαiαj(⃗ui ·⃗uj)\\n=\\nn\\n∑\\ni=1\\nα2\\ni\\nwhere for the last equality, we use the fact that ⃗ui’s are orthonormal\\n— that is, ⃗ui ·⃗uj = 0 if i ̸= j and ⃗ui ·⃗ui = 1. Since⃗x has norm 1, we see\\nthat\\nn\\n∑\\ni=1\\nα2\\ni = 1. Now notice that\\nA⃗x = A(α1⃗u1 + α2⃗u2 + . . . + αn⃗un)\\n= α1A⃗u1 + α2A⃗u2 + . . . + αnA⃗un\\n= α1λ1⃗u1 + α2λ2⃗u2 + . . . + αnλn⃗un'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 254}, page_content='linear algebra\\n255\\nwhere λi is the eigenvalue for the eigenvector ⃗ui. Following a similar\\ncomputation as above,\\n∥A⃗x∥2 =\\nn\\n∑\\ni=1\\nα2\\ni λ2\\ni\\nThe allocation of weights αi that will maximize\\nn\\n∑\\ni=1\\nα2\\ni λ2\\ni while main-\\ntaining\\nn\\n∑\\ni=1\\nα2\\ni = 1 is assigning αi = ±1 to the eigenvalue λi that has\\nthe highest value of λ2\\ni . This shows that the unit vector⃗x = ±⃗ui is an\\neigenvector with the eigenvalue λi.\\nWe now prove one last preliminary result.\\nTheorem 20.3.4. For a matrix A ∈Rm×n, the matrix AA⊺is symmetric\\nand its eigenvalues are non-negative.\\nProof. The first part can be verified easily by observing that\\n(AA⊺)⊺= (A⊺)⊺A⊺= AA⊺\\nNow assume⃗x is an eigenvector of A with eigenvalue λ. Then\\nAA⊺⃗x = λ⃗x\\nWe multiply by⃗x⊺on the left on both sides of the equation.\\n⃗x⊺AA⊺⃗x =⃗x⊺(λ⃗x) = λ ∥⃗x∥2\\nAt the same time, notice that\\n⃗x⊺AA⊺⃗x = (A⊺⃗x)⊺(A⊺⃗x) = ∥A⊺⃗x∥2\\nwhich shows that\\nλ ∥⃗x∥2 = ∥A⊺⃗x∥2\\nSince ∥⃗x∥2 , ∥A⊺⃗x∥2 are both non-negative, λ is also non-negative.\\nWe are now ready to (partially) prove the main result of this\\nsection.\\nProof of Theorem 20.3.1. We prove the case where k = 1. Recall\\nthat we want to find a vector ⃗u that minimizes the error of the low-\\ndimensional representation:\\nN\\n∑\\ni=1\\n\\r\\r\\r⃗vi −b⃗vi\\n\\r\\r\\r\\n2\\nwhere b⃗vi is the low-dimensional representation of ⃗vi that can be\\ncomputed as\\nb⃗vi = (⃗vi ·⃗u)⃗u'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.23', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-08T03:30:34+00:00', 'source': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'file_path': 'rag-dataset-main/machine-learning/COS324_Course_Notes.pdf', 'total_pages': 256, 'format': 'PDF 1.5', 'title': 'Introduction to  Machine Learning  Lecture Notes for COS 324 at Princeton University', 'author': 'Sanjeev Arora, Simon Park, Dennis Jacob, Danqi Chen', 'subject': '', 'keywords': '', 'moddate': '2024-09-08T03:30:34+00:00', 'trapped': '', 'modDate': 'D:20240908033034Z', 'creationDate': 'D:20240908033034Z', 'page': 255}, page_content='256\\nintroduction to machine learning lecture notes for cos 324 at princeton university\\nby the result of Problem 7.1.3. Now by Proposition 20.1.15, we see\\nthat\\nN\\n∑\\ni=1\\n∥⃗vi −(⃗vi ·⃗u)⃗u∥2 =\\nN\\n∑\\ni=1\\n\\x10\\n∥⃗vi∥2 −∥(⃗vi ·⃗u)⃗u∥2\\x11\\n=\\nN\\n∑\\ni=1\\n\\x10\\n∥⃗vi∥2 −(⃗vi ·⃗u)2\\x11\\nSince we are already given a fixed set of vectors ⃗vi, we cannot change\\nthe values of ∥⃗vi∥2. Therefore, minimizing the last term of the equa-\\ntion above amounts to maximizing\\nN\\n∑\\ni=1\\n(⃗vi ·⃗u)2. Notice that\\nN\\n∑\\ni=1\\n(⃗vi ·⃗u)2 = ∥A⊺⃗u∥2 = ⃗u⊺AA⊺⃗u\\nBy Theorem 20.3.2 and by Theorem 20.3.4, there is an orthonormal\\nbasis {⃗u1, . . . ,⃗un} of Rn that consist of the eigenvectors of the matrix\\nAA⊺. Let λi be the eigenvalue corresponding to the eigenvector ⃗ui.\\nThen similarly to the proof of Theorem 20.3.3, we can represent any\\nvector ⃗u as a linear combination of the eigenvectors as\\n⃗u = α1⃗u1 + α2⃗u2 + . . . + αn⃗un\\nThen we have\\nn\\n∑\\ni=1\\nα2\\ni = 1 and\\n⃗u⊺AA⊺⃗u = (α1⃗u1 + α2⃗u2 + . . . + αn⃗un)⊺AA⊺(α1⃗u1 + α2⃗u2 + . . . + αn⃗un)\\n= (α1⃗u1 + α2⃗u2 + . . . + αn⃗un)⊺(α1λ1⃗u1 + α2λ2⃗u2 + . . . + αnλn⃗un)\\n=\\nn\\n∑\\ni,j=1\\nαiαjλj(⃗ui ·⃗uj)\\n=\\nn\\n∑\\ni=1\\nα2\\ni λi\\nAgain, the allocation of αi’s that maximize\\nn\\n∑\\ni=1\\nα2\\ni λi while maintaining\\nn\\n∑\\ni=1\\nα2\\ni = 1 is assigning αi = ±1 to the eigenvector corresponding to\\nthe highest value of λi.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='arXiv:2505.03861v1  [cs.LG]  6 May 2025\\nMachine Learning: a Lecture Note\\nKyunghyun Cho\\nNew York University & Genentech\\nJune 10, 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='2\\nPREFACE\\nI prepared this lecture note in order to teach DS-GA 1003 “Machine Learn-\\ning” at the Center for Data Science of New York University. This is the first\\ncourse on machine learning for master’s and PhD students in data science, and\\nmy goal was to provide them with a solid foundation on top of which they can\\ncontinue on to learn more advanced and modern topics in machine learning,\\ndata science as well as more broadly artificial intelligence. Because of this goal,\\nthis lecture note has quite a bit of mathematical derivations of various concepts\\nin machine learning. This should not deter students from reading through this\\nlecture note, as I have interleaved these derivations with accessible explana-\\ntions on the intuition and insights behind these derivations. Of course, as I was\\npreparing this note, it only became clear how shallow my own foundation in\\nmachine learning was. But, I tried.\\nIn preparing this lecture note, I tried my best to constantly remind my-\\nself of “Bitter Lesson” by Richard Sutton [Sutton, 2019]. I forced myself to\\npresent various algorithms, models and theories in ways that support scalable\\nimplementations, both for compute and data. All machine learning algorithms\\nin this lecture are thus presented to work with stochastic gradient descent and\\nits variants. Of course, there are other aspects of scalability, such as distributed\\ncomputing, but I expect and hope that other more advanced follow-up courses\\nwould teach students with these advanced topics based on the foundation this\\ncourse has equipped those students with.\\nDespite my intention to cover as much foundational topics as possible in\\nthis course, it only became apparent that one course is not long enough to\\ndig deeper into all of these topics. I had to make a difficult decision to omit\\nsome topics I find foundational, interesting and exciting, such as online learning,\\nkernel methods and how to handle missing values. There were on the other hand\\nsome topics I intentionally omitted, although I believe them to be foundational\\nas well, because they are covered extensively in various other courses, such\\nas sequence modeling (or large-scale language modeling). I have furthermore\\nrefrained from discussing any particular application, hoping that there are other\\nfollow-up courses focused on individual application domains, such as computer\\nvision, computational biology and natural language processing.\\nThere are a few more modern topics I hoped I could cover but could not\\ndue to time. To list a few of those, they include ordinary differential equation\\n(ODE) based generative models and contrastive learning for both representation\\nlearning and metric learning. Perhaps in the future, I could create a two-course\\nseries in machine learning and add these extra materials. Until then, students\\nwill have to look for other materials to learn about these topics.\\nThis lecture note is not intended to be a reference book but was created to\\nbe a teaching material. This is my way of apologizing in advance that I have\\nnot been careful at all on extensively and exhaustively citing all relevant past\\nliterature. I will hopefully add citations more thoroughly the next time I teach\\nthis same course, although there is no immediate plan to do so anytime soon.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Contents\\n1\\nAn Energy Function\\n1\\n2\\nBasic Ideas in Machine Learning with Classification\\n5\\n2.1\\nClassification . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5\\n2.1.1\\nPerceptron and margin loss functions . . . . . . . . . . . .\\n6\\n2.1.2\\nSoftmax and cross entropy loss . . . . . . . . . . . . . . .\\n7\\n2.2\\nBackpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n11\\n2.2.1\\nA Linear Energy Function . . . . . . . . . . . . . . . . . .\\n11\\n2.2.2\\nA Nonlinear Energy Function . . . . . . . . . . . . . . . .\\n13\\n2.3\\nStochastic Gradient Descent . . . . . . . . . . . . . . . . . . . . .\\n17\\n2.3.1\\nDescent Lemma . . . . . . . . . . . . . . . . . . . . . . . .\\n19\\n2.3.2\\nStochastic Gradient Descent . . . . . . . . . . . . . . . . .\\n20\\n2.3.3\\nAdaptive Learning Rate Methods . . . . . . . . . . . . . .\\n21\\n2.4\\nGeneralization and Model Selection . . . . . . . . . . . . . . . . .\\n23\\n2.4.1\\nExpected risk vs. empirical risk: a generalization bound .\\n23\\n2.4.2\\nBias, Variance and Uncertainty . . . . . . . . . . . . . . .\\n28\\n2.4.3\\nUncertainty in the error rate\\n. . . . . . . . . . . . . . . .\\n30\\n2.5\\nHyperparameter Tuning: Model Selection\\n. . . . . . . . . . . . .\\n34\\n2.5.1\\nSequential model-based optimization for hyperparameter\\ntuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n36\\n2.5.2\\nWe still need to report the test set accuracy separately . .\\n37\\n3\\nBuilding blocks of neural networks\\n39\\n3.1\\nNormalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n40\\n3.2\\nConvolutional blocks . . . . . . . . . . . . . . . . . . . . . . . . .\\n42\\n3.3\\nRecurrent blocks . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n44\\n3.4\\nPermutation equivariance: attention\\n. . . . . . . . . . . . . . . .\\n44\\n4\\nProbabilistic Machine Learning and Unsupervised Learning\\n49\\n4.1\\nProbabilistic interpretation of the energy function . . . . . . . . .\\n49\\n4.2\\nVariational inference and Gaussian mixture models . . . . . . . .\\n51\\n4.2.1\\nVariational Gaussian mixture models . . . . . . . . . . . .\\n52\\n4.2.2\\nK-means clustering . . . . . . . . . . . . . . . . . . . . . .\\n55\\n4.3\\nContinuous latent variable models\\n. . . . . . . . . . . . . . . . .\\n56\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='4\\nCONTENTS\\n4.3.1\\nVariational autoencoders\\n. . . . . . . . . . . . . . . . . .\\n59\\n4.3.2\\nImportance sampling and its variance. . . . . . . . . . . .\\n63\\n5\\nUndirected Generative Models\\n67\\n5.1\\nRestricted Boltzmann machines: the Product of Experts . . . . .\\n67\\n5.1.1\\nMarkov Chain Monte Carlo (MCMC) Sampling . . . . . .\\n70\\n5.1.2\\n(Persistent) Contrastive Divergence . . . . . . . . . . . . .\\n74\\n5.2\\nEnergy-based generative adversarial networks . . . . . . . . . . .\\n75\\n5.3\\nAutoregressive models . . . . . . . . . . . . . . . . . . . . . . . .\\n79\\n6\\nFurther Topics\\n83\\n6.1\\nReinforcement Learning . . . . . . . . . . . . . . . . . . . . . . .\\n83\\n6.2\\nEnsemble Methods . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n6.3\\nMeta-Learning\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n96\\n6.4\\nRegression: Mixture Density Networks . . . . . . . . . . . . . . .\\n98\\n6.5\\nCausality\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='Chapter 1\\nAn Energy Function\\nA usual way to teaching machine learning is to go through different problem\\nsetups. It often starts with binary classification, when perceptron, logistic regres-\\nsion and support vector machines are introduced, and continues with multi-class\\nclassification. At this point, it is usual to introduce regression as a continu-\\nous version of classification. Often, at this point, one would learn about kernel\\nmethods and neural networks, with focus on backpropagation (a more recent\\ndevelopment in terms of teaching machine learning.) This is also at a point\\nwhere one would take a detour by learning probabilistic machine learning, with\\nthe eventual goal of introducing a Bayesian approach to machine learning, i.e.,\\nmarginalization over optimization. The latter half of the course would closely\\nresemble the contents so far however in an unsupervised setting, where we learn\\nthat machine learning can be useful even when observations are not associated\\nwith outcomes (labels). One would learn about a variety of matrix factorization\\ntechniques, clustering as well as probabilistic generative modeling. If the lec-\\nturer were ambitious, they would sneak in one or two lectures on reinforcement\\nlearning at the very end.\\nA main issue of teaching machine learning in such a conventional way is that\\nit is extremely inconvenient for students to see a common foundation underlying\\nall these different techniques and paradigms. It is often challenging for students\\nto see how supervised and unsupervised learning connect with each other. It is\\neven more challenging for students to figure out that classification and clustering\\nare simply two sides of the same coin. In my opinion, it is simply impossible to\\nmake a majority of students see the unifying foundation behind all these different\\ntechniques and paradigms if we stick to enumerating all these paradigms and\\ntechniques. In this course, I thus try to take a new approach to teaching machine\\nlearning, largely based on and inspired by an earlier tutorial paper authored by\\nYann LeCun and his colleagues [LeCun et al., 2006]. Other than this tutorial\\npaper, this approach does not yet exist and will take a shape I continue to write\\nthis lecture note as the course continues.\\nTo begin on this journey, we start by defining an energy function, or a\\nnegative compatibility score. This energy function e assigns a real value to a\\n1'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='2\\nCHAPTER 1. AN ENERGY FUNCTION\\npair of an observed instance and a latent instance (x, z) and is parametrized by\\na multi-dimensional vector θ.\\ne : X × Z × Θ →R.\\n(1.1)\\nX is a set of all possible observed instances, Z is a set of all possible latent\\ninstances, and Θ is a set of all possible parameter configurations.\\nWhen the energy function is low (that is, the compatibility is high,) we say\\nthat a given pair (x, z) is highly preferred given θ. When the energy function is\\nhigh, unsurprisingly we say that the given pair is not as preferred.\\nThe latent observation z is, as the name suggests, not observed directly. It\\nnevertheless plays an important role in capturing uncertainty. When we only\\nobserve x, but not z, we cannot fully determine how preferable x is. With a\\ncertain set of values of z, the energy may be low, while it may be high with\\nother values of z. This gives us a sense of the uncertainty. For instance, we can\\ncompute both the mean and variance of the energy of an observed instance x\\nby\\neµ(x, θ) = E[e(x, z, θ)] =\\nX\\nz∈Z\\np(z)e(x, z, θ),\\n(1.2)\\nev(x, θ) = E[(e(x, z, θ) −eµ(x, θ))2].\\n(1.3)\\nGiven an energy function e and the parameter θ, we can derive a variety\\nof paradigms in machine learning by minimizing the energy function with re-\\nspect to different variables. For instance, let the observation be partitioned into\\ntwo parts; input and output and assume that there is no latent variable, i.e.,\\ne([x, y], ∅, θ). Given a new input x′, we can solve the problem of supervised\\nlearning by\\nˆy = arg min\\ny∈Y e([x′, y], ∅, θ),\\n(1.4)\\nwhere Y is the set of all possible outcomes y. When Y consists of discrete items,\\nwe call it classification. If y is a continuous variable, we call it regression.\\nWhen Z is a finite set of discrete items, a given energy function e defines\\nthe cluster assignment of an observation x, resulting in clustering:\\nˆz = arg min\\nz∈Z e(x, z, θ).\\n(1.5)\\nIf z is a continuous variable, we would solve the same problem but call it repre-\\nsentation learning.\\nAll these different paradigms effectively correspond to solving a minimization\\nproblem with respect to some subset of the inputs to the energy function e. In\\nother words, given a partially-observed input, we infer the unobserved part that\\nminimizes the energy function. This is often why people refer to using any\\nmachine learning model after training as inference.\\nIt is not trivial to solve such a minimization problem. The level of difficulty\\ndepends on a variety of factors, including how the energy function is defined,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='3\\nthe dimensionalities of the observed as well as latent variables as well as the\\nparameters themselves. Throughout the course, we will consider different setups\\nin which efficient and effective optimization algorithms are known and used for\\ninference.\\nAs the name ‘machine learning’ suggests, a bulk of machine learning is on\\nestimating θ. Based on what we have seen above, it may be tempting to think\\nthat learning is nothing but\\nmin\\nθ∈Θ Ex∼pdata [e(x, ∅, θ)] ,\\n(1.6)\\nwhen there is no latent variable. It turned out unfortunately that learning is not\\nas easy, since we must ensure that the energy assigned to undesirable observa-\\ntion, i.e. pdata(x) ↓, must be relatively high. In other words, we must introduce\\nan extra term that regularizes learning:\\nmin\\nθ∈Θ Ex∼pdata [e(x, ∅, θ) −R(θ)] .\\n(1.7)\\nThe choice of R must be made appropriately for each problem we solve, and\\nthroughout the course, we will learn how to design appropriate regularizers to\\nensure proper learning.\\nOf course it becomes even more involved when there are latent (unobserved)\\nvariables z, since it require us to solve the problem of inference simultaneously as\\nwell. This happens for problems such as clustering where the cluster assignment\\nof each observation is unknown and factor analysis where latent factors are\\nunknown in advance. We will learn how to interpret such latent variables and\\nalgorithms that allow us to estimate θ in the absence of latent variables.\\nIn summary, there are three aspects to every machine learning problem; (1)\\ndefining an energy function e (parametrization), (2) estimating the parameters\\nθ from data (learning), and (3) inferring a missing part given an partial obser-\\nvation (inference). Across these three steps sits one energy function, and once\\nwe obtain an energy function e, we can easily mix and match these steps from\\ndifferent paradigms of machine learning.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='4\\nCHAPTER 1. AN ENERGY FUNCTION'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='Chapter 2\\nBasic Ideas in Machine\\nLearning with Classification\\n2.1\\nClassification\\nIn the problem of classification, an observation x can be split into the input and\\noutput; [x, y]. The output y takes one of the finite number of categories in Y.\\nFor now, we assume that there is no latent variable, i.e., Z = ∅. Inference is\\nquite trivial in this case, since all we need to do is to pick the category that has\\nthe lowest energy, after computing the energy for all possible categories one at\\na time:\\nˆy(x) = arg min\\ny∈Y e([x, y], ∅, θ).\\n(2.1)\\nOf course, this can be computationally costly if either |Y| is large or x is high-\\ndimensional. We can overcome this issue by cleverly parametrizing the energy\\nfunction for instance as\\ne([x, y], ∅, θ) = 1(y)⊤f(x, θ),\\n(2.2)\\nwhere 1(y) = [0, . . . , 0, 1, 0, . . . , 0] is an one-hot vector. This one-hot vector is\\nall zeroes except for the y-th element which is set to 1.\\nf : X × Θ →R|Y| is a feature extractor that returns as many real values\\nas there are categories. With this parametrization, we can compute the energy\\nvalues of all categories in parallel. A relatively simple example of f is a linear\\nfunction, defined as\\nf(x, θ) = Wx + b,\\n(2.3)\\nwhere θ = (W, b) with W ∈R|Y|×|x| and b ∈R|Y|. When such a linear feature\\nextractor is used, we call it a linear classifier.\\nA natural next question is how we can learn the parameters θ (e.g. W and b).\\nWe approach learning from the perspective of optimization. That is, we establish\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='6CHAPTER 2. BASIC IDEAS IN MACHINE LEARNING WITH CLASSIFICATION\\na loss function first and figure out how to minimize the loss function averaged\\nover a training set D, where the training set D is assumed to consist of N\\nindependently sampled observations from the identical distribution (i.i.d.):\\nD = {[xn, yn]}N\\nn=1 .\\n(2.4)\\nPerhaps the most obvious loss function we can imagine is a so-called zero-one\\n(0-1) loss:\\nL0−1([x; y], θ) = 1(y ̸= ˆy(x)),\\n(2.5)\\nwhere\\nˆy(x) = arg min\\ny′∈Y e([x, y′], ∅, θ),\\n(2.6)\\nas described earlier (reproduced here for emphasis.) 1(a) is an indicator function\\ndefined as\\n1(a) =\\n(\\n1,\\nif a is true.\\n0,\\notherwise.\\n(2.7)\\nWith this zero-one loss function, the overall objective of learning is then\\nmin\\nθ\\n1\\nN\\nN\\nX\\nn=1\\nL0−1([xn, yn], θ).\\n(2.8)\\nThis optimization problem is unfortunately very difficult, because there is\\nalmost no signal on how we can incrementally change θ to gradually decrease the\\nloss function. The zero-one loss is a piece-wise constant function with respect\\nto θ. It is either 0 or 1, and any infinitesimal change to θ is unlikely to change\\nthe loss value. In other words, the only way to tackle this problem is to sweep\\nthrough many (if not all) possible values of θ and to identify the one that has\\nthe lowest overall loss. Such an approach is called blackbox optimization, and is\\nknown to be notoriously difficult.\\n2.1.1\\nPerceptron and margin loss functions\\nInstead, we can come up with a proxy to this zero-one loss function, that is easier\\nto optimize. We do so by assuming that the energy function is differentiable with\\nrespect to θ, that is, ∇θe exists and is easily computable.1 Then, we just need\\nto ensure that the loss function is not piece-wise constant with respect to the\\nenergy function itself.\\nWe start by noticing that the zero-one loss is minimized (= 0) when y′\\nassociated with the lowest energy (= ˆy) coincides with y from the training data.\\nIn other words, the zero-one loss is minimized when the energy associated with\\n1We will shortly see why it is find to assume that it is easily computable.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='2.1. CLASSIFICATION\\n7\\nthe true outcome y, i.e., e([x, y], ∅, θ), is lower than the energy associated with\\nany other y′ ̸= y. This goal can then be written down as satisfying the following\\ninequality:\\ne([x, y], ∅, θ) ≤e([x, ˆy′], ∅, θ) −m,\\n(2.9)\\nwhere m > 0 and\\nˆy′ = arg\\nmin\\ny′∈Y\\\\{y} e([x, y′], ∅, θ).\\n(2.10)\\nBy rearranging terms in this inequality we get\\nm + e([x, y], ∅, θ) −e([x, ˆy′], ∅, θ) ≤0.\\n(2.11)\\nIn order to satisfy this inequality, we need to minimize the left hand side (l.h.s.)\\nuntil it hits 0. We do not need to further minimize l.h.s. after hitting 0, since the\\ninequality is already satisfied. This translates to the following so-called margin\\nloss (or a hinge loss):\\nLmargin([x, y], θ) = max(0, m + e([x, y], ∅, θ) −e([x, ˆy′], ∅, θ)).\\n(2.12)\\nThis loss is called a margin loss, because it ensures that there exists at least\\nthe margin of m between the energy values of the correct outcome y and the\\nsecond best outcome ˆy′. The margin loss is at the heart of support vector ma-\\nchines [Cortes, 1995].\\nConsider the case where m = 0:\\nLperceptron([x, y], θ) = max(0, e([x, y], ∅, θ) −e([x, ˆy′], ∅, θ)).\\n(2.13)\\nIf y = ˆy (not ˆy′), the loss is already minimized at 0, since\\ne([x, y], ∅, θ) < e([x, ˆy′], ∅, θ).\\n(2.14)\\nIn other words, if a given example [x, y] is already correctly solved, we do not\\nneed to change θ for this example. We only update θ when y ̸= ˆy. This loss is\\ncalled a perceptron loss and dates back to 1950’s [Rosenblatt, 1958].\\n2.1.2\\nSoftmax and cross entropy loss\\nIt is often convenient to rely on the probabilistic framework, since it allows us\\nto use a large set of tools developed for probabilistic inference and statistical\\ntechniques. As an example of doing so, we will now derive a probabilistic classi-\\nfier from the energy function e([x, y], ∅, θ). The first step is to turn this energy\\nfunction into a Categorical distribution over Y given the input x.\\nLet pθ(y|x) be the Categorical probability of y given x. There are two major\\nconstraints that must be satisfied:\\n1. Non-negativity: pθ(y|x) ≥0 for all y ∈Y.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='8CHAPTER 2. BASIC IDEAS IN MACHINE LEARNING WITH CLASSIFICATION\\n2. Normalization: P\\ny′∈Y pθ(y′|x) = 1.\\nOf course, there can be many (if not infinitely many) different ways to map\\ne([x, y], ∅, θ) to pθ(y|x), while satisfying these two conditions [Peters et al.,\\n2019]. We thus need to impose a further constraint to narrow down on one\\nparticular mapping from the energy function to the Categorical probability. A\\nnatural such constraint is the maximum entropy criterion.\\nThe (Shannon) entropy is defined as\\nH(y|x; θ) = −\\nX\\ny∈Y\\npθ(y|x) log pθ(y|x).\\n(2.15)\\nThe entropy is large if there is a large degree of uncertainty. In order to cope\\nwith the issue of log 0, we assume that\\nH(y|x; θ) = 0, if pθ(y|x) =\\n(\\n0\\n1\\n.\\n(2.16)\\nWhy is this natural? Because, it is our way to explicitly concede that we\\nare not fully aware of the world and that there may be somethings that are not\\nknown, resulting in some uncertainty about our potential choice. This is often\\nreferred to as the principle of maximum entropy [Jaynes, 1957].\\nThen, we can convert the energy values {a1 = e([x, y = 1], ∅, θ), . . . , ad = e([x, y = d], ∅, θ)}\\nassigned to different outcome classes Y = {1, 2, . . . , d} into the Categorical prob-\\nabilities {p1, . . . , pd} by solving the following constrained optimization problem:\\nmax\\np1,...,pd −\\nd\\nX\\ni=1\\naipi −\\nd\\nX\\ni=1\\npi log pi\\n(2.17)\\nsubject to\\npi ≥0, for all i = 1, . . . , d\\n(2.18)\\nd\\nX\\ni=1\\npi = 1.\\n(2.19)\\nWe can solve this optimization problem with the method of Lagrangian\\nmultipliers. First, we write the unconstrained objective function:\\nJ(p1, . . . , pd, λ1, . . . , λd, γ) = −\\nd\\nX\\ni=1\\naipi −\\nd\\nX\\ni=1\\npi log pi +\\nd\\nX\\ni=1\\nλi(pi −s2\\ni ) + γ(\\nd\\nX\\ni=1\\npi −1),\\n(2.20)\\nwhere λ1, . . . , λd and γ are Lagragian multipliers, and s1, . . . , sd are slack vari-\\nables.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='2.1. CLASSIFICATION\\n9\\nLet us first compute the partial derivative of J with respect to pi and set it\\nto 0:\\n∂J\\n∂pi\\n= −ai −log pi −1 + λi + γ = 0\\n(2.21)\\n⇐⇒log pi = −ai + λi −1 + γ\\n(2.22)\\n⇐⇒pi = exp(−ai + λi −1 + γ) > 0.\\n(2.23)\\nWe notice that pi is already greater than 0 at this extreme point, meaning\\nthat the first constraint pi ≥0 is already satisfied. We can just set λi to any\\narbitrary value, and we will pick 0, i.e., λi = 0 for all i = 1, . . . , d. This results\\nin\\npi = exp(−ai) exp(−1 + γ).\\n(2.24)\\nLet us now plug it into the second constraint and solve for γ:\\nexp(−1 + γ)\\nd\\nX\\ni=1\\nexp(−ai) = 1\\n(2.25)\\n⇐⇒−1 + γ + log\\nd\\nX\\ni=1\\nexp(−ai) = 0\\n(2.26)\\n⇐⇒γ = 1 −log\\nd\\nX\\ni=1\\nexp(−ai).\\n(2.27)\\nBy plugging it into pi above, we get\\npi = exp(−ai) exp(−1 + 1 −log\\nd\\nX\\nj=1\\nexp(−aj))\\n(2.28)\\n=\\nexp(−ai)\\nPd\\nj=1 exp(−aj)\\n.\\n(2.29)\\nThis formulation is often referred to as softmax [Bridle, 1990].\\nNow, we have the Categorical probability pi = pθ(y = i|x). We can then\\ndefine an objective function under the probabilistic framework, as\\nLce([x, y]; θ) = −log pθ(y|x) = e([x, y], ∅, θ) + log\\nX\\ny′∈Y\\nexp(−e([x, y′], ∅, θ)).\\n(2.30)\\nWe often call this a cross-entropy loss, or equivalently negative log-likelihood.\\nUnlike the margin and perceptron losses from above, it is more informative'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='10CHAPTER 2. BASIC IDEAS IN MACHINE LEARNING WITH CLASSIFICATION\\nto consider the gradient of the cross-entropy loss:\\n∇θLce([x, y], ∅, θ) = ∇θe([x, y], ∅, θ) −\\nX\\ny′∈Y\\nexp(−e([x, y′], ∅, θ))\\nP\\ny′′∈Y exp(−e([x, y′′], ∅, θ))\\n|\\n{z\\n}\\n=pθ(y′|x)\\n∇θe([x, y′], ∅, θ))\\n(2.31)\\n= ∇θe([x, y], ∅, θ)\\n|\\n{z\\n}\\n(a)\\n−Ey|x;θ [∇θe([x, y′], ∅, θ))]\\n|\\n{z\\n}\\n(b)\\n.\\n(2.32)\\nThis gradient, or an update rule since we update θ following this direction, is\\ncalled a Boltzmann machine learning [Ackley et al., 1985].\\nThere are two terms in this update rule; (a) positive and (b) negative terms.\\nThe positive term corresponds to increasing the energy value associated with\\nthe true outcome y.2 The negative term corresponds to decreasing the energy\\nvalues associated with all possible outcomes, but they are weighted according\\nto how likely they are under the current parameters.\\nLet us consider the negative term a bit more carefully:\\n−\\nX\\ny′∈Y\\nexp(−βe([x, y′], ∅, θ))\\nP\\ny′′∈Y exp(−βe([x, y′′], ∅, θ))∇θe([x, y′], ∅, θ)).\\n(2.33)\\nβ was added to make our analysis easier. We often call β an inverse temperature.\\nβ is by default 1, but by varying β, we can gain more insights into the negative\\nterm.\\nConsider the case where β = 0, the negative term reduces to\\n−1\\n|Y|\\nX\\ny′∈Y\\n∇θe([x, y′], ∅, θ)).\\n(2.34)\\nThis would correspond to increasing the energy associated with each outcome\\nequally.\\nHow about when β →∞? In that case, the negative term reduces to\\n−∇θe([x, ˆy], ∅, θ),\\n(2.35)\\nwhere\\nˆy = arg min\\ny∈Y e([x, y], ∅, θ).\\n(2.36)\\nWhen β →∞, we end up with two cases. First, the classifier makes the\\ncorrect prediction; ˆy = y. In this case, the positive and negative terms cancel\\neach other, and there is no gradient. Hence, there is no update to the param-\\neters. This reminds us of the perceptron loss from the earlier section. On the\\nother hand, if ˆy ̸= y, it will try to lower the energy value associated with the\\n2Recall that this is a loss which is minimized.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='2.2. BACKPROPAGATION\\n11\\ncorrect outcome y while increasing the energy value associated with the current\\nprediction ˆy. This continues until the prediction matches the correct outcome.\\nThese two extreme cases tell us what happens with the cross entropy loss.\\nIt softly adjust the energy values associated with all possible outcomes however\\nbased on how likely they are to be the prediction. The cross entropy loss has\\nbecome more or less de facto standard when it comes to training a neural network\\nin recent years.\\n2.2\\nBackpropagation\\nOnce you decide the loss function, it is time for us to train a classifier to minimize\\nthe average loss. In doing so, one of the most effective approaches has been\\nstochastic gradient descent, or its variant. Stochastic gradient descent, which\\nwe will discuss more in-depth later, takes a subset of training instances from D,\\ncomputes and averages the gradients of the loss of each instance in this subset\\nand updates the parameters in the negative direction of this stochastic gradient.\\nThis makes it both interesting and important for us to think of how to compute\\nthe gradient of a loss function.\\nLet us consider both the margin loss and cross entropy loss, since there is\\nno meaningful gradient of the zero-one loss function and the perceptron loss is\\na special case of the margin loss:\\n∇θLmargin([x, y], θ) =\\n(\\n∇θe([x, y], ∅, θ) −∇θe([x, ˆy′], ∅, θ),\\nif Lmargin([x, y], θ) > 0.\\n0,\\notherwise.\\n(2.37)\\n∇θLce([x, y], θ) = ∇θe([x, y], ∅, θ) −Ey|x;θ [∇θe([x, y′], ∅, θ))] .\\n(2.38)\\nIn both cases, the gradient of the energy function shows up: ∇θe([x, y], ∅, θ)).\\nWe thus focus on the gradient of the energy function in this case.\\n2.2.1\\nA Linear Energy Function\\nLet us start with a very simple case we considered earlier. We assume that x is\\na real-valued vector of d dimensions, i.e., x ∈Rd. We will further assume that y\\ntakes one of K potential values, i.e., y ∈{1, 2, . . . , K}. The parameters θ consist\\nof\\n1. The weight matrix W =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nw1\\nw2\\n...\\nwK\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb∈RK×d\\n2. The bias vector b =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nb1\\nb2\\n...\\nbK\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb∈RK'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='12CHAPTER 2. BASIC IDEAS IN MACHINE LEARNING WITH CLASSIFICATION\\nWe can now define the energy function as\\ne([x, y], ∅, θ) = −w⊤\\ny x −by.\\n(2.39)\\nThe gradient of the energy function with respect to the associated weight\\nvector wy is then\\n∇wye = −x.\\n(2.40)\\nSimilarly, for the bias:\\n∂e\\n∂by\\n= −1.\\n(2.41)\\nThe first one (the gradient w.r.t. wy) states that for the energy to be lowered\\nfor this particular combination (x, y), we should add the input x to the weight\\nvector wy. The second one (the gradient w.r.t. by) lowers the energy for the\\noutcome y regardless of the input.\\nLet us consider the perceptron loss, or the margin loss with zero margin.\\nThe first-term gradient, ∇θe([x, y], ∅, θ), updates the weight vector and the\\nbias value associated with the correct outcome. With a learning rate η > 0,\\nthe updated energy associated with the correct outcome, where we follow the\\nnegative gradient,3 is then smaller than the original energy function:\\n−(wy + ηx)⊤x −(by + η) = −w⊤\\ny x −by −η(∥x∥2 + 1)\\n(2.42)\\n=e([x, y], ∅, θ) −η(∥x∥2 + 1)\\n(2.43)\\n< e([x, y], ∅, θ).\\n(2.44)\\nThis is precisely what we intended, since we want the energy value to be lower\\nwith a good combination of the input and outcome.\\nThis alone is however not enough as a full learning rule. Even if the energy\\nvalue associated with the right combination is lowered, it may not be lowered\\nenough, so that the correct outcome is selected when the input is presented\\nagain. The second-term gradient compliments this by having the opposite sign in\\nfront of it. By following the negative gradient of the negative energy associated\\nwith the input and the predicted outcome ˆy, we ensure that this particular\\nenergy value is increased:\\n−(wˆy −ηx)⊤x −(bˆy −η) = e([x, ˆy], ∅, θ) + η(∥x∥2 + 1)\\n(2.45)\\n> e([x, ˆy], ∅, θ).\\n(2.46)\\nSo, this learning rule would lower the energy value associated with the correct\\noutcome and increase that associated with the incorrectly-predicted outcome,\\nuntil the outcome with the lowest energy coincides with the correct outcome.\\nWhen that happens, the loss is constant, and no learning happens, because\\ny = ˆy.\\n3We will shortly discuss why we do so later in this chapter.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='2.2. BACKPROPAGATION\\n13\\nAt this point, we start to see that the derivation and argument above equally\\napply to x, the input. Instead of the gradient of the energy w.r.t. the weight\\nvector wy, but we can compute that w.r.t. the input x as well:\\n∇xe = −wy,\\nassuming that x is continuous and the energy function is differentiable w.r.t. x.\\nBy following the (opposite of the) gradient in the input space, we can alter the\\nloss function, instead of modifying the weight vectors and biases.\\nOf course this is absolutely the opposite of what we are trying to do here,\\nsince the main goal is to find a classifier that classifies a given input x into the\\ncorrect category y. This perspective however leads us naturally to the idea of\\nbackpropagation [Rumelhart et al., 1986].\\n2.2.2\\nA Nonlinear Energy Function\\nInstead of adjusting the weight vector W and the bias vector b, we can adjust the\\ninput x directly in order to modify the associated energy value. More specifically,\\nwith the perceptron loss, that is the margin loss with zero margin, when the\\nprediction is incorrect, i.e. y ̸= ˆy, the gradient of the perceptron loss with respect\\nto the input x is4\\n∇xLperceptron([x, y], θ) = ∇xe([x, y], θ) −∇xe([x, ˆy], θ) = −wy + wˆy.\\n(2.47)\\nSimilarly to the weight matrix and bias vector above, if we update the input\\nx following the opposite of this direction, we can increase the energy value\\nassociated with the correct outcome y while lowering that with the incorrectly-\\npredicted outcome ˆy. Although this is generally useless with a linear energy\\nfunction, as we discussed just now, this is an interesting thought experiment, as\\nthis tells us that we can solve the problem either by adapting the parameters,\\ni.e. the weight matrix and bias vector, or by adapting the input data points\\nthemselves. The latter sounds like an attractive alternative, because it would\\nbreak us free from being constrained by the linearity of the energy function.\\nThere is however a major issue with the latter alternative. That is, we do not\\nknow how to change the new input in the future (not included in the training\\nset), since such a new input may not come together with the associated correct\\noutcome. We thus need to build a system that predicts what the altered input\\nwould be given a new input in the future.\\nTo overcome this issue, we start by using some transformation h of the\\ninput x, with its own parameters θ′, instead of the original input x. That is,\\nh = F(x, θ′). Analogously, we refer to the newly updated input by ˆh. We obtain\\nˆh by following the gradient direction from Eq. (2.47). We now define a new\\nenergy function e′ such that the combination (h, ˆh) is assigned a lower energy\\nthan the other combinations if h and ˆh are close to each other. Under this energy\\nfunction, the energy is low if this transformation of the input h = F(x, θ′) is\\n4When it is clear that there is no latent (unobserved) variable z, I will skip φ for brevity.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='14CHAPTER 2. BASIC IDEAS IN MACHINE LEARNING WITH CLASSIFICATION\\nsimilar to the updated input ˆh. This intuitively makes sense, since ˆh is the\\ndesirable transformation of the input x, as it lowers the overall loss function\\nabove.\\nA typical example of such an energy function would be\\ne′([h, ˆh], θ′) = 1\\n2∥σ(U ⊤x + c)\\n|\\n{z\\n}\\n=h\\n−ˆh∥2,\\n(2.48)\\nwhere U and c are the weight matrix and bias vector, respectively, and σ is an\\narbitrary nonlinear function. h = σ(U ⊤x + c) would be some transformation of\\nthe input x, as described above.\\nThe loss function in this case can be simply the energy function itself:\\nLℓ2([h, ˆh], θ′) = e′([h, ˆh], θ′).\\n(2.49)\\nThe gradient of the loss function w.r.t. the transformation matrix U is then:\\n∇U = x\\n\\x10\\n(h −ˆh) ⊙h′\\x11⊤\\n(2.50)\\nwhere\\nh′ = σ′(U ⊤x + c)\\n(2.51)\\nwith σ′(a) =\\n∂σ\\n∂a(a), according to the chain rule of derivatives. ⊙denotes\\nelement-wise multiplication. Similarly, the gradient w.r.t. the bias vector c is\\n∇c = (h −ˆh) ⊙h′.\\n(2.52)\\nBefore continuing further, let us examine these gradients. If we look at ∇c, the\\nfirst term, or its negation, since we want to minimize the energy, states that we\\nshould change c toward ˆh away from h. If h is further away from ˆh, we need to\\nchange c more. The second term h′ is multiplied to (h −ˆh). This term h′ is the\\nslope of the nonlinear activation function σ at the current input U ⊤x + c. If the\\nslope is positive, we should update c following the sign of ˆh−h , as usual. But, if\\nthe slope is negative, we should flip the direction of c’s update, since increasing\\nc would result in decreasing ˆh −h.\\nIn order to analyze the gradient w.r.t. U, let us consider the gradient w.r.t.\\none particular element of U, i.e., uij. uij can be thought of as the weight between\\nthe i-th dimention of the input, xi, and the j-th dimension of the transformation,\\nhj. This gradient is written down as\\n∂\\n∂uij\\n= xi(hj −ˆhj)h′\\nj = (xihj −xiˆhj)h′\\nj.\\n(2.53)\\nWe already know what h′\\nj does: it decides whether the slope was positive or\\nnegative, and thereby whether the update direction should flip. Because we\\nfollow the opposite direction (since we want to lower the energy), the first term'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='2.2. BACKPROPAGATION\\n15\\nxihj is subtracted from uij. This term tells us how strongly the value of xi is\\nreflected on the value of hj. Since hj is now being updated away, the effect of\\nxi on the j-th dimension of the transformation via uij must be reduced. On the\\nother hand, the second term xiˆhj does the opposite. It states that the effect of\\nxi on the j-th dimension of the transformation, according to the newly updated\\nvalue ˆhj, must be reflected more on uij. If the new value of the j-th dimension\\nhas the same sign as xi, uij should tend toward the positive value. Otherwise,\\nit should tend toward the negative value.\\nWe can now imagine a procedure where we alternate between computing\\nˆh and updating U and c to match ˆh. Of course, this procedure may not be\\noptimal, since there is no guarantee (or it is difficult to obtain any guarantee)\\nthat repeatedly updating U and c following the gradient of the second energy\\nfunction leads to improvement in the overall loss when h = σ(U ⊤x + c) is used\\nin place of the target ˆh. When the second energy function is truly minimized\\nso that σ(U ′⊤x + c′) coincides with ˆh, the loss will be smaller than the original\\nh = σ(U ⊤x+c). It is however unclear whether the loss will be smaller until this\\nminimum is achieved.\\nInstead, we can think of a procedure in which we update U and c directly\\nwithout producing ˆh as an intermediate quantity. Assume we take just a unit\\nstep to update ˆh:\\nˆh = h + (wy −wˆy)\\n(2.54)\\n⇐⇒ˆh −h = −∇hL(h).\\n(2.55)\\nThat is, we use the learning rate (or step size) of 1.\\nThen,\\n∇U = x (∇hL(h) ⊙h′)⊤\\n(2.56)\\n∇c = ∇hL(h) ⊙h′.\\n(2.57)\\nIn other words, we can skip computing ˆh and directly compute the gradients of\\nthe loss w.r.t. U and c using the gradient w.r.t. h.\\nJust like what we did with h (or originally x), we can check how we would\\nchange this new x to minimize the second energy function e′. This is done by\\ncomputing the gradient of e′ w.r.t. x:\\n∇x = U\\n\\x10\\n(h −ˆh) ⊙h′\\x11\\n,\\n(2.58)\\nwhich is similar to the gradient w.r.t. U. If we replace (h −ˆh) with ∇hL(h), we\\nget\\n∇x = U (∇hL(h) ⊙h′) .\\n(2.59)\\nIt is the third time we are discussing it, but yes, we know what h′ does here:\\nit decides the sign of the update. If we ignore h′ by simply assuming that σ'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 19}, page_content='16CHAPTER 2. BASIC IDEAS IN MACHINE LEARNING WITH CLASSIFICATION\\nwas e.g. an identity map (which would mean that h′ = 1), we realize that ∇x is\\nlinear transformation of ∇h 5 , as\\n∇x = U∇h.\\n(2.60)\\nContrast it against the red-coloured term below:\\nh = σ(U ⊤x + c)\\n(2.61)\\nThe red-colour term above can be thought of as propagating the input signal x\\nvia U ⊤to h. In contrast U∇h can be thought of as back-propagating the error\\nsignal ∇h via U to the input x.\\nYou must see where we are heading toward now. Let us replace x once more,\\nthis time, with z. In other words,\\nh = σ(U ⊤z + c)\\nand\\nz = σ(V ⊤x + s).\\nWe can analogously introduce yet another energy function e′′ defined as\\ne′′([z, ˆz], θ′′) = 1\\n2∥z −ˆz∥2,\\n(2.62)\\nwhere\\nˆz = z −∇z\\n(2.63)\\n= z −U∇h.\\n(2.64)\\nFollowing the exactly same steps of derivation from above, we end up with\\n∇V = x (∇z ⊙z′)⊤\\n(2.65)\\n∇s = ∇z ⊙z′,\\n(2.66)\\nwhere\\n∇z = U∇h.\\n(2.67)\\nIn one single sweep, we could backpropagate the error signal from the loss func-\\ntion all the way back to x and compute the gradient of the loss function w.r.t.\\nall the parameters, W, b, U, c, V and s. Of course, in doing so, we had to store\\nthe so-called forward activation vectors, x, z and h, which is often referred to\\nbook-keeping.\\nThis process of computing the gradient of the loss fucntion w.r.t. all the pa-\\nrameters from multiple stages of nonlinear transformation of the input is called\\n5Whenever it is clear, we will drop some terms for both brevity and clarify. In this case,\\n∇h is ∇hL(h).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 20}, page_content='2.3. STOCHASTIC GRADIENT DESCENT\\n17\\nbackpropgation. This can be generalized to any computation graph without any\\nloops (though, loops can be unrolled for a finite number of cycles in practice)\\nand is a special case of automatic differentiation [Baydin et al., 2018], called\\nreverse-mode automatic differentiation.\\nBecause reverse-mode automatic differentiation is efficient both in terms of\\ncomputation and memory (both linear), it is universally used for computing\\nthe gradient and is well-implemented in many widely used deep learning tools,\\nsuch as PyTorch and Jax. This universality implies that once we decide on a\\nloss function and an energy function such that the loss function is differentiable\\nw.r.t. the parameters of the energy function, we can simply assume the gradient\\nwould be readily available.\\n2.3\\nStochastic Gradient Descent\\nOnce we have defined an energy function and an associated loss function, we\\ncan compute the gradient of this loss function w.r.t. the parameters. With the\\ngradient, we can update the parameters repeatedly so that we can minimize the\\nloss function. It is important to observe that we have defined the loss function for\\neach individual training example, and eventually our goal becomes minimizing\\nthe average of the loss of all training examples. For a random reason, we will\\nuse fi(θ) to denote the loss function of the i-th example at θ, and thereby the\\noverall loss is\\nf(θ) = 1\\nN\\nN\\nX\\ni=1\\nfi(θ).\\n(2.68)\\nWhen the overall loss is the average (or sum) of the individual loss functions,\\nwe say that the loss is decomposable.\\nWe can view such an overall loss function as computing the expected indi-\\nvidual loss function:\\nf(θ) = Ei [fi(θ)] ,\\n(2.69)\\nwhere i ∼U(1, . . . , N). Of course, we can replace this uniform distribution with\\nan arbitrary data distribution and write this as\\nf(θ) = Ex∼pdata [f(x; θ)] ,\\n(2.70)\\nalthough we will for now stick to the uniform indexing over the training set.\\nWith Eq. (2.69), we also get\\n∇f = Ei [∇fi] ,\\n(2.71)\\nbecause the expectation over a finite, discrete random variable can be written\\ndown using a finite sum.\\nThere are two constants we should consider when deciding how we are going\\nto minimize f w.r.t. θ. They are the number of training examples N and the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 21}, page_content='18CHAPTER 2. BASIC IDEAS IN MACHINE LEARNING WITH CLASSIFICATION\\nnumber of parameters dim(θ) (if not confusing, we would use dim(θ) and |θ|\\ninterchangeably.) Let us start with the latter |θ|. If the number of parameters is\\nlarge, we cannot expect to compute any high-order derivative information of the\\nfunction f beyond the first-order derivative, that is its gradient. Without access\\nto higher-order derivative, we cannot benefit from advanced optimization algo-\\nrithms, such as Newton’s algorithm. Unfortunately, in modern machine learning,\\n|θ| can be as larger as tens of billions, and we are often stuck with first-order\\noptimization algorithms.\\nIf N is large, it becomes increasingly burdensome to compute f not to men-\\ntion ∇f directly each update. In other words, we can only expect to use the\\ntrue gradient of f only when there are few training examples only, i.e., small N.\\nIn modern machine learning, we are often faced with hundreds of thousands, if\\nnot millions or billions, of training examples, and it is often impossible for us to\\nexactly compute the overall loss. In short, we are in a situation where we cannot\\neven use the full, true gradient information to update the parameters.\\nIn order to cope with large N and large |θ|, we often resort to a stochastic\\ngradient estimate rather than the full gradient, where the stochastic gradient is\\ndefined as\\ngit = ∇fit(θt),\\n(2.72)\\nwhere it was drawn from the uniform distribution over {1, . . . , N}. We then\\nupdate the parameters using this stochastic gradient estimate by\\nθt+1 = θt −αtgit.\\n(2.73)\\nIn doing so, it is a usual practice to maintain a set of so-called checkpoints and\\npick the best one within this checkpoint set. We will discuss how we pick the\\nbest checkpoint according to which criteria in the next section in more detail,\\nas this is where optimization and learning deviate from each other.\\nFor now, let us stick to optimization and in particular iterative optimization.\\nWhen thinking about optimization, there are two distinct concepts that are\\nequally important. The first one is convergence. With convergence we mean\\nwhether iterative optimization gradually moves the iterate θt toward a desirable\\nsolution. A desirable solution could the global minimum (if it exists), any local\\nminimum or any extremum (where the gradient is zero.) It is important to know\\nwhether the iterate converges to such a desirable solution and if so, at which rate.\\nThe second important concept is the descent property. An iterative optimization\\nalgorithm is descent if it always makes progress, that is, f(θt+1) ≤f(θt) for all\\nt.\\nAs we will learn about it shortly in the next section, the desirable solution\\nis not defined with the overall loss function f. Rather, the desirable solution for\\nus is defined using another function f ∗. This function f ∗is similar to f almost\\neverywhere over θ but these two functions differ. It is thus more desirable for\\nus to enumerate a series of θt’s with small f(θt) values and eventually pick one\\nusing f ∗(θt). In other words, it is not convergence but the descent property.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 22}, page_content='2.3. STOCHASTIC GRADIENT DESCENT\\n19\\n2.3.1\\nDescent Lemma\\nWe start by stating and proving one of the most fundamental results in opti-\\nmization, called the descent lemma. According to the descent lemma, the fol-\\nlowing inequality holds when ∇f is an L-Lipschitz continuous function, i.e.,\\n∥∇f(x) −∇f(y)∥≤L∥x −y∥:\\nf(y) ≤f(x) + ∇f(x)⊤(y −x) + L\\n2 ∥y −x∥2.\\n(2.74)\\nThis inequality allow us to upper-bound the value of a function at a point y\\ngiven the value as well as the gradient at another point x.\\nLet g(t) = f(x + t(y −x)) so that g(0) = f(x) and g(1) = f(y). Then,\\nf(y) −f(x) = g(1) −g(0) =\\nZ 1\\n0\\ng′(u)du =\\nZ 1\\n0\\n∇f(x + t(y −x))⊤(y −x)dt.\\n(2.75)\\nBy subtracting ∇f(x)⊤(y −x) from both sides, we get\\nf(y) −f(x) −∇f(x)⊤(y −x) =\\nZ 1\\n0\\n(∇f(x + t(y −x)) −∇f(x))⊤(y −x)dt.\\n(2.76)\\nWe can upperbound it using the Cauchy-Schwarz inequality, i.e. a⊤b ≤∥a∥∥b∥:\\nf(y) −f(x) −∇f(x)⊤(y −x) ≤\\nZ 1\\n0\\n∥∇f(x + t(y −x)) −∇f(x)∥∥y −x∥dt.\\n(2.77)\\nWe can use the assumption above that ∇f is an L-Lipschitz function to simplify\\nit into\\nf(y) −f(x) −∇f(x)⊤(y −x) ≤\\nZ 1\\n0\\nLt∥y −x∥2dt = L\\n2 ∥y −x∥2,\\n(2.78)\\nwhich is in turn\\nf(y) ≤f(x) + ∇f(x)⊤(y −x) + L\\n2 ∥y −x∥2.\\n(2.79)\\nIf we assume that N is not too large, we can compute the gradient exactly and\\nupdate the parameters following the negative gradient direction:\\nθt+1 = θt −αt∇f(θt)\\n(2.80)\\n⇐⇒θt+1 −θt = −αt∇f(θt)\\n(2.81)\\nLet us plug (θt, θt+1) into (x, y) in the descent lemma:\\nf(θt+1) ≤f(θt) −αt∥∇f(θt)∥2 + α2\\nt\\nL\\n2 ∥∇f(θt)∥2\\n(2.82)\\n= f(θt) −(αt −L\\n2 α2\\nt)∥∇f(θt)∥2.\\n(2.83)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 23}, page_content='20CHAPTER 2. BASIC IDEAS IN MACHINE LEARNING WITH CLASSIFICATION\\nSince ∥∇f(θt)∥2 ≥0, we want to find αt that maximizes −L\\n2 α2\\nt +αt. We simply\\ncompute the derivative of this expression w.r.t. αt and set it to zero:\\n−Lαt + 1 = 0 ⇐⇒αt = 1\\nL.\\n(2.84)\\nIn other words, if we set the learning rate to 1/L (that is, inverse proportionally\\nto how rapidly the function changes), we can make the most progress each time.\\nOf course, this does not directly apply to the stochastic case, since the descent\\nlemma does not apply to the stochastic gradient estimate as it is.\\n2.3.2\\nStochastic Gradient Descent\\nResuming from the descent lemma above, we will use the stochastic gradient\\nupdate rule from Eq. (2.73). Let’s restate the stochastic gradient rule:\\nθt+1 = θt −αtgit ⇐⇒θt+1 −θt = −αtgit.\\n(2.85)\\nPlugging in (θt, θt+1) into the descent lemma, we get\\nf(θt+1) ≤f(θt) −αt∇f(θt)⊤git + α2\\nt\\nL\\n2 ∥git∥2.\\n(2.86)\\nWe are interested in the expected progress here over it ∼U(1, . . . , N):\\nE [f(θt+1)] ≤f(θt) −αt∇f(θt)⊤E [git] + α2\\nt\\nL\\n2 E∥git∥2\\n(2.87)\\n= f(θt) −αt∥∇f(θt)∥2\\n|\\n{z\\n}\\n=(a)\\n+ α2\\nt\\nL\\n2 E∥git∥2\\n|\\n{z\\n}\\n=(b)\\n,\\n(2.88)\\nbecause ∇f(θt) = Eit [git].\\nThere are two terms that are both positive but have opposing signs. The first\\nterm (a) is good news. It states that on expectation we would make a positive\\nprogress, that is, to lower the expected value after a stochastic gradient step.\\nSince this term is multiplied with αt, we may be tempted to simply set αt to\\na large value to make a big improvement on expectation. Unfortunately, this is\\nnot the case because of the second term (b).\\nAlthough the stochastic gradient is an unbiased estimate of the full gradient,\\nit is still a noisy estimate. The second term (b) reflect this noise. Imagine we\\nare close to the/a minimum of f such that ∇f(θt) = 0. The second term (b) is\\nthen the trace of the covariance of the stochastic gradient. Because it is not zero\\n(i.e. noisy), stochastic gradient descent will not decrease the objective function\\non expectation but may increase it.\\nIn order to control away the second term (b), we must ensure that αt is small\\nenough so that αt ≫α2\\nt, or must assume further constraints on f. If we decrease\\nαt over t , stochastic gradient descent will on expectation make progress (i.e.,\\ndescent) and eventually passes by the/a minimum of f. More details on the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 24}, page_content='2.3. STOCHASTIC GRADIENT DESCENT\\n21\\nconvergence rate(s) of stochastic gradient descent are out of the scope of this\\ncourse.\\nIn summary, we use stochastic gradient descent in modern machine learning,\\nand with a small learning rate stochastic gradient descent exhibits the descent\\nproperty on expectation. We will therefore worry less and rely on stochastic\\ngradient descent throughout the course.\\n2.3.3\\nAdaptive Learning Rate Methods\\nAlthough we have approached the problem of stochastic optimization by stating\\nthat we follow the (negative) stochastic gradient estimate at each update, it is\\nnot necessarily the only way to view this problem. We can instead view the\\nproblem of learning as online optimization. In online optimization, or online\\nlearning, we play a game in which at each turn t we receive the stochastic\\ngradient estimate gt = ∇θfit(θt−1) and use it to update our estimate of the\\nparameters, θt−1, gt →θt. We receive the penalty as the difference between\\nthe stochastic estimate of the loss at the updated parameter and that at the\\noptimal parameter configuration,6 i.e., fit(θt) −fit(θ∗). We call this penalty a\\nregret, since this quantifies how much better we could’ve done in hindsight (that\\nis, regret.) The goal is to minimize the regret over time:\\nR(T) =\\nT\\nX\\nt=1\\nfit(θt) −fit(θ∗)\\n|\\n{z\\n}\\n≥0\\n.\\n(2.89)\\nThe regret must grow sub-linearly, i.e, R(T) = o(T), since linear growth, i.e.,\\nR(T) = O(T), implies that the learning algorithm is not converging toward the\\noptimal solution (or its associated minimum value.)\\nWe (try to) achieve this goal by finding an appropriate update rule that\\nmaps θt−1 and gt to θt. In doing so, it is relatively straightforward to think of\\nthe following simplified framework, that generalizes stochastic gradient descent:\\nθt ←θt−1 + ηt ⊙gt,\\n(2.90)\\nwhere ηt is a collection of learning rates for all parameters.7 By adapting ηt\\nappropriately, we can achieve the sublinear regret. In SGD above, ηt was often a\\nscalar, i.e. ηi\\nt = ηj\\nt for all i ̸= j. SGD in fact achieves the sublinear regret, O(\\n√\\nT)\\nwith ηt =\\n1\\n√\\nT , but it turned out that we can do better either asymptotically or\\n6The optimality in this context of online adaptation is defined as the final solution reached\\nby the online optimization procedure. If we follow the direction that is correlated with the\\ngradient, we know that we are making progress on average toward the local extreme configu-\\nration due to the decent lemma above. We thus know that asymptotically the optimal solution\\nhere θ∗would have a lower loss than any other intermediate points. This makes the online\\nlearning perspective different from the optimization perspective from above.\\n7It is possible to use a matrix ηt instead of a vector ηt, and there could be a good chance that\\nwe would achieve a better regret bound. Unfortunately, this could increase the computationally\\ncomplexity dramatically for each update, from O(|θ|) to O(|θ|2), which can be prohibitive in\\nmany modern applications.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 25}, page_content='22CHAPTER 2. BASIC IDEAS IN MACHINE LEARNING WITH CLASSIFICATION\\npractically by taking into account the loss function landscape, that is, how the\\nloss changes w.r.t. the parameters, more carefully.\\nAdagrad [Duchi et al., 2011].\\nFor each parameter θi, the magnitude of the\\npartial derivative of the loss, (gi\\nt)2, tells us how sensitive the loss value was to\\nthe change in θi. Or, another way is to view it as the impact of the change in\\nθi on the loss. By accumulating this over time,\\nqPt\\nt′=1(gi\\nt′)2, we can measure\\nthe overall impact of θi on the loss. We can then normalize each update inverse-\\nproportionally in order to ensure each and every parameter has more or less the\\nequal impact on the loss. That is,\\nθt ←θt−1 +\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n√Pt\\nt′=1(g1\\nt′)2\\n...\\n1\\nqPt\\nt′=1(g|θ|\\nt′ )2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb⊙gt\\n(2.91)\\nThe regret of Adagrad is O(\\n√\\nT), just like that of SGD, assuming ∥gt∥≪∞.\\nIt however often decreases faster especially when many parameters are inconse-\\nquential (sparse) and/or quickly learned (because the accumulated magnitude\\nrapidly grows and its inverse converges to zero quickly.)\\nAdam [Kingma and Ba, 2014].\\nA major disadvantage of Adagrad above is\\nthat the per-parameter learning rate decays monotonically, often resulting in a\\npremature termination. This is especially problematic with a non-convex opti-\\nmization problem, such as the ones in training deep neural networks, as it may\\nrequire many updates for the optimizer to get close enough to a good solution\\nin the parameter space. We can address it by not accumulating the magnitude\\nof the gradient over the full duration but using exponential smoothing:\\nvt ←βvvt−1 + (1 −βv)g2\\nt ,\\n(2.92)\\nwhere βv ∈[0, 1]. Then, we use √vt as the learning rate instead, leading to\\nθi\\nt ←θi\\nt−1 +\\ngi\\nt\\np\\nvi\\nt + ϵ\\n,\\n(2.93)\\nwhere ϵ > 0 is a small scalar to prevent the degenerate case.\\nAdam furthermore uses exponential smoothing to reduce the variance of the\\ngradient estimate as well:\\nmt ←βmmt−1 + (1 −βm)gt.\\n(2.94)\\nThis results in the following final update rule:\\nθi\\nt ←θi\\nt−1 + α\\nmi\\nt\\np\\nvi\\nt + ϵ\\n,\\n(2.95)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 26}, page_content='2.4. GENERALIZATION AND MODEL SELECTION\\n23\\nwhere α ∈(0, 1] is a default step size.\\nAdam also has O(\\n√\\nT) regret and exhibits an overall similar asymptotic\\nbehaviour to Adagrad. Adam is however often favoured over Adagrad, because\\nthe per-parameter learning rate is not monotonically decreasing anymore. Since\\nit was proposed earlier, there have been a number of improvements to Adam,\\nalthough they are out of the scope of this course.\\nOverall, whenever we refer to stochastic gradient descent in the rest of the\\ncourse, we are generally referring to Adam or its variants that adaptively update\\nthe learning rate of each parameter on the fly. Although it is just a folk wisdom,\\nquite a few researchers, including myself, attribute the recently-observed surpris-\\ning successes of many conventional machine learning algorithms with gradient-\\ndescent optimization to these adaptive learning rate algorithms.\\n2.4\\nGeneralization and Model Selection\\n2.4.1\\nExpected risk vs. empirical risk: a generalization\\nbound\\nA risk is another word we use to refer to the loss. In this section, we will use risk\\ninstead of loss, as the former is more often used in this particular context. If\\nyou are confused by the term “risk”, simply read it out loud as “loss” whenever\\nyou run into it.\\nFor each example (x, y), we now know how to construct an energy function\\nand also an associate loss function L([x, y], θ). Let pdata(x, y) be some unknown\\ndistribution from which we draw an example (x, y). We do not know what\\nthis distribution is, but we assume that this is the distribution from which the\\ntraining examples were drawn and any future instance would be drawn as well.8\\nThen, our goal must be to minimize\\nR(θ) = Edata [L([x, y], θ)] .\\n(2.96)\\nUnfortunately, this expected risk is not computable, and we only have access\\nto a sample-based proxy to the expected risk, called the empirical risk:\\nˆR(θ) = 1\\nN\\nN\\nX\\nn=1\\nL([xn, yn], θ).\\n(2.97)\\nFor brevity and clarity, let Sn = Pn\\nk=1 L([xk, yk], θ). Then, we can express these\\nrisks as\\nR(θ) = Edata×···×data\\n\\x14 1\\nN SN\\n\\x15\\n, and ˆR(θ) = 1\\nN SN.\\n(2.98)\\nThe former holds because each instance (x, y) is drawn independently from the\\nsame data distribution.\\n8This is certainly not true in reality but is a reasonable starting point. We will discuss later\\nin the course what we can do if this assumption does not hold, hopefully if time permits.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 27}, page_content='24CHAPTER 2. BASIC IDEAS IN MACHINE LEARNING WITH CLASSIFICATION\\nLet’s assume that an individual loss is bounded between 0 and 1 (which\\nwould be the case for the 0-1 loss.) Then, we can use the Hoeffding’s inequality\\nto get\\np(|R(θ) −ˆR(θ)| ≥ϵ) ≤2 exp\\n \\n−\\n2(Nϵ)2\\nPN\\nn=1(1 −0)2\\n!\\n= 2 exp\\n\\x00−2Nϵ2\\x01\\n.\\n(2.99)\\nThis inequality tells us that the gap between the expected and empirical risks\\nshrinks exponentially with N, the number of training examples we use to com-\\npute the empirical risk. This inequality applies to any θ, implying that this\\nconvergence of the empirical risk toward the expected risk is uniform over the\\nparameter space (or the corresponding classifier space.) Such uniform conver-\\ngence is nice in that we do not have to worry about how well learning works\\n(that is, what kind of solution we end up with after optimization), in order to\\ndetermine how much deviation we would anticipate between the empirical risk\\n(the one we can compute) and the expected risk at any θ. On the other hand,\\nthere is a big question of whether we actually care about most of the parameter\\nspace; it is likely that we do not and we only care about a small subset of the\\nparameter space over which iterative optimization, such as stochastic gradient\\ndescent, explores. We will discuss this a bit more later, but for now, let’s assume\\nwe are happy with this uniform convergence.9\\nLet’s imagine that someone (or some learning algorithm) gave me θ that is\\nsupposed to be good with a particular empirical risk ˆR(θ). Is there any way\\nfor me to check how much worse the expected risk R(θ) would be, based on\\nthe Hoeffding’s inequality above? Of course, such a statement would have to be\\nprobabilistic, since we are working with random variables, R(θ) and ˆR(θ).\\nThe inequality above allows us to express that\\n|R(θ) −ˆR(θ)| < ϵ\\n(2.100)\\nwith some probability at least 1−δ. Be aware that the direction of the inequality\\nhas flipped.\\nIf |R(θ) −ˆR(θ)| < ϵ, we know that R(θ) < ˆR(θ) + ϵ. We are interested in\\nthis latter inequality, because we want to upper-bound the expected (true) risk.\\nIf the true risk was lower than the empirical risk, we are happy and do not care\\nabout it. We want to know if we were to be unhappy (that is, the expected risk\\nwas greater than the empirical risk), how unhappy we would be in the worst\\ncase.\\nBecause we want to make such a statement with the probability of at least\\n9In practice, we are not.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 28}, page_content='2.4. GENERALIZATION AND MODEL SELECTION\\n25\\n1 −δ, we equate the right-hand side above with δ:\\n2 exp(−2Nϵ2) = δ\\n(2.101)\\n⇐⇒−2Nϵ2 = log δ\\n2\\n(2.102)\\n⇐⇒ϵ2 =\\n1\\n2N log 2\\nδ\\n(2.103)\\n⇐⇒ϵ =\\nr\\n1\\n2N log 2\\nδ .\\n(2.104)\\nCombining these two together, we can now state that with probability at\\nleast 1 −δ, we have\\nR(θ) < ˆR(θ) +\\nr\\n1\\n2N log 2\\nδ .\\n(2.105)\\ngiven the model parameter θ.\\nThis generalization bound makes sense. If we want to get a strong guarantee,\\ni.e., (1 −δ) →1 (equivalently δ →0), we end up with a much loser bound,\\nsince the bound is O(\\nq\\nlog 1\\nδ ). We can counter this by collecting more training\\nexamples, i.e., N →∞, since the bound shrinks rapidly as N grows: O(N −1\\n2 ).\\nThis bound looks reasonable, but there is a catch. The catch is that this is\\nbased on a single, given model θ. In other words, this bound is too optimistic,\\nas in reality, we often need to choose θ ourselves among many alternatives by\\nthe process of learning. In doing so, we need to consider the possibility that\\nwe somehow picked one that has the worst generalization gap |R(θ) −ˆR(θ)|. In\\nother words, we need to consider the generalization bounds of all possible model\\nparameters.\\nFor simplicity, we assume that θ ∈Θ where Θ is a finite set of size K.\\nLearning is then a process of selecting one of K possible parameter configura-\\ntions based on data. We use the idea of so-called union bound from the basic\\nprobability theory, which states that\\np(e1 ∪e2 ∪· · · ∪eN) ≤\\nN\\nX\\ni=1\\np(ei).\\n(2.106)\\nThis is somewhat obvious, because a pair (ei, ej) may not be mutually exclusive.\\nThink of a Venn diagram. With this, we want to compute\\np(∪θ∈Θ|R(θ) −ˆR(θ)| ≥ϵ) ≤\\nX\\nθ∈Θ\\np(|R(θ) −ˆR(θ)| ≥ϵ) ≤2|Θ| exp\\n\\x00−2Nϵ2\\x01\\n|\\n{z\\n}\\n=2 exp(log |Θ|−2Nϵ2)\\n.\\n(2.107)\\nWe can follow the exactly same logic above:\\n2 exp(log |Θ| −2Nϵ2) = δ\\n(2.108)\\n⇐⇒ϵ =\\nr\\nlog |Θ| −log 2δ\\n2N\\n.\\n(2.109)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 29}, page_content='26CHAPTER 2. BASIC IDEAS IN MACHINE LEARNING WITH CLASSIFICATION\\nThis makes sense, as the generalization bound now depends on the size of Θ,\\nour hypothesis space. If the hypothesis set is large, there is a greater chance of\\nus finding a solution that is good empirically ˆR(θ) ↓but is on expectation very\\nbad R(θ) ↑. This also implies that we need N (the number of training examples)\\nto grow exponentially w.r.t. the size of the hypothesis space Θ.\\nThis bound only works with a finite-size hypothesis set Θ without favouring\\nany particular parameter configuration. In order to work with an infinitely large\\nhypothesis set, we must come up with different approaches. For instance, the\\nVapnik–Chervonenkis (VC) dimension can be used to bound the complexity\\nof the infinitely large hypothesis set [Vapnik and Chervonenkis, 1971]. Or, we\\ncan use the PAC-Bayes bound, where a prior distribution over the (potentially\\ninfinitely large) hypothesis set is introduced [McAllester, 1999]. These are all\\nout of the scope of this course, but we briefly touch upon the idea of PAC-Bayes\\nbound here before ending this section.\\nPAC-Bayesian bound.\\nThe original PAC-Bayes result states that\\nDKL(B( ˆR(Q))∥B(R(Q))) ≤1\\nN\\n\\x12\\nDKL(Q∥P) + log N + 1\\nδ\\n\\x13\\n(2.110)\\nwith probability at least 1−δ. Although this inequality looks quite dense, these\\nterms are extremely descriptive, once we define and learn how to read them.\\nFirst, R(Q) and ˆR(Q) are defined analogously to R(θ) and ˆR(θ), except that\\nwe marginalize out θ using the so-called posterior distribution Q(θ). That is,\\nR(Q) = EQ [R(θ)]\\n(2.111)\\nˆR(Q) = EQ\\nh\\nˆR(θ)\\ni\\n.\\n(2.112)\\nQ can be any distribution and can depend on data D consisting of N examples.\\nBecause we continue to assume we work with a bounded loss, we can assume\\nthat R(Q) ∈[0, 1] and ˆR(Q) ∈[0, 1]. Then, we can define Bernoulli distributions\\nusing these two values as the means. We denote these distributions as B(R(Q))\\nand B( ˆR(Q)), respectively. You can think of these distributions as how expected\\nand empirical risks vary as θ follows the distribution Q. We can then measure\\nthe discrepancy between these two quantities, which is by definition the general-\\nization gap, by using KL divergence. This is the left-hand side of the inequality\\nabove.\\nThe right-hand side is then the bound on how much discrepancy between\\nthe empirical and expected risks there could be on average given Q. There are\\ntwo terms here. The first term is the KL divergence between the posterior Q\\nand the so-called prior P, where P is constrained to be independent of data\\nD. You can think of P as our prior belief about which parameter θ would be\\ngood. On the other hand Q is our belief after observing the data D. The first\\nterm therefore states that the discrepancy will be greater if our prior belief was\\nincorrect, that is, our belief after observing data changed dramatically from the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 30}, page_content='2.4. GENERALIZATION AND MODEL SELECTION\\n27\\nprior belief. This effect will however vanish rapidly as the number of training\\nexamples increases due to\\n1\\nN .\\nWe can read two things from the second term 1\\nN log N+1\\nδ\\n. Because δ is in the\\ndenominator, we know that we would potentially get a greater discrepancy if\\nwe want to get a stronger guarantee, that is, δ →0. log(N+1)\\nN\\nvanishes toward 0\\nas the data size increases, i.e. N →∞. The rate of this convergence is however\\nquite slow, i.e. sublinear.\\nSimilarly to what we did earlier, we can turn this inequality in Eq. (2.110)\\ninto a generalization bound. In particular, we use the Pinsker’s inequality. In\\nour case with Bernoulli random variables, we get\\n\\x10\\nˆR(Q) −R(Q)\\n\\x112\\n≤1\\n2DKL(B( ˆR(Q))∥B(R(Q))).\\n(2.113)\\nThen,\\n| ˆR(Q) −R(Q)| ≤\\ns\\n1\\n2N\\n\\x12\\nDKL(Q∥P) + log N + 1\\nδ\\n\\x13\\n.\\n(2.114)\\nWe end up with the following generalization bound:\\nR(Q) ≤ˆR(Q) +\\ns\\n1\\n2N\\n\\x12\\nDKL(Q∥P) + log N + 1\\nδ\\n\\x13\\n.\\n(2.115)\\nUnlike the earlier generalization bound, and its variants, this PAC-Bayesian\\nbound provides us with more actionable insights. First, we want the posterior\\ndistribution Q to be good in that it results in a lower empirical risk on average. It\\nsounds obvious, but the earlier generalization bound was designed to work with\\nany parameter configuration (uniform convergence) and did not tell us what\\nit means to choose a good parameter configuration. With the PAC-Bayesian\\nbound, we already know that we want to choose the parameter configuration so\\nthat the empirical risk is low on average. In other words, we should use a good\\nlearning algorithm.\\nThe posterior distribution Q however cannot be too far away from where\\nwe start from. As the bound is a function of the discrepancy between Q and\\nour prior belief P. Flipping the coin around, it also states that we must choose\\nour prior P so that it puts high probabilities on parameter configurations that\\nare likely to be probable under the posterior distribution Q. In other words, we\\nwant to ensure that we need a minimum amount of work to go from P to Q, in\\norder to minimize the generalization bound.\\nIn summary, the PAC-Bayesian bound tells us that we should have some\\ngood prior knowledge of the problem and that we should not train a predictive\\nmodel too much, thereby ensuring that the posterior distribution Q stays close\\nto the prior distribution P. This will ensure that the expected risk does not\\ndeviate too much from the empirical risk.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 31}, page_content='28CHAPTER 2. BASIC IDEAS IN MACHINE LEARNING WITH CLASSIFICATION\\n2.4.2\\nBias, Variance and Uncertainty\\nAn alternative way to write the 0-1 loss is to rely on the squared difference\\nbetween the true label and predicted label, in the case of binary classification,\\nwhere there are only two categories to which the input may belong. Let us use\\ny ∈{−1, 1} to indicate two classes. Then,\\nL([x, y], θ) = 1\\n4(y −ˆy(x, θ))2,\\n(2.116)\\nwhere ˆy(x, θ) = arg minc∈{−1,1} e([x, c], θ). If y and ˆy are the same, this loss is\\nzero. Otherwise, it is\\nAs we discussed earlier, an instance (x, y) is drawn from an underlying data\\ndistribution pdata(x, y) which can be written down as\\npdata(x, y) = pdata(x)pdata(y|x)\\n(2.117)\\nfollowing the definition of conditional probability.\\nWe can furthermore imagine a distribution over θ as well: q(θ). This distri-\\nbution may be to have come out of nowhere. It is however only natural to have\\na distribution over θ rather than a single value of θ if we realize that learning\\nalways depends on some randomness, either due to arbitrary symmetry breaking\\nin optimization, random sampling of training examples or sometimes the lack\\nof technical capabilities in reducing noise in our systems. We will discuss this\\nuncertainty in the model parameters in depth later, and for now, we assume\\nthat this q(θ) is given to us.\\nWe can then write down the expected 0-1 loss for binary classification under\\nthis (unknown) data distribution over the model parameters as:\\n1\\n4Ex,y,θ(y −ˆy(x, θ))2 ∝Ex\\n\\x02\\nEy|x\\n\\x02\\n(y −µy)2\\x03\\n+ µ2\\ny\\n(2.118)\\n+ Eθ\\n\\x02\\n(ˆy(x, θ) −ˆµy)2\\x03\\n+ ˆµ2\\ny\\n(2.119)\\n−2Ey|xEθ [(y −µy)(ˆy(x, θ) −ˆµy)] −2µy ˆµy\\n\\x03\\n(2.120)\\n= Ex\\n\\uf8ee\\n\\uf8ef\\uf8f0Ey|x\\n\\x02\\n(y −µy)2\\x03\\n|\\n{z\\n}\\n=(a)\\n+ Eθ\\n\\x02\\n(ˆy(x, θ) −ˆµy)2\\x03\\n|\\n{z\\n}\\n=(b)\\n(2.121)\\n−2\\n((((((((((((((\\nEy|xEθ [(y −µy)(ˆy(x, θ) −ˆµy)]\\n|\\n{z\\n}\\n=0\\n+ (µy −ˆµy)2\\n|\\n{z\\n}\\n=(c)\\n\\uf8f9\\n\\uf8fa\\uf8fb,\\n(2.122)\\nwhere\\nµy = Ey|x [y] ,\\n(2.123)\\nˆµy = Eθ [ˆy(x, θ)] .\\n(2.124)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 32}, page_content='2.4. GENERALIZATION AND MODEL SELECTION\\n29\\nThere are so many terms we need to consider in this equation, but we will\\nconsider them one at a time, from the back. First, let us start with (µy −ˆµy)2.\\nThis term (c) tells us about how well our learner captures the mean of the true\\noutput y. This term does not care about how much variance there is either under\\nthe data distribution pdata(y|x) nor under the model distribution q(θ). It only\\ntalks about getting the outcome correct on average. This term is referred to as\\na bias. When this term (c) is zero, we call our predictor unbiased.\\nThe second term from the back, which is zero, is the (negative) covariance\\nbetween the true outcome y and the predicted one ˆy(x, θ), both of which are\\nrandom variables. Because we did not assume anything about q(θ), in general\\nwe cannot assume θ is in anyway correlated with y|x, implying that there should\\nnot be any covariance. We can ignore this term.\\nLet us continue with the two remaining terms, (a) and (b). The first term (a)\\nis the variance of the true outcome y. This reflect inherent uncertainty present in\\nthe true outcome given an input x. This inherent uncertainty cannot be reduced,\\nsince it is not what we control but is given to us by the nature of the problem\\nwe are tackling. If this quantity is large, there is only so much we can do. We\\noften refer to this as aleatoric uncertainty or irreducible uncertainty.\\nThe second term (b) is also uncertainty, as it measures the variance arising\\nfrom the uncertainty in the model parameters. This uncertainty is however con-\\ntrollable and thereby reducible with efforts, since it arises from our uncertainty\\nq(θ) in choosing the parameters θ. When the model is simpler, we tend to have a\\nbetter grasp at learning and can reduce this reducible (or epistemic) uncertainty\\ngreatly. When the model is complex and thereby exhibits many symmetries that\\nmust be broken arbitrarily, it is difficult (if not impossible) to reduce this epis-\\ntemic uncertainty much. This term is often referred to as variance.\\nIt should be quite clear at this point that there must be some inherent trade\\noff between the bias (c) and the variance (b). The more complex a classifier\\nis the higher variance we end up with, but due to its complexity, it would be\\nable to fit data well, resulting in a lower bias. When a classifier is simple, the\\nvariance will be lower, but the bias will be higher. Learning can thus be thought\\nof as finding a good balance between these two competing quantities.10\\nThe explanation above is slightly different from a usual way in which bias-\\nvariance tradeoff is described [Wikipedia contributors, 2023]. In particular, we\\nare considering a generic distribution q(θ) that may or may not be directly\\nrelated to any particular training dataset, when the conventional approach often\\nsticks to the strong dependence on the training dataset and the distribution over\\nthe training set. This is a minor difference, but this can come in handy when\\nwe start thinking about more exotic ways by which we come with q(θ). If time\\nand space permits later in the course, we may learn one or two techniques that\\ninvolve such exotic techniques, such as transfer learning and multi-task learning.\\n10I must emphasize here that the complexity of a classifier is not easy to quantify. When I\\nspeak of ‘complex’ or ‘simple’ here, I am referring to this mythical measure of the classifier’s\\ncomplexity and do not mean that we can compute it easily.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 33}, page_content='30CHAPTER 2. BASIC IDEAS IN MACHINE LEARNING WITH CLASSIFICATION\\n2.4.3\\nUncertainty in the error rate\\nWe first need to talk about random variables. In probability courses you prob-\\nably have taken earlier, you must have learned about the strict distinction be-\\ntween random variables and non-random variables. In fact, a random variable\\ndoes not take any particular value but carries with it a probability distribution\\nover all possible values it can take. Once we draw a sample from this distribu-\\ntion, this value is not random anymore but is deterministic.\\nIt unfortunately becomes easily cumbersome to explicitly distinguish be-\\ntween random variables and the samples drawn from their distributions. That\\nis one of the reasons why we have not explicitly stated whether any particular\\nvariable is random or not so far. Another reason, perhaps more important, is\\nthat almost every variable in machine learning is random, because almost every\\nvariable depends on a set of samples drawn from an unknown underlying dis-\\ntribution. For instance, the parameters θ are random, because either they were\\ninitialized by drawing a sample from a so-called prior distribution, or because\\nthey were updated using a stochastic gradient estimate that is a function of a\\nset of samples drawn from the data distribution. From this perspective, in fact,\\nprediction ˆy we make using a model parametrized with θ is a random variable\\nas well. The loss, or the risk, is thereby a random variable, as we have seen in\\n§2.4.1.\\nConfidence interval: capturing test set variation.\\nLet us stick to the\\nzero-one loss (although this is not strictly necessary, it makes the following\\nargument easier to follow.) The loss l is a function of (1) a particular observation\\n[x, y] drawn from the data distribution pdata and (2) the parameters θ. Both of\\nthese are sources of randomness, but for now, let’s assume that θ is given to us\\nas a fixed value, rather than as a random variable with a distribution attached\\nto it. If we assume to have access to N test examples, that were independently\\ndrawn from the identical distribution pdata, we have\\n(l1, l2, . . . , lN),\\n(2.125)\\nwhere each ln is itself a random variable. Each and every one of these N ran-\\ndom variables follows the same distribution. Because these all follow the same\\ndistribution, they also share the mean and variance:\\nµ = E[l] and σ2 = V[l] < ∞,\\n(2.126)\\nwhere we safely assume that the variance is finite.\\nAccording to the central limit theorem, we then know that\\n√\\nN(¯lN −µ) →d N(0, σ2),\\n(2.127)\\nwhere →d refers to the convergence in distribution, and\\n¯lN = 1\\nN\\nN\\nX\\nn=1\\nln.\\n(2.128)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 34}, page_content='2.4. GENERALIZATION AND MODEL SELECTION\\n31\\n¯lN is a random variable that refers to the average loss computed over the N\\nexamples. In other words, with larger N, we expect that the average accuracy\\nwe get from considering N examples is centered at the true average µ with the\\nvariance σ2\\nN . So, the more N , the more confidence we have in trusting that the\\nsample average does not deviate too much from the true average. With small\\nN, however, we cannot be confident that our sample average accuracy is close\\nenough to the true average, and this lack of confidence is proportional to the\\ntrue variance underlying the accuracy. Unfortunately, we do not have access\\nto the true variance of the accuracy but often can get a rough sense of it by\\nconsidering the sample variance.\\nIf N is large, we can compute the confidence interval11 and use it to compare\\nagainst another classifier or your prior expectation on the accuracy. For instance,\\nbecause the accuracy estimate converges to the normal distribution, we can use\\nso-called t-test, since the difference between the true mean and the mean of\\nthe estimate converges toward the Student’s t distribution. In that case, the\\nconfidence interval for the binary accuracy (simply 1 −l⋆, where l⋆is the true\\nloss of the classifier) is given by\\nCI ≈\\n\"\\n(1 −¯lN) −Z\\nr¯lN(1 −¯lN)\\nN\\n, (1 −¯lN) + Z\\nr¯lN(1 −¯lN)\\nN\\n#\\n,\\n(2.129)\\nwhere Z is determined based on the target confidence level γ. If γ = 0.99, Z\\nwould be approximately 2.576.\\nLet l0 be the accuracy by the existing classifier. We will assume this is the\\nexact quantity because we have been running this classifier for a very long time.\\nWe can use this confidence interval to get some sense of whether we want to\\nreplace the existing classifier with this new one. If l0 lies comfortably outside\\nthis confidence interval, we would feel more comfortable considering this option.\\nThis approach focuses on estimating the error rate, and associated confi-\\ndence, given a classifier θ. In other words, the randomness we are considering\\nstems from the choice of the test set D. If we repeatedly obtain new test sets and\\ncompute the associated confidence intervals, we anticipate the true accuracy to\\nbe included in the confidence interval approximately γ times. This however tells\\nus only one side of the story. Let us consider two additional aspects.\\nCredible interval: capturing model variations.\\nThere are quite a few\\nfactors that make our learning algorithm stochastic. First, our objective function\\ntends to have many local minima, arising from reasons such as co-linear features\\nand scaling invariance. For instance, if we use the zero-one loss, the following\\nclassifiers are all equivalent:\\nˆy = arg\\nmax\\ny=1,...,|y|\\n\\x00α\\n\\x00W ⊤x + b\\n\\x01\\x01\\ny ,\\nα > 0,\\n(2.130)\\n11The confidence interval for a quantity with the confidence level γ means that if we repeat\\nthe process of inferring the target quantity and measure the confidence interval, the true target\\nquantity would be included in the confidence interval proportional to γ.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 35}, page_content='32CHAPTER 2. BASIC IDEAS IN MACHINE LEARNING WITH CLASSIFICATION\\nwhere (·)j refers to the j-th element of the vector, because the zero-one loss is\\ninvariant to the multiplicative scaling of the energy value. A pair of co-linear\\nfeatures are defined to have linear relationship given the target outcome. Imagine\\nthat\\nxj = αxi,\\n(2.131)\\nwhen y = c. We then say that (xi, xj) are co-linear given y = c. In this case,\\nthe following two energy functions are equivalent:\\ne([x, c], θ) = −\\n\\uf8ee\\n\\uf8f0wc,1, . . . , wc,i, . . . ,\\n0\\n|{z}\\n=wc,j\\n, . . . , wc,|x|\\n\\uf8f9\\n\\uf8fbx −bc,\\n(2.132)\\ne′([x, c], θ′) = −\\n\\uf8ee\\n\\uf8f0wc,1, . . . ,\\n0\\n|{z}\\n=wc,i\\n, . . . , 1\\nαwc,i, . . . , wc,|x|\\n\\uf8f9\\n\\uf8fbx −bc,\\n(2.133)\\nfor any α ̸= 0. We cannot really distinguish these two energy functions.\\nThere are more of these, which we will touch upon over the rest of the\\ncourse, and they all lead to the issue that our learner will pick one of these\\nequivalent (or nearly equivalent) solutions at random. Such randomness arises\\nfrom many factors, including stochastic initialization, stochastic construction\\nof minibatches in stochastic gradient descent and even non-determinism in the\\nimplementation of underlying compute architectures. That is, learning is not\\nreally a deterministic process but a random process, resulting in a random ˆθ. In\\nother words, every time we train a model, we are effectively sampling ˆθ from a\\nconditional distribution over a random variable θ given the training set D, i.e.,\\nˆθ ∼p(θ|D). This distribution is often referred as a posterior distribution, and if\\ntime permits, we will learn about this distribution more carefully in the context\\nof Bayesian machine learning later.\\nWe considered ¯lN, the test set accuracy, in Eq. (2.128) as a random variable\\nwhose stochasticity arose from the choice of the test set. Here we however con-\\nsider it as a random variable whose randomness is induced by the choice of the\\nparameters θ rather than the test set D′. This is understandable now that θ is\\na random variable rather than a given deterministic variable as before. We can\\nthen write the probability of ¯lN as\\np(¯lN|D, D′) =\\nZ\\np(¯lN|θ, D′)p(θ|D)dθ,\\n(2.134)\\nwhere we safely assume θ is independent of the test set D′.\\nIt may be confusing to see p(¯lN|θ, D′), since we often get one test accuracy\\n(loss) once we have a model and a fixed test set. This is however not true in\\ngeneral, as running a model, that is performing arg max on the energy function,\\nis often either noisy on its own or computational intractable so that we must\\nresort to some kind of randomization.\\nWe can then derive a so-called credible interval of the test-set accuracy, such\\nthat the true test-set accuracy would be contained within this interval with the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 36}, page_content='2.4. GENERALIZATION AND MODEL SELECTION\\n33\\nprobability γ. Let γ = 1−α for convenience. Then, we are looking for an interval\\n[l, u]:\\np(¯lN ≤l|D, D′) = α\\n2\\nand\\np(¯lN ≥u|D, D′) = α\\n2 .\\n(2.135)\\nThis credible interval is reasonable when p(¯lN|D, D′) is unimodal, but this may\\nnot be the case. The probability density may be concentrated in two well-\\nseparated sub-regions, in which case this credible interval would be unnecessarily\\nwide and uninformative.\\nIn that case, we can try to define a credible region C, which may not be\\ncontiguous. The credible region is define to satisfy\\nZ\\n¯lN∈C\\np(¯lN|D, D′)d¯lN = γ,\\n(2.136)\\np(¯lN|D, D′) ≥p(¯l′\\nN|D, D′) for all ¯lN ∈C ∧¯l′\\nN /∈C.\\n(2.137)\\nThe second condition is often referred as density dominance. Effectively, the\\ncredible region consists of one or more contiguous sub-regions such that no point\\nwithin these sub-regions have lower densities than any other points outside these\\nregions. By inspecting this credible region, we can get a good sense of how the\\ntrue accuracy (or error) rate would be with the probability of γ.\\nIn practice, we often cannot compute any of those quantities exactly, because\\nthe posterior distribution θ|D is tractable nor not even known. Instead, we use\\nMonte Carlo approximation by training models many times, benefitting from\\nthe stochasticity in learning. Let {θ1, . . . , θM} be a set of resulting models. For\\neach θm, we draw a sample of the test loss ¯lm\\nN, resulting in\\n\\x08¯l1\\nN, . . . , ¯lM\\nN\\n\\t\\n. We can\\nthen use these samples to characterize, understand and analyze how the true\\ntest accuracy would be with the learning algorithm given the training and test\\nsets, D and D′.\\nCapturing training set variations.\\nIn addition to the randomness arising\\nfrom the construction of the test set as well as the learning process itself, there\\nis yet another source of randomness we want to take into account. This source\\nof randomness arises from the construction of the training set D. If we continue\\nfrom the credible region above, we do not want p(¯lN|D, D′) but rather\\np(¯l) =\\nX\\nD\\nX\\nD′\\np(¯l|D′||D, D′)p(D)p(D′)dDdD′.\\n(2.138)\\nIn words, we want to check the variability of the test-set accuracy ¯l after\\nmarginalizing out both training and test sets. Unfortunately, we often do not\\nhave access to the distribution over the dataset. Rather, we are only given a\\nsingle dataset which is split into two data sets; one for training and the other\\nfor evaluation.\\nIn this case, we can resort to the idea of so-called bootstrap resampling. The\\nidea is simple: (1) we resample N examples from the original set of N training'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 37}, page_content='34CHAPTER 2. BASIC IDEAS IN MACHINE LEARNING WITH CLASSIFICATION\\nexamples with replacement, (2) compute the sample statistics of interest and\\n(3) repeat (1-2) M times. In step (2), we can split the resampled set into the\\nresampled training set and the resampled test set. We use the resampled training\\nset to train a model and then the resampled test set to evaluate the trained\\nmodel to obtain ¯lm\\n|D′|. After M such iterations, we end up with\\nn\\n¯l(m)\\nN\\noM\\nm=1.\\nThese sampled statistics then serve as a set of samples drawn from p(¯l), allowing\\nus to get a good sense of how the proposed learning algorithm works on this\\nparticular problem (not a particular dataset).\\nThere are many ways to characterize the uncertainty in evaluating how well\\nany learning algorithm works. Although we have considered a few aspects of un-\\ncertainty we should consider in this section, there are many more ways to think\\nof this problem. For instance, if we want to compare two learning algorithms,\\nhow should we take into account the uncertainty? If there is uncertainty in my\\nlearning algorithm, is there a better way to benefit from this uncertainty? We\\nwill touch upon some of these questions in the rest of the course.\\n2.5\\nHyperparameter Tuning: Model Selection\\nWe often use the term ‘hyperparameter’ to refer to anything that we can con-\\ntrol in order to affect learning. For instance, in the case of stochastic gradient\\ndescent, a learning rate α (or any knobs in a learning rate scheduler) is a ma-\\njor hyperparameter. There are so many hyperparameters in machine learning.\\nFor instance, the parameters of the disdtribution one uses to initialize the model\\nparameters are hyperparameters. The choice/parametrization of an energy func-\\ntion is yet another hyperparameter which is highly complex. We will use λ to\\nrefer to the collection of all hyperparameters.\\nWe have learned so far that the model parameters θ should be estimated\\nfrom data D. How should we then estimate the hyperparameters λ ? We start\\nby realizing that learning corresponds to\\nLearn(D; λ, ϵ) = arg min\\nθ\\nˆR(θ; D).\\n(2.139)\\nIn other words, learning is the process of minimizing the empirical risk. This\\nlearning process is however not only a function of data D but also of the hyper-\\nparameters λ and noise ϵ.\\nWe now need to find the right set of hyperparameters. What should be the\\nobjective function here? We can use a separate dataset Dval ∩D = ∅, called a\\nvalidation set, to measure how good each hyperparamer set is:\\nTune(Dval, D; ϵ′) = arg min\\nλ Eϵ\\nh\\nˆR(Learn(D; λ, ϵ); Dval)\\ni\\n.\\n(2.140)\\nThis hyperparameter tuning process is a function of both the training and val-\\nidation sets as well as some source of noise ϵ′.\\nWe can then obtain the final model by\\nˆθ = Learn(D; Tune(Dval, D; ϵ′), ϵ),\\n(2.141)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 38}, page_content='2.5. HYPERPARAMETER TUNING: MODEL SELECTION\\n35\\nor\\nˆθ = Learn(D ∪Dval; Tune(Dval, D; ϵ′), ϵ).\\n(2.142)\\nWe can furthermore obtain several such models by repeated sampling ϵ.12 We\\nwill learn about what we can do with such a case of having multiple models\\nand what it means to have them later when we talk about Bayesian machine\\nlearning (if time permits) in §6.2.\\nThe question is then how to implement and execute hyperparameter opti-\\nmization in Eq. (2.140). One could be tempted to use gradient-based optimiza-\\ntion here as well, which is perfectly the right first reaction. There is however a\\nmajor issue. We already saw this issue earlier and had to come up with stochas-\\ntic gradient descent, and this issue is the computational cost of computing the\\ngradient, since the gradient requires us to compute\\nJacλLearn(D; λ, ϵ).\\n(2.143)\\nThere are many different ways to approximate this quantity, such as forward-\\nmode automatic differentiation as well as implicit function theorem. Neverthe-\\nless, this quantity is ultimately a fairly expensive quantity to compute due to\\nmany factors including the ever-increasing dataset size |D| and thereby the\\never-increasing optimization cost of learning.\\nIt is thus more usual to treat hyperparameter optimization as a black-box\\noptimization problem, where we can evaluate the outcome (that is, the loss\\ncomputed on the validation set) of a particular hyperparameter combination\\nbut cannot access anything else of this learning process.\\nRandom search is one of the most widely used black-box optimization based\\napproaches to hyperparameter optimization. In random search, we start by\\ndefining a prior distribution p(λ) over the hyperparameters λ. We draw K sam-\\nples from this prior distribution, {λ1, . . . , λK}, and in parallel evaluate them by\\ntraining a model using each of these sampled hyperparameters. We then pick the\\nbest hyperparameter based on the validation risk, rk = ˆR(Learn(D; λk, ϵ); Dval).\\nInstead of simply picking the best one, one can update the prior over the\\nhyperparameters based on\\n{(λ1, r1), . . . , (λK, rK)} ,\\n(2.144)\\nsuch that the probability is concentrated in the neighbourhood of low-risk hy-\\nperparameter configurations. The whole process can then be repeated using this\\nupdated distribution as the prior over the hyperparameters. This iterative ap-\\nproach is akin to the widely used method called the cross-entropy method [Ru-\\nbinstein and Kroese, 2004].\\n12We can also repeatedly sample ϵ′ to obtain more than one set of good hyperparameters\\nas well, but this process tends to be too expensive computationally to be practical, since we\\nneed to repeatedly train many new models for the purpose of optimization.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 39}, page_content='36CHAPTER 2. BASIC IDEAS IN MACHINE LEARNING WITH CLASSIFICATION\\n2.5.1\\nSequential model-based optimization for hyperpa-\\nrameter tuning\\nInstead of drawing independent hyperparameter configurations, we can think\\nof drawing a series of correlated hyperparameter configurations. Let Dn−1 =\\n((λ1, r1), . . . , (λn−1, rn−1)) be a series of hyperparameter configurations and\\ntheir associated validation risks, selected and tested so far. At time n, we need\\nto decide which hyperparameter to test next. This decision requires us to ask\\nwhich criteria we want the next hyperparameter configuration to satisfy. There\\nare many possible criteria, but one particular easy-to-understand criterion is\\nexpected improvement.\\nThe expected improvement literally computes how much improvement we\\nwould see in the risk on expectation. This expectation is computed over the\\nposterior distribution, similarly to Eq. (2.134):\\np(r|λ, Dn−1) =\\nZ\\np(r|λ, θ)p(θ|Dn−1)dθ.\\n(2.145)\\np(r|λ, θ) is a model that predicts the output r given the hyperparameter con-\\nfiguration λ, using the parameters θ. See Eq. (6.64) and surrounding discussion\\non how to create such a model. The expected improvement of a hyperparameter\\nconfiguration λ is then defined as\\nEI(λ) = Er|λ,Dn−1 [max (0, ˆrn−1 −r)] ,\\n(2.146)\\nwhere\\nˆrn−1 =\\nmin\\ni=1,...,n−1 ri.\\n(2.147)\\nThis can often be approximated using samples:\\nEI(λ) ≈1\\nM\\nM\\nX\\nm=1\\nmax(0, ˆrn−1 −rm),\\n(2.148)\\nwhere rm ∼r|λ, Dn−1.\\nWe then want to draw the next hyperparameter configuration from the fol-\\nlowing distribution:\\nq(λ|Dn−1) ∝exp (βEI(λ)) ,\\n(2.149)\\nwhere β ≥0. When β = 0, we recover the random search, and when β →∞,\\nwe always choose the hyperparameter configuration with the best expected im-\\nprovement. It is however intractable often to search for the best hyperparameter\\nconfiguration each time to maximize the expected improvement, and we only\\nsample the next hyperparameter configuration proportionally to the expected\\nimprovement.\\nWhen the number of hyperparameter is large, i.e. |λ| ≫1, it can be chal-\\nlenging to sample exactly from this distribution. In that case, it makes sense'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 40}, page_content='2.5. HYPERPARAMETER TUNING: MODEL SELECTION\\n37\\nto narrow down the space by make the density concentrated locally around the\\nbest hyperparameter so far:\\nq(λ|Dn−1) ∝exp(βEI(λ) −αD(λ, ˆλn−1),\\n(2.150)\\nwhere ˆλn−1 is the best hyperparameter configuration so far, and D is a problem-\\nspecific distance metric. We can then readily sample from this distribution by\\nfirst drawing a random set of samples in the neighbourhood of the best hyper-\\nparameter configuration so far and picking one of them proportionally to the\\nexpected improvement. This variation resembles iterative optimization, such as\\nstochastic gradient descent.\\nOverall, this approach, often called sequential model based optimization [Jones\\net al., 1998], consists of repeating three steps; (1) fit an uncertainty-aware pre-\\ndictor of the risk given a hyperparameter configuration, (2) draw the next hyper-\\nparameter configuration that maximizes the expected improvement according\\nto the trained predictor, and (3) test the newly selected hyperparameter con-\\nfiguration. Of course, it is easy to see that we do not have to test only one\\nhyperparameter configuration at a time. Instead, we can draw many samples\\nfrom the proposal distribution q, test all of them (by training multiple models\\nand evaluating them on the validation set) and update the uncertainty-aware\\npredictor on all accumulated pairs of hyperparameter configuration and associ-\\nated validation risk. This approach has become de facto standard when training\\na new deep neural network with many hyperparameters [Bergstra et al., 2011].\\n2.5.2\\nWe still need to report the test set accuracy sepa-\\nrately\\nThe hyperparameter optimization algorithm above can be thought of as the\\nimplementation of Tune in\\nˆθ = Learn(D; Tune(Dval, D; ϵ′), ϵ),\\n(2.151)\\nOnce we found the best hyperparameter configuration, we train the final model\\non the training set D to obtain our final model parameter ˆθ. How well would it\\nwork?\\nUnfortunately, we cannot use the validation risk, as that was the objective\\nby which ˆθ was selected. Meanwhile, when this model is deployed in the wild,\\nthe world will not be so kind and a set of examples thrown at this model will\\nnot be so perfect for the model. We thus need another set, called the test set,\\nDtest in order to check the test accuracy. This set must be separate from both\\nthe training and validation sets, and we can report the risk on this set as is, or\\nwe can report more statistics, as we discussed earlier in §2.4.3.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 41}, page_content='38CHAPTER 2. BASIC IDEAS IN MACHINE LEARNING WITH CLASSIFICATION'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 42}, page_content='Chapter 3\\nBuilding blocks of neural\\nnetworks\\nEarlier in §2.2.2, we talked about how general transformation F(x; θ) can be.\\nAs an example back then, we considered\\nF σ\\nlinear(x; θ) = σ(U ⊤x + c),\\n(3.1)\\nwhere σ is a point-wise nonlinearity such as a rectified linear unit:\\nσ(a) = max(0, a).\\n(3.2)\\nBy stacking this block repeatedly, we can create an increasingly more nonlinear\\ntransformation, which is the basic idea behind multi-layer perceptrons [Rumel-\\nhart et al., 1986]. We often call such a nonlinear transformation function that\\nconsists of a stack of such nonlinear layers a deep neural network.\\nThis linear layer1 is not the only option, although this is widely used due to\\nits lack of inductive biases. That is, if we do not possess any particular knowledge\\nabout the input x, it is safe to treat it as a flat finite-dimensional vector and\\nfeed it through a stack of these linear layers. It is however often the case that\\nwe know about underlying structures of an observation. For instance, if we are\\ndealing with a set of items as an observation, we want our transformation to be\\npermutation equivariant or invariant, as there is no inherent order among the\\nitems within a set.\\nIn this (short) chapter, we will introduce a few more of these basic building\\nblocks to build a deep neural network. In addition to these blocks, we can be as\\ncreative as possible as long as your newly designed blocks are differentiable w.r.t.\\nboth their own parameters and inputs. Some blocks may lack any parameters,\\nand that is perfectly fine. For instance, I can have a block that simply reverses\\nthe order of items within an input in a deterministic manner.\\n1Although this block is far from being linear, we often refer to this block as a linear block\\nor a linear layer.\\n39'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 43}, page_content='40\\nCHAPTER 3. BUILDING BLOCKS OF NEURAL NETWORKS\\n3.1\\nNormalization\\nLet us consider the simple squared energy function from Eq. (6.64), with an\\nidentity nonlinearlity:\\ne′([x, y], (u, c)) = 1\\n2(u⊤x + c −y)2.\\n(3.3)\\nWe will further assume that y is a scalar and thereby u is a vector rather than\\na matrix.\\nThe overall loss is then\\nJ(θ) = 1\\nN\\nN\\nX\\nn=1\\ne′([xn, yn], (u, c)) =\\n1\\n2N\\nN\\nX\\nn=1\\n(u⊤xn + c −yn)2.\\n(3.4)\\nThe gradient of the loss w.r.t. u is then\\n∇u = 1\\nN\\nN\\nX\\nn=1\\n(u⊤xn + c −yn)x⊤\\nn ,\\n(3.5)\\n∇c = 1\\nN\\nN\\nX\\nn=1\\n(u⊤xn + c −yn).\\n(3.6)\\nSo far, there is nothing different from our earlier exercises. We now consider\\nthe Hessian of the loss:\\nH =\\n\"\\n1\\nN\\nPN\\nn=1 xnx⊤\\nn\\n1\\nN\\nPN\\nn=1 xn\\n1\\nN\\nPN\\nn=1 xn\\n1\\n#\\n.\\n(3.7)\\nThe Hessian matrix tells us about the curvature of the objective function and\\ndirectly relates to the difficulty of optimization by a gradient-based approach. In\\nparticular, gradient-based optimization is more challenging when the condition\\nnumber is larger, where the condition number is defined as\\nκ = | maxi λi(H)|\\n| mini λi(H)| ≥1,\\n(3.8)\\nwhere λi(H) is the i-th eigenvalue of H.\\nIt is out of scope of this course to discuss in depth why the condition number\\nmatters for optimization. At a high level, you can think of the eigenvalues of the\\nHessian of the objective function as quantifying how stretched out this function\\nis along the associated eigenvector directions. That is, if the eigenvalue of an\\neigenvector is large, it means that the function value changes more dramatically\\nalong this eigenvector direction. When the objective value changes very differ-\\nently across all these directions (orthogonal directions, as they are eigenvector\\ndirections of a symmetric matrix), stochastic gradient descent suffers, as it will\\neasily oscillate along the directions with steep changes while it will not make\\nmuch progress along the directions with only little changes. We thus want the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 44}, page_content='3.1. NORMALIZATION\\n41\\neigenvalues of the Hessian to be similar to each other, for such an iterative op-\\ntimization algorithm to work well. For more rigorous discussion, refer to your\\nfavourite convex optimization book [Nocedal and Wright, 2006].\\nBased on this definition, an identity matrix has the minimal condition num-\\nber. In other words, we can transform the Hessian matrix into the identity\\nmatrix, in order to facilitiate gradient-based optimization [LeCun et al., 1998].\\nIn this particular case, because the Hessian matrix does not depend on θ but\\nonly on the observations xn’s, we can simply transform the input in advance by\\n(1) xn ←xn −1\\nN\\nN\\nX\\nn′=1\\nxn′\\n(centering)\\n(3.9)\\n(2) xn ←\\n \\n1\\nN\\nN\\nX\\nn′=1\\nxn′x⊤\\nn′\\n!−1\\n2\\nxn\\n(whitening)\\n(3.10)\\nThis will result in the identity Hessian matrix, improving the convergence of\\ngradient-based optimization.\\nSuch normalization is a key to the success in optimization, but it is challeng-\\ning to apply it in practice exactly, as the Hessian matrix is often non-stationary\\nwhen we train a deep neural network. The Hessian matrix changes as we update\\nthe model parameters, and there is no tractable way to turn the Hessian matrix\\ninto the identity matrix. Furthermore, it is even more challenging to invert this\\nHessian matrix. It however turned out that normalizing (as a weaker version\\nof whitening) of the input to each block helps in learning. Such normalization\\ncould also be considered as a building block, and let us look at a few widely\\nused ones here.\\nBatch normalization [Ioffe and Szegedy, 2015].\\nThis is one of the build-\\ning blocks that sparked the revolution in deep learning, greatly facilitating learn-\\ning:\\nFbatch−norm(x; θ = (m, s)) = m + exp(s) · ((x −µ) ⊘σ) ,\\n(3.11)\\nwhere µ and σ2 are the mean and diagonal covariance of the input to this\\nblock. Because the inverse of a full covariance matrix, which is often similar to\\nthe Hessian matrix up to an additive term, is costly, we are only consider the\\ndiagonal of the covariance matrix, which is readily invertible.\\nInstead of using the full training set to estimate µ and σ2 , which will be\\nprohibitively expensive, we use the minibatch at each update during training to\\nget stochastic estimates of these two quantities. This practice is perfectly fine\\nduring training but it becomes problematic when the model is deployed, as the\\nmodel will receive one example at a time. With a single example, we cannot\\nestimate either µ nor σ2, or if we do, it will simply subtract out the input in its\\nentirety. It is a usual practice instead to either fully re-estimate µ and σ2 using\\nthe full training set once training is over or keep the running estimates of µ and\\nσ2 during training and use them after training is over.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 45}, page_content='42\\nCHAPTER 3. BUILDING BLOCKS OF NEURAL NETWORKS\\nLayer normalization [Ba et al., 2016].\\nInstead of normalizing values across\\nexamples, it is possible to normalize values within each example across dimen-\\nsions. When we do so, we call it layer normalization:\\nFlayer−norm(x; θ = (m, s)) = m +\\nexp(s)\\nv\\nu\\nu\\nt 1\\n|x|\\n|x|\\nX\\ni=1\\n(xi −µ)2\\n|\\n{z\\n}\\n=σ\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nx −1\\n|x|\\n|x|\\nX\\ni=1\\nxi\\n|\\n{z\\n}\\n=µ\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n, (3.12)\\nwhere we assume x is a finite-dimensional vector. We can certainly modify it to\\ncope better with other types of the input, but that is out of the scope of this\\ncourse. It is rather unclear why this should help with optimization, but it has\\nbeen found to greatly facilitate learning in many large-scale experiments.\\nUnlike batch normalization, one must be careful when using layer normal-\\nization, as it can easily break the relationships among different examples. For\\ninstance, imagine a simple binary classification problem, where the positive class\\nconsists of all input vectors whose Euclidean norms are less than 1 and the neg-\\native class of all input vectors whose Euclidean norms are greater than or equal\\nto 1. Let x+ = [0.9, 0] and x−= [2, 0]. After layer normalization, they are trans-\\nformed into ˆx+ = [0.5, −0.5] and ˆx−= [0.5, −0.5], respectively. Suddenly, these\\ntwo inputs, which belong to two separate classes, are not distinguishable from\\neach other. This happens, unlike with batch normalization, because normal-\\nization is applied differently to different instances, while batch normalization\\napplies the same normalization to all instances simultaneously.\\n3.2\\nConvolutional blocks\\nMany problems in machine learning boil down to detecting patterns within an\\ninput that repeatedly appear within the training data set. Consider for instance\\nan object detection algorithm. Initially we do not know what kind of patterns are\\nconsidered representative of each object. Learning thus must figure out which\\npatterns repeatedly appear whenever the input was associated with a particular\\nobject label. These patterns are however not global but localized, since the\\nobject may appear anywhere within an input image. It may appear at the center\\nbut also appear at any corner of the image, without any impact on the object\\nidentity. In other words, an object detector should be translation invariant.2\\n2We say F is equivariant to a particular transformation T when\\nF(T (X)) = T (F(X)).\\n(3.13)\\nWe say F is invairant to a particular transformation T when\\nF(T (X)) = F(X).\\n(3.14)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 46}, page_content='3.2. CONVOLUTIONAL BLOCKS\\n43\\nAny invariance could be implemented as a stack of equivariant blocks fol-\\nlowed by a reduction operator, such as summation. We thus need to implement a\\ntranslation equivariant block. In this section, we consider a so-called convolution\\nblock, or more precisely correlation block.\\nWe start by considering an infinitely long discretized time series, x = [. . . , xt−1, xt, xt+1, . . .]\\nwith |x| →∞, as an input to this block. Each item xt is a finite-dimensional\\nreal vector. The parameter of this block is a set of finite-length filter sequences,\\nf k = [f k\\n1 , f k\\n2 , . . . , f k\\n2M+1] with M ≪∞and k = 1, . . . , K. Similarly to xt, each\\nf k\\nt is also a finite-dimensional real vector with |f k\\nt | = |xt|. The convolution\\nblock then returns an infinitely-long time series, h = [. . . , ht−1, ht, ht+1, . . .],\\nwhere |ht| = K.\\nLet hk\\nt be the k-th element of ht. We then compute it as\\nhk\\nt =\\nm′=M\\nX\\nm′=−M\\nx⊤\\nt+m′f k\\nm′+M+1.\\n(3.15)\\nIn other words, we apply the k-th filter f k at each position t to check how similar\\n(in the sense of dot product) the signal centered at t is to the filter f k.\\nAnother way to write it down is\\nht =\\nm′=M\\nX\\nm′=−M\\nFm′+M+1xt+m′,\\n(3.16)\\nwhere\\nFm =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nf 1\\nm\\nf 2\\nm\\n...\\nvK\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb∈Rd×K\\n(3.17)\\nwith d = |xt|. The full parameters of this 1-D convolution block can be summa-\\nrized as a 3-D tensor of size d × K × (2M + 1).\\nIt is pretty straightforward to see that this operation is translation equivari-\\nant. If we shift every xt by δ, the resulting ht will shift by δ without any impact\\non its computed value. Unfortunately, in practice, this does not hold perfectly,\\nas we do not work with an infinitely long sequence. We must decide how to\\nhandle the boundaries of the sequence with a finite-length sequence, and this\\nchoice will impact the degree of translation equivariance near the boundaries.\\nDetailed discussion on how we handle boundaries is out of the scope of this\\ncourse, though.\\nWe can now readily extend this 1-D convolution to N-D convolution. For\\ninstance, 2-D convolution would work on an infinitely large image, and 3-D\\nconvolution on an infinitely large-and-long video. Furthermore, we can extend\\nit by introducing various features, such as a stride. These are also out of the\\nscope of this course, but I recommend the first half of the classic by LeCun et al.\\n[1998].'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 47}, page_content='44\\nCHAPTER 3. BUILDING BLOCKS OF NEURAL NETWORKS\\n3.3\\nRecurrent blocks\\nOften, strong equivariance or invariance tends to be too strict. Perhaps we\\nwant equivariance only in a particular context and not in another context. It\\nis however difficult to implement it in a strict sense. We can take one step up\\nin the level of abstraction and work on applying the same operator repeatedly\\nover the input. This is the core idea behind a recurrent block.\\nA recurrent block works on a sequence of input items (x1, x2, . . . , xT ), just\\nlike the 1-D convolution block above. This block consists of a neural network\\nthat is applied repeated to xt sequentially (that is, one at a time.) This neural\\nnet takes as input the concatenation of xt and the memory (or hidden state)\\nht−1 and returns an updated memory ht:\\nht = F([xt, ht−1]; θr),\\n(3.18)\\nwhere θr is the parameters of this recurrent function F. θr includes the intial\\nhidden state h0. Once we sweep the input sequence with F, the recurrent block\\nreturns the same-length sequence by concatenating all ht’s: (h1, h2, . . . , hT ).\\nThe advantage of such a recurrent block over e.g. the 1-D convolution above\\nis that it effectively has an unlimited context size. In the 1-D convolution, any\\noutput at time t depends only on 2M + 1 input vectors centered at t. On the\\nother hand, the recurrent block takes into account all inputs up to t to compute\\nthe hidden state ht at time t. Furthermore, by simply stacking two recurrent\\nblocks with the sequence reversal inbetween, we can make each output vector\\nto depend on the entire sequence readily.\\nA representative (and simple) example of widely-used (and easy-to-use) re-\\ncurrent blocks is a gated recurrent unit [GRU; Cho et al., 2014] which is defined\\nas\\nFGRU = ut ⊙ht−1 + (1 −ut) ⊙˜ht,\\n(3.19)\\nwhere\\nrt = σ (Wrxt + Urht−1 + br)\\n(Reset Gate)\\n(3.20)\\nut = σ (Wuxt + Uuht−1 + bu)\\n(Update Gate)\\n(3.21)\\n˜ht = tanh (Whxt + Uh(rt ⊙ht−1) + bh)\\n(Candidate State)\\n(3.22)\\nThis (weighted) linear combination has shown to effectively address the issue of\\nvanishing gradient [Bengio et al., 1994], and has become a standard practice in\\nmachine learning over the past decade or so [He et al., 2016].\\n3.4\\nPermutation equivariance: attention\\nWe are often faced with a situation where the input to a block is a set of vectors\\nX = {xi}N\\ni=1. We want to transform each item xk in the context of all the other\\nitems in this set, resulting in another set of vector H = {hi}N\\ni=1. We want this'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 48}, page_content='3.4. PERMUTATION EQUIVARIANCE: ATTENTION\\n45\\nlayer to be equivariant to permutation such that F((xσ(i))N\\ni=1) = (hσ(i))N\\ni=1,\\nwhere σ : {1, . . . , N} →{1, . . . , N} is a permutation operator. Let’s consider\\none canonical way to build such a permutation equivariant block.\\nThis block begins with three linear blocks:\\nki = Flinear(xi; θk),\\n(3.23)\\nqi = Flinear(xi; θq),\\n(3.24)\\nvi = Flinear(xi; θv).\\n(3.25)\\nWe are referred to as the key, query and value vectors of the i-th item xi.\\nFor each j-th item xj, we check how compatible it is to the current i-th item\\nxi:\\nαj\\ni =\\nexp(q⊤\\ni kj)\\nPN\\nj′=1 exp(q⊤\\ni kj′)\\n.\\n(3.26)\\nWe normalize it to sum to one using softmax.\\nNow, we use these importance weights to compute the weighted average of\\nthe values:\\nˆvi =\\nN\\nX\\nj=1\\nαj\\nivi.\\n(3.27)\\nIt is a usual practice to repeat this process K times to produce\\nˆvi ←\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nˆv1\\ni\\n...\\nˆvK\\ni\\n\\uf8f9\\n\\uf8fa\\uf8fb.\\n(3.28)\\nWhen we do this, each of K such processes is called an attention head.\\nAt this point, ˆvi is a linear function of the input X = {x1, . . . , xN}. We\\nwant to introduces some nonlinearity here by introducing the final linear layer\\ntogether with a residual connection:\\nhi = Flayer−norm(F σ\\nlinear(ˆvi; θh); θl) + Flinear(xi; θr),\\n(3.29)\\nwhere the lack of the superscript in the second term means that there is no\\nnonlinearity. If |hi| = |xi|, it is customary to fix θr = (I, 0). It is usual to add a\\nlayer normalization block after ˆvi or at hi, to facilitate optimization.\\nWhen implemented in a single block, this block is often referred to as the\\n(multi-headed) attention block [Bahdanau et al., 2015, Vaswani et al., 2017].\\nPositional Encoding.\\nAnother way to think of the attention block above is\\nto view it as a way to handle a variable-sized input. Regardless of the size of\\nthe input set, this attention block can work with the input. It is thus tempting\\nto use the attention block for a variable-length sequence, which was the orig-\\ninal motivation behind the attention block. There is one hurdle that must be'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 49}, page_content='46\\nCHAPTER 3. BUILDING BLOCKS OF NEURAL NETWORKS\\novercome in that case. That is, we must ensure that each item in a sequence is\\nmarked with its position.\\nThere are two major approaches to this. The first approach is based on\\nadditive marking. For each position i, let ei be a vector of size |x| and represent\\nthe i-th position. There are many ways to construct this vector, and sometimes\\nit is even possible to learn this vector from data, although we can only handle\\nthe length seen during training in the latter case. One particular approach is to\\nuse sinusoidal functions so that each dimension of ei captures different rates at\\nwhich the position changes. For instance,\\ned\\ni =\\n(\\nsin\\n\\x00i\\nLi/|x|\\n\\x01\\n,\\nif i\\nmod 2 = 0\\ncos\\n\\x00i\\nL(i−1)/|x|\\n\\x01\\n,\\nif i\\nmod 2 = 1\\n(3.30)\\nwhere L is a hyperparameter and is often set to 10000. This vector is then added\\nto each input item, i.e., xi + ei before being fed to the attention block.\\nThe first approach, the additive approach, makes it easy for the attention\\nblock to capture the locality of each vector, because neary vectors tend to have\\nsimilar positional embeddings, and to capture the absolute position based pat-\\ntern, as each absolute position is represented by its unique positional embedding\\nvector. It is however challenging for the attention block to capture the patterns\\nbased on relative positions beyond simple locality.\\nIn particular, consider how the so-called attention weight on the j-th item\\nfor the i-th item was computed in Eq. (3.26). The weight is proportional to the\\ndot product between the i-th query vector and the j-th key vector:\\nq⊤\\ni kj = (Wq(xi + ei))⊤(Wk(xj + ej))\\n(3.31)\\n= x⊤\\ni W ⊤\\nq Wkxj + e⊤\\ni W ⊤\\nq Wkxj + x⊤\\ni W ⊤\\nq Wkek + e⊤\\ni W ⊤\\nq Wkek,\\n(3.32)\\nwhere we assumed zero bias vectors for both query and key vectors. From from\\nthe first term in the expanded expression, we notice that the content-based\\nrelationship between the i-th input and j-th input is largely independent of\\ntheir positions. In other words, the semantic relationship between these two\\ninputs is stationary across their relative positions, which may be restrictive in\\nmany downstream applications.\\nFocusing on the first term above, we can think of a way to ensuring that\\nthis pairwise semantic relationship is position-dependent. More specifically, we\\nwant it to depend on the relative position between xi and xj:\\n⟨qi, kj⟩j−i = q⊤\\ni R⊤\\ni Rjxj,\\n(3.33)\\nwhere Rm is an orthogonal matrix that is parameterized by a scalar position m\\nand changes smoothly w.r.t. m. One way to construct such an orthogonal matrix\\nis to build a block diagonal matrix where a 2-D rotation matrix is repeated along'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 50}, page_content='3.4. PERMUTATION EQUIVARIANCE: ATTENTION\\n47\\nthe diagonal:\\nRm =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nR2\\n1(m)\\n0\\n· · ·\\n0\\n0\\nR2\\n2(m)\\n· · ·\\n0\\n0\\n0\\n· · ·\\n0\\n0\\n· · ·\\n0\\nR2\\n|x|/2(m)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,\\n(3.34)\\nwhere R2\\nk(m) is a 2-dimensional rotation matrix that rotates a 2-dimensional\\nreal vector and defined as\\nR2\\nk(m) =\\n\\x14\\ncos(mLk/|x|)\\n−sin(mLk/|x|)\\nsin(mLk/|x|)\\ncos(mLk/|x|)\\n\\x15\\n.\\n(3.35)\\nIn other words, we rotate every pair of elements of the query/key vector based on\\nits position before computing the dot product between these two vectors. Since\\nthis rotation depends on the relative position between the query and key vectors,\\nthis approach can capture position-dependent semantic relationship between\\nthe i-th input and the j-th input. This idea has become one of the standard\\napproaches to incorporating positional information in the attention block in\\nrecent years [Su et al., 2021].'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 51}, page_content='48\\nCHAPTER 3. BUILDING BLOCKS OF NEURAL NETWORKS'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 52}, page_content='Chapter 4\\nProbabilistic Machine\\nLearning and Unsupervised\\nLearning\\n4.1\\nProbabilistic interpretation of the energy func-\\ntion\\nAlthough we already learned about how to turn an energy function into a prob-\\nability function in §2.1.2, we will go slightly deeper in this section, as it will help\\nus derive a series of machine learning algorithms in this chapter.\\nThe energy function is defined w.r.t. the observation x, the unobserved (la-\\ntent) variable z and the model parameters θ: e(x, z, θ). For now, we will assume\\nthat θ is not a random variable, unlike x and z. We can then compute the joint\\ndistribution over x and z as\\np(x, z; θ) =\\nexp(−e(x, z, θ))\\nRR\\nexp(−e(x′, z′, θ))dx′dz′ .\\n(4.1)\\nOf course, it is often (if not almost always) challenging to compute the normal-\\nization constant (or the partition function) in the denominator. Such a challenge\\nhints at a different approach to the same problem. Instead of defining an en-\\nergy function first and then deriving the probability function, why not directly\\ndefine the probability function? After all, we can recover the underlying energy\\nfunction given a probability function up to a constant:\\ne(x, z, θ) = −log p(x, z; θ) + log Z(θ).\\n(4.2)\\nIn fact, it may be even easier to decompose the joint probability function p(x, z)\\n49'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 53}, page_content='50CHAPTER 4. PROBABILISTIC MACHINE LEARNING AND UNSUPERVISED LEARNING\\nfurther,1 using the chain rule of probability:\\np(x, z) = p(z)p(x|z).\\n(4.3)\\nSuch a decomposition gives us an interesting way to interpret this proba-\\nbilistic model. z is a latent variable that determines the intrinsic properties of\\nthe observation x. We therefore first draw an intrinsic property configuration z\\nfrom the prior distribution p(z). Given this intrinsic property configuration z,\\nwe draw the actual observation x.\\nFor instance, you can imagine that z refers to an object category (a dog, a cat,\\na car, etc.) We first draw a category of an object we want to paint by selecting\\nz according to the prior distribution p(z). This prior distribution reflects the\\nfrequencies of these object categories in the world. Given the object category\\nz, we can now paint the object by drawing x from p(x|z). This conditional\\ndistribution encapsulates all variations of the object z in its visual form, such\\nas lightning condition, background, texture, etc.\\nAnother distribution, or the probability function, of our interest is the pos-\\nterior distribution over z given the observation x. Continuing from the example\\nabove, we can think of trying to infer which object z a given painting x de-\\npicts. Such inference is often imperfect and results in a distribution over the\\nobject categories rather than picking one correct category. We can derive this\\ndistribution using the Bayes’ rule:\\np(z|x) =\\np(x|z)p(z)\\nR\\np(x|z′)p(z′)dz′ = p(x|z)p(z)\\np(x)\\n.\\n(4.4)\\nJust like earlier when we tried to turn the energy function into a probability\\nfunction, posterior inference is often computationally intractable due to the\\nnormalization constant in the denominator:\\nR\\np(x|z′)p(z′)dz′.\\nWith these probability functions in our hands, we can now define a generic\\nloss function:\\nLll(x, θ) = −log\\nZ\\np(x|z; θ)p(z; θ)dz.\\n(4.5)\\nWe often refer to this loss function as the negative log-likelihood or log-probability.\\nIf you are not comfortable with having a single-variable observation x, we\\ncan write this down in terms of the input-outcome pair (x, y):\\nLll([x, y], θ) = −log\\nZ\\np(y|x, z; θ)p(x)p(z; θ)dz\\n(4.6)\\n= −log\\nZ\\np(y|x, z; θ)p(z; θ)dz + const.,\\n(4.7)\\nwhere we assume that p(x) is simply given and is not optimized with its own\\nparameters. When z does not exist, it reduces to the cross-entropy loss from\\nEq. (2.30):\\nLll([x, y], θ) = −log p(y|x; θ).\\n(4.8)\\n1As usual, we will omit θ if its existence, or lack thereof, is clear from the context.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 54}, page_content='4.2. VARIATIONAL INFERENCE AND GAUSSIAN MIXTURE MODELS51\\nIn the rest of this chapter, we focus on the case where we have input-only\\nobservations. We often call such a setup unsupervised learning.\\n4.2\\nVariational inference and Gaussian mixture\\nmodels\\nWe will derive something magical in this section, although it will not look mag-\\nical at all in hindsight by the end of this section. To do so, let us first re-state\\nthat it is challenging to derive the posterior distribution p(z|x) over the latent\\nvariable given an observation, unless we explicitly put severe constraints on the\\nforms of p(x|z) and p(z).2 Instead of computing p(z|x) directly, we can perhaps\\nfind a proxy q(z; ϕ(x)) to this exact posterior distribution, called an approximate\\nposterior. This approximate posterior probability function is parametrized by\\nϕ(x), where we use (x) to denote that these parameters are specific to x. When\\nit is not confusing, we would drop (x) here and there for both brevity and clarity.\\nWe are now faced with a task to make the proxy q(z; ϕ(x)) a good approxi-\\nmation to the true posterior p(z|x). We will do so by minimizing the Kullback-\\nLeibler (KL) divergence which is defined as\\nDKL(q∥p) = −\\nZ\\nq(z; ϕ(x)) log\\np(z|x)\\nq(z; ϕ(x))dz\\n(4.9)\\n= −Ez∼q [log p(z|x)] −H(q) ≥0\\n(4.10)\\nwhere H(q) is the entropy of q defined as\\nH(q) = −\\nZ\\nq(z) log q(z)dz.\\n(4.11)\\nIt is important to notice the inequality above, that is, the KL divergence is by\\ndefinition non-negative.\\nLet us continue from the KL divergence:\\nDKL(q∥p) = −\\nZ\\nq(z; ϕ(x)) log\\np(z|x)\\nq(z; ϕ(x))dz\\n(4.12)\\n= −\\nZ\\nq(z; ϕ(x)) log\\np(x|z)p(z)\\np(x)q(z; ϕ(x))dz\\n(4.13)\\n= log p(x) −\\nZ\\nq(z; ϕ(x)) log p(x|z)dz −\\nZ\\nq(z; ϕ(x)) log\\np(z)\\nq(z; ϕ(x))dz\\n(4.14)\\n= log p(x) −Ez∼q [log p(x|z)] + DKL(q(z; ϕ(x))∥p(z)).\\n(4.15)\\n2In short, p(z) should be a so-called conjugate prior to the likelihood p(x|z), so that the\\nposterior p(z|x) follows the same distributional family as p(x|z).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 55}, page_content='52CHAPTER 4. PROBABILISTIC MACHINE LEARNING AND UNSUPERVISED LEARNING\\nTo find q (or its parameters ϕ(x)), we minimize the second and third terms\\nabove, since the first term log p(x) is not a function of q . In other words,\\nˆϕ(x) = arg min\\nϕ(x) −Ez∼q [log p(x|z)] + DKL(q(z; ϕ(x))∥p(z))\\n(4.16)\\n= arg max\\nϕ(x) Ez∼q [log p(x|z)] −DKL(q(z; ϕ(x))∥p(z))\\n|\\n{z\\n}\\n=J(ϕ(x))\\n.\\n(4.17)\\nIf we design q(z; ϕ(x)), it is often possible to compute the (stochastic) gradient\\nof this objective function J w.r.t. ϕ(x) and use stochastic gradient descent to\\nupdate ϕ(x) iteratively to find q that is a better proxy to the true distribution\\nthan at the beginning.\\nIn an interesting twist, this objective function J is a lower bound to log p(x),\\nbecause the KL divergence is greater than or equal to 0:\\nlog p(x; θ) ≥Ez∼q [log p(x|z; θ)] −DKL(q(z; ϕ(x))∥p(z))\\n|\\n{z\\n}\\n=J(θ)\\n.\\n(4.18)\\nThis means that we can indirectly maximize the log-probability assigned to\\nan observation x by the model by maximizing its lowerbound. Maximizing the\\nlowerbound does not guarantee that the actual quantity increases, but it ensures\\nthat the actual quantity is higher than the achieved maximum lowerbound. The\\nquality of doing so is determined by the gap between the lowerbound and the\\nactual quantity, and this gap turned out to be exactly the KL divergence between\\nthe approximate posterior and the true posterior, DKL(q∥p). In other words, if\\nour approximation to the posterior is good, we get a tighter lowerbound and\\nconsequently can maximize the true target quantity better.\\nSince the same objective function J is used for both minimizing the KL\\ndivergence in order to find a better approximate posterior (4.16) and maximizing\\nthe lowerbound to the true quantity (4.18), we can perform both optimization\\nsimultaneously [Neal and Hinton, 1998]:\\nmax\\nϕ(x1),...,ϕ(xN),θ\\n1\\nN\\nN\\nX\\nn=1\\nEz∼q(z;ϕ(xn)) [log p(xn|z; θ)] −DKL(q(z; ϕ(xn))∥p(z)),\\n(4.19)\\nwhere x1, . . . , xN are the training examples. This formulation furthermore allows\\nus to use stochastic gradient descent from §2.3.2. This procedure is often referred\\nto as stochastic variational inference and learning. Inference refers to estimating\\nϕ(xn), and learning refers to estimating θ.\\n4.2.1\\nVariational Gaussian mixture models\\nLet us consider a practical use case of stochastic variational inference and learn-\\ning above. We start by defining a mixture of Gaussians. A generative story\\nbehind a mixture of Gaussians (or equivalently a Gaussian mixture model) is'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 56}, page_content='4.2. VARIATIONAL INFERENCE AND GAUSSIAN MIXTURE MODELS53\\nthat there exist a finite number of Gaussian distributions, which are referred to\\nas “components”, and a latent variable z selects one of these components. Once\\nthe component is selected, an observation x is drawn from the corresponding\\nGaussian distribution.\\nTo map this story onto the probability functions, we begin with a prior\\ndistribution over the components:\\np(z) = 1\\nM ,\\n(4.20)\\nwhere M is the number of Gaussian components. This prior states that each\\nand every component is equally likely to be selected. This can be relaxed, but\\nwe will stick to this for now. z can take any one of {1, . . . , M}.\\nOnce the component is selected, we draw an observation x from\\np(x|z) = N(x|µz, Σz),\\n(4.21)\\nwhere µz and Σz are the mean and covariance of the z-th Gaussian component.\\nFor simplicity, let us assume that Σz = I, that is, the covariance is an identity\\nmatrix. In such a case, we say that the component is spherical Gaussian.\\nWe introduce an approximate posterior for each training example xn. This\\napproximate posterior is\\nq(z = k; ϕn) = αn\\nk,\\n(4.22)\\nwhere αn\\nz ≥0 and PM\\nk=1 αn\\nk = 1 for all n = 1, . . . , N.\\nWe can now write down the objective J:\\nJ(α1, . . . , αN, µ1, . . . , µM) = 1\\nN\\nN\\nX\\nn=1\\n M\\nX\\nm=1\\nαn\\nm\\n\\x12\\n−1\\n2∥xn −µm∥2 −d\\n2 log 2π\\n\\x13\\n(4.23)\\n+\\nM\\nX\\nm=1\\nαn\\nm log M −\\nM\\nX\\nm=1\\nαn\\nm log αn\\nm\\n!\\n,\\n(4.24)\\nwhere d = dim(xn).\\nLet’s compute the gradient of J w.r.t. µk:\\n∇µk = 1\\nN\\nX\\nn\\n(αn\\nk(xn −µk)) = 1\\nN\\n X\\nn\\nαn\\nkxn −µk\\nX\\nn\\nαn\\nk\\n!\\n= 0\\n(4.25)\\n⇐⇒µk =\\nN\\nX\\nn=1\\nαn\\nk\\nPN\\nn′=1 αn′\\nk\\nxn.\\n(4.26)\\nWe can compute the exact solution to µk analytically, that maximizes J.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 57}, page_content='54CHAPTER 4. PROBABILISTIC MACHINE LEARNING AND UNSUPERVISED LEARNING\\nLet’s do the same for αn\\nk:\\n∇αn\\nk = −1\\n2∥xn −µk∥2 −d\\n2 log 2π −log M −log αn\\nk −1 = 0\\n(4.27)\\n⇐⇒log αn\\nk = −1\\n2∥xn −µk∥2 −d\\n2 log 2π −log M −1\\n(4.28)\\n⇐⇒αn\\nk =\\nexp\\n\\x00−1\\n2∥xn −µk∥2 −d\\n2 log 2π −log M\\n\\x01\\nPK\\nk′=1 exp\\n\\x00−1\\n2∥xn −µk′∥2 −d\\n2 log 2π −log M\\n\\x01,\\n(4.29)\\n⇐⇒αn\\nk =\\nexp\\n\\x00−1\\n2∥xn −µk∥2\\x01\\nPK\\nk′=1 exp\\n\\x00−1\\n2∥xn −µk′∥2\\x01,\\n(4.30)\\nbecause PK\\nk=1 αn\\nk = 1.\\nFor the approximate posterior, we can solve it analytically and exactly. In\\nfact, if we analyze the solution above more carefully, we realize that it is identical\\nto the true posterior:\\nlog\\nαn\\nk\\n|{z}\\np(z=k|xn)\\n= log N(xn|µk, I)\\n|\\n{z\\n}\\n=p(xn|z=k;θ)\\n+ log\\n1\\nM\\n|{z}\\n=p(z=k)\\n−log Z.\\n(4.31)\\nwhere Z is the normalization constant. In other words, the KL divergence be-\\ntween q(z; ϕ(x)) and p(z|x) is zero. It also implies that there is no gap between\\nthe variational lowerbound and the true log-evidence log p(x).\\nGaussian mixture models are special in that the variational lowerbound is\\ntight, i.e., there is no gap. They are also special in that we can find the analytical\\nsolution to posterior inference and likelihood maximization in a relatively simple\\nmanner. Even in this case, one should notice that the solutions to setting these\\ngradients to zero are co-dependent. We thus need to iteratively update these\\nquantities multiple times until some kind of convergence is achieved. This pro-\\ncess is called expectation-maximization (E-M), or more generally a coordinate-\\nascent algorithm. Each of these two steps (updating the posterior and updating\\nthe parameters) is guaranteed to improve the variational lowerbound, and this\\nalternating procedure will ultimately find a local maximum.\\nAlthough there is an analytical solution to the parameters at each E-M iter-\\nation, it may not be desirable to use this analytical solution, because it requires\\nus to use the posterior means of all N training examples. When N is large, this\\nstep, which needs to be repeated, can be prohibitively expensive. We can in-\\nstead use stochastic gradient descent by computing the posterior means of only\\na small subset of training set (which can be done exactly as we have derived ear-\\nlier) and only slightly updating the parameters following the stochastic gradient\\ncomputed using this minibatch alone. Each E-M iteration is not guaranteed to\\nimprove the overall variational lowerbound, but on average with small enough\\nstep sizes, stochastic gradient descent makes progress. This would be a good\\napproach to implement the Gaussian mixture model on a very large dataset.\\nOnce learning is over, we can use the fitted Gaussian mixture model to (a)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 58}, page_content='4.2. VARIATIONAL INFERENCE AND GAUSSIAN MIXTURE MODELS55\\ndraw more samples and (b) infer the posterior distribution over the components\\ngiven a new observation.\\n4.2.2\\nK-means clustering\\nLet us introduce a temperature β ≥0 as a hyperparameter in Eq. (4.30):\\nαn\\nk =\\nexp\\n\\x10\\n−1\\n2β ∥xn −µk∥2\\x11\\nPK\\nk′=1 exp\\n\\x10\\n−1\\n2β ∥xn −µk′∥2\\n\\x11.\\n(4.32)\\nWhen the temperature is high, i.e. β →∞, the posterior distribution is\\ncloser to the uniform distribution. This is understandable if you think of sta-\\ntistical thermodynamics. When the temperature is high, there is no particular\\nconfiguration that is more likely than others, since all molecules are bouncing\\naround non-stop with high energy. On the other hand, when the temperature\\napproaches 0, the posterior converges toward one of the corners of the (K −1)-\\ndimensional simplex, meaning that only one of the components is probable and\\nall the others are not at all. This would be an extreme case that interests us\\nhere.\\nWhen β →0, we can rewrite the solution to posterior inference as\\nαn\\nk =\\n(\\n1,\\nif ∥xn −µk∥2 = mink′=1,...,K ∥xn −µk′∥2\\n0,\\notherwise.\\n(4.33)\\nIn this case, we can be more economical by storing\\nˆzn = arg\\nmax\\nk=1,...K αn\\nk,\\n(4.34)\\ninstead of K values for each n-th training example. In other words, we need\\nonly ⌈log2 K⌉bits as opposed to K × B bits where B is the number of bits for\\nrepresenting a real value in one’s system.\\nIn this case, the update rule for the mean of each component in Eq. (4.26)\\ncan be simplified as well:\\nµk =\\nN\\nX\\nn=1\\nαn\\nk\\nPN\\nn′=1 αn′\\nk\\nxn\\n(4.35)\\n=\\nN\\nX\\nn=1\\n1(ˆzn = k)xn.\\n(4.36)\\nThat is, we collect all training examples that belong to the k-th component and\\ncompute the average of these training examples. This further saves a significant\\namount of compute, as we only go through on average N/K training examples\\nfor each component to compute its mean vector.\\nBecause we are effectively making the hard choice of to which component\\neach training example belongs to (4.34), we often refer to this special case as'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 59}, page_content='56CHAPTER 4. PROBABILISTIC MACHINE LEARNING AND UNSUPERVISED LEARNING\\nhard expectation-maximization (EM). Furthermore, because we are effectively\\ngrouping the training examples into K clusters, and each cluster is represented\\nby its mean, this algorithm is called K-means clustering as well. This is one\\nof the most widely used algorithms in unsupervised learning and data analysis,\\nand the variational inference based approach we started with allows us to more\\nflexibly extend this algorithm to work with more non-trivial distributions.\\n4.3\\nContinuous latent variable models\\nLet’s restate the objective function derived from the variational inference prin-\\nciple earlier in Eq. (4.19):\\nmax\\nϕ(x1),...,ϕ(xN),θ\\n1\\nN\\nN\\nX\\nn=1\\nEz∼q(z;ϕ(xn)) [log p(xn|z; θ)] −DKL(q(z; ϕ(xn))∥p(z)).\\n(4.37)\\nLooking at this formula, there is absolutely no reason for us to assume that z is\\na discrete variable, as we did with the mixture of Gaussians above. z can very\\nwell be a continuous real-valued vector.\\nLet us try a simple case here by assuming that\\np(z) = N(z; 0, σ2I|z|)\\n(4.38)\\np(x|z; θ) = N(x; Wz + b, I|x|),\\n(4.39)\\nwhere θ = (W, b) with W ∈R|x|×|z| and b ∈R|z|. σ2 is a hyperparameter and\\ncontrols the strength of regularization. We will discuss more what we mean by\\nthis. We further use a simple approximate posterior for each example xn:\\nq(z; ϕ(xn)) = N(z; µn, I|z|),\\n(4.40)\\nwhere ϕ(xn) = (µn).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 60}, page_content='4.3. CONTINUOUS LATENT VARIABLE MODELS\\n57\\nThen, the objective for each training example xn becomes\\nJn = Ez∼qn\\n\\x14\\n−1\\n2∥xn −Wz −b∥2 −|x|\\n2 log 2π\\n\\x15\\n−1\\n2 ∗\\n\\x14K + ||µn||2\\nσ2\\n−K + 2K ln(σ)\\n\\x15\\n(4.41)\\n= −Ez\\n\\x141\\n2∥xn∥2 + 1\\n2∥Wz + b∥2 −x⊤\\nn (Wz + b)\\n\\x15\\n−\\n1\\n2σ2 ∥µn∥2\\n(4.42)\\n= −Ez\\n\\x141\\n2z⊤W ⊤Wz + 1\\n2∥b∥2 + b⊤Wz −x⊤\\nn Wz −x⊤\\nn b\\n\\x15\\n−\\n1\\n2σ2 ∥µn∥2 + const.\\n(4.43)\\n= −1\\n2tr WEz[(z −µn)(z −µn)⊤]W ⊤−1\\n2tr WµnEz[z]⊤W ⊤−1\\n2tr WEz[z]µ⊤\\nn W ⊤+ 1\\n2tr Wµnµ⊤\\nn W ⊤\\n(4.44)\\n−1\\n2∥b∥2 −b⊤Wµn + x⊤\\nn Wµn + x⊤\\nn b −\\n1\\n2σ2 ∥µn∥2 + const.\\n(4.45)\\n= −1\\n2tr WW ⊤−1\\n2µ⊤\\nn W ⊤Wµn −1\\n2∥b∥2 −b⊤Wµn + x⊤\\nn Wµn + x⊤\\nn b −\\n1\\n2σ2 ∥µn∥2 + const.\\n(4.46)\\nwhere const. refers to the terms that do not depend on either ϕ(xn) nor θ.\\nLet’s perform posterior inference first by computing the gradient of J =\\n1\\nN\\nP\\nn Jn w.r.t. ϕ(xn):\\n∇µn = −W ⊤Wµn + W ⊤(xn −b) −1\\nσ2 µn = 0\\n(4.47)\\n−(W ⊤W + σ−2I)µn + W ⊤(xn −b) = 0\\n(4.48)\\nµn = (W ⊤W + σ−2I)−1W ⊤(xn −b).\\n(4.49)\\nJust like with the MoG above, we get a clean, analytical solution to each µn.\\nBecause we need to compute the inverse of W ⊤W + I ∈RK×K, this may be\\nsomewhat expensive, but we need to compute it once and use it for all N µn’s.\\nLet’s look at the role of σ2 from the prior p(z) earlier in this context. When\\nσ2 →∞, the expression above simplifies to\\nµn = (W ⊤W)−1W ⊤\\n|\\n{z\\n}\\n=(a)\\n(xn −b),\\n(4.50)\\nwhere (a) is the pseudoinverse of W. When W is a square and invertible matrix,\\nthis corresponds to W −1. In that case, we can think of µn as the solution to\\nµn = W −1(xn −b),\\n(4.51)\\nwhich is equivalent to\\nxn = Wµn + b.\\n(4.52)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 61}, page_content='58CHAPTER 4. PROBABILISTIC MACHINE LEARNING AND UNSUPERVISED LEARNING\\nThis expression is the mean of the p(x|z; θ) from above.\\nIf there is no prior information available for z, i.e. σ2 →∞, our best guess\\nat which latent configuration led to xn (that is, posterior inference) is to mul-\\ntiply xn (after subtracting the bias b) with the inverse of the forward matrix\\nW. In other words, the prior knowledge we have (in this case, that the latent\\nconfigurations are more probably if they are closer to the origin) would affect\\nposterior inference. This is what we meant earlier by regularization and that σ2\\ncontrols the strength of regularization.\\nWe now compute the gradient of J w.r.t. W and b given µn’s. Let’s begin\\nwith b:\\n∇b = 1\\nN\\nN\\nX\\nn=1\\n(−b −Wµn + xn) = 0\\n(4.53)\\n⇐⇒b = 1\\nN\\nN\\nX\\nn=1\\n(Wµn −xn) .\\n(4.54)\\nThis expression makes an intuitive sense. b, the bias, is the average offset between\\nwhat we get given the latent configuration and what we actually observe.\\nLet us continue with W:\\n∇W = 1\\nN\\nN\\nX\\nn=1\\n\\x00−W −Wµnµ⊤\\nn −bµ⊤\\nn + xnµ⊤\\nn\\n\\x01\\n(4.55)\\n= −W\\n \\nI + 1\\nN\\nN\\nX\\nn=1\\nµnµ⊤\\nn\\n!\\n+ 1\\nN\\nN\\nX\\nn=1\\n(xn −b)µ⊤\\nn .\\n(4.56)\\nThen,\\nW =\\n \\n1\\nN\\nN\\nX\\nn=1\\n(xn −b)µ⊤\\nn\\n!  \\nI + 1\\nN\\nN\\nX\\nn=1\\nµnµ⊤\\nn\\n!−1\\n.\\n(4.57)\\nThe first term in the product in the right-hand side can be thought of im-\\nplementing so-called a Hebbian learning rule: “neurons that fire together, wire\\ntogether” [Hebb, 1949]. If the i-th dimension of the observation xi fires (that\\nis, beyond the bias bi) and the j-th dimension of the latent variable µj fires\\ntogether (where ‘fire’ is defined as any deviate away from the bias or zero),\\nthe strength of the weight value wij between them must be large. This already\\nshows up as the second term in the gradient w.r.t. W above.\\nThe second term in the right-hand side (which corresponds to the first term\\nin the gradient) is more complicated. This works as whitening µn inside the first\\nterm. That is, it makes µn’s to be distributed so that the covariance is closer\\nto the identity. This works as making W capture the covariance between the\\nmean-subtracted observations (xn −b)’s and the whitened latent configurations\\n\\x12\\nµn\\n\\x10\\nI + 1\\nN\\nPN\\nn=1 µnµ⊤\\nn\\n\\x11−1\\x13\\n’s. By doing so, in the next iteration of this EM'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 62}, page_content='4.3. CONTINUOUS LATENT VARIABLE MODELS\\n59\\nprocedure, µn’s will be distributed such that their collectively covariance will be\\ncloser to the identity, which is what we imposed by saying that the prior over\\nthe latent variable should be a spherical Gaussian distribution. In other words,\\nthis is also the effect of regularization due to the prior distribution.\\nBy cycling through these steps of updating µn’s, W and b, the variational\\nlowerbound will improve gradually until convergence. In the limit of σ2 →∞\\nand with the constraints that W is orthogonal, i.e., WW ⊤= I and that\\nb = 1\\nN\\nPN\\nn=1 xn, we recover principal component analysis [Hotelling, 1933]. Our\\nderivation here is a special case of a more general version called probabilis-\\ntic principal component analysis [Tipping and Bishop, 1999]. In particular, we\\nfollow the variational inference approach [Ilin and Raiko, 2010].\\nThis variational lowerbound based approach again enables us to use stochas-\\ntic gradient descent which is much more scalable than the exact EM procedure.\\nAt each iteration, we pick a minibatch of training examples, infer the (approxi-\\nmate) posterior means and use them to compute the gradient of the variational\\nlowerbound w.r.t. W and b . Instead of computing the optimal values given this\\nminibatch, we simply update them slightly following the stochastic gradient\\ndirection.\\n4.3.1\\nVariational autoencoders\\nA natural question, based on what we have already seen in §2.2.2, is whether\\nwe can use a nonlinear transformation for p(x|z) instead of the linear one in\\nEq. (4.38). This is totally possible with\\np(x|z; θ) = N(x|F(z; θ), I|x|),\\n(4.58)\\nwhere F is an arbitrary nonlinear function, parametrized by θ, that maps z to\\nx and is differentiable w.r.t. θ. In other words, we are okay with any kind of\\nparametrization as long as we can compute3\\nJacθF(z; θ) = ∂F\\n∂θ (z; θ).\\n(4.59)\\nThis small change has a big consequence in terms of the modeling power of\\nthe continuous latent variable model. This is due to the peculiar (and amazing)\\nproperties of normal distributions. Let us revisit the linear case above (4.38):\\np(z) = N(z; 0, σ2I|z|)\\n(4.60)\\np(x|z; θ) = N(x; Wz + b, I|x|),\\n(4.61)\\n3I will use\\n∂\\n∂x notation to refer to the Jacobian matrix unless it is confusing.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 63}, page_content='60CHAPTER 4. PROBABILISTIC MACHINE LEARNING AND UNSUPERVISED LEARNING\\nThen, the joint probability can be written down as\\nlog p(x, z; θ) = log p(z) + log p(x|z; θ) = −1\\n2σ2 ∥z∥2 −1\\n2∥x −Wz −b∥2 + const.\\n(4.62)\\n= −1\\n2\\n\\x00z⊤(σ−2I)z + (x −b)⊤I(x −b) + z⊤W ⊤Wz −(x −b)⊤Wz −zW ⊤(x −b)\\n\\x01\\n+ const.\\n(4.63)\\n= −1\\n2\\n\\x00(x −b)⊤I(x −b) + z⊤(W ⊤W + σ−2I)z −(x −b)⊤Wz −z⊤W ⊤z\\n\\x01\\n+ const.\\n(4.64)\\nLet v = [x, z]⊤and µ = [b, 0|z|]⊤. Then,\\nlog p(x, z; θ) −log Z(θ) = −1\\n2\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed(v −µ)⊤\\n\\x14\\nI\\n−W\\n−W ⊤\\nW ⊤W + σ−2I\\n\\x15\\n|\\n{z\\n}\\n=Σ−1\\n(v −µ)\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8+ const.\\n(4.65)\\nThis shows that the joint distribution over [x, z] is also Gaussian with the mean\\nµ. Although we just needed to show this for our further argument, let us also\\ncheck the covariance matrix of the joint distribution.\\nThere is a magical formula called the block matrix inversion lemma:\\n\\x14A\\nB\\nC\\nD\\n\\x15−1\\n=\\n\\x14A−1 + A−1B(D −CA−1B)−1CA−1\\n−A−1B(D −CA−1B)−1\\n−(D −CA−1B)−1CA−1\\n(D −CA−1B)−1\\n\\x15\\n.\\n(4.66)\\nWe can use this to write down the covariance of the joint distribution p(x, z; θ):\\nΣ =\\n\\x14I + W(W ⊤W + σ−2I −W ⊤W)−1W ⊤\\nW(W ⊤W + σ−2I −W ⊤W)−1\\n(W ⊤W + σ−2I −W ⊤W)−1W ⊤\\n(W ⊤W + σ−2I −W ⊤W)−1\\n\\x15\\n(4.67)\\n=\\n\\x14I + σ2WW ⊤\\nσ2W\\nσ2W ⊤\\nσ2I\\n\\x15\\n.\\n(4.68)\\nBecause the marginal distribution over any subset of dimensions of a normal\\nrandom variable is also normal, we know that the marginal distribution over x,\\ni.e., p(x) =\\nR\\np(x|z)p(z)dz, is also normal. In other words, the linear relationship\\nbetween x and z in the probabilistic principal component analysis (PCA) above\\ncan only represent a Gaussian distribution over x. This is a critical limitation.\\nSuch a limitation does not hold anymore if we use a nonlinear function\\nto model the relationship between x and z. In that case, the joint distribution\\np(x, z) would not be in general Gaussian, because the covariance structure would'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 64}, page_content='4.3. CONTINUOUS LATENT VARIABLE MODELS\\n61\\nnot be stationary but change dynamically depending on x and z. We would\\nrather get a mixture of Gaussians with an infinitely many components:\\np(x) =\\nZ\\np(x|z)p(z)dz =\\nZ\\np(z)N(x|F(z; θ), I)dz.\\n(4.69)\\nLet’s consider the variational lowerbound with this nonlinear formulation for\\none particular example xn:\\nJn = Ez∼qn\\n\\x14\\n−1\\n2∥xn −F(z; θ)∥2 −|x|\\n2 log 2π\\n\\x15\\n−1\\n2\\n\\x14K + ∥µn∥2\\nσ2\\n−K + 2K ln(σ)\\n\\x15\\n.\\n(4.70)\\nThe gradient of Jn w.r.t. µn is then\\n∇µn = −\\nZ exp\\n\\x00−1\\n2∥z −µn∥2\\x01\\n(2π)|z|/2\\n1\\n2(z −µn)∥xn −F(z; θ)∥2 −µn\\nσ2\\n(4.71)\\n= −1\\n2Ez\\n\\x02\\n(z −µn)∥xn −F(z; θ)∥2\\x03\\n−µn\\nσ2\\n(4.72)\\n= −1\\n2\\n\\x002xnEz [zF(z; θ)] −2xnµnEz [F(z; θ)] + Ez\\n\\x02\\nz∥F(z; θ)∥2\\x03\\n−µnEz\\n\\x02\\n∥F(z; θ)∥2\\x03\\x01\\n−µn\\nσ2 .\\n(4.73)\\nIt is clear that without knowing the form of F, it is not possible to come up with\\nan analytical solution to µn in general. Even worse, it is unclear how to compute\\nthe gradient analytically either, due to the challenging expectations that must\\nbe computed. We can however use sampled-based Monte Carlo approximation,\\nsince we can choose the approximate posterior q to be readily samplable:\\n∇µn ≈˜∇µn = −1\\n2(˜z −µn)∥xn −F(˜z; θ)∥2 −µn\\nσ2 .\\n(4.74)\\nIn this particular case of Gaussian posterior, we can draw a sample using a\\nreparametrization trick:4\\n˜z = µn + σϵ,\\n(4.76)\\nwhere ϵ ∼N(0, I|z|). Plugging this in, we get\\n˜∇µn = −σ\\n2 ϵ∥xn −F(µn + σϵ; θ)∥2 −µn\\nσ2 .\\n(4.77)\\n4A reparametrization trick refers to formulating the process of sampling from a particu-\\nlar distribution as nonlinearly and deterministically transforming noise drawn from another\\ndistribution:\\nz = g(ϵ; ϕ),\\nϵ ∼p(ϵ).\\n(4.75)\\nThis allows us to compute the derivative of the sample z w.r.t. the parameters of this de-\\nterministic function, i.e.,\\n∂g\\n∂ϕ(z). This is a handy trick, since sampling is often considered a\\nnon-differentiable operator. Despite its usefulness, it is not always possible to come up with\\nsuch reparametrization.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 65}, page_content='62CHAPTER 4. PROBABILISTIC MACHINE LEARNING AND UNSUPERVISED LEARNING\\nWe can then use this stochastic gradient estimate to find the solution to µn.\\nLooking at the gradient above, we can however see what this gradient direc-\\ntion points at. Particularly, the first term considers directions that are similar\\nto the mean µn but with some noise. We then weigh each such direction (or the\\ndifference between this direction and the current estimate of µn) by their qual-\\nity, where the quality is defined as how similar the decoded observation, F(z; θ),\\nis to the actual observation xn (notice the negative sign that turns this distance\\ninto the quality.) In other words, we look for the change to µn that makes the\\ndecoded observation closer to the actual observation. This makes perfect sense,\\nfrom the perspective of p(y|z). The second term simply brings µn toward the\\norigin at the rate inversely proportional to the prior variance which is inversely\\nproportional to the regularization strength.\\nThis lack of an analytical solution to µn is problematic, because we must\\nkeep µn for all N training examples across E-M iterations, even with stochas-\\ntic gradient descent. At each iteration, we would select a small number M of\\ntraining examples, retrieve the associated observations {xm}M\\nm=1 as well as the\\nassociated current estimate of the posterior means {µm}M\\nm=1, update the pos-\\nterior means slightly following the gradient direction, update the parameters\\nslightly following the stochastic gradient direction, and finally store back the\\nupdated estimate of the posterior means. This does not put too much pressure\\non computation but it puts a huge pressure on the storage and I/O, as we need\\nO(b × |z| × N) bits to store the posterior means.\\nAmortized inference.\\nInstead of storing the approximate posterior means\\nfor all the training examples, we can compress them into a powerful deep neural\\nnetwork. Let G : X →R|z| be an inference network, as we will ask G to ap-\\nproximately infer the posterior over the latent variable given an input x. This\\nG is also parametrized by its own set of parameters θG. This inference network\\neffectively work as a compressed version of the table containing {µm}M\\nm=1, since\\nwe can retrieve µm by\\nµm = G(xm; θG).\\n(4.78)\\nIn fact, this inference network even allows us to retrieve an approximate poste-\\nrior distribution given a novel input x′ /∈D thanks to its generalization capa-\\nbility.\\nLet us now plug this inference network G into the per-instance objective\\nfunction from Eq. (4.70):\\nJn = Ez∼qn(z;G(xn;θG),σ2)\\n\\x14\\n−1\\n2∥xn −F(z; θ)∥2 −|x|\\n2 log 2π\\n\\x15\\n(4.79)\\n−1\\n2\\n\\x14K + ∥G(xn; θG)∥2\\nσ2\\n−K + 2K ln(σ)\\n\\x15\\n.\\n(4.80)\\nBecause of the expectation is difficult to evaluate, we will consider a single-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 66}, page_content='4.3. CONTINUOUS LATENT VARIABLE MODELS\\n63\\nsample estimate of Jn:\\n˜Jn = −1\\n2∥xn −F(G(xn; θG) + σϵ; θ)∥2 −\\n1\\n2σ2 ∥G(xn; θG)∥2 + const.,\\n(4.81)\\nwhere ϵ ∼N(0, I).\\nThere are two non-constant terms in this approximate objective. The first\\nterm is the reconstruction error. The input xn is processed by the inference\\nnetwork G first, and then the noisy version of the output of G is then processed\\nby F to reconstruct the input. The objective is maximized when the difference\\nbetween the original input and the reconstructed input is minimized (see the\\nnegation in front of the L2 norm.) This process is often referred as autoencoding,\\nand this is why this whole framework is called a variational autoencoder [Kingma\\nand Welling, 2013].\\nThe second term is a regularizer that pushes the L2 norm of the output from\\nthe inference network to be small. This ensures that all the inputs {x1, . . . , xM}\\nare mapped to the latent space, i.e. the space of the latent variable z, as tightly\\nas possible. Without this term, the norm of the output of G can grow indefinitely,\\npushing the inferred posteriors of all the inputs to be as far away as possible,\\nsince this would ensure that F can reconstruct the original input perfectly even\\nwith the injected noise. This would however make it impossible for F to cope\\nwith any z sampled from the prior or located between any pair of inputs’ inferred\\nposterior distributions, resulting in a lousy generative model.\\nThanks to the reparametrization trick, we can compute the gradient of ˜Jn\\nw.r.t. all parameters, including those of F and those of G. In other words, we\\ncan use backpropagation to train both the inference and generation networks,\\nG and F, respectively. This allows us to train the inference network extremely\\nefficiently, without having to maintain a whole database of instance-specific ap-\\nproximate posterior parameters. Furthermore, as discussed earlier, this inference\\nnetwork can be used with a novel input, making it useful for analyzing a set\\nof inputs that are not present during training. The approximate posterior com-\\nputed by the inference network can be further finetuned using gradient descent\\nto match the true posterior better [Hjelm et al., 2016].\\nPerhaps more importantly, this implies that such end-to-end learning of in-\\nference and generation networks is possible with backpropagation and stochastic\\ngradient descent as long as we can use a reparametrization trick to sample from\\nan approximate posterior without breaking the differentiability. This opens wide\\na door to a whole new set of opportunities to scale up various probabilistic mod-\\nels that were cumbersome to derive and use before, although these are out of\\nthe scope of this course.\\n4.3.2\\nImportance sampling and its variance.\\nBefore ending this section, let us think of how we should compute the log-\\nmarginal probability of an observation x from Eq. (4.69):\\np(x) = Ez∼p(z) [p(x|z)] .\\n(4.82)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 67}, page_content='64CHAPTER 4. PROBABILISTIC MACHINE LEARNING AND UNSUPERVISED LEARNING\\nUnlike in the training time, we are less under time pressure, and therefore a\\nnatural approach would be a naive Monte-Carlo approximation:\\np(x) ≈1\\nM\\nM\\nX\\nm=1\\np(x|zm),\\n(4.83)\\nwhere zm ∼pz(z).\\nUnfortunately this naive approach can have a large variance. For brevity, let\\nf(z) = p(x|z) and p(z) = pz(z). Because we already know that it is unbiased,\\nwe can then write the variance as\\nV\\n\"\\n1\\nM\\nM\\nX\\nm=1\\nf(zm)\\n#\\n=\\n1\\nM 2 V\\n\" M\\nX\\nm=1\\nf(zm)\\n#\\n=\\n1\\nM 2\\nM\\nX\\nm=1\\nV [f(zm)] =\\n1\\nM 2 MV [f(z)] = V [f(z)]\\nM\\n,\\n(4.84)\\nbecause zm’s are identically distributed according to p(x).\\nIt turned out that we can reduce this variance by avoiding sampling from pz\\ndirectly but from another distribution qz. This technique is called importance\\nsampling:\\nEz∼pz [f(z)] = Ez∼qz\\n\\x14pz(z)\\nqz(z)f(z)\\n\\x15\\n≈1\\nM\\nM\\nX\\nm=1\\npz(zm)\\nqz(zm)f(zm).\\n(4.85)\\nWe can then control the variance of this estimator by choosing qz carefully.\\nTo understand how we can choose qz carefully, consider the variance of this\\nestimator:\\nV\\n\"\\n1\\nM\\nM\\nX\\nm=1\\npz(zm)\\nqz(zm)f(zm)\\n#\\n= 1\\nM V\\n\\x14pz(zm)\\nqz(zm)f(zm)\\n\\x15\\n.\\n(4.86)\\nLet’s plug p(x|z) and pz(z) back in:\\n1\\nM V\\n\\x14pz(zm)\\nqz(zm)f(zm)\\n\\x15\\n= 1\\nM V\\n\\x14p(x|z)p(z)\\nq(z)\\n\\x15\\n.\\n(4.87)\\nBy using V[X] = E[X2] −E[X]2, we get\\nV\\n\\x14p(x|z)p(z)\\nq(z)\\n\\x15\\n=\\nZ p(x|z)2pz(z|)2\\nq(z)\\ndz −const.\\n(4.88)\\nThe second term is constant w.r.t. q, because that is nothing but the original\\nquantity we are trying to approximate.\\nRecall the following definition of Cauchy-Schwarz inequality:\\n|⟨u, v⟩|2 ≤⟨u, u⟩⟨v, v⟩,\\n(4.89)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 68}, page_content='4.3. CONTINUOUS LATENT VARIABLE MODELS\\n65\\nwhere ⟨·, ·⟩is an inner product that generalizes a standard dot product. We can\\ndefine an inner product on the square-integrable functions5 as\\n⟨f, g⟩=\\nZ\\nf(x)g(x)dx\\n(4.91)\\nover the domain of x. Then, we can write the Cauchy-Schwarz inequality as\\nZ\\nf(x)g(x)dx ≤\\nZ\\nf 2(x)dx\\nZ\\ng2(x)dx.\\n(4.92)\\nBecause\\nR\\nq(z)dz = 1 by definition, we observe that\\n\\uf8eb\\n\\uf8ed\\nZ  \\np(x|z)pz(z)\\np\\nq(z)\\n!2\\ndz\\n\\uf8f6\\n\\uf8f8\\n\\x12Z\\nq(z)dz\\n\\x13\\n|\\n{z\\n}\\n=1\\n≥\\n Z p(x|z)pz(z)\\np\\nq(z)\\np\\nq(z)dz\\n!2\\n=\\n\\x12Z\\np(x|z)pz(z)dz\\n\\x132\\n.\\n(4.93)\\nConsidering both sides carefully, we see that they are equal when\\nCq(z) = p(x|z)pz(z).\\n(4.94)\\nThis is easy to check by plugging it into the left hand side of the inequality\\nabove:\\n\\uf8eb\\n\\uf8ed\\nZ  \\nCq(z)\\np\\nq(z)\\n!2\\ndz\\n\\uf8f6\\n\\uf8f8\\n\\x12Z\\nq(z)dz\\n\\x13\\n= C2,\\n(4.95)\\nand then into the right hand side of the inequality:\\n\\x12Z\\nCq(z)dz\\n\\x132\\n= C2.\\n(4.96)\\nBecause\\nR\\nq(z)dz = 1,\\nC =\\nZ\\np(x|z)pz(z)dz.\\n(4.97)\\nPutting them all together, we get the following optimal q:\\nq∗(z) =\\np(x|z)pz(z)\\nR\\np(x|z′)pz(z′)dz′ ,\\n(4.98)\\n5A function f is square-integrable when\\nZ\\nf(x)dx < ∞.\\n(4.90)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 69}, page_content='66CHAPTER 4. PROBABILISTIC MACHINE LEARNING AND UNSUPERVISED LEARNING\\nwhich turned out to be exactly the posterior distribution over z given x. In other\\nwords, if we sample from the posterior distribution instead of the prior distri-\\nbution and reweigh p(x|z) according to their ratio\\np(z)\\np(z|x), our approximation is\\nboth unbiased and has the minimal variance.\\nThis is however not the right way forward, since the posterior probability\\nhas in its own denominator the intractable integral. Rather, this says that the\\nso-called proposal distribution q must be close to the true posterior distribution\\np(z|x), which is in fact exactly the criterion we used to derive the variational\\nlowerbound earlier in §4.2. When the variational lowerbound, which serves as the\\nobjective function for latent-variable models, is maximized, the KL divergence\\nbetween q (the approximate posterior) and p (the true posterior) shrinks. In\\nother words, we can simply use the trained q inference network as the proposal\\ndistribution to approximate the log-marginal probability of an observation x\\nafter training to obtain an unbiased, low-variance estimator of the quantity.6 It\\nturned out maximizing variational inference had yet another advantage.\\n6The variational lowerbound can be used as a proxy to the log-marginal probability as well.\\nThis is indeed a standard practice during training, to monitor the progress of learning. This\\nquantity is however a biased estimate of the log-marginal probability, and it is important to\\nuse importance sampling to check the true log-marginal probability.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 70}, page_content='Chapter 5\\nUndirected Generative\\nModels\\nWe have studied a few different approaches to generative modeling in the pre-\\nvious chapter. These approaches can be thought of as fitting a directed graph-\\nical model where there are two variables, the observation x and the latent z.\\nThese two variables are connected by a directed edge going from z to x. In\\nthis model, we defined two relatively simple, or perhaps more correctly rela-\\ntively easily-described, distributions, p(z) and p(x|z) but were able to model a\\ncomplicated distribution over the observation by the process of marginalization,\\np(x) =\\nR\\np(z)p(x|z)dz. Now, we must ask whether there are other ways to do\\nthe same.\\n5.1\\nRestricted Boltzmann machines: the Prod-\\nuct of Experts\\nWe begin with a pretty old idea called restricted Boltzmann machines [RBM;\\nSmolensky, 1986]. An RBM defines a bipartite graph with undirected edges\\nbetween two groups; x and z. Each partition consists of the dimensions of the\\nobservation x or the latent z . These partitions are fully connected with each\\nother, but there is no edges within each partition. Each edge has a weight\\nvalue, resulting in a matrix W ∈R|x|×|z| . Each node also has its own scalar\\nbias, resulting in two vectors b ∈R|x| and c ∈R|z|. We then define an energy\\nfunction as\\ne(x, z, θ = (W, b, c)) = −x⊤Wc −x⊤b −z⊤c\\n(5.1)\\n= −\\n|x|\\nX\\ni=1\\n|z|\\nX\\nj=1\\nwijxizj −\\n|x|\\nX\\ni=1\\nxibi −\\n|z|\\nX\\nj=1\\nzjcj.\\n(5.2)\\n67'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 71}, page_content='68\\nCHAPTER 5. UNDIRECTED GENERATIVE MODELS\\nAlthough it is not necessary for x, we restrict z to be a binary vector: z ∈\\n{0, 1}|z|.\\nAs we have done over and over so far, we can turn this energy function into\\nthe joint probability function:\\nlog p(x, z; θ) = −e(x, z, θ) −log\\nZ\\nx′∈X\\nX\\nz′∈{0,1}|z|\\nexp (−e(x′, z′, θ)) dx′,\\n(5.3)\\nwhere X is a set of all possible values x can take. If X is a finite set, we replace\\nR\\nwith P.\\nLet us focus on the normalization constant, P\\nz′∈{0,1}|z| exp(−e(x, z′, θ)).\\nSince\\nexp(a + b) = exp(a) exp(b),\\n(5.4)\\nwe can rewrite it into\\nexp(−e(x, z, θ)) =\\n\\uf8eb\\n\\uf8ed\\n|x|\\nY\\ni=1\\n|z|\\nY\\nj=1\\nexp(wijxizj)\\n\\uf8f6\\n\\uf8f8\\n\\uf8eb\\n\\uf8ed\\n|x|\\nY\\ni=1\\nexp(xibi)\\n\\uf8f6\\n\\uf8f8\\n\\uf8eb\\n\\uf8ed\\n|z|\\nY\\nj=1\\nexp(zjcj)\\n\\uf8f6\\n\\uf8f8\\n(5.5)\\n=\\n|x|\\nY\\ni=1\\nexp(xibi)\\n|z|\\nY\\nj=1\\nexp(wijxizj + zjcj).\\n(5.6)\\nNow, I want to marginalize out z from this expression. In most cases, this\\nwould be intractable, because there are 2|z| possible values z can take. This\\nbipartite structure however turned out to be a blessing we can rely on.\\nLet’s consider the following simple case:\\nX\\nz∈{0,1}2\\n2\\nY\\nj=1\\nfj(zj) = f1(0)f2(0) + f1(0)f2(1) + f1(1)f2(0) + f1(1)f2(1)\\n(5.7)\\n= f1(0)(f2(0) + f2(1)) + f1(1)(f2(0) + f2(1))\\n(5.8)\\n= (f1(0) + f1(1))(f2(0) + f2(1))\\n(5.9)\\n=\\n2\\nY\\nj=1\\n(fj(0) + fj(1)).\\n(5.10)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 72}, page_content='5.1. RESTRICTED BOLTZMANN MACHINES: THE PRODUCT OF EXPERTS69\\nInstead of summing exponentially many terms, we can multiply |z| terms only:\\nX\\nz∈{0,1}|z|\\nexp(−e(x, z, θ)) =\\nX\\nz∈{0,1}|z|\\n|x|\\nY\\ni=1\\nexp(xibi)\\n|z|\\nY\\nj=1\\nexp(wijxizj + zjcj)\\n(5.11)\\n=\\n|x|\\nY\\ni=1\\nexp(xibi)\\nX\\nz∈{0,1}|z|\\n|z|\\nY\\nj=1\\nexp(wijxizj + zjcj)\\n(5.12)\\n= exp\\n\\uf8eb\\n\\uf8ed\\n|x|\\nX\\ni=1\\nxibi\\n\\uf8f6\\n\\uf8f8\\n|z|\\nY\\nj=1\\n(1 + exp(wijxi + cj))\\n(5.13)\\n(5.14)\\nYou can think of the left-hand side of this derivation as the unnormalized\\nprobability function ˜p(x; θ) of x, since the normalization constant of p(x, z; θ) is\\nneither a function of x nor z. In that case, we can write it down as\\n˜p(x; θ) ∝ϕ0(x)\\n|z|\\nY\\nj=1\\nϕj(x),\\n(5.15)\\nwhere\\nlog ϕ0(x) = x⊤b,\\n(5.16)\\nlog ϕj(x) = log(1 + exp(w⊤\\n·jx + cj)).\\n(5.17)\\nWe call each ϕk an expert, and this is a typical formulation of a product of\\nexperts [PoE; Hinton, 2002].\\nPoE’s are unlike a mixture of experts (MoE), such as a mixture of Gaus-\\nsians from §4.2.1. MoE’s have a significant advantage over PoE’s in that they are\\nreadily normalized as long as each and every expert is well-normalized. PoE’s\\nhowever can model a much sharper distribution, unlike MoE’s. The entropy of a\\nMoE is always lowerbounded by the entropy of an individual component. This\\nis not the case with a PoE, because the scores from the experts are multiplied\\nrather than averaged. It is possible for any one expert to simply veto by out-\\nputing a value close to 0, while this wouldn’t affect the overall outcome in the\\ncase of an MoE.\\nWe use the log-likelihood objective, averaged over the whole training set, for\\ntraining this RBM:\\nLll(x, θ) = e(x, θ) + log\\nZ\\nexp (−e(x′, θ)) dx′.\\n(5.18)\\nJust like earlier, we use stochastic gradient descent, and to do so, we need to\\nbe able to compute the gradient of this per-example loss w.r.t. the energy e.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 73}, page_content='70\\nCHAPTER 5. UNDIRECTED GENERATIVE MODELS\\nOnce we can compute it, we can use the chain rule of derivatives to compute\\nthe gradient w.r.t. each parameter. So,\\n∇θLll =∇θe(x, θ) −\\nZ\\nexp(−e(x′, θ))\\nR\\nexp(−e(x′′, θ))dx′′\\n|\\n{z\\n}\\n=p(x′;θ)\\n∇θe(x′, θ)dx′\\n(5.19)\\n= ∇θe(x, θ)\\n|\\n{z\\n}\\n=(a)\\n−Ex′;θ∇e(x′, θ)\\n|\\n{z\\n}\\n=(b)\\n.\\n(5.20)\\nThere are two terms in this gradient. The first term (a) is called a positive\\nphase, since it proactively decreases (recall that we are taking the negative gra-\\ndient direction) the energy of the positive example, where the positive example\\nrefers to one of the training examples x from the training set. The second term\\n(b) is called a negative phase, where it proactively increases the energy of a\\nconfiguration x′ that is highly probable under the current model, i.e. p(x′; θ) ↑.\\nThis is exactly what we saw earlier when we learned about the cross-entropy\\nloss for classification in §2.1.2.\\nUnlike the cross entropy with softmax earlier, we are in a worse situation\\nhere, because the number of possible values x can take is much greater. In fact,\\nit is exponentially larger, since we often use RBMs or any of these generative\\nmodels to model a distribution over a high-dimensional space. In other words, we\\ncannot compute the negative phase (b) exactly in a tractable time, or sometimes\\nwe just do not know how to compute it at all.\\nIn the remainder of this section, we study how we can efficiently draw these\\nnegative samples and use them for learning.\\n5.1.1\\nMarkov Chain Monte Carlo (MCMC) Sampling\\nLet’s imagine that we want to draw a set of samples from a complicated target\\ndistribution p∗(x). It would be great if we could draw samples independently in\\nparallel, but this is often impossible. Rather, we need to come up with a way to\\ndraw a series of samples such that collectively they form a set of independent\\nsamples from the target distribution. How would we do this?\\nWe do so by defining a Markov chain (X, p0, T ), where X is the set of all\\npossible observations (i.e. the state space), p0 is the initial distribution over X,\\nand T is a transition operator. The transition operator is really nothing but a\\nconditional distribution over X given a sample from X, i.e., T (x|x′). We can\\ndraw a series of observations (x1, x2, . . .) by repeatedly sampling xt ∼T (x|xt−1)\\nwith x0 ∼p0(x). Eventually, that is, the latter part of this series of repeated\\nsampling, we want those samples to be drawn from the target distribution p∗(x).\\nIn other words, we want a stationary distribution p∞, which is the normalized\\ncumulative visit counts for all states and satisfies\\np∞= T p∞,\\n(5.21)\\nto match p∗. Once we converge to the stationary distribution, which matches\\nthe target distribution, we can simply apply the transition operator repeatedly'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 74}, page_content='5.1. RESTRICTED BOLTZMANN MACHINES: THE PRODUCT OF EXPERTS71\\nand be convinced that the collected series of samples form collectively a set of\\nsamples from the target distribution.\\nIn addition to this condition (p∞= p∗), we need to meet an extra condi-\\ntion. That is, this stationary distribution has to be unique. If there are other\\nstationary distributions, we may not be able to tell that even after running\\nthis transition operator indefinitely that we are collecting samples from the true\\ndistribution. To do so, we further put a constraint that this Markov chain is\\nergodic. In an ergodic Markov chain, any state (or a region of the state space, in\\nthe case of an infinitely large X) is reachable from any other state within a finite\\nnumber of transition steps. This ergodicity guarantees that there is only one sta-\\ntionary distribution, and that repeated applications of the transition operator\\nwill eventually converge toward this unique stationary distribution.\\nSampling from a complicated target distribution p∗then boils down to de-\\nsigning a transition operator T such that the resulting Markov chain has a\\nunique stationary distribution. The next question is how we can guarantee that\\nthere exists a stationary distribution, since the ergodicity tells us that there is\\na unique stationary distribution if there is a stationary distribution under this\\nMarkov chain. There are more than one way to do so, and one relatively well-\\nknown way is the principle of detailed balance. Detailed balance in a Markov\\nchain is defined as having the transition operator T satisfy\\nT (x′|x)p∞(x) = T (x|x′)p∞(x′).\\n(5.22)\\nAs pretty clear from the equation, it says that whatever flows from one state\\nto another must flow back. This is stronger than having a stationary distribution,\\nas a stationary distribution p∞may not satisfy this. When detailed balance is\\nsatisfied, we often refer to such a Markov chain as a reversible Markov chain,\\nsince we will not be able to tell the direction of time once it converged.\\nOur goal is then to design a transition operator T such that the resulting\\nMarkov chain is ergodic and satisfies detailed balance.1 We refer to the procedure\\nof sampling by collecting a series of visited states from such a Markov chain by\\nMarkov Chain Monte Carlo (MCMC) methods.\\nOne of the most popular and widely-used MCMC algorithm is Metropolis-\\nHastings (M-H) algorithm [Hastings, 1970]. The M-H algorithm assumes that\\nwe have access to the unnormalized probability ˜p∗(x) of the target distribution:\\np∗(x) =\\n˜p∗(x)\\nR\\n˜p∗(x)dx.\\n(5.23)\\nThis assumption makes the M-H algorithm particularly suitable for many energy-\\nbased models, such as restricted Boltzmann machines (RBM), since we can easily\\noften the unnormalized probability but cannot tractably compute the normal-\\nization constant.\\n1This statement does not exclude the possibility of designing a Markov chain that allows\\nus to sample from a target distribution even when it does not satisfy detailed balance. Fur-\\nthermore, this statement does not exclude the possibility of expanding the state space by\\naugmenting x with an extra variable. It has been shown that this can be beneficial with\\nso-called Hamiltonian Monte Carlos methods [Neal, 1993].'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 75}, page_content='72\\nCHAPTER 5. UNDIRECTED GENERATIVE MODELS\\nWe first assume we are given (or can create) a proposal distribution q(x|x′)\\nthat is often centered at x′ and whose probability mass is largely concentrated\\nin the neighbourhood of x′. q must be ergodic, that is, if we repeatedly sample\\nfrom q(x|x′), we should be able to reach any state (or a region of the state space)\\nwithin a finite number of steps. We then define an acceptance probability α(x|x′)\\nsuch that\\nα(x|x′) = min\\n\\x12\\n1, ˜p∗(x)q(x′|x)\\n˜p∗(x′)q(x|x′)\\n\\x13\\n(5.24)\\nThen, the transition operator is\\nT (x|x′) = α(x|x′)q(x|x′) + (1 −α(x|x′))δx′(x),\\n(5.25)\\nwhere\\nδx′(x) =\\n(\\n∞,\\nif x = x′\\n0,\\notherwise\\n(5.26)\\nand\\nZ\\nδx′(x)dx = 1.\\n(5.27)\\nWe can sample from this transition operator given the past sample x′ by\\n(1) ˜x ∼q(x|x′)\\n(Candidate generation)\\n(5.28)\\n(2) ˜u ∼U[0, 1]\\n(Random draw)\\n(5.29)\\n(3) x =\\n(\\n˜x,\\nif ˜u ≤α(˜x|x′)\\nx′,\\notherwise\\n(Acceptance)\\n(5.30)\\nThis transition operator satisfies both ergodicity and detailed balance, and\\na lot of MCMC algorithms can be viewed as variants of the M-H algorithm with\\nparticular choices of the proposal distribution q.\\nGibbs Sampling.\\nLet’s assume that x is a finite-dimensional vector. We can\\nthen define a conditional probability over one particular dimension d given all\\nthe other dimensions ̸= d as\\npd(xd|x′\\n1,...,d−1,d+1,...,|x|) =\\np([x′\\n1, . . . , x′\\nd−1, x, x′\\nd+1, . . . , x|x|])\\nR\\np([x′\\n1, . . . , x′\\nd−1, ˜x, x′\\nd+1, . . . , x|x|])d˜x.\\n(5.31)\\nAssume d follows a uniform distribution, i.e. d ∼U {1, 2, . . . , |x|} and we start\\nfrom x′ = [x′\\n1, . . . , x′\\n|x|]. We now replace the d-th dimension of x by sampling\\nfrom the conditional distribution pd, resulting in x = [x′\\n1, . . . , x′\\nd−1, ˜xd, x′\\nd+1, . . . , x′\\n|x|].'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 76}, page_content='5.1. RESTRICTED BOLTZMANN MACHINES: THE PRODUCT OF EXPERTS73\\nIn order to compute the acceptance probability, we must compute\\n˜p∗(x)pd(x′|x)\\n˜p∗(x′)pd(x|x′) =\\n˜p∗([x′\\n1, . . . , x′\\nd−1, xd, x′\\nd+1, . . . , x′\\n|x|])pd(x′\\nd|[x′\\n1, . . . , x′\\nd−1, x′\\nd+1, . . . , x′\\n|x|])\\n˜p∗([x′\\n1, . . . , x′\\n|x|])pd(xd|[x′\\n1, . . . , x′\\nd−1, x′\\nd+1, . . . , x′\\n|x|])\\n(5.32)\\n= ((((((((((((((((\\n˜p∗([x′\\n1, . . . , x′\\nd−1, xd, x′\\nd+1, . . . , x′\\n|x|])\\n((((((((((((((((\\n˜p∗([x′\\n1, . . . , x′\\nd−1, x′\\nd, x′\\nd+1, . . . , x′\\n|x|])\\n(((((((((((((\\nC([x′\\n1, . . . , x′\\nd−1, x′\\nd+1, . . . , x′\\n|\\n((((((((\\n˜p∗([x′\\n1, . . . , x′\\n|x|])\\n((((((((((((((((\\n˜p∗([x′\\n1, . . . , x′\\nd−1, xd, x′\\nd+1, . . . , x′\\n|x|])\\n((((((((((((((\\nC([x′\\n1, . . . , x′\\nd−1, x′\\nd+1, . . . , x′\\n|x|])\\n(5.33)\\n= 1\\n(5.34)\\nIn other words, the acceptance probability is 1, and we always accept this new\\nsample which differs from the previous sample in just one dimension d.\\nThis procedure is called Gibbs sampling. We pick one coordinate, sample\\nfrom the conditional distribution of that particular coordinate, replace it with\\nthe newly sampled coordinate value and repeat it. This procedure is often ap-\\nplicable even when we have access only to the unnormalized probability, since\\nthe conditional probability is often tractable in that case. Furthermore, because\\nevery sample is automatically accepted, there is almost no extra overhead in\\nimplementation, which makes it an attractive algorithm choice.\\nVariational inference would not work.\\nBased on what we have learned\\nin §4.2, one may wonder whether we could use variational inference instead of\\nMCMC sampling. The answer is unfortunately no. The core idea of variational\\ninference is to approximate a complex target distribution (the posterior distri-\\nbution in §4.2, and here the target distribution p∗) with a simpler distribution\\nq by minimizing\\nKL(q∥p∗) = −Ex∼q [log ˜p∗(x)] + log\\nZ\\n˜p∗(x)dx\\n|\\n{z\\n}\\nconst. w.r.t. q\\n+H(q).\\n(5.35)\\nIf we focus on the first term of the KL divergence, we observe that we only\\ncare about the region of the observation space where q is high. That is, the KL\\ndivergence only cares about the highly probable regions under q and ignores any\\nother regions that are highly probable under p∗but not under q. In other words,\\nsamples we draw from q after minimizing the KL divergence above would not be\\nrepresentative of p∗, because they will largely miss high probable regions under\\np∗.\\nThis issue disappears as the complexity of q increases and approaches that\\nof p∗. This however comes with the very issue we want to solve; that is, we must\\nsample from this equally complex q in order to approximately compute and\\nminimize the KL divergence above. Later in this chapter, we consider directly\\nbuilding a sampler so that an implicitly defined q is both complex enough and\\napproximately minimizes the KL divergence above.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 77}, page_content='74\\nCHAPTER 5. UNDIRECTED GENERATIVE MODELS\\n5.1.2\\n(Persistent) Contrastive Divergence\\nWe need to sample from p(x; θ), to train an RBM. One way to produce a set of\\nsamples from p(x; θ) is to draw a set of (x, z) samples from p(x, z; θ) and discard\\nz from each pair. In doing so, we want to use Gibbs sampling. Let us first try\\nto write down the conditional probability of z given x:\\nlog p(z|x; θ) =\\n|z|\\nX\\nj=1\\nzj\\n\\uf8eb\\n\\uf8ed\\n|x|\\nX\\ni=1\\nwijxi + cj\\n\\uf8f6\\n\\uf8f8+ const.\\n(5.36)\\nThis implies that the z1, . . . , z|z| are conditional independent given x, as\\np(z|x; θ) =\\n|z|\\nY\\nj=1\\nexp\\n\\x00zj\\n\\x00w⊤\\n·jx + cj\\n\\x01\\x01\\nexp (0) + exp\\n\\x00w⊤\\n·jx + cj\\n\\x01.\\n(5.37)\\nWe thus can look at each dimension of z separately:\\np(zj = 1|x; θ) =\\nexp(w⊤\\n·jx + cj)\\n1 + exp(w⊤\\n·jx + cj) =\\n1\\n1 + exp(−w⊤\\n·jx −cj) = σ(w⊤\\n·jx + cj),\\n(5.38)\\nwhere σ is a sigmoid function we saw earlier:\\nσ(a) =\\n1\\n1 + exp(−a).\\n(5.39)\\nSampling all |z| dimensions is embarrassingly parallelizable, since they are\\nconditionally independent. Let’s say we have sampled a new z. We now need to\\nsample a new x given z. Following a similar derivation, we end up with\\np(x|z; θ) =\\n|x|\\nY\\ni=1\\np(xi|z; θ),\\n(5.40)\\nwhere\\np(xi = 1|z; θ) = σ(w⊤\\ni· z + bi).\\n(5.41)\\nIn other words, we can also sample all dimensions of x in parallel as well.\\nWe can then alternate between sampling x and z repeatedly to collect a\\nseries of (x, z) pairs that collectively constitute a set of samples drawn from\\np(x, z; θ). Of course, we want to probably throw away quite a few pairs from the\\nearly stage of sampling, as they have likely been collected before the Markov\\nchain converged. Furthermore, in order to avoid the potentially slowly mixing\\nrate of the Markov chain, we might want to use only every k-th sample. This\\nstrategy is often referred to as thinning.\\nOf course, this does not really help us too much, since we must run a pretty\\nlong chain of Gibbs sampling in order to collect enough independent samples.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 78}, page_content='5.2. ENERGY-BASED GENERATIVE ADVERSARIAL NETWORKS\\n75\\nIf we run it too short, our stochastic gradient estimate will likely be incorrect,\\nresulting in a disastrous outcome.\\nInstead, it turned out that we can simply start the Gibbs sampling chain\\nfrom a positive example, run it only a small number of steps (as few as just one)\\nand use the resulting sample as the negative example. That is,\\n∇θLk(θ; x) = ∇θe(x, θ)\\n|\\n{z\\n}\\n=positive\\n−1\\nS\\nS\\nX\\ns=1\\n∇e(x′\\ns, θ)\\n|\\n{z\\n}\\n=negative\\n,\\n(5.42)\\nwhere x′\\ns is one of the S samples drawn after running k steps of Gibbs sampling\\nstarting from x. It is usual to set S to 1. In the limit of k →∞, this is exact,\\nsince the negative sample x′ would be from the stationary distribution which\\ncoincides with the true distribution p(x; θ). It is however not so with a finite k,\\nand there is not even a guarantee that a larger k leads to a better approximation,\\nwhen k is small. This strategy nevertheless results in a reasonably well trained\\nRBM and is often called contrastive divergence.\\nIt turned out that we can maintain the computational complexity with a\\nminimal overhead in memory complexity by maintaining S samples across mul-\\ntiple stochastic gradient steps while ensuring that learning converges to the\\nexact solution asymptotically. We do so by running S chains of Gibbs sampling\\nin parallel to stochastic gradient descent. Between consecutive steps of SGD,\\nwe run S chains of Gibbs sampling for T ≈1 steps each to update the a set\\nof S samples that are more likely to have been drawn from the latest model.\\nThen, we use these newly updated samples to compute the stochastic gradient\\nestimate, to update the model parameters.\\nAs learning continues, the change to the model parameters slows down (since\\nwe are getting increasingly closer to a local minimum), and thereby Gibbs sam-\\npling chains in the background are increasingly getting closer to the stationary\\ndistribution of the final model. This makes it such that the early stage of learn-\\ning is inexact but has a low variance (because we are not perturbing negative\\nexamples too much) but the later stage is exact since the model parameters\\nchange very slowly. This strategy is called persistent contrastive divergence.\\n5.2\\nEnergy-based generative adversarial networks\\nIt is challenging to draw samples from a complex, high-dimensional distribution\\neven with an advanced MCMC algorithm. Instead, we may want to consider\\ntraining a neural network to draw samples from such a distribution. Any such\\nneural network can be described as\\nx = g(ϵ; θg),\\n(5.43)\\nwhere ϵ ∼p(ϵ) and p(ϵ) is some easy-to-sample distribution of our choice. This\\nsampler is parametrized by θg.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 79}, page_content='76\\nCHAPTER 5. UNDIRECTED GENERATIVE MODELS\\nWe can train this sampler by minimizing the following loss function:\\nLrkl(θg) = KL(pg∥pe) = −Ex∼pg [log pe(x) −log pg(x)]\\n(5.44)\\n= −Ex∼pg [log pe(x)]\\n|\\n{z\\n}\\n=(a)\\n−H(pg)\\n| {z }\\n=(b)\\n,\\n(5.45)\\nwhere pg is the distribution underlying the sampler g and pe is the distribution\\ndefined from the energy function e using the Boltzmann formulation. We will\\nconsider two terms in this loss function separately.\\nThe first term (a) is the negative expected energy of x plus some constant:\\n(a) = Ex∼pg [log pe(x)] = E\\n\\x14\\n−e(x) −log\\nZ\\nexp(−e(x′))dx′\\n\\x15\\n(5.46)\\n= E [−e(x)] + const.\\n(5.47)\\nAlthough we do not have pg, we can draw samples from this distribution with\\ng. We can thus compute the stochastic gradient of (a):\\n∇a\\nθg ≈−1\\nM\\nM\\nX\\nm=1\\n∇θge(g(ϵm)),\\n(5.48)\\nwhere ϵm ∼p(ϵ). As long as g is differentiable w.r.t θg and e is differentiable\\nw.r.t. the input, we can compute this stochastic gradient using backpropagation.\\nBy following the opposite direction to this stochastic gradient, we can effectively\\nminimize the first term (a).\\nUnfortunately, (b) is less trivial to compute, since we do not have access to\\npg. Instead of maximizing the entropy (see the negative sign in front of (b),) we\\ncan try to make pg closer to another distribution that potentially has a higher\\nentropy. Assuming that X is a multi-dimensional real space, i.e. Rd, the normal\\ndistribution is the maximum entropy distribution given a mean and a covariance\\nmatrix. We can thus draw many samples from pg using g, estimate the mean µg\\nand covariance Σg from these samples and then use the normal distribution with\\nµg and αΣg as the mean and covariance, respectively, as the target distribution\\nwith a higher entropy than pg, with α > 1.\\nWhen we have two sets of samples drawn from two distributions, we can\\nuse a kernelized maximum mean discrepancy (MMD) to measure the similarity\\nbetween these two distributions. Unfortunately, it is definitely out of the scope\\nof the course to discuss MMD and its kernelized estimator [Gretton et al., 2012].\\nInstead, we will trust that the following measures the discrepancy between two'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 80}, page_content='5.2. ENERGY-BASED GENERATIVE ADVERSARIAL NETWORKS\\n77\\ndistributions when we have only two sets of samples:\\nMMD2(D, D′) =\\n1\\n|D|(|D| −1)\\nX\\nx∈D\\nX\\nx′∈D′:x′̸=x\\nk(x, x′)\\n|\\n{z\\n}\\n=(a)\\n(5.49)\\n+\\n1\\n|D′|(|D′| −1)\\nX\\nx∈D′\\nX\\nx′∈D′:x′̸=x\\nk(x, x′)\\n|\\n{z\\n}\\n=(b)\\n(5.50)\\n−\\n2\\n|D||D′|\\nX\\nx∈D\\nX\\nx′∈D′\\nk(x, x′)\\n|\\n{z\\n}\\n=(c)\\n,\\n(5.51)\\nwhere k(·, ·) is a kernel function. We will not discuss what kernel functions are,\\nbut you can think of the kernel function as some kind of a distance metric, such\\nthat any kernel function k(a, b) satisfies two properties. First, it is symmetric:\\nk(a, b) = k(b, a).\\n(5.52)\\nSecond, it is semi-positive definite:\\nx⊤Kx ≥0, for all x ∈Rn,\\n(5.53)\\nwhere K is an n × n matrix with each entry Kij = k(vi, vj) for any set {vi}n\\ni=1.\\nFor real vectors, one conventional choice is a Gaussian kernel defined as\\nk(a, b) = exp\\n\\x12\\n−1\\nσ2 ∥a −b∥2\\n\\x13\\n.\\n(5.54)\\nBecause the kernelized MMD above is differentiable w.r.t. the samples, as long\\nas the kernel function was selected to be differentiable, we can compute the\\ngradient of the MMD w.r.t. the parameters of the sampler g and use it in place\\nof the gradient of (b) from Eq. (5.55).\\nAlthough we will not go into any technical detail behind this kernelized\\nMMD, it is instructive to inspect it at an intuitive level. Let us start from\\nthe back. The third term (c) is intuitively correct, as it computes the average\\npair-wise distance between all possible pairs of samples from two distributions. If\\nthe average pair-wise distance is larger, the discrepancy between two underlying\\ndistributions must be high as well.\\nLet’s assume |D| = |D′| (that is, we have the same number of samples\\nfrom each distribution.) Then, the minimum this pair-wise distance can at-\\ntain is determined by the average pair-wise distance within each set, since all\\nthese samples would be placed on top of the samples from the other distribu-\\ntion. Furthermore, when this happens, the first two terms, (a) and (b), would\\ncoincide with each other. Considering that the first two terms and the final\\nterm have opposite signs, they would cancel out each other, resulting in 0, as'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 81}, page_content='78\\nCHAPTER 5. UNDIRECTED GENERATIVE MODELS\\ndesirable. In other words, (c) determines the overall discrepancy between two\\ndistributions, while (a) and (b) are there to take into account that the mini-\\nmum discrepancy between two distributions is largely bounded from below by\\nthe intra-distribution dispersion.\\nBy minimizing the following loss, we can train a sampling network g that\\ntransforms a sample from a simple distribution p(ϵ) into a sample from the\\ntarget distribution defined from the energy function e:\\nJg(θg; e) = −1\\nM\\nN\\nX\\nm=1\\ne(g(ϵm)) −λ MMD2 \\x10\\n{sn}N\\nn=1 , {g(ϵm)}M\\nm=1\\n\\x11\\n|\\n{z\\n}\\n=R(θg)\\n,\\n(5.55)\\nwhere\\nsn ∼N\\n \\nµ = 1\\nM\\nM\\nX\\nm=1\\ng(ϵm), αΣ\\n!\\n(5.56)\\nwith\\nΣ = 1\\nM\\nM\\nX\\nm=1\\n(g(ϵm) −µ) (g(ϵm) −µ)⊤.\\n(5.57)\\nIt is important to treat sn’s as constants rather than the functions of ϵm’s. λ > 0\\ncontrols the balance between these two terms.\\nNow, we can use this sampler g instead of using a costly MCMC sampler\\nto draw samples from an energy function. In other words, we can compute the\\ngradient for the energy function from Eq. (5.19) by drawing samples from this\\nsampler g:\\n˜∇θ = ∇θe(x, θ) −1\\nM\\nM\\nX\\nm=1\\n∇θe(xm, θ),\\n(5.58)\\nwhere xm = g(ϵm; θg) with ϵm ∼p(ϵ).\\nIf you look at the first term from Eq. (5.55) (the objective function to be\\nmaximized for training g) and the second term above, it is easy to see that they\\nare identical. We can then put these two together into a single objective function\\nand then see that we can train both the energy function and the sampler jointly\\nby solving a minimax problem:\\nmin\\nθ\\nmax\\nθg Ex∼D [e(x, θ)] −Eϵ∼p(ϵ) [e(g(ϵ; θg), θ)] −λR(θg).\\n(5.59)\\nIn words, we try to adjust θ to ensure training instances are assigned lower\\nenergy values, while the samples drawn from pg are assigned higher energy\\nvalues. Meanwhile, we ensure that the sampler g draws samples that are assigned\\nlower energy values and that the implicit distribution pg ’s entropy is maximized.\\nBecause we are not reliant on Gibbs sampling, we can be much more re-\\nlaxed about how to design an energy function, unlike with the RBM above. A'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 82}, page_content='5.3. AUTOREGRESSIVE MODELS\\n79\\nnatural choice is a deterministic autoencoder which is similar to the variational\\nautoencoder from §4.3.1 however without any noise in the middle. With the\\ndeterministic autoencoder, the energy function is defined as\\ne(x; θ) = ∥F(G(x; θG); θF ) −x∥2,\\n(5.60)\\nwhere θ = θG ∪θF . The energy value is lower if x can be reconstructed better.\\nOne can view this as the energy function e and the sampler g are playing\\nan adversarial game. The energy function’s job is to ensure that the sampler’s\\nsamples are less likely than the true inputs, while the sampler’s job is to ensure\\nthat the generated samples are as likely as true inputs according to the energy\\nfunction. This approach was pioneered by Goodfellow et al. [2014], and this\\nparticular way to describe this approach using the energy function was explored\\nsoon after by Zhao et al. [2016]. Once training is over, one can either use the\\nsampler as is, or can use the sampler as the initialization for sampling from the\\ntrained energy function.\\n5.3\\nAutoregressive models\\nWe have so far considered a family of generative models, called latent variable\\nmodels. Regardless of whether the probabilistic dependencies were described\\nusing directed or undirected edges, we used unobserved variables, or latent vari-\\nables, in order to capture complex distributions. For each latent variable config-\\nuration, we define a relatively simple distribution over the observation. We call\\na distribution simple when this distribution has a small number of parameters\\nand if we can build a differentiable neural net that maps the latent variable con-\\nfiguration to these parameters of the distribution. By marginalizing out these\\nlatent variables, we end up with a model that is able to capture a complex\\ndistribution. Then, is there any alternative?\\nSuch a simple distribution is often inadequate to capture all variations of a\\nfull observation X which almost always consists of simpler (lower-dimensional)\\nconstituents, i.e., X = {x1, . . . , xd}. Such a simple distribution is however often\\nenough to capture the conditional distribution over an individual constituent\\nwhich is often significantly lower-dimensional. For instance, if xi is a categorical\\nvariable with C categories, we can easily use softmax with C parameters to\\ncapture this distribution. X however can take Cd many possible values, and\\nthis will not be easy to capture with simple softmax based parametrization. It\\nis then tempting to imagine modeling these d constituents of X separately and\\ncombine them to build a model of X.\\nRecall the chain rule of probabilities:\\np(X) = p(xΠ(1))p(xΠ(2)|xΠ(1))p(xΠ(3)|xΠ(1), xΠ(2)) · · ·\\n(5.61)\\n=\\nd\\nY\\ni=1\\np(xΠ(i)|xΠ(1), . . . , xΠ(i−1))\\n|\\n{z\\n}\\n=(a)\\n,\\n(5.62)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 83}, page_content='80\\nCHAPTER 5. UNDIRECTED GENERATIVE MODELS\\nwhere Π is an arbitrary permutation of (1, 2, . . . , d). This chain rule states that\\nthe probability of any configuration of X can be computed as the product of\\nthe probabilities of the d constituents, appropriately conditioned on a subset of\\nconstituents. Without loss of generality, we assume Π(i) = i.\\nOur goal is to build a neural network that models (a) above and thereby\\nmodel the joint probability function p(X). There are two things to consider.\\nFirst, we do not want to have d separate neural networks to capture d conditional\\nprobability distributions. We instead want to have a single neural network that\\nis able to model the relationship between any pair of the target dimension xi\\nand the context dimensions x<i = (x1, . . . , xi−1). This allows the predictor to\\nbenefit from patterns shared across these pairs. For instance, if xi was the i-th\\npixel in an image, we know that the pixel value of xi must be somewhat similar\\nto xi−1, regardless of i, due to the locality of pixel values. This knowledge should\\nbe more readily captured if a single predictor is used for all i.\\nSecond, the number of parameters should not grow w.r.t. d, i.e., |θ| = o(d). It\\nis in fact desirable to have |θ| = O(1), by having absolutely no dependency on d.\\nThis enables us to build an unsupervised model that can work on a variable-sized\\nobservation, which is critically important when dealing with variable-length se-\\nquences, such as natural language text and video.\\nCombining these two considerations, we can now write this approach in the\\nform of\\nxi ∼G(F((x1, x2, . . . , xi−1); θ), ϵ),\\n(5.63)\\nwhere ϵ is noise. This reminds us of autoregressive modeling in signal process-\\ning,2 and thus we refer to such an approach as autoregressive modeling, as this is\\nakin to a nonlinear autoregressive model with an unbounded context (p →∞).\\nTwo building blocks from §3 are particularly suitable for implementing F; a\\nrecurrent block and an attention block (with a positioning encoding.) In the case\\nof a recurrent block, we do not need any modification, but can simply feed in the\\nentire sequence (x0, x1, x2, . . . , xd) and read out (p(x1), p(x2|x1), . . . , p(xd|x<d)).\\nMore specifically, if we use gated recurrent units,\\nhi = FGRU([xi, hi−1]; θr)\\n(5.65)\\np(xi+1|x≤i) =\\nexp(u⊤\\nxi+1hi + cxi+1)\\nP\\nx∈C exp(u⊤\\nx hi + cx),\\n(5.66)\\nwhere h0 is a part of the parameters, and x0 is a placeholder vector. We can\\nthen train this recurrent network to minimize the average log-loss:\\nmin\\nθr,U,c −1\\nN\\nN\\nX\\nn=1\\ndn\\nX\\ni=1\\nlog p(xn\\ni |xn\\n<i; θr, U, c),\\n(5.67)\\n2A typical autoregressive model of order p is signal processing is defined as\\nxi =\\np\\nX\\nk=1\\nθkxi−k + ϵi.\\n(5.64)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 84}, page_content='5.3. AUTOREGRESSIVE MODELS\\n81\\nwhere we are being explicit about the possibility of variable-size observations\\nby writing dn.\\nThe attention block however requires one small modification. This mod-\\nification is necessary, since we must ensure that hi is computed only using\\n(x0, x1, . . . , xi). This can be implemented by masking out the attention weights\\nfrom Eq. (3.26) as\\nαj\\ni =\\nexp(q⊤\\ni kj −mij)\\nPN\\nj′=1 exp(q⊤\\ni kj′ −mij′)\\n,\\n(5.68)\\nwhere\\nmij =\\n(\\n0,\\nif j < i\\n∞,\\nif j ≥i\\n(5.69)\\nThis would ensure that the output ˆvi from the attention block is not computed\\nusing any input vectors (xi, xi+1, . . . , xd). Some refer to this as causal masking\\nby borrowing from the concept of a causal system in signal processing.\\nWe must be careful when we are dealing with continuous xi’s. We will discuss\\nwhy this is the case, and how we can deal with it properly in §6.4, if time permits.\\nA major advantage of this autoregressive modeling approach is that we can\\ncompute the log-probability of any observation exactly. We simply need to com-\\npute the conditional log-probabilities and sum them to get the log-probability\\nof the observation. This is unlike any latent variable approaches we have con-\\nsidered above. In the case of a variational autoencoder, we have to solve an\\nintractable marginalization problem, and in the case of RBM’s, we must com-\\npute the intractable log-partition function, or the log-normaliztaion constant.\\nFurthermore, we can readily draw independent samples tractably with this au-\\ntoregressive model, which is a great advantage for RBM’s which require costly\\nand challenging MCMC sampling.\\nThis autoregressive modeling paradigm has become de facto standard in\\nbuilding conversational agents in recent years since the successful demonstra-\\ntions by Brown et al. [2020] and Ouyang et al. [2022]. To learn more about the\\nfundamentals behind language modeling and related ideas, see this somewhat\\noutdated lecture note [Cho, 2015]. We do not go into any further detail, as these\\ntopics are out of the scope of this course.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 85}, page_content='82\\nCHAPTER 5. UNDIRECTED GENERATIVE MODELS'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 86}, page_content='Chapter 6\\nFurther Topics\\n6.1\\nReinforcement Learning\\nSingle-step reinforcement learning.\\nWe are in a situation where we must\\ntrain a classifier but we are not given input-output pairs, but rather a black\\nbox that takes as input one of the outputs and returns a scalar reward, i.e.,\\nR : {1, . . . , C} →R. This is truly a blackbox, unlike learning from §2.5 where\\nlearning was a blackbox due to its intractability. We want to train a classifier\\nso that we maximize the reward by the blackbox on expectation:\\nmax\\nθ\\nExEy|x;θ [R(y)] .\\n(6.1)\\nThe first question we often need to ask is whether we can compute the stochastic\\ngradient of this objective w.r.t. the parameters θ. Let us try that ourselves here:\\n∇θ\\nZ\\np(x)\\nC\\nX\\ny=1\\np(y|x; θ)R(y)dx =\\nZ\\np(x) ∇θ\\nC\\nX\\ny=1\\np(y|x; θ)R(y)\\n|\\n{z\\n}\\n=∇Ey|x;θR(y)\\ndx.\\n(6.2)\\nWe continue with ∇Ey|x;θR(y) :\\n∇θ\\nC\\nX\\ny=1\\np(y|x; θ)R(y) =\\nC\\nX\\ny=1\\n∇θp(y|x; θ)R(y)\\n(6.3)\\n=\\nC\\nX\\ny=1\\np(y|x; θ)R(y)∇log p(y|x; θ)\\n(6.4)\\n=Ey|x;θ[R(y)∇log p(y|x; θ)],\\n(6.5)\\nwhere we used the so-called log-derivative trick.1\\n1\\nf′ = f · (log f)′,\\n(6.6)\\n83'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 87}, page_content='84\\nCHAPTER 6. FURTHER TOPICS\\nIn other words, the stochastic gradient of the expected reward given an\\ninput x is the weighted sum of the stochastic gradient of the log-probability\\nassigned to each possible output, where the weights are the associated rewards\\nand the outputs are drawn according to the classifier’s output distribution.\\nThis intuitively makes sense. We want to follow the gradient direction that\\nwould encourage the classifier to put a higher probability on an output that is\\nassociated with a higher reward, more so than the other directions. Because it is\\noften expensive (or even impossible) to run this blackbox, it is a usual practice\\nto use a single sample dranw from y|x; θ to approximate this stochastic gradient:\\n∇θEy|x;θ [R(y)] ≈R(˜y)∇θ log p(˜y|x; θ) = ˆg,\\n(6.8)\\nwhere ˜y ∼y|x; θ.\\nBefore declaring the victory, let us compute the variance of this stochastic\\ngradient estimator:\\nV[ˆg] = E[ˆg2] −E[ˆg]2.\\n(6.9)\\nAlthough we know that this is an unbiased estimator because we have de-\\nrived it fully until we used single-sample Monte Carlos approximation (which is\\nunbiased on its own), let’s first compute E[ˆg]:\\nE[ˆg] = Ey|x;θ [R(y)∇θ log p(y|x; θ)]\\n(6.10)\\n=\\nX\\ny\\np(y|x; θ)R(y)∇θ log p(y|x; θ)\\n(6.11)\\n=\\nX\\ny\\nR(y)∇θp(y|x; θ)\\n(6.12)\\n= ∇θ\\nX\\ny\\nR(y)p(y|x; θ)\\n(6.13)\\n= ∇θEy|x;θ [R(y)]\\n(6.14)\\nThen, we need to compute the first term of the variance above:\\nE\\n\\x02\\nˆg2\\x03\\n= Ey|x;θ\\n\\x02\\nR2(y)∥∇θ log p(y|x; θ)∥2\\x03\\n(6.15)\\nPutting them together, we get\\nV [ˆg] = E\\n\\x02\\nR2(y)∥∇log p(y|x; θ)∥2\\x03\\n−∥∇θE[R(y)]∥2.\\n(6.16)\\nLooking at the first term of the variance, we notice that there are two things\\nthat affect the variance greatly. The first factor is the magnitude of the reward.\\nIf the reward has a high magnitude, it results in an increased variance of the\\nbecause\\n(log f)′ = f′\\nf\\n(6.7)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 88}, page_content='6.1. REINFORCEMENT LEARNING\\n85\\nstochastic gradient estimate. This suggests that it is critical for us to control\\nthe magnitude of the reward, although this is impossible if we are working with\\nthe truly black box R. The second factor is the norm of the gradient of the\\nlog-probability of the selected action w.r.t. the parameters θ. In other words,\\nthe variance will be greater if the predictive probability computed by the model\\nis sensitive to the change in the parameters. This suggests a very explicit way\\nto regularize learning by minimizing this quantity directly, in order to stabilize\\nlearning. This technique is often referred to as gradient penalty.\\nAt this point, we begin to wonder if there is another stochastic estimator that\\nis equally unbiased but potentially has a lower variance. Consider the following\\nestimator, which is often referred to as a policy gradient estimator:\\n∇θEy|x;θ [R(y)] ≈(R(˜y) −b(x))∇θ log p(˜y|x; θ),\\n(6.17)\\nwhere b may be a function of x but is independent of y. If we consider the\\nexpected value of the left-hand side, we notice that\\nE [R(˜y)∇θ log p(y|x; θ)] −b(x) E [∇θ log p(y|x; θ)]\\n|\\n{z\\n}\\n=(a)\\n.\\n(6.18)\\nLet us dig deeper into (a) above:2\\nC\\nX\\ny=1\\np(y)∇log p(y) =\\nC\\nX\\ny=1\\n\\x08\\x08\\np(y) 1\\n\\x08\\x08\\np(y)∇p(y) = ∇\\nC\\nX\\ny=1\\np(y)\\n|\\n{z\\n}\\n=1\\n= 0.\\n(6.19)\\nIn other words, the estimator in Eq. (6.17) is an unbiased estimator.\\nAlthough this estimator is identical to the original one in terms of the bias,\\nthis extra subtraction of b(x) from R(˜y) has an important consequence for the\\nvariance. Let us consider the first term of the variance using the new estimator\\nin Eq. (6.16). We are particularly interested to find b(x) that minimizes this\\nterm:\\n∇b\\n1\\n2Ey|x;θ\\n\\x02\\n(R(y) −b)2∥s(y)∥2\\x03\\n= −E\\n\\x02\\n(R(y) −b)∥s(y)∥2\\x03\\n= 0\\n(6.20)\\n⇐⇒bE∥s(y)∥2 −E\\n\\x02\\nR(y)∥s(y)∥2\\x03\\n= 0\\n(6.21)\\n⇐⇒b∗= E\\n\\x02\\nR(y)∥s(y)∥2\\x03\\nE∥s(y)∥2\\n.\\n(6.22)\\nwhere s(y) = ∇log p(y|x; θ). Unfortunately, this optimal baseline is intractable\\nor impossible to compute, since it requires us to query the blackbox R for each\\nand every possible outcome for the input x.\\nIt is rather more informative to consider the upperbound to the first term\\nof the variance. Let cmax = maxy=1,...,C ∥s(y)∥2 ≪∞, which we can encourage\\nby the technique of gradient penalty. Then,\\nE\\n\\x02\\n(R(y) −b(x))2∥s(y)∥2\\x03\\n≤cmaxE\\n\\x02\\n(R(y) −b(x))2\\x03\\n.\\n(6.23)\\n2I will omit |x; θ for the brevity without loss of generality.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 89}, page_content='86\\nCHAPTER 6. FURTHER TOPICS\\nThe optimal baseline to minimize the upperbound on the right hand side is\\n∇b\\ncmax\\n2\\nE (R(y) −b)2 = −cmax (ER(y) −b) = 0\\n(6.24)\\n⇐⇒b∗= Ey|x;θ [R(y)] .\\n(6.25)\\nIn other words, the optimal baseline is the expected reward we anticipated given\\nthe input x.\\nOf course, this quantity is again intractable or impossible to compute exactly.\\nWe can however now fit a predictor of b∗given x using all the past observations of\\n(x, R(˜y)), because each R(˜y) is a single-sample approximation to Ey|x;θ [R(y)].3\\nBecause we update θ along the way, many of the past samples would not be\\nvalid under the current θ. If we however assume that θ is updated slowly and\\nthat the predictor is adapted rapidly, asymptotically this is an exact procedure,\\njust like persistent contrastive divergence from §5.1.2.\\nWe then need to maintain two predictors. One predictor is often called a\\npolicy network that maps the current input, or state, x to the distribution\\nover possible outputs, or actions. The other predictor is often referred to as a\\nvalue network that maps the current state x to the expected reward. The latter\\nis called a value network, because it predicts the value of the current state,\\nregardless of the action to be taken by the policy. These networks are trained\\nin parallel.\\nThe case of noisy reward: an actor critic method.\\nLet’s imagine that\\nthe reward R depends on both x and y and that it is random as well. That is,\\nwe observe only a noisy estimate of the reward at x given the output choice y.\\nWe probably want then to maximize the expected, expected reward:\\nmax\\nθ\\nEy|x;θEϵ [R(y, x; ϵ)] ,\\n(6.28)\\nwhere we use ϵ to collectively refer to as any kind of uncertainty in the reward\\nR. Then, we have to further approximate the policy gradient with a sample\\nreward ˜R(y, x):\\n∇θEy|x;θEϵ [R(y, x; ϵ)] ≈(Eϵ[R(˜y, x; ϵ)] −b(x))∇θ log p(˜y|x; θ)\\n(6.29)\\n≈( ˜R(˜y, x) −ˆb(x; θb))∇θ log p(˜y|x; θ),\\n(6.30)\\n3In particular, we should use a mean-squared error as the loss function when fitting a\\npredictor to estimate the expected reward. This comes from the fact that the optimal solution\\nto minimizing the mean-squared error corresponds to computing the average, as easily seen\\nbelow:\\n∇µ\\n1\\n2N\\nN\\nX\\nn=1\\n(µ −xn)2 = 1\\nN\\nN\\nX\\nn=1\\n(µ −xn) = µ −1\\nN\\nN\\nX\\nn=1\\nxn = 0\\n(6.26)\\n⇐⇒µ = 1\\nN\\nN\\nX\\nn=1\\nxn.\\n(6.27)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 90}, page_content='6.1. REINFORCEMENT LEARNING\\n87\\nwhere ˆb(x) refers to a predicted baseline, e.g. the value of x. θb is the parameters\\nof this value function.\\nUnfortunately, this estimator will have an extra variance due to noisy reward.\\nSimilarly to what we did with the baseline above, we can lower the variance by\\npredicting the expected reward at (x, ˜y) using a predictor trained on samples.\\nThat is,\\n∇θEy|x;θEϵ [R(y, x; ϵ)] ≈( ˆR(˜y, x; θr) −ˆb(x; θb)\\n|\\n{z\\n}\\n=(a)\\n)∇θ log p(˜y|x; θ),\\n(6.31)\\nwhere ˆR is the reward predictor, parametrized by θr. Such a reward predictor\\nis often referred to as a Q value of the state-action (x, y) pair.4 This difference\\n(a) between the Q value ˆR and the value ˆb is called an advantage, since this is\\ntells us about the advantage of choosing ˜y over other outputs/actions.\\nAn interesting observation here is that if we have ˆR(y, x; θr) and if C, the\\nnumber of all possible y values, is small, we can replace the value network ˆb with\\nˆb(x) = Ey|x;θ\\nh\\nˆR(y, x; θr)\\ni\\n,\\n(6.32)\\nwhich may help reduce the variance from having to train two separator pre-\\ndictors. With a reasonable C, this can implemented quite efficiently by having\\nˆR to output a C-dimensional real-valued vector, multiplying the output with\\nthe output from the y predictor (which is often called a policy) and sum these\\nvalues. Sometimes we call this ˆR a critic and p(y|x; θ) an actor. This approach\\nis thus called an actor-critic algorithm.\\nMulti-step reinforcement learning\\nLet us assume there exist C-many\\n|X| × |X| stochastic transition matrix Σ(y) such that\\nΣij(y) ≥0\\nand\\nC\\nX\\ni=1\\nΣi·(y) = 1,\\n(6.33)\\nfor y ∈{1, 2, . . . , C}. This transition matrix gives us the distribution over the\\nnext state given the current state xt−1 and the selected action yt, as\\nq(x = k|xt−1, yt) = Σxt−1,k(yt),\\n(6.34)\\nwhere we assume X is a finite set, although it is easy to extend it to a continuous\\nstate space X.\\nIn defining this transition operator q, we have made an important assumption\\ncalled Markov assumption. That is, at time t −1, where we will end up at time\\nt given my choice of yt is independent of the past states (x1, . . . , xt−2) I have\\nvisited so far nor the action choices (y1, . . . , yt−1) I have made so far. We further\\n4Although almost no paper explicitly mentions what ‘Q’ stands for, it is widely acknowl-\\nedged that it stands for quality.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 91}, page_content='88\\nCHAPTER 6. FURTHER TOPICS\\nassume that a reward function S⋆that is defined on each state and returns a\\nscalar, i.e. S⋆: X →R. Each time we transit from xt−1 to xt due to yt, we\\nreceive a reward S⋆(xt).\\nTogether with a policy π(y|x; θ), it defines a distribution over trajectories, or\\noften called episodes. We can then sample a (potentially infinitely long) sequence\\nof tuples of previous state xt−1, selected action yt, next state xt and received\\nreward st = S⋆(xt). Of course, these tuples are highly correlated with each\\nother, since they are collected from a single trajectory defined by a shared set\\nof distributions, the policy, transition and reward. We will however for now\\nignore this by saying that we are considering a particular time step t from many\\nindependent trajectories.\\nLet us use n to refer to each of these trajectories. In order to apply the policy\\ngradient, or actor-critic algorithm, from above, we must start with the Q network\\nˆQ(xn\\nt−1, yn\\nt ). This Q network approximates the expected quality of (xn\\nt−1, yn\\nt ).\\nWe define the expected quality by first defining the quality of (xn\\nt−1, yn\\nt ) from\\nthe n-th trajectory as\\n˜Q(xn\\nt−1, yn\\nt ) = sn\\nt +\\nTn\\nX\\nt′=t+1\\nγt′−tsn\\nt′,\\n(6.35)\\nwhere γ ∈[0, 1] is a so-called discounting factor and Tn is the length of the n-th\\ntrajectory.\\nThis formulation tells us that the quality of any particular state-action pair\\nis determined by the accumulated rewards from there on throughout the full\\ntrajectory. Because we assumed the Markov property, it is perfectly fine for us\\nto ignore how we arrived at (xt−1, yt). With γ < 0, we are specifying that we do\\nnot want to take into account what happens too far into the future. This is often\\na good strategy to facilitate learning in the case of finite-length episodes, i.e.\\nTn < ∞, and is necessary to define the quality to be finite with infinitely-long\\nepisodes, i.e. Tn →∞.5\\nThis particular quality from the n-th trajectory can be thought of a sample\\nfrom a random variable Q(xn\\nt−1, yn\\nt ) which is defined as\\nQ(xn\\nt−1, yn\\nt ) = sn\\nt +Eq(xt|xt−1,yn\\nt ) [\\n(6.36)\\nγEπ(yt+1|xt)q(xt+1|xt,yt+1) [s⋆(xt+1)+\\n(6.37)\\nγEπ(yt+2|xt+1)q(xt+2|xt+1,yt+2) [s⋆(xt+2) + · · · ]\\n\\x03\\x03\\n.\\n(6.38)\\nIn other words, the expected quality is the weighted sum of all future per-step\\nrewards after marginalizing out all possible future trajectories according to the\\ntransition model and the policy.\\nWhen we are working with finite-length trajectories, we can easily train the\\nQ network to minimize the following quantity:\\nmin\\nθr\\n1\\nN\\nN\\nX\\nn=1\\nLn\\nX\\nt=2\\n1\\n2\\n\\x10\\nˆR(xn\\nt−1, yn\\nt ) −˜Q(xn\\nt−1, yn\\nt )\\n\\x112\\n,\\n(6.39)\\n5Unless γ < 1, the quality easily diverges, assuming st > 0 even when |st| < ∞.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 92}, page_content='6.1. REINFORCEMENT LEARNING\\n89\\nbecause ˆQ is an unbiased sample drawn from the true distribution of the quality\\ndefined immediately above.\\nUnfortunately this is not possible, if we are working with an infinitely-long\\nepisode. Such an infinitely-long episode is not common in the current-day setups,\\nbut it is something we aspire to working with in the future, where we would\\nanticipate a learning based system to be deployed in real world situations and\\nadapt itself on the fly. Of course, in this case, we must updaate the Q network\\nalso on-the-fly. It is unfortunately not possible to get even a single sample ˜Q,\\nsince we never see the end of any episode.\\nLet us re-arrange terms in Eq. (6.35):\\n˜Q(xn\\nt−1, yn\\nt ) =sn\\nt +\\nTn\\nX\\nt′=t+1\\nγt′−tsn\\nt′\\n(6.40)\\n=sn\\nt + γ\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nsn\\nt+1 +\\nTn\\nX\\nt′=2\\nγt′−tsn\\nt′\\n|\\n{z\\n}\\n= ˜\\nQ(xn\\nt ,yn\\nt+1)\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n.\\n(6.41)\\nWe see that the quality is recursively defined:\\n˜Q(xn\\nt−1, yn\\nt ) = sn\\nt + γ ˜Q(xn\\nt , yn\\nt+1).\\n(6.42)\\nThis allows us to write a loss function to train the Q network without waiting\\nfor the full episode to end (or never end) by considering the temporal difference\\nat time t:\\nmin\\nθr\\n1\\nN\\nN\\nX\\nn=1\\n\\x10\\nˆR(xn\\nt−1, yn\\nt ; θr) −γ\\n\\x10\\nsn\\nt + ˆR(xn\\nt , yn\\nt+1; ˜θr)\\n\\x11\\x112\\n(6.43)\\n⇐⇒min\\nθr\\nγ2\\nN\\nN\\nX\\nn=1\\n\\x12\\x12 1\\nγ\\nˆR(xn\\nt−1, yn\\nt ; θr) −ˆR(xn\\nt , yn\\nt+1; ˜θr)\\n\\x13\\n−sn\\nt\\n\\x132\\n,\\n(6.44)\\nwhere ˜θr is a previous estimate of θr. We bootstrap from some random Q func-\\ntion (or its estimate) and iteratively improve our estimate of the Q function by\\nlearning to predict the temporal difference. Unsurprisingly, we refer to this kind\\nof learning as the method of temporal difference [Sutton, 1988].\\nIt turned out that such temporal difference methods are effective even when\\nwe are dealing with finite-length episodes, when these episodes are long. It is\\nhowever generally challenging to train the Q network with such a temporal\\ndifference method due to many factors. For instance, the objective function\\nabove effectively tells us that the objective function itself is a function of our\\nprevious estimate ˜θr, meaning that a minimum one finds now will not continue\\nto be a minimum once you plug the new estimate ˆθr into ˜θr. Furthermore,\\nit will take a long time for the quality estimate the capture the longer-term'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 93}, page_content='90\\nCHAPTER 6. FURTHER TOPICS\\ndependencies of the choice of a particular action yn\\nt at xn\\nt−1 on many steps later,\\nsince the naive temporal difference only considers one step deviation at a time.\\nThere have been many improvements proposed since the initial work, but it is\\nout of the scope of this course to cover those.\\nWith this Q network (or the critic network, as we learned to call it above,)\\nwe can rely on the policy gradient to update the policy (or the actor network)\\nfrom Eq. (6.31). There are of course many different ways to improve the actor\\nupdate, for instance by constraining the update to be somewhat limited. Again,\\nthese are more or less out of the scope of this course.\\n6.2\\nEnsemble Methods\\nBagging.\\nAs we discussed already multiple times throughout the course (see\\ne.g. §2.4.3,) we are often in a situation where we do not have just one predictor\\nbut have access to many different predictors. These predictors can be thought\\nof as samples drawn from some distribution over all possible predictors:\\n˜θn ∼q(θ).\\n(6.45)\\nWe will discuss where such a distribution comes from later, but for now, we will\\nassume it magically exists and that we can readily draw N classifiers from this\\ndistribution q.\\nWe already considered the case of having q earlier in §2.4.2 when we consid-\\nered the following bias-variance decomposition from Eq. (2.118):\\nEx,y,θ(y −ˆy(x, θ))2 ∝Ex\\n\\uf8ee\\n\\uf8ef\\uf8f0Ey|x\\n\\x02\\n(y −µy)2\\x03\\n|\\n{z\\n}\\n=(a)\\n+ Eθ\\n\\x02\\n(ˆy(x, θ) −ˆµy)2\\x03\\n|\\n{z\\n}\\n=(b)\\n+ (µy −ˆµy)2\\n|\\n{z\\n}\\n=(c)\\n\\uf8f9\\n\\uf8fa\\uf8fb,\\n(6.46)\\nwhere\\nµy = Ey|x [y] ,\\n(6.47)\\nˆµy = Eθ [ˆy(x, θ)] .\\n(6.48)\\nThis decomposition was done on the loss averaged over the predictors drawn\\nfrom the posterior distribution q. We can instead consider the loss computed\\nusing the average prediction from the predictors drawn from the posterior dis-\\ntribution. That is, our prediction is\\nˆy(x) = Eθ [ˆy(x, θ)] .\\n(6.49)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 94}, page_content='6.2. ENSEMBLE METHODS\\n91\\nThen,\\nEx,y (y −ˆy(x))2 ∝Ex\\n\\uf8ee\\n\\uf8ef\\uf8f0Ey|x (y −µy)2\\n|\\n{z\\n}\\n=(a’)\\n−2 ˆy(x)\\n|{z}\\n=ˆµy\\nEy|x [y]\\n| {z }\\n=µy\\n+ ˆy2(x)\\n| {z }\\n=ˆµ2y\\n\\uf8f9\\n\\uf8fa\\uf8fb\\n(6.50)\\n= Ex\\n\\uf8ee\\n\\uf8ef\\uf8f0Ey|x (y −µy)2\\n|\\n{z\\n}\\n=(a’)\\n−µ2\\ny\\n|{z}\\n=(b’)\\n+ (ˆµy −µy)2\\n|\\n{z\\n}\\n=(c’)\\n\\uf8f9\\n\\uf8fa\\uf8fb.\\n(6.51)\\nNow, let us consider the difference between these two loss function. Since (a)\\nand (a’) are equivalent and (c) and (c’) are equivalent, we just need to consider\\n(b) and (b’):\\nEθ (ˆy(x; θ) −ˆµy)2 + µ2\\ny = Eθˆy2(x; θ) −2ˆµy Eθ [ˆy(x; θ)]\\n|\\n{z\\n}\\n=ˆµy\\n+ˆµ2\\ny + ˆµ2\\ny = Eθˆy2(x; θ) ≥0.\\n(6.52)\\nIn other words, this tells us that the average loss over the predictors is always\\ngreater than or equal to the loss of the average prediction by the predictors.\\nThis motivates the idea of bagging [Breiman, 1996].\\nAs long as we have q, or a sampler that draws predictors, or the correspond-\\ning parameters, from this distribution q, bagging tells us that it is never a bad\\nidea to use many of those sampled predictors and average their predictions,\\nrather than using any one of them solely, on average. It turned out there are\\nmany different ways that make our predictor θ random rather than determin-\\nistic. We have already covered most of them earlier in the course, but let us\\nbriefly go through them here once more.\\nIn modern machine learning, a major source of randomness is the use of\\nstochastic gradient descent on a non-convex loss function. The loss function is\\nnot convex w.r.t. the parameters, as we stack highly nonlinear blocks to build\\na deep neural network based predictor, and in doing so, we introduce a large\\ndegree of redundancies (or ambiguities). These ambiguities are more or less\\narbitrarily resolved by randomness in stochastic gradient descent. For instance,\\nour choice of the initial values of the parameters affect a subspace over which\\nstochastic gradient descent can explore and find a local minimum. In addition to\\ninitialization, there are other types of randomness in stochastic gradient descent,\\nthat is, how we construct minibatches by selecting random subsets of the training\\nset. Furthermore, quite a few building blocks are inherently stochastic. Recall\\nthe variational autoencoder from §4.3.1, where we injected noise for processing\\neach and every instance during training. In other words, we can think of the\\nresulting solution by running stochastic gradient descent as a sample drawn\\nfrom some distribution implicitly defined by this process of learning.\\nOf course, another major source of randomness is the choice of the training\\nset. As we have discussed earlier in §2.4.3, we can imitate the randomness in\\ndata collection even when we have a single set of data points drawn from the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 95}, page_content='92\\nCHAPTER 6. FURTHER TOPICS\\nunderlying distribution by the process of bootstrap resampling. Instead of using\\nthe training set as it is, we can resample it to match its original size however by\\nresampling training examples with replacement. Each time, we use a different\\nresampled training set, we end up with a somewhat different solution which can\\nbe considered a sample drawn from the distribution again implicitly defined by\\nthe process of training set construction.\\nIn summary, we should embrace stochasticity inherent in learning and data\\ncollection in order to produce a set of distinct predictors and average their\\npredictions for each input. On average, this will give us a low-loss predictor,\\nthanks for the theory of bagging, above.\\nBayesian machine learning.\\nOur discussion so far has progressed assuming\\nthat we are given this distribution q(θ). When q(θ) is given, bagging tells us that\\nwe want to use the average prediction from many sampled predictors from q to\\nbuild a lower-loss predictor on average. This however does not tell me anything\\nabout how we can create this distribution ourselves, or what this distribution q\\nis.\\nIt turned out that we can rely on probability to guide us in designing as well\\nas understanding this distribution q(θ). This will resemble much what we have\\ndone in §4, and if you did not have much trouble following that section, you\\nwould not find it any confusing. Let us try to derive this q distribution by first\\ntreating the loss value on the training set of a single predictor θ as the energy\\nfunction:\\ne(θ; D) =\\nX\\nx∈D\\nL(x; θ),\\n(6.53)\\nwith a very generic loss function L.\\nWe can interpret this energy function just like any other energy function\\nwe have defined and used throughout the semester. We want the predictor\\nparametrized by θ to be assigned a low energy value when it is good. The\\ngoodness of the predictor is defined as how low the loss function this predictor\\nattains on the training set D.\\nWe can now turn this energy function into the probability function using the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 96}, page_content='6.2. ENSEMBLE METHODS\\n93\\nBoltzmann formulation, as we have done over and over by now:\\nq(θ|D, β) =\\nexp\\n\\x00−β P\\nx∈D L(x; θ)\\n\\x01\\nR\\nΘ exp\\n\\x00−β P\\nx∈D L(x; θ′)\\n\\x01\\ndθ′\\n(6.54)\\n=\\nY\\nx∈D\\nexp (−βL(x; θ))\\nR\\nΘ exp\\n\\x00−β P\\nx∈D L(x; θ′)\\n\\x01\\ndθ′\\n(6.55)\\n=\\nY\\nx∈D\\nexp (−βL(x; θ))\\nR\\nX exp (−βL(x′; θ)) dx′\\nR\\nX exp (−βL(x′; θ)) dx′\\nR\\nΘ exp\\n\\x00−β P\\nx∈D L(x; θ′)\\n\\x01\\ndθ′ (6.56)\\n=\\nY\\nx∈D\\np(x|θ, β)\\nR\\nX exp (−βL(x′; θ)) dx′\\nR\\nΘ\\nR\\nX exp (−βL(x′; θ′)) dx′dθ′\\nR\\nΘ\\nR\\nX exp (−βL(x′; θ′)) dx′dθ′\\nR\\nΘ exp\\n\\x00−β P\\nx∈D L(x; θ′)\\n\\x01\\ndθ′\\n(6.57)\\n=\\nY\\nx∈D\\np(x|θ, β)\\np(θ)\\nQ\\nx′∈D p(x′|β).\\n(6.58)\\nThis is precisely the posterior distribution over θ, where we consider θ to be a\\nrandom variable. It states that our belief (probability) of a particular parameter\\nconfiguration θ is proportional to the product of the likelihood p(D|θ, β) =\\nQ\\nx∈D p(x|θ, β) and the prior belief of θ.\\nWith this our updated (that is, posterior) belief over θ, we probably want\\nto marginalize θ out when we make a prediction on a new instance x′ /∈D:\\np(x′|D, β) =\\nZ\\nΘ\\np(x′|θ, β)q(θ|D, β)dθ.\\n(6.59)\\nThis formulation tells us that we should sample many predictors according to\\nq(θ|D, β) and average their predictions, just like bagging above:\\np(x′|D, β) ≈1\\nM\\nM\\nX\\nm=1\\np(x′|θm, β),\\n(6.60)\\nwhere θm ∼q(θ|D, β). In other words, if we follow the Bayes’ rule and think of\\nthe loss function as an energy function of the parameter θ given an individual\\ninstance, we arrive at the conclusion that we should draw predictors from the\\nposterior distribution q(θ|D, β). This is a great property, since we now have a\\ngood guideline on what we should do, although the inclusion of β here was quite\\nintentional, as it says that we still need some kind of hyperparameter search even\\nin so-called Bayesian machine learning.\\nLet us now connect this (log-)posterior distribution with what we have\\nlearned so far by writing it as\\nlog p(θ|D, β) =\\nX\\nx∈D\\nlog p(x|θ, β) + log p(θ) −log Z(D, β)\\n(6.61)\\n= −β\\nX\\nx∈D\\nL(x; θ) + log p(θ) −log Z′(D, β),\\n(6.62)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 97}, page_content='94\\nCHAPTER 6. FURTHER TOPICS\\nwhere we collect all terms that are constant w.r.t. β into log Z′.\\nBy setting β =\\nα\\n|D|, we end up with\\n−log p(θ|D) = α\\n|D|\\nX\\nx∈D\\nL(x; θ) −log p(θ)\\n|\\n{z\\n}\\n=−log p∗(θ|D,α)\\n+const.\\n(6.63)\\nIf we minimize this, that is, if we maximize the log-posterior, this is precisely\\nwhat we have already been doing all along. We look for the parameter configu-\\nration θ that minimizes the average loss but use the regularizer to ensure that\\nwe end up with a parameter that generalizes. The balance between these two\\nare determined by the constant α.\\nSince we can exactly compute the unnormalized posterior probability, we\\ncan think of using an advanced sampling technique, based on Markov Chain\\nMonte Carlo methods, from §5.1.1 [Neal, 1996]. Unfortunately, this is often\\ncomputationally too costly, because we must evaluate the loss over the entire\\ntraining set D each time we evaluate the acceptance probability. After all, the\\nwhole reason why we introduced stochastic gradient descent earlier was precisely\\nbecause it was too costly to evaluate the loss over the whole training set.\\nFortunately, or obviously in retrospect, researchers have realized that stochas-\\ntic gradient descent, with some adjustments or sometimes without much of ad-\\njustment, draws samples from this particular posterior distribution [see, e.g.,\\nWelling and Teh, 2011]. A general idea behind these recent algorithms, or find-\\nings, is that if we do not try to reduce the effect of noise, i.e. (b) in Eq. (2.87),\\nstochastic gradient descent will tend toward a local minimum but will not tend\\nto stay at the local minimum and jump out toward another local minimum.\\nThese local minima correspond to modes of the posterior distribution. By col-\\nlecting all the parameter configurations visited by stochastic gradient descent,\\nor some subset of them via thinning, we call parameter samples approximately\\naccording to the posterior distribution.\\nThis view of stochastic gradient descent as a posterior sampler tells us one\\nmore alternative to create a set of predictors for bagging. That is, we simply\\nrun stochastic gradient descent, without annealing the learning rate toward\\nzero or while explicitly adding extra noise, and collect every once a while a\\npredictor, to form a set of predictors for bagging. This approach explains why it\\nhas been successful to build a bag of deep neural networks to build an ensemble\\nclassifier [Krizhevsky et al., 2012], because those were approximate samples from\\nthe posterior distribution.\\nGradient Boosting.\\nConsider a regression problem in which the target is\\ny ∈Rd, and the energy function is defined as\\ne([x; y], θ) = 1\\n2∥y −f(x; θ)∥2.\\n(6.64)\\nLet us imagine that we already have a trained predictor f(x; θ) which is not\\nperfect. We want to fit another predictor g(x; θ′) in order to ensure that we can'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 98}, page_content='6.2. ENSEMBLE METHODS\\n95\\nmake better prediction on x. We can approach this by first defining an aggregate\\npredictor as\\nh(x; {θ, θ′}) = f(x; θ) + αg(x; θ),\\n(6.65)\\nwhere α > 0. We can then write the energy function that includes h as\\ne′([x; y], {θ, θ′}) = ∥y −h(x; {θ, θ′})∥2\\n(6.66)\\n= ∥(y −f(x; θ))\\n|\\n{z\\n}\\n(a)\\n−αg(x; θ′)∥2.\\n(6.67)\\nWe can minimize this energy function w.r.t. α and θ′ , which results in g that\\ncomplements the existing predictor f to minimize any remaining error by f.\\nThis idea is often referred as boosting [Schapire, 1990], as it boosts the represen-\\ntational power of weak predictors by combining two weak predictors, here f and\\ng, to form a stronger predictor. This procedure can be repeated by considering\\nh as f and introducing yet another weak predictor g into the mix, until the\\npoint at which a satisfyingly low level of the loss is achieved. Although we have\\nderived it in the context of a single example (x, y), it should be readily extended\\nto multiple example pairs.\\nBy carefully inspecting (a), we realize that this term is the negative gradient\\nof Eq. (6.64) w.r.t. f(x; θ):\\n∂e\\n∂f(x; θ) = −(y −f(x; θ)).\\n(6.68)\\nInstead of e, which was equivalent to the loss, because it was formulated using\\nL2 distance, we can use a more generic loss l(θ; [x, y]). We can then further\\nrewrite e′ as\\ne′([x; y], {θ, θ′}) ∝∥−∇ˆyl(θ; [x, y]) −αg(x; θ′)∥2,\\n(6.69)\\nwhere ˆy = f(x; θ) . By minimizing e′ w.r.t. θ′ and α, we effectively let g capture\\nthe (scaled) negative gradient of the loss w.r.t. ˆy.\\nAs we have learned repeatedly over the course so far, the gradient is only\\nmeaningful in some small neighbourhood. In other words, taking the full step in\\nthe direction of the negative gradient may not necessarily decrease the overall\\nloss, and we must scale the gradient accordingly. We thus search for the right\\nstep size by solving\\nmin\\nγ≥0 l({θ, θ′}; [x, y]),\\n(6.70)\\nwhere the loss l is computed by comparing y and\\nˆy = f(x; θ) + γg(x; θ′).\\n(6.71)\\nThis procedure resembles the process of gradient descent from §2.3.2, and is\\nthereby referred to as gradient boosting [Friedman, 2001].'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 99}, page_content='96\\nCHAPTER 6. FURTHER TOPICS\\nBoosting does not specify how to estimate β and g (or equivalently θ′) at each\\niteration, and it is up to the practitioner to decide which (weak) learner g they\\nuse and which loss function l they choose. Popular choices include decision trees\\nand kernel-based support vector machines. In this sense, this is not a learning\\nalgorithm but more a meta-heuristics.\\n6.3\\nMeta-Learning\\nIn the previous section §6.2, we learned that it is a good idea to average the\\npredictions from multiple models if we have a distribution q(θ) over the models\\n(or predictors) rather than a single predictor. We then learned that Bayesian\\nmachine learning tells us that this distribution should be conditioned on the\\ntraining set, resulting q(θ|D), and that we can obtain this posterior distribution\\nfollowing the Bayes’ rule:\\nq(θ|D) ∝p(θ)\\nY\\nx∈D\\np(x|θ).\\n(6.72)\\nIt is a fair question at this point whether we must follow this particular for-\\nmulation based on the Bayes’ rule. Perhaps there is a better way to map the\\ntraining set D to the posterior distribution over θ.\\nLet us assume that we have not one but multiple training set\\n\\x08\\nD1, D2, . . . , DM\\t\\n,\\ncorresponding to the M prediction tasks. For each training set, we can define a\\nso-called K-fold cross-validation loss as\\nLKCV(ϕ; Dm) = −1\\nK\\nK\\nX\\nk=1\\nX\\nx∈Dm\\nσk(1):σk(⌈1\\nK |Dm|⌉)\\nlog\\nZ\\nΩ\\np(x|θ)q\\n\\x10\\nθ|Dm\\nσk(⌈1\\nK |Dm|⌉+1):σk(|Dm|); ϕ\\n\\x11\\ndθ,\\n(6.73)\\nwhere σk is the k-th permutation of the indices from 1 to |Dm|. To compute\\nthis loss, we often partition the data Dm into K partitions. For each partition,\\nwe use the rest of the partitions to train a predictor (or a set of predictors) and\\nuse this predictor to compute the loss. We average these K loss values and use\\nit as a proxy to the generalization loss [Kohavi, 1995].\\nIn this particular case above, this cross-validation loss is a function ϕ which\\nparameterizes the posterior distribution q over the parameters θ. This parametriza-\\ntion effectively turns the posterior inference problem in Bayesian machine learn-\\ning into building a predictor that maps a set of training data points into a dis-\\ntribution over the parameters, where this predictor is parametrized using θ. In\\nother words, we train a predictor that solves the posterior inference problem by\\nsolving\\nmin\\nϕ\\n1\\nM\\nM\\nX\\nm=1\\nLKCV(ϕ; Dm).\\n(6.74)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 100}, page_content='6.3. META-LEARNING\\n97\\nIn this case, we would call\\n\\x08\\nD1, . . . , DM\\t\\na meta-training set.\\nJust like what we have seen earlier in §4, this K-fold cross-validation loss is\\nnot easy to compute nor to minimize. Instead, we can use the same technique\\nfrom variational inference from earlier to minimize the upperbound to LKCV:\\nLKCV(ϕ; Dm) ≤−1\\nK\\nK\\nX\\nk=1\\nX\\nx∈Dm\\nσk(1):σk(⌈1\\nK |Dm|⌉)\\n1\\nB\\nB\\nX\\nb=1\\nlog p(x|θb),\\n(6.75)\\nwhere θb ∼q\\n\\x10\\nθ|Dm\\nσk(⌈1\\nK |Dm|⌉+1):σk(|Dm|); ϕ\\n\\x11\\n. Since θ is often continuous, we can\\nfor instance compute the gradient of LKCV w.r.t. ϕ with the reparametrization\\ntrick as long as q is differentiable w.r.t. ϕ.\\nThis is interesting, since we can be flexible about how we parametrize q,\\nand this q is directly optimized to result in a distribution over θ or a set of\\nθ’s under which the predictive loss is minimal. In other words, q is a learning\\nalgorithm, and we are training a learning algorithm by minimizing the meta-\\nobjective function in Eq. (6.74) w.r.t. q.\\nFor instance, we can define q implicitly by drawing a sample of the parame-\\nters θ from q using just a few steps N of stochastic gradient descent, as opposed\\nto running it until convergence as from §2.3.2. In doing so, we can consider the\\ninitialization θ0 of the parameters as ϕ. By minimizing the meta-objective func-\\ntion w.r.t. θ0, we are looking for the initialization of the parameters that are\\noptimal with N SGD steps. If the new training set after such meta-learning is\\nsimilar to the meta-training sets, we would expect that N SGD steps would be\\nenough if not optimal to obtain the best predictor. This approach was originally\\nproposed by [Finn et al., 2017] and called model-agnostic meta-learning.\\nOf course, we can completely forego of any iterative optimization when de-\\nsigning q and build a predictor that directly maps a set of training data points\\nD to the prediction on a new observation x′. In doing so, it is important to real-\\nize that this predictor cannot simply take as input D but needs to model noise\\nin learning itself. This naturally calls for including latent variables z into this\\npredictor, just like how we did earlier with generative models in §4. In this case,\\nthe posterior distribution q(θ) is implicit, and we directly predict the predictive\\nprobability by\\np(x|D; ϕ) =\\nZ\\nZ\\np(x|z; ϕx)pz(z|D; ϕz)dz,\\n(6.76)\\nwhere pz is the prior over z and we marginalize out z. This approach is often\\nreferred as neural processes [Garnelo et al., 2018]. Because this marginalization\\nis often intractable, it is a common practice to approach it from variational\\ninference and learning which we learned already in §4.3.1.\\nOverall, these approaches are referred as meta-learning, since such a proce-\\ndure results in a predictor that knows how to learn to solve a problem given a\\nset of new examples. Meta-learning can then be used to solve not only learning\\nproblems but also any kind of set-to-set problems, such as causal discovery and\\nstatistical inference problems. This is an exciting and active area of research.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 101}, page_content='98\\nCHAPTER 6. FURTHER TOPICS\\n6.4\\nRegression: Mixture Density Networks\\nLet e([x, y], θ) be the energy function where y is not categorical with a small\\nnumber of categories.6 Without loss of generality, let y ∈Rd. We can turn this\\ninto a probability density function by\\np(y|x; θ) =\\nexp (−e([x, y]; θ))\\nR\\nRd exp (−e([x, y′]; θ)) dy′ .\\n(6.77)\\nUnlike the classification problem we saw in Eq. (2.30), it is extremely challenging\\nto compute the normalization constant in this case with a non-categorical y,\\nin general. In fact, this problem is identical to undirected graphical models,\\nsuch as restricted Boltzmann machines from §5.1, which requires costly MCMC\\nsampling [Boulanger-Lewandowski et al., 2012].\\nIt is thus natural to consider the parametrization of the energy function so\\nthat the normalization constant is automatically 1. We have already considered\\none particular approach under this paradigm earlier in §4.3.1. With a latent\\nvariable z (an unobserved variable), we can make it readily normalized:\\np(y|x; θ) =\\nZ\\nZ\\np(z)\\nexp(−e([x, y], z, θ))\\nR\\nRd exp(−e([x, y′], z, θ)dy′ dz.\\n(6.78)\\nIf we choose the following parametrization of the energy function, we know how\\nto compute the normalization constant exactly, because we end up with the\\nGaussian distribution over y given x and z:\\ne([x, y], z, θ) = 1\\n2∥y −µ(x, z; θ)∥2.\\n(6.79)\\nUnfortunately, this approach is not trivial either, as we must marginalize out the\\nlatent variable z. This marginalization problem is not easily solvable in general,\\nand we often need to resort to an approximate approach, such as variational\\ninference [Chung et al., 2015].\\nBecause y is often lower-dimensional than x , there is a tractable alternative\\nto these two intractable approaches. This approach constrains the latent variable\\napproach above so that |Z| ≪∞, that is, z can take one of only a few possible\\nvalues, i.e., Z = {1, 2, . . . , K}. In that case, we can solve the marginalization\\nproblem exactly and arrive at\\np(y|x; θ) =\\nK\\nX\\nz=1\\n1\\nK N\\n\\x00y; µz(x), σ2\\nz(x)I\\n\\x01\\n,\\n(6.80)\\nassuming\\ne([x, y], z, θ) =\\n1\\n2σ2z(F(x; θF ); θσ) ∥y −µz(F(x; θF ); θµ)∥2 .\\n(6.81)\\n6A categorical variable takes a value out of a small number of predefined possible values,\\njust like classification.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 102}, page_content='6.4. REGRESSION: MIXTURE DENSITY NETWORKS\\n99\\nThis is exactly a mixture of Gaussians however conditioned on x. F(x; θF ) is a\\nfeature extractor of the input x, and this extractor is shared between µ and σ2.\\nWe further assumed that the prior over the mixture components was uniform,\\ni.e. p(z) =\\n1\\nK . This is not strictly necessary, but simply makes learning easier,\\nas we remove any extra parameters for computing the prior from the input x.\\nAs long as µz and σ2\\nz are differentiable w.r.t. θµ , θσ and θF (collectively,\\ncomprising θ,) we can train this predictor all together without having to rely on\\nsome approximate marginalization by\\nmin\\nθF ,θµ,θσ −1\\nN\\nN\\nX\\nn=1\\nlog\\nK\\nX\\nz=1\\n1\\nK N(yn; µz(xn), σ2\\nz(xn)I).\\n(6.82)\\nSuch a predictor is called a mixture density network and outdates all the other\\napproaches above [Bishop, 1994].\\nA main special case of this mixture density network is when there is only\\none mixture component, i.e. K = 1. In that case, this reduces to a more famil-\\niar linear regression with the mean squared error loss function, assuming the\\nconstant variance, i.e., σ2\\nz(x) = c. Although this is a usual approach and also\\nwhat we did earlier when we derived backpropagation in §2.2.2, this approach\\nof a single mixture component has a major disadvantage is that there can only\\nbe a single mode in the predictive distribution. This is particularly problematic\\nwhen the underlying true distribution has multiple local modes, as learning with\\nthe criterion above would make this predicted distribution to be dispersed in\\norder to cover all those multiple modes of the true distribution, resulting in an\\nunnecessarily uncertain prediction with the probability mass concentrated on a\\nregion of the output space that is relevant to where true modes are. By increas-\\ning K beyond 1, we increase the chance of capturing the inherent uncertainty\\nin regression.\\nAlthough training can be done exactly, this does not imply that we can make\\nprediction readily with the mixture density network. Unless K = 1, there is no\\nanalytical solution to\\nˆy(x) = arg max\\ny∈Rd log\\nK\\nX\\nz=1\\nN(y; µz(x), σ2\\nz(x)I) −log K.\\n(6.83)\\nWe can solve this problem by gradient descent which will find one of at most K\\nmodes of this complex distribution or find a saddle point.\\nIt however is unsatisfactory to return a single point estimate of the solution,\\nwhen we trained our predictor to capture the full distribution over the output\\nspace. Rather, it may be desirable to return a set of possible values of the\\noutcome y that are within a credible region, following the procedure from §2.4.3.\\nThis is particularly desirable, as we can readily draw as many independent\\nsamples from the mixture of Gaussians. Once the samples {y1, . . . , yM} are\\ndrawn, we score each sample with the mixture density network, which is again\\ntrivial, resulting in {p1, . . . , pM}. We can then fit a cumulative density function\\non these scores and pick only those that are above a predefined threshold. These\\nselected outputs can be considered a credible set of outputs for x.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 103}, page_content='100\\nCHAPTER 6. FURTHER TOPICS\\n6.5\\nCausality\\nA major limitation of all methods in this lecture note, perhaps except for rein-\\nforcement learning in §6.1, is that they all rely almost entirely on association, or\\ncorrelation. These algorithms all look for which patterns appear together with\\nwhich other patterns frequently within a given dataset.\\nAlready in §2.2.2, this was apparent. For instance, recall the following update\\nrule for a linear block in Eq. (2.53):\\n∂\\n∂uij\\n= xihj −xiˆhj,\\n(6.84)\\nwhere we assume there was no nonlinearity, i.e. h′\\nj = 1. The first term decreases\\nthe value of uij toward the origin 0 if xi and the old, undesired value of the\\nj-th hidden neuron had the same sign.7 The second term on the other hand\\nincreases the value of uij away from the origin if xi and the new, desirable ˆhj\\nhave the same sign. In other words, uij, one of the many parameters of this\\npredictor, encodes how correlated the i-th dimension of the observation and the\\nj-th dimension of the hidden variable are with each other.\\nThis is perfectly fine, if the goal is to capture such correlations and use them\\nto impute missing values, such as outputs associated with test-time observa-\\ntions. This is not enough however if we want to infer the causal relationship\\namong variables, because as we often say casually, “correlation does not imply\\ncausation.”8\\nLet us dig slightly deeper into this statement and consider a few cases where\\ncorrelation exists but causation does not. The first case is when there exists an\\nunobserved confounder, where the confounder z is defined to affect both the\\ninput x and the outcome y, such that\\nx\\ny\\nz\\nBoth x and y are caused by this unobserved confounder z in this diagram,\\nand we can write down the marginal distribution over (x, y) as\\np(x, y) =\\nZ\\np(x|z)p(y|z)p(z)dz.\\n(6.85)\\nIt is relatively straightforward to see that this would not be factorized into the\\nproduct of p(x) and p(z), i.e.\\nZ\\np(x|z)p(y|z)p(z)dz ̸= p(x)p(z),\\n(6.86)\\n7We are following the opposite of the gradient direction.\\n8When we say this, we are referring to dependence by correlation, but unless it is technically\\nconfusing, I will interchangeably use correlation and dependence in this section.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 104}, page_content='6.5. CAUSALITY\\n101\\nunless\\nZ\\np(x|z)p(y|z)p(z)dz = p(x)\\nZ\\np(y|z)p(z)dz,\\n(6.87)\\nwhich would imply that there is no edge going from z to x in the first place.\\nThat we cannot factor p(x, y) into the product of the marginals of x and\\ny implies that x and y are dependent on each other. Equivalently, we can say\\nthat x and y are correlated with each other (potentially nonlinearly.) They are\\nhowever unrelated to each other causally, since intervening on x would not cause\\nany change in y and vice versa.\\nAn example of this case of an unobserved confounder can be found in driving.\\nIf one is not aware of how driving works and only looks at the dashboard of a\\ncar,9 it is easy to see that the turn indicator and the steering wheel angle\\nare highly correlated with each other, which may result in an incorrect causal\\nconclusion that the turn indicator causes the steering wheel to turn, or vice\\nversa. This is missing a big confounder that is a driver and their intention to\\nturn the car.\\nThe second case is what we often referred to as confirmation bias. Consider\\nthe following causal model:\\nx\\ny\\nz\\nIn this case, x and y are independent of each other a priori. It is clear\\nthat they are not causally related to each other, since manually setting one\\nof these to a particular value should not change the value taken by the other\\nvariable. It is however interesting to observe that these two variables, x and y,\\nare suddenly dependent on each other, once we observe z. That is, under the\\nposterior distribution, x and y are not independent:\\np(x, y|z) =\\np(x)p(y)p(z|x, y)\\nR\\np(x′)p(y′)p(z|x′, y′)dx′dy′ .\\n(6.88)\\nBecause of p(z|x, y), we cannot factor p(x, y|z) into the product of two terms,\\neach of which depends only on either x or y. If we could, that would imply that\\nz is caused by either one of x or y (or neither.) The input and outcome are\\ncorrelated in this case, because we only selectively consider a subset of (x, y)\\npairs that are associated with a particular value of z. This is thus also called a\\nselection bias.\\nLet us consider an example, where x corresponds to a burglary and y to an\\nearthquake. z is a house alarm. The house alarm goes off (z = 1) when either\\nthere is burglary (x = 1) or there is an earthquake (y = 1). It is pretty safe for\\n9Imagine you are collecting data from the car to build a self-driving model.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 105}, page_content='102\\nCHAPTER 6. FURTHER TOPICS\\nnow to assume that the chances of burglary and earthquake are pretty much\\nindependent of each other. If you however hear that your alarm went off, that\\nis, if you condition on z = 1, burglary and earthquake are not independent\\nanymore, since I would be able to explain away the chance of burglary if I felt\\nearthquake myself. That is, what’s the chance that earthquake and burglary\\nhappened together and triggered the alarm. Although there is no causal rela-\\ntionship between the earthquake and burglary, they are now correlated with\\neach other negatively because we are conditioned on alarm going off.\\nThese cases emphasize the difference between association (correlation) and\\ncausality. In order to capture causal relationships among variables and use them\\nto control the underlying system, we must use an extra set of assumptions and\\ntools to rule out non-causal associations, or so-called spurious correlations. Once\\nwe are equipped with such tools, we can make machine learning more robust in\\nmore realistic scenarios, for instance where the distribution from which obser-\\nvations are drawn shifts between training and test times. This is a fascinating\\ntopic in machine learning and more broadly artificial intelligence, but is out\\nof the scope of this course. I suggest you check out my lecture note “A Brief\\nIntroduction to Causal Inference in Machine Learning” [Cho, 2024] and then\\nmove on to more in-depth materials on causal inference, causal discovery and\\ncausal representation learning.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 106}, page_content='Bibliography\\nD. H. Ackley, G. E. Hinton, and T. J. Sejnowski.\\nA learning algorithm for\\nboltzmann machines. Cognitive science, 9(1):147–169, 1985.\\nJ. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\nD. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly\\nlearning to align and translate.\\nIn International Conference on Learning\\nRepresentations, 2015.\\nA. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind. Automatic\\ndifferentiation in machine learning: a survey. Journal of Machine Learning\\nResearch, 18(153):1–43, 2018. URL http://jmlr.org/papers/v18/17-468.\\nhtml.\\nY. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with\\ngradient descent is difficult.\\nIEEE transactions on neural networks, 5(2):\\n157–166, 1994.\\nJ. Bergstra, R. Bardenet, Y. Bengio, and B. K´egl.\\nAlgorithms for hyper-\\nparameter optimization.\\nIn J. Shawe-Taylor, R. Zemel, P. Bartlett,\\nF.\\nPereira,\\nand\\nK.\\nWeinberger,\\neditors,\\nAdvances\\nin\\nNeural\\nInfor-\\nmation Processing Systems, volume 24. Curran Associates, Inc., 2011.\\nURL https://proceedings.neurips.cc/paper_files/paper/2011/file/\\n86e8f7ab32cfd12577bc2619bc635690-Paper.pdf.\\nC. M. Bishop. Mixture density networks. Technical report, Neural Computing\\nResearch Group, Aston University, Birmingham, UK, 1994.\\nN. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Modeling temporal de-\\npendencies in high-dimensional sequences: Application to polyphonic music\\ngeneration and transcription. In Proceedings of the 29th International Con-\\nference on Machine Learning (ICML-12), pages 1159–1166. ICML, 2012.\\nL. Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.\\n103'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 107}, page_content='104\\nBIBLIOGRAPHY\\nJ. S. Bridle. Probabilistic interpretation of feedforward classification network\\noutputs, with relationships to statistical pattern recognition. In Neurocom-\\nputing: Algorithms, architectures and applications, pages 227–236. Springer,\\n1990.\\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Nee-\\nlakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot\\nlearners. Advances in neural information processing systems, 33:1877–1901,\\n2020.\\nK. Cho. Natural language understanding with distributed representation. arXiv\\npreprint arXiv:1511.07916, 2015.\\nK. Cho. A brief introduction to causal inference in machine learning. arXiv\\npreprint arXiv:2405.08793, 2024.\\nK. Cho, B. van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,\\nH. Schwenk, and Y. Bengio.\\nLearning phrase representations using RNN\\nencoder-decoder for statistical machine translation. In Proceedings of the 2014\\nConference on Empirical Methods in Natural Language Processing (EMNLP),\\npages 1724–1734, Doha, Qatar, 2014. Association for Computational Linguis-\\ntics. URL https://aclanthology.org/D14-1179.\\nJ. Chung, K. Kastner, L. Dinh, K. Goel, A. Courville, and Y. Bengio. A recurrent\\nlatent variable model for sequential data. In Advances in neural information\\nprocessing systems, pages 2980–2988, 2015.\\nC. Cortes. Support-vector networks. Machine Learning, 1995.\\nJ. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online\\nlearning and stochastic optimization. Journal of machine learning research,\\n12(7), 2011.\\nC. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adapta-\\ntion. International Conference on Machine Learning, pages 1126–1135, 2017.\\nJ. H. Friedman.\\nGreedy function approximation: A gradient boosting ma-\\nchine. The Annals of Statistics, 29(5):1189–1232, 2001. doi: 10.1214/aos/\\n1013203451. URL https://projecteuclid.org/euclid.aos/1013203451.\\nM. Garnelo, J. Schwarz, D. Rosenbaum, F. Viola, D. J. Rezende, S. Eslami, and\\nY. W. Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018.\\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,\\nA. Courville, and Y. Bengio. Generative adversarial nets. In Advances in\\nneural information processing systems, pages 2672–2680, 2014.\\nA. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch¨olkopf, and A. Smola. A\\nkernel two-sample test. Journal of Machine Learning Research, 13(Mar):723–\\n773, 2012.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 108}, page_content='BIBLIOGRAPHY\\n105\\nW. K. Hastings. Monte carlo sampling methods using Markov chains and their\\napplications. Biometrika, 57(1):97–109, 1970. doi: 10.1093/biomet/57.1.97.\\nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recogni-\\ntion. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition, pages 770–778, 2016.\\nD. O. Hebb. The Organization of Behavior: A Neuropsychological Theory. Wiley\\n& Sons, New York, 1949.\\nG. E. Hinton. Training products of experts by minimizing contrastive diver-\\ngence. Neural computation, 14(8):1771–1800, 2002.\\nD. Hjelm, R. R. Salakhutdinov, K. Cho, N. Jojic, V. Calhoun, and J. Chung.\\nIterative refinement of the approximate posterior for directed belief networks.\\nAdvances in neural information processing systems, 29, 2016.\\nH. Hotelling. Analysis of a complex of statistical variables into principal com-\\nponents. Journal of educational psychology, 24(6):417, 1933.\\nA. Ilin and T. Raiko. Practical approaches to principal component analysis in\\nthe presence of missing values. The Journal of Machine Learning Research,\\n11:1957–2000, 2010.\\nS. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training\\nby reducing internal covariate shift. Proceedings of the 32nd International\\nConference on Machine Learning, 37:448–456, 2015.\\nE. T. Jaynes. Information theory and statistical mechanics. Physical review,\\n106(4):620, 1957.\\nD. R. Jones, M. Schonlau, and W. J. Welch. Efficient global optimization of\\nexpensive black-box functions. Journal of Global Optimization, 13(4):455–492,\\n1998. doi: 10.1023/A:1008352424803.\\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv\\npreprint arXiv:1412.6980, 2014.\\nD. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint\\narXiv:1312.6114, 2013.\\nR. Kohavi. A study of cross-validation and bootstrap for accuracy estimation\\nand model selection. Ijcai, 14(2):1137–1145, 1995.\\nA. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep\\nconvolutional neural networks. In Advances in neural information processing\\nsystems, pages 1097–1105, 2012.\\nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning ap-\\nplied to document recognition. Proceedings of the IEEE, 86(11):2278–2324,\\n1998.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 109}, page_content='106\\nBIBLIOGRAPHY\\nY. LeCun, S. Chopra, R. Hadsell, M. Ranzato, F. Huang, et al. A tutorial on\\nenergy-based learning. Predicting structured data, 1(0), 2006.\\nD. A. McAllester. Some PAC-Bayes theorems. In Proceedings of the Twelfth\\nAnnual Conference on Computational Learning Theory, pages 230–234. ACM,\\n1999.\\nR. M. Neal. Hybrid monte carlo. Technical Report CRG-TR-93-1, Department\\nof Computer Science, University of Toronto, 1993.\\nR. M. Neal. Bayesian learning for neural networks, volume 118 of Lecture Notes\\nin Statistics. Springer Science & Business Media, New York, 1996.\\nR. M. Neal and G. E. Hinton. A view of the em algorithm that justifies in-\\ncremental, sparse, and other variants. In Learning in graphical models, pages\\n355–368. Springer, 1998.\\nJ. Nocedal and S. J. Wright.\\nNumerical Optimization.\\nSpringer Science &\\nBusiness Media, 2nd edition, 2006. ISBN 978-0-387-30303-1.\\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang,\\nS. Agarwal, K. Slama, A. Ray, et al. Training language models to follow in-\\nstructions with human feedback. Advances in Neural Information Processing\\nSystems, 35:27730–27744, 2022.\\nB. Peters, V. Niculae, and A. F. Martins. Sparse sequence-to-sequence models.\\narXiv preprint arXiv:1905.05702, 2019.\\nF. Rosenblatt. The perceptron: a probabilistic model for information storage\\nand organization in the brain. Psychological review, 65(6):386, 1958.\\nR. Y. Rubinstein and D. P. Kroese. The cross-entropy method: a unified ap-\\nproach to combinatorial optimization, Monte-Carlo simulation and machine\\nlearning. Springer Science & Business Media, 2004.\\nD. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations\\nby back-propagating errors. nature, 323(6088):533–536, 1986.\\nR. E. Schapire.\\nThe strength of weak learnability. Machine Learning, 5(2):\\n197–227, 1990. doi: 10.1007/BF00116037.\\nP. Smolensky.\\nInformation processing in dynamical systems: Foundations of\\nharmony theory. In D. E. Rumelhart and J. L. McClelland, editors, Paral-\\nlel Distributed Processing: Explorations in the Microstructure of Cognition,\\nVolume 1: Foundations, pages 194–281. MIT Press, Cambridge, MA, 1986.\\nJ. Su, Y. Lu, S. Pan, B. Murtadha, S. Wen, and Y. Liu. Roformer: Enhanced\\ntransformer with rotary position embedding. In Proceedings of the 2021 In-\\nternational Conference on Learning Representations, 2021.\\nR. Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'file_path': 'rag-dataset-main/machine-learning/2505.03861v1.pdf', 'total_pages': 111, 'format': 'PDF 1.5', 'title': 'Machine Learning: a Lecture Note', 'author': 'Kyunghyun Cho', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 110}, page_content='BIBLIOGRAPHY\\n107\\nR. S. Sutton. Learning to predict by the methods of temporal differences. Ma-\\nchine learning, 3:9–44, 1988.\\nM. E. Tipping and C. M. Bishop. Probabilistic principal component analysis.\\nJournal of the Royal Statistical Society: Series B (Statistical Methodology),\\n61(3):611–622, 1999.\\nV. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of rela-\\ntive frequencies of events to their probabilities. Theory of Probability & Its\\nApplications, 16(2):264–280, 1971.\\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\n L. Kaiser, and I. Polosukhin.\\nAttention is all you need.\\nIn Advances in\\nneural information processing systems, pages 5998–6008, 2017.\\nM. Welling and Y. W. Teh. Bayesian learning via stochastic gradient langevin\\ndynamics. Proceedings of the 28th international conference on machine learn-\\ning (ICML-11), pages 681–688, 2011.\\nWikipedia contributors.\\nBias–variance tradeoff — Wikipedia, the free en-\\ncyclopedia, 2023. URL https://en.wikipedia.org/w/index.php?title=\\nBias%E2%80%93variance_tradeoff&oldid=1178072263.\\n[Online; accessed\\n16-October-2023].\\nJ. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative adversarial net-\\nwork. arXiv preprint arXiv:1609.03126, 2016.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='MACHINE LEARNING  \\n[R17A0534] \\nLECTURE NOTES \\n \\nB.TECH IV YEAR – I SEM(R17) \\n(2020-21) \\n \\n \\n \\n \\n \\n \\nDEPARTMENT OF \\nCOMPUTER SCIENCE AND ENGINEERING \\nMALLA REDDY COLLEGE OF ENGINEERING & \\nTECHNOLOGY \\n(Autonomous Institution – UGC, Govt. of India) \\nRecognized under 2(f) and 12 (B) of UGC ACT 1956 \\n(Affiliated to JNTUH, Hyderabad, Approved by AICTE - Accredited by NBA & NAAC – ‘A’ Grade - ISO 9001:2015 Certified) \\nMaisammaguda, Dhulapally (Post Via. Hakimpet), Secunderabad – 500100, Telangana State, India'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='IV Year B. Tech. CSE –II Sem   \\n \\n \\n \\n \\n \\n                L   T/P/D   C  \\n  4   1/- / -   3  \\n(R17A0534) Machine Learning \\nObjectives:  \\n\\uf0b7 Acquire theoretical Knowledge on setting hypothesis for pattern recognition. \\n\\uf0b7 Apply suitable machine learning techniques for data handling and to gain knowledge from it. \\n\\uf0b7 Evaluate the performance of algorithms and to provide solution for various real world \\napplications. \\n \\nUNIT I:  \\nIntroduction to Machine Learning \\nIntroduction ,Components of Learning , Learning Models , Geometric Models, Probabilistic \\nModels, Logic Models, Grouping and Grading, Designing a Learning System, Types of \\nLearning, Supervised, Unsupervised, Reinforcement, Perspectives and Issues, Version Spaces, \\nPAC Learning, VC Dimension.  \\n \\nUNIT II:  \\nSupervised and Unsupervised Learning \\nDecision Trees: ID3, Classification and Regression Trees, Regression: Linear Regression, Multiple Linear \\nRegression, Logistic Regression, Neural Networks: Introduction, Perception, Multilayer Perception, \\nSupport Vector Machines: Linear and Non-Linear, Kernel Functions, K Nearest Neighbors.  \\nIntroduction to clustering, K-means clustering, K-Mode Clustering. \\n \\nUNIT III:  \\nEnsemble and Probabilistic Learning \\nModel Combination Schemes, Voting, Error-Correcting Output Codes, Bagging: Random Forest Trees, \\nBoosting: Adaboost, Stacking. \\nGaussian mixture models - The Expectation-Maximization (EM) Algorithm, Information Criteria, Nearest \\nneighbour methods - Nearest Neighbour Smoothing, Efficient Distance Computations: the KD-Tree, \\nDistance Measures. \\n \\n \\nUNIT IV:  \\nReinforcement Learning and Evaluating Hypotheses \\nIntroduction, Learning Task, Q Learning, Non deterministic Rewards and actions, temporal-difference \\nlearning, Relationship to Dynamic Programming, Active reinforcement learning, Generalization in \\nreinforcement learning. \\nMotivation, Basics of Sampling Theory: Error Estimation and Estimating Binomial Proportions, The \\nBinomial Distribution, Estimators, Bias, and Variance   \\n \\n \\nUNIT V:  \\nGenetic Algorithms: Motivation, Genetic Algorithms: Representing Hypotheses, Genetic Operator, \\nFitness Function and Selection, An Illustrative Example, Hypothesis Space Search, Genetic \\nProgramming, Models of Evolution and Learning: Lamarkian Evolution, Baldwin Effect, Parallelizing \\nGenetic Algorithms.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='TEXT BOOKS:  \\n \\n1. Ethem  Alpaydin, ”Introduction to Machine Learning”, MIT Press, Prentice Hall of India, 3rd \\nEdition2014. \\n2. Mehryar Mohri, Afshin Rostamizadeh, Ameet Talwalkar ” Foundations of Machine Learning”, MIT \\nPress,2012. \\n3. Tom Mitchell, “Machine Learning”, McGraw Hill, 3rdEdition, 1997. \\n4. MACHINE LEARNING - An Algorithmic Perspective, Second Edition, Stephen Marsland, 2015. \\n \\nREFERENCE BOOKS:  \\n1. \\nCharuC.Aggarwal,“DataClassificationAlgorithmsandApplications”,CRCPress,2014. \\n2. \\nCharu C. Aggarwal, “DATA CLUSTERING Algorithms and Applications”, CRC Press,  \\n       2014. \\n3. \\nKevin P. Murphy ”Machine Learning: A Probabilistic Perspective”, The MIT Press, 2012 \\n4. \\nJiawei Han and Micheline Kambers and JianPei, “Data Mining Concepts  \\n      andTechniques”,3rd edition, Morgan Kaufman Publications, 2012. \\n \\n \\nOUTCOMES:  \\n1. Recognize the characteristics of Machine Learning techniques that enable to solve real world \\nproblems \\n2. Recognize the characteristics of machine learning strategies \\n3. Apply various supervised learning methods to appropriate problems \\n4. Identify and integrate more than one techniques to enhance the performance of learning \\n5. Create probabilistic and unsupervised learning models for handling unknown pattern \\n6. Analyze the co-occurrence of data to find interesting frequent patterns'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='INDEX \\nUNIT NO \\nTOPIC \\nPAGE NO \\nI \\nIntroduction \\n1 \\nLearning Models \\n3 \\nDesigning a Learning System \\n7 \\nTypes of Learning \\n12 \\nPerspectives and Issues \\n13 \\nVersion Spaces \\n14 \\nPAC Learning \\n19 \\nVC Dimension \\n21 \\nII \\nDecision Trees \\n23 \\nClassification and Regression Trees \\n27 \\nNeural Networks \\n37 \\nSupport Vector Machines \\n45 \\nIntroduction to clustering \\n49 \\nK-means clustering \\n52 \\nIII \\nModel Combination Schemes \\n55 \\nVoting, Error-Correcting Output Codes \\n57 \\nBagging, Random Forest Trees \\n61 \\nBoosting, Adaboost \\n65 \\nGaussian mixture models \\n68 \\nEM Algorithms \\n69 \\nEfficient Distance Computations \\n73'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='IV \\nReinforcement Learning \\n78 \\nLearning Task \\n79 \\nQ Learning \\n82 \\nEvaluating Hypotheses \\n86 \\nBasics of Sampling Theory \\n88 \\nV \\nGenetic Algorithms \\n92 \\nAn Illustrative Example \\n96 \\nHypothesis Space Search \\n98 \\nGenetic Programming \\n101 \\nModels of Evolution and Learning \\n104 \\nParallelizing Genetic Algorithms. \\n105'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='1 \\n \\nUNIT I  \\nIntroduction to Machine Learning \\n1. Introduction \\n \\n1.1 What Is Machine Learning?  \\nMachine learning is programming computers to optimize a performance criterion using example \\ndata or past experience. We have a model defined up to some parameters, and learning is the \\nexecution of a computer program to optimize the parameters of the model using the training data or \\npast experience. The model may be predictive to make predictions in the future, or descriptive to gain \\nknowledge from data, or both. \\nArthur Samuel, an early American leader in the field of computer gaming and artificial intelligence, \\ncoined the term “Machine Learning” in 1959 while at IBM. He defined machine learning as “the field of \\nstudy that gives computers the ability to learn without being explicitly programmed.” However, there is \\nno universally accepted definition for machine learning. Different authors define the term differently. \\n \\nDefinition of learning \\nDefinition \\nA computer program is said to learn from experience E with respect to some class of tasks T and \\nperformance measure P, if its performance at tasks T, as measured by P, improves with experience E. \\n \\nExamples \\ni) Handwriting recognition learning problem \\n• Task T: Recognising and classifying handwritten words within images \\n• Performance P: Percent of words correctly classified \\n• Training experience E: A dataset of handwritten words with given classifications \\nii) A robot driving learning problem \\n• Task T: Driving on highways using vision sensors \\n• Performance measure P: Average distance traveled before an error \\n• training experience: A sequence of images and steering commands recorded while  \\n  observing a human driver \\niii) A chess learning problem \\n• Task T: Playing chess \\n• Performance measure P: Percent of games won against opponents \\n• Training experience E: Playing practice games against itself \\nDefinition \\nA computer program which learns from experience is called a machine learning program or \\nsimply a learning program. Such a program is sometimes also referred to as a learner. \\n \\n1.2 Components of Learning \\n Basic components of learning process \\nThe learning process, whether by a human or a machine, can be divided into four components, \\nnamely, data storage, abstraction, generalization and evaluation. Figure 1.1 illustrates the \\nvariouscomponents and the steps involved in the learning process.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='2 \\n \\n \\n1. Data storage \\nFacilities for storing and retrieving huge amounts of data are an important component of the \\nlearning process. Humans and computers alike utilize data storage as a foundation for advanced \\nreasoning. \\n• In a human being, the data is stored in the brain and data is retrieved using electrochemical    signals. \\n• Computers use hard disk drives, flash memory, random access memory and similar devices    to store \\ndata and use cables and other technology to retrieve data. \\n \\n2. Abstraction \\nThe second component of the learning process is known as abstraction. \\nAbstraction is the process of extracting knowledge about stored data. This involves creating general \\nconcepts about the data as a whole. The creation of knowledge involves application of known models \\nand creation of new models. \\nThe process of fitting a model to a dataset is known as training. When the model has been trained, the \\ndata is transformed into an abstract form that summarizes the original information. \\n \\n3. Generalization \\nThe third component of the learning process is known as generalisation. \\nThe term generalization describes the process of turning the knowledge about stored data into a form \\nthat can be utilized for future action. These actions are to be carried out on tasks that are similar, but \\nnot identical, to those what have been seen before. In generalization, the goal is to discover those \\nproperties of the data that will be most relevant to future tasks. \\n \\n4. Evaluation \\nEvaluation is the last component of the learning process. \\nIt is the process of giving feedback to the user to measure the utility of the learned knowledge. This \\nfeedback is then utilised to effect improvements in the whole learning process \\n \\nApplications of machine learning \\nApplication of machine learning methods to large databases is called data mining. In data \\nmining, a large volume of data is processed to construct a simple model with valuable use, for example, \\nhaving \\nhigh predictive accuracy. \\n \\nThe following is a list of some of the typical applications of machine learning. \\n1. In retail business, machine learning is used to study consumer behaviour. \\n2. In finance, banks analyze their past data to build models to use in credit applications, fraud \\ndetection, and the stock market. \\n3. In manufacturing, learning models are used for optimization, control, and troubleshooting.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='3 \\n \\n4. In medicine, learning programs are used for medical diagnosis. \\n5. In telecommunications, call patterns are analyzed for network optimization and maximizing the \\nquality of service. \\n6. In science, large amounts of data in physics, astronomy, and biology can only be analyzed fast \\nenough by computers. The World Wide Web is huge; it is constantly growing and searching for \\nrelevant information cannot be done manually. \\n7. In artificial intelligence, it is used to teach a system to learn and adapt to changes so that the \\nsystem designer need not foresee and provide solutions for all possible situations.  \\n8. It is used to find solutions to many problems in vision, speech recognition, and robotics.  \\n9. Machine learning methods are applied in the design of computer-controlled vehicles to steer \\ncorrectly when driving on a variety of roads. \\n10. Machine learning methods have been used to develop programmes for playing games such as \\nchess, backgammon and Go. \\n \\n1.3 Learning Models \\nMachine learning is concerned with using the right features to build the right models that \\nachieve the right tasks.  The basic idea of Learning models has divided into three categories. \\nFor a given problem, the collection of all possible outcomes represents the sample space or instance \\nspace. \\n \\n\\uf0b7 \\nUsing a Logical expression. (Logical models) \\n\\uf0b7 \\nUsing the Geometry of the instance space. (Geometric models)  \\n\\uf0b7 \\nUsing Probability to classify the instance space. (Probabilistic models) \\n\\uf0b7 \\nGrouping and Grading \\n \\n1.3.1  Logical models \\nLogical models use a logical expression to divide the instance space into segments and hence \\nconstruct grouping models. A logical expression is an expression that returns a Boolean value, i.e., a \\nTrue or False outcome. Once the data is grouped using a logical expression, the data is divided into \\nhomogeneous groupings for the problem we are trying to solve.  For example, for a classiﬁcation \\nproblem, all the instances in the group belong to one class. \\n \\nThere are mainly two kinds of logical models: Tree models and Rule models. \\n \\nRule models consist of a collection of implications or IF-THEN rules. For tree-based models, the ‘if-part’ \\ndeﬁnes a segment and the ‘then-part’ deﬁnes the behaviour of the model for this segment. Rule models \\nfollow the same reasoning. \\n \\nLogical models and Concept learning \\nTo understand logical models further, we need to understand the idea of Concept Learning. \\nConcept Learning involves learning logical expressions or concepts from examples. The idea of Concept \\nLearning fits in well with the idea of Machine learning, i.e., inferring a general function from specific \\ntraining examples. Concept learning forms the basis of both tree-based and rule-based models.  More \\nformally, Concept Learning involves acquiring the definition of a general category from a given set of \\npositive and negative training examples of the category. A Formal Definition for Concept Learning is \\n“The inferring of a Boolean-valued function from training examples of its input and output.” In \\nconcept learning, we only learn a description for the positive class and label everything that doesn’t \\nsatisfy that description as negative.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='4 \\n \\n \\n \\n \\n \\n \\n \\n \\nThe following example explains this idea in more detail. \\n \\n \\n \\nA Concept Learning Task called “Enjoy Sport” as shown above is defined by a set of data from \\nsome example days. Each data is described by six attributes. The task is to learn to predict the value of \\nEnjoy Sport for an arbitrary day based on the values of its attribute values. The problem can be \\nrepresented by a series of hypotheses. Each hypothesis is described by a conjunction of constraints on \\nthe attributes. The training data represents a set of positive and negative examples of the target \\nfunction. In the example above, each hypothesis is a vector of six constraints, specifying the values of \\nthe six attributes –  Sky, AirTemp, Humidity, Wind, Water, and Forecast. The training phase involves \\nlearning the set of days (as a conjunction of attributes) for which Enjoy Sport = yes. \\n \\nThus, the problem can be formulated as: \\n \\n\\uf0b7 \\nGiven instances X  which represent a set of all possible days, each described by the attributes: \\no \\nSky – (values: Sunny, Cloudy, Rainy), \\no \\nAirTemp – (values: Warm, Cold), \\no \\nHumidity – (values: Normal, High), \\no \\nWind – (values: Strong, Weak), \\no \\nWater – (values: Warm, Cold), \\no \\nForecast – (values: Same, Change). \\n \\nTry to identify a function that can predict the target variable Enjoy Sport as yes/no, i.e., 1 or 0. \\n \\n1.3.2 Geometric models \\nIn the previous section, we have seen that with logical models, such as decision trees, a logical \\nexpression is used to partition the instance space. Two instances are similar when they end up in the \\nsame logical segment. In this section, we consider models that define similarity by considering the \\ngeometry of the instance space.  In Geometric models, features could be described as points in two \\ndimensions (x- and y-axis) or a three-dimensional space (x, y, and z). Even when features are not'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='5 \\n \\nintrinsically geometric, they could be modelled in a geometric manner (for example, temperature as a \\nfunction of time can be modelled in two axes). In geometric models, there are two ways we could \\nimpose similarity. \\n\\uf0b7 \\nWe could use geometric concepts like lines or planes to segment (classify) the instance space. \\nThese are called Linear models. \\n\\uf0b7 \\nAlternatively, we can use the geometric notion of distance to represent similarity. In this case, if \\ntwo points are close together, they have similar values for features and thus can be classed as \\nsimilar. We call such models as Distance-based models. \\n \\n \\nLinear models \\nLinear models are relatively simple. In this case, the function is represented as a linear \\ncombination of its inputs. Thus, if x1 and x2 are two scalars or vectors of the same dimension \\nand a and b are arbitrary scalars, then ax1 + bx2 represents a linear combination of x1 and x2. In the \\nsimplest case where f(x) represents a straight line, we have an equation of the form f (x) \\n= mx + c where c represents the intercept and m represents the slope. \\n \\n \\nLinear models are parametric, which means that they have a ﬁxed form with a small number of numeric \\nparameters that need to be learned from data. For example, in f (x) = mx + c, m and c are the \\nparameters that we are trying to learn from the data. This technique is different from tree or rule \\nmodels, where the structure of the model (e.g., which features to use in the tree, and where) is not \\nﬁxed in advance. \\n \\nLinear models are stable, i.e., small variations in the training data have only a limited impact on the \\nlearned model. In contrast, tree models tend to vary more with the training data, as the choice of a \\ndifferent split at the root of the tree typically means that the rest of the tree is different as well.  As a \\nresult of having relatively few parameters, Linear models have low variance and high bias. This implies \\nthat Linear models are less likely to overfit the training data than some other models. However, they \\nare more likely to underfit. For example, if we want to learn the boundaries between countries based \\non labelled data, then linear models are not likely to give a good approximation. \\n \\nDistance-based models \\nDistance-based models are the second class of Geometric models. Like Linear models, distance-\\nbased models are based on the geometry of data. As the name implies, distance-based models work on \\nthe concept of distance.  In the context of Machine learning, the concept of distance is not based on \\nmerely the physical distance between two points. Instead, we could think of the distance between two \\npoints considering the mode of transport between two points. Travelling between two cities by plane'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='6 \\n \\ncovers less distance physically than by train because a plane is unrestricted. Similarly, in chess, the \\nconcept of distance depends on the piece used – for example, a Bishop can move diagonally.   Thus, \\ndepending on the entity and the mode of travel, the concept of distance can be experienced differently. \\nThe distance metrics commonly used are Euclidean, Minkowski, Manhattan, and Mahalanobis. \\n \\n \\nDistance is applied through the concept of neighbours and exemplars. Neighbours are points in \\nproximity with respect to the distance measure expressed through exemplars. Exemplars are \\neither centroids that ﬁnd a centre of mass according to a chosen distance metric or medoids that ﬁnd \\nthe most centrally located data point. The most commonly used centroid is the arithmetic mean, which \\nminimises squared Euclidean distance to all other points. \\n \\nNotes: \\n\\uf0b7 \\nThe centroid represents the geometric centre of a plane figure, i.e., the arithmetic mean \\nposition of all the points in the figure from the centroid point. This definition extends to any \\nobject in n-dimensional space: its centroid is the mean position of all the points. \\n\\uf0b7 \\nMedoids are similar in concept to means or centroids. Medoids are most commonly used on \\ndata when a mean or centroid cannot be defined. They are used in contexts where the centroid \\nis not representative of the dataset, such as in image data. \\n \\nExamples of distance-based models include the nearest-neighbour models, which use the training data \\nas exemplars – for example, in classification. The K-means clustering algorithm also uses exemplars to \\ncreate clusters of similar data points. \\n \\n1.3.3 Probabilistic models \\nThe third family of machine learning algorithms is the probabilistic models. We have seen \\nbefore that the k-nearest neighbour algorithm uses the idea of distance (e.g., Euclidian distance) to \\nclassify entities, and logical models use a logical expression to partition the instance space. In this \\nsection, we see how the probabilistic models use the idea of probability to classify new entities. \\n \\nProbabilistic models see features and target variables as random variables. The process of modelling \\nrepresents and manipulates the level of uncertainty with respect to these variables. There are two \\ntypes of probabilistic models: Predictive and Generative. Predictive probability models use the idea of \\na conditional probability distribution P (Y |X) from which Y can be predicted from X.  Generative models \\nestimate the joint distribution P (Y, X).  Once we know the joint distribution for the generative models, \\nwe can derive any conditional or marginal distribution involving the same variables. Thus, the \\ngenerative model is capable of creating new data points and their labels, knowing the joint probability \\ndistribution. The joint distribution looks for a relationship between two variables. Once this relationship \\nis inferred, it is possible to infer new data points. \\nNaïve Bayes is an example of a probabilistic classifier. \\n \\nWe can do this using the Bayes rule defined as'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='7 \\n \\n \\n \\n \\nThe Naïve Bayes algorithm is based on the idea of Conditional Probability.  Conditional probability is \\nbased on finding the probability that something will happen, given that something else has already \\nhappened. The task of the algorithm then is to look at the evidence and to determine the likelihood of a \\nspecific class and assign a label accordingly to each entity. \\n \\nSome broad categories of models: \\nGeometric models \\nProbabilistic models \\nLogical models \\nE.g. K-nearest neighbors, linear \\nregression, \\nsupport \\nvector \\nmachine, logistic regression, … \\nNaïve Bayes, Gaussian process \\nregression, conditional random \\nfield, … \\nDecision tree, random forest, … \\n \\n1.3.4 Grouping and Grading \\nGrading vs grouping is an orthogonal categorization to geometric-probabilistic-logical-compositional.  \\n\\uf0b7 \\nGrouping models break the instance space up into groups or segments and in each segment \\napply a very simple method (such as majority class). \\no E.g. decision tree, KNN. \\n\\uf0b7 \\nGrading models form one global model over the instance space. \\no E.g. Linear classifiers – Neural networks \\n1.4 Designing a Learning System \\nFor any learning system, we must be knowing the three elements — T (Task), P (Performance \\nMeasure), and E (Training Experience). At a high level, the process of learning system looks as below. \\n \\nThe learning process starts with task T, performance measure P and training experience E and objective \\nare to find an unknown target function. The target function is an exact knowledge to be learned from the \\ntraining experience and its unknown. For example, in a case of credit approval, the learning system will \\nhave customer application records as experience and task would be to classify whether the given \\ncustomer application is eligible for a loan. So in this case, the training examples can be represented as'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content=\"8 \\n \\n(x1,y1)(x2,y2)..(xn,yn) where X represents customer application details and y represents the status of \\ncredit approval. \\nWith these details, what is that exact knowledge to be learned from the training experience? \\nSo the target function to be learned in the credit approval learning system is a mapping function f:X →y. \\nThis function represents the exact knowledge defining the relationship between input variable X and \\noutput variable y.  \\nDesign of a learning system \\nJust now we looked into the learning process and also understood the goal of the learning. When we \\nwant to design a learning system that follows the learning process, we need to consider a few design \\nchoices. The design choices will be to decide the following key components: \\n1. Type of training experience \\n2. Choosing the Target Function \\n3. Choosing a representation for the Target Function \\n4. Choosing an approximation algorithm for the Target Function \\n5. The final Design \\n \\nWe will look into the game - checkers learning problem and apply the above design choices. For a \\ncheckers learning problem, the three elements will be, \\n \\n1. Task T: To play checkers \\n2. Performance measure P: Total percent of the game won in the tournament. \\n3. Training experience E: A set of games played against itself \\n \\n1.4.1 Type of training experience \\nDuring the design of the checker's learning system, the type of training experience available for a \\nlearning system will have a significant effect on the success or failure of the learning. \\n \\n1. Direct or Indirect training experience — In the case of direct training experience, an individual board \\nstates \\nand \\ncorrect \\nmove \\nfor \\neach \\nboard \\nstate \\nare \\ngiven. \\nIn case of indirect training experience, the move sequences for a game and the final result (win, loss \\nor draw) are given for a number of games. How to assign credit or blame to individual moves is the \\ncredit assignment problem. \\n2. Teacher or Not — Supervised — The training experience will be labeled, which means, all the board \\nstates will be labeled with the correct move. So the learning takes place in the presence of a \\nsupervisor \\nor \\na \\nteacher. \\nUnsupervised — The training experience will be unlabeled, which means, all the board states will not \\nhave the moves. So the learner generates random games and plays against itself with no supervision \\nor \\nteacher \\ninvolvement.\"),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='9 \\n \\nSemi-supervised — Learner generates game states and asks the teacher for help in finding the \\ncorrect move if the board state is confusing. \\n3. Is the training experience good — Do the training examples represent the distribution of examples \\nover which the final system performance will be measured? Performance is best when training \\nexamples and test examples are from the same/a similar distribution. \\n \\nThe checker player learns by playing against oneself. Its experience is indirect. It may not encounter \\nmoves that are common in human expert play. Once the proper training experience is available, the next \\ndesign step will be choosing the Target Function. \\n \\n1.4.2 Choosing the Target Function \\nWhen you are playing the checkers game, at any moment of time, you make a decision on \\nchoosing the best move from different possibilities. You think and apply the learning that you have \\ngained from the experience. Here the learning is, for a specific board, you move a checker such that your \\nboard state tends towards the winning situation. Now the same learning has to be defined in terms of \\nthe target function. \\n \\nHere there are 2 considerations — direct and indirect experience. \\n \\n\\uf0b7 \\nDuring the direct experience, the checkers learning system, it needs only to learn how to choose \\nthe best move among some large search space. We need to find a target function that will help \\nus choose the best move among alternatives. Let us call this function ChooseMove and use the \\nnotation ChooseMove : B →M to indicate that this function accepts as input any board from the \\nset of legal board states B and produces as output some move from the set of legal moves M. \\n\\uf0b7 \\nWhen there is an indirect experience, it becomes difficult to learn such function. How about \\nassigning a real score to the board state.  \\n \\nSo the function be V : B →R indicating that this accepts as input any board from the set of legal board \\nstates B and produces an output a real score. This function assigns the higher scores to better board \\nstates. \\n \\n \\nIf the system can successfully learn such a target function V, then it can easily use it to select the best \\nmove from any board position.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='10 \\n \\nLet us therefore define the target value V(b) for an arbitrary board state b in B, as follows: \\n1. if b is a final board state that is won, then V(b) = 100 \\n2. if b is a final board state that is lost, then V(b) = -100 \\n3. if b is a final board state that is drawn, then V(b) = 0 \\n4. if b is a not a final state in the game, then V (b) = V (b’), where b’ is the best final board state that can \\nbe achieved starting from b and playing optimally until the end of the game. \\n \\nThe (4) is a recursive definition and to determine the value of V(b) for a particular board state, it \\nperforms the search ahead for the optimal line of play, all the way to the end of the game. So this \\ndefinition is not efficiently computable by our checkers playing program, we say that it is a \\nnonoperational definition. \\n \\nThe goal of learning, in this case, is to discover an operational description of V ; that is, a description \\nthat can be used by the checkers-playing program to evaluate states and select moves within realistic \\ntime bounds. \\nIt may be very difficult in general to learn such an operational form of V perfectly. We expect learning \\nalgorithms to acquire only some approximation to the target function ^V. \\n \\n1.4.3 Choosing a representation for the Target Function \\nNow that we have specified the ideal target function V, we must choose a representation that \\nthe learning program will use to describe the function ^V that it will learn. As with earlier design \\nchoices, we again have many options. We could, for example, allow the program to represent using a \\nlarge table with a distinct entry specifying the value for each distinct board state. Or we could allow it to \\nrepresent using a collection of rules that match against features of the board state, or a quadratic \\npolynomial function of predefined board features, or an artificial \\nneural network. In general, this choice of representation involves a crucial tradeoff. On one hand, we \\nwish to pick a very expressive representation to allow representing as close an approximation as \\npossible to the ideal target function V.  \\n \\nOn the other hand, the more expressive the representation, the more training data the program \\nwill require in order to choose among the alternative hypotheses it can represent. To keep the \\ndiscussion brief, let us choose a simple representation:  \\nfor any given board state, the function ^V will be calculated as a linear combination of the following \\nboard features: \\n\\uf0b7 \\nx1(b) — number of black pieces on board b \\n\\uf0b7 \\nx2(b) — number of red pieces on b \\n\\uf0b7 \\nx3(b) — number of black kings on b \\n\\uf0b7 \\nx4(b) — number of red kings on b \\n\\uf0b7 \\nx5(b) — number of red pieces threatened by black (i.e., which can be taken on black’s next turn) \\n\\uf0b7 \\nx6(b) — number of black pieces threatened by red \\n \\n^V = w0 + w1 · x1(b) + w2 · x2(b) + w3 · x3(b) + w4 · x4(b) +w5 · x5(b) + w6 · x6(b) \\n \\nWhere w0 through w6 are numerical coefficients or weights to be obtained by a learning algorithm.  \\nWeights w1 to w6 will determine the relative importance of different board features.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='11 \\n \\nSpecification of the Machine Learning Problem at this time — Till now we worked on choosing the type \\nof training experience, choosing the target function and its representation. The checkers learning task \\ncan be summarized as below. \\n\\uf0b7 \\nTask T : Play Checkers \\n\\uf0b7 \\nPerformance Measure : % of games won in world tournament \\n\\uf0b7 \\nTraining Experience E : opportunity to play against itself \\n\\uf0b7 \\nTarget Function : V : Board → R \\n\\uf0b7 \\nTarget Function Representation : ^V = w0 + w1 · x1(b) + w2 · x2(b) + w3 · x3(b) + w4 · x4(b) +w5 · \\nx5(b) + w6 · x6(b) \\nThe first three items above correspond to the specification of the learning task,whereas the final two \\nitems constitute design choices for the implementation of the learning program. \\n \\n1.4.4 Choosing an approximation algorithm for the Target Function \\nGenerating training data — \\nTo train our learning program, we need a set of training data, each describing a specific board state b and \\nthe training value V_train (b) for b. Each training example is an ordered pair <b,V_train(b)> \\nFor example, a training example may be <(x1 = 3, x2 = 0, x3 = 1, x4 = 0, x5 = 0, x6 = 0), +100\">. This is an \\nexample where black has won the game since x2 = 0 or red has no remaining pieces. However, such clean \\nvalues of V_train (b) can be obtained only for board value b that are clear win, loss or draw. \\nIn above case, assigning a training value V_train(b) for the specific boards b that are clean win, loss or \\ndraw is direct as they are direct training experience. But in the case of indirect training experience, \\nassigning a training value V_train(b) for the intermediate boards is difficult. In such case, the training \\nvalues are updated using temporal difference learning. Temporal difference (TD) learning is a concept \\ncentral to reinforcement learning, in which learning happens through the iterative correction of your \\nestimated returns towards a more accurate target return. \\nLet Successor(b) denotes the next board state following b for which it is again the program’s turn to \\nmove. ^V is the learner’s current approximation to V. Using these information, assign the training value \\nof V_train(b) for any intermediate board state b as below :  \\nV_train(b) ← ^V(Successor(b)) \\n \\nAdjusting the weights \\nNow its time to define the learning algorithm for choosing the weights and best fit the set of \\ntraining examples. One common approach is to define the best hypothesis as that which minimizes the \\nsquared error E between the training values and the values predicted by the hypothesis ^V. \\n \\n \\nThe learning algorithm should incrementally refine weights as more training examples become available \\nand it needs to be robust to errors in training data Least Mean Square (LMS) training rule is the one \\ntraining algorithm that will adjust weights a small amount in the direction that reduces the error. \\n \\nThe LMS algorithm is defined as follows:'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='12 \\n \\n \\n \\n1.4.5 Final Design for Checkers Learning system \\nThe final design of our checkers learning system can be naturally described by four distinct \\nprogram modules that represent the central components in many learning systems. \\n1. The performance System — Takes a new board as input and outputs a trace of the game it played \\nagainst itself. \\n2. The Critic — Takes the trace of a game as an input and outputs a set of training examples of the \\ntarget function. \\n3. The Generalizer — Takes training examples as input and outputs a hypothesis that estimates the \\ntarget function. Good generalization to new cases is crucial. \\n4. The Experiment Generator — Takes the current hypothesis (currently learned function) as input and \\noutputs a new problem (an initial board state) for the performance system to explore. \\n \\n \\nFinal design of the checkers learning program. \\n \\n1.5 Types of Learning \\nIn general, machine learning algorithms can be classified into three types. \\n\\uf0b7 \\nSupervised learning \\n\\uf0b7 \\nUnsupervised learning \\n\\uf0b7 \\nReinforcement learning \\n \\n1.5.1 Supervised learning \\nA training set of examples with the correct responses (targets) is provided and, based on this \\ntraining set, the algorithm generalises to respond correctly to all possible inputs. This is also called \\nlearning from exemplars. Supervised learning is the machine learning task of learning a function that \\nmaps an input to an output based on example input-output pairs. \\n \\nIn supervised learning, each example in the training set is a pair consisting of an input object \\n(typically a vector) and an output value. A supervised learning algorithm analyzes the training data and \\nproduces a function, which can be used for mapping new examples. In the optimal case, the function \\nwill correctly determine the class labels for unseen instances. Both classification and regression'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='13 \\n \\nproblems are supervised learning problems. A wide range of supervised learning algorithms are \\navailable, each with its strengths and weaknesses. There is no single learning algorithm that works best \\non all supervised learning problems. \\n \\n \\nFigure 1.4: Supervised learning \\n \\n \\n \\nRemarks \\nA “supervised learning” is so called because the process of an algorithm learning from the \\ntraining dataset can be thought of as a teacher supervising the learning process. We know the correct \\nanswers (that is, the correct outputs), the algorithm iteratively makes predictions on the training data \\nand is corrected by the teacher. Learning stops when the algorithm achieves an acceptable level of \\nperformance. \\n \\nExample \\nConsider the following data regarding patients entering a clinic. The data consists of the gender \\nand age of the patients and each patient is labeled as “healthy” or “sick”. \\n \\n \\n \\n1.5.2 Unsupervised learning \\nCorrect responses are not provided, but instead the algorithm tries to identify similarities \\nbetween the inputs so that inputs that have something in common are categorised together. The \\nstatistical approach to unsupervised learning is \\nknown as density estimation. \\n \\nUnsupervised learning is a type of machine learning algorithm used to draw inferences from \\ndatasets consisting of input data without labeled responses. In unsupervised learning algorithms, a \\nclassification or categorization is not included in the observations. There are no output values and so \\nthere is no estimation of functions. Since the examples given to the learner are unlabeled, the accuracy \\nof the structure that is output by the algorithm cannot be evaluated. The most common unsupervised \\nlearning method is cluster analysis, which is used for exploratory data analysis to find hidden patterns'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content=\"14 \\n \\nor grouping in data. \\n \\nExample \\nConsider the following data regarding patients entering a clinic. The data consists of the gender \\nand age of the patients. \\n \\nBased on this data, can we infer anything regarding the patients entering the clinic? \\n \\n1.5.3 Reinforcement learning \\nThis is somewhere between supervised and unsupervised learning. The algorithm gets told \\nwhen the answer is wrong, but does not get told how to correct it. It has to explore and try out different \\npossibilities until it works out how to get the answer right. Reinforcement learning is sometime called \\nlearning with a critic because of this monitor that scores the answer, but does not suggest \\nimprovements. \\n \\nReinforcement learning is the problem of getting an agent to act in the world so as to maximize \\nits rewards. A learner (the program) is not told what actions to take as in most forms of machine \\nlearning, but instead must discover which actions yield the most reward by trying them. In the most \\ninteresting and challenging cases, actions may affect not only the immediate reward but also the next \\nsituations and, through that, all subsequent rewards. \\n \\nExample \\nConsider teaching a dog a new trick: we cannot tell it what to do, but we can reward/punish it if \\nit does the right/wrong thing. It has to find out what it did that made it get the reward/punishment. We \\ncan use a similar method to train computers to do many tasks, such as playing backgammon or chess, \\nscheduling jobs, and controlling robot limbs. Reinforcement learning is different from supervised \\nlearning. Supervised learning is learning from examples provided by a knowledgeable expert. \\n \\n1.6 PERSPECTIVES AND ISSUES IN MACHINE LEARNING \\n \\nPerspectives in Machine Learning \\nOne useful perspective on machine learning is that it involves searching a very large space of \\npossible hypotheses to determine one that best fits the observed data and any prior knowledge held by \\nthe learner. \\nFor example, consider the space of hypotheses that could in principle be output by the above checkers \\nlearner. This hypothesis space consists of all evaluation functions that can be represented by some \\nchoice of values for the weights wo through w6. The learner's task is thus to search through this vast \\nspace to locate the hypothesis that is most consistent with the available training examples. The LMS \\nalgorithm for fitting weights achieves this goal by iteratively tuning the weights, adding a correction to \\neach weight each time the hypothesized evaluation function predicts a value that differs from the \\ntraining value. This algorithm works well when the hypothesis representation considered by the learner \\ndefines a continuously parameterized space of potential hypotheses.\"),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 19}, page_content=\"15 \\n \\nMany of the chapters in this book present algorithms that search a hypothesis space defined by \\nsome underlying representation (e.g., linear functions, logical descriptions, decision trees, artificial \\nneural networks). These different hypothesis representations are appropriate for learning different \\nkinds of target functions. For each of these hypothesis representations, the corresponding learning \\nalgorithm takes advantage of a different underlying structure to organize the search through the \\nhypothesis space.  \\n \\nThroughout this book we will return to this perspective of learning as a search problem in order \\nto characterize learning methods by their search strategies and by the underlying structure of the \\nsearch spaces they explore. We will also find this viewpoint useful in formally analyzing the relationship \\nbetween the size of the hypothesis space to be searched, the number of training examples available, \\nand the confidence we can have that a hypothesis consistent with the training data will correctly \\ngeneralize to unseen examples. \\n \\nIssues in Machine Learning \\nOur checkers example raises a number of generic questions about machine learning. The field of \\nmachine learning, and much of this book, is concerned with answering questions such as the following: \\n \\n\\uf0b7 \\nWhat algorithms exist for learning general target functions from specific training examples? In \\nwhat settings will particular algorithms converge to the desired function, given sufficient \\ntraining data? Which algorithms perform best for which types of problems and representations? \\n\\uf0b7 \\nHow much training data is sufficient? What general bounds can be found to relate the \\nconfidence in learned hypotheses to the amount of training experience and the character of the \\nlearner's hypothesis space? \\n\\uf0b7 \\nWhen and how can prior knowledge held by the learner guide the process of generalizing from \\nexamples? Can prior knowledge be helpful even when it is only approximately correct? \\n\\uf0b7 \\nWhat is the best strategy for choosing a useful next training experience, and how does the \\nchoice of this strategy alter the complexity of the learning problem?  \\n\\uf0b7 \\nWhat is the best way to reduce the learning task to one or more function approximation \\nproblems? Put another way, what specific functions should the system attempt to learn? Can \\nthis process itself be automated?  \\n\\uf0b7 \\nHow can the learner automatically alter its representation to improve its ability to represent \\nand learn the target function? \\n \\n1.7 Version Spaces \\nDefinition (Version space). A concept is complete if it covers all positive examples. \\n \\nA concept is consistent if it covers none of the negative examples. The version space is the set of all \\ncomplete and consistent concepts. This set is convex and is fully defined by its least and most general \\nelements. \\n \\nThe key idea in the CANDIDATE-ELIMINATION algorithm is to output a description of the set of all \\nhypotheses consistent with the training examples \\n \\n1.7.1 Representation \\nThe Candidate – Elimination  algorithm finds all describable hypotheses that are consistent with the\"),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 20}, page_content=\"16 \\n \\nobserved training examples. In order to define this algorithm precisely, we begin with a few basic \\ndefinitions. First, let us say that a hypothesis is consistent with the training examples if it correctly \\nclassifies these examples. \\n \\nDefinition: A hypothesis h is consistent with a set of training examples D if and only if h(x) = c(x) for \\neach example (x, c(x)) in D. \\n \\n \\n \\nNote difference between definitions of consistent and satisfies  \\n\\uf0b7 \\nAn example x is said to satisfy hypothesis h when h(x) = 1, regardless of whether x is a positive \\nor negative example of the target concept.  \\n\\uf0b7 \\nAn example x is said to consistent with hypothesis h iff h(x) = c(x)  \\n \\nDefinition: version space- The version space, denoted V SH, D with respect to hypothesis space H and \\ntraining examples D, is the subset of hypotheses from H consistent with the training examples in D \\n \\n \\n \\n1.7.2 The LIST-THEN-ELIMINATION algorithm  \\nThe LIST-THEN-ELIMINATE algorithm first initializes the version space to contain all hypotheses in H \\nand then eliminates any hypothesis found inconsistent with any training example. \\n \\n1. VersionSpace c a list containing every hypothesis in H  \\n2. \\nFor each training example, (x, c(x)) remove from VersionSpace any hypothesis h for which h(x) ≠ c(x)  \\n3. \\nOutput the list of hypotheses in VersionSpace \\n \\n\\uf0b7 \\nList-Then-Eliminate works in principle, so long as version space is finite. \\n\\uf0b7 \\nHowever, since it requires exhaustive enumeration of all hypotheses in practice it is not feasible.  \\n \\nA More Compact Representation for Version Spaces \\nThe version space is represented by its most general and least general members. These members form general \\nand specific boundary sets that delimit the version space within the partially ordered hypothesis space.  \\nDefinition: The general boundary G, with respect to hypothesis space H and training data D, is the set of \\nmaximally general members of H consistent with D \\n \\nG \\uf0ba{g \\uf0ce H | Consistent (g, D)\\uf0d9(\\uf0d8\\uf024g' \\uf0ce H)[(g' \\uf03e g) \\uf0d9 Consistent(g', D)]} \\ng \\n \\nDefinition: The specific boundary S, with respect to hypothesis space H and training data D, is the set of \\nminimally general (i.e., maximally specific) members of H consistent with D. \\n \\nS \\uf0ba{s \\uf0ce H | Consistent (s, D)\\uf0d9(\\uf0d8\\uf024s' \\uf0ce H)[(s \\uf03e s') \\uf0d9 Consistent(s', D)]} \\ng \\n \\nTheorem: Version Space representation theorem  \\nTheorem: Let X be an arbitrary set of instances and Let H be a set of Boolean-valued hypotheses defined over X. \\nLet c: X →{O, 1} be an arbitrary target concept defined over X, and let D be an arbitrary set of training examples\"),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 21}, page_content='17 \\n \\n{(x, c(x))). For all X, H, c, and D such that S and G are well defined, \\n \\nVS \\n={ h \\uf0ce H | (\\uf024s \\uf0ce S ) (\\uf024g \\uf0ce G ) ( g \\uf0b3 h \\uf0b3 s )} \\nH,D \\ng \\ng \\n \\nTo Prove: \\n1. Every h satisfying the right hand side of the above expression is in VS \\n                                                                                                                                            H, D \\n2. Every member of VS satisfies the right-hand side of the expression \\n                                         H, D \\n \\nSketch of proof: \\n1. let g, h, s be arbitrary members of G, H, S respectively with g \\uf0b3g h \\uf0b3g s \\n\\uf0b7 By the definition of S, s must be satisfied by all positive examples in D. Because h \\uf0b3g s, \\nh must also be satisfied by all positive examples in D. \\n\\uf0b7 By the definition of G, g cannot be satisfied by any negative example in D, and because g \\uf0b3g h \\nh cannot be satisfied by any negative example in D. Because h is satisfied by all positive \\nexamples in D and by no negative examples in D, h is consistent with D, and therefore h is a \\nmember of VSH,D. \\n2. It can be proven by assuming some h in VSH,D,that does not satisfy the right-hand side of \\nthe expression, then showing that this leads to an inconsistency \\n1.7.3 CANDIDATE-ELIMINATION Learning Algorithm \\n \\nThe CANDIDATE-ELIMINTION algorithm computes the version space containing all hypotheses \\nfrom H that are consistent with an observed sequence of training examples. \\n \\nInitialize G to the set of maximally general hypotheses in H Initialize S to the set of maximally specific \\nhypotheses in H For each training example d, do \\n• \\nIf d is a positive example \\n• \\nRemove from G any hypothesis inconsistent with d \\n• \\nFor each hypothesis s in S that is not consistent with d \\n• \\nRemove s from S \\n• \\nAdd to S all minimal generalizations h of s such that \\n• \\nh is consistent with d, and some member of G is more general than h \\n• \\nRemove from S any hypothesis that is more general than another hypothesis in S \\n \\n• If d is a negative example \\n• \\nRemove from S any hypothesis inconsistent with d \\n• \\nFor each hypothesis g in G that is not consistent with d \\n• \\nRemove g from G'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 22}, page_content='18 \\n \\n• \\nAdd to G all minimal specializations h of g such that \\n• \\nh is consistent with d, and some member of S is more specific than h \\n• \\nRemove from G any hypothesis that is less general than another hypothesis in G \\nCANDIDATE- ELIMINTION algorithm using version spaces \\n \\n1.7.4 An Illustrative Example \\n \\n \\nExample \\nSky \\nAirTemp \\nHumidity \\nWind \\nWater \\nForecast \\nEnjoySport \\n1 \\nSunny \\nWarm \\nNormal \\nStrong \\nWarm \\nSame \\nYes \\n2 \\nSunny \\nWarm \\nHigh \\nStrong \\nWarm \\nSame \\nYes \\n3 \\nRainy \\nCold \\nHigh \\nStrong \\nWarm \\nChange \\nNo \\n4 \\nSunny \\nWarm \\nHigh \\nStrong \\nCool \\nChange \\nYes \\n \\nCANDIDATE-ELIMINTION algorithm begins by initializing the version space to the set of all \\nhypotheses in H; \\n \\nInitializing the G boundary set to contain the most general hypothesis in H \\nG0  \\uf0b3?,  ?,  ?,  ?,  ?, ?\\uf0b3\\n \\n \\nInitializing the S boundary set to contain the most specific (least general) hypothesis \\nS0  \\uf0b3\\uf0b3, \\uf0b3, \\uf0b3, \\uf0b3, \\uf0b3, \\uf0b3\\uf0b3\\n \\n \\n\\uf0b7 When the first training example is presented, the CANDIDATE-ELIMINTION algorithm checks the \\nS boundary and finds that it is overly specific and it fails to cover the positive example. \\n\\uf0b7 The boundary is therefore revised by moving it to the least more general hypothesis that covers \\nthis new example \\n\\uf0b7 No update of the G boundary is needed in response to this training example because Go \\ncorrectly covers this example \\n \\n \\n \\n \\n\\uf0b7 \\nWhen the second training example is observed, it has a similar effect of generalizing S further to S2, \\nleaving G again unchanged i.e., G2 = G1 = G0'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 23}, page_content='19 \\n \\n \\n \\n \\n\\uf0b7 Consider the third training example. This negative example reveals that the G boundary of \\nthe version space is overly general, that is, the hypothesis in G incorrectly predicts that this \\nnew example is a positive example. \\n\\uf0b7 The hypothesis in the G boundary must therefore be specialized until it correctly classifies \\nthis new negative example. \\n \\n \\n \\nGiven that there are six attributes that could be specified to specialize G2, why are there only three \\nnew hypotheses in G3? \\nFor example, the hypothesis h = (?, ?, Normal, ?, ?, ?) is a minimal specialization of G2 that \\ncorrectly labels the new example as a negative example, but it is not included in G3. The \\nreason this hypothesis is excluded is that it is inconsistent with the previously encountered \\npositive examples \\n \\nConsider the fourth training example.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 24}, page_content='20 \\n \\n \\n \\n \\n\\uf0b7 This positive example further generalizes the S boundary of the version space. It also \\nresults in removing one member of the G boundary, because this member fails to cover \\nthe new positive example \\n \\nAfter processing these four examples, the boundary sets S4 and G4 delimit the version space of all \\nhypotheses consistent with the set of incrementally observed training examples. \\n \\n \\n \\n \\n \\n1.8 Probably approximately correct learning \\n \\nIn computer science, computational learning theory (or just learning theory) is a subfield of \\nartificial intelligence devoted to studying the design and analysis of machine learning algorithms. In \\ncomputational learning theory, probably approximately correct learning (PAC learning) is a framework \\nfor mathematical analysis of machine learning algorithms. It was proposed in 1984 by Leslie Valiant. \\n \\nIn this framework, the learner (that is, the algorithm) receives samples and must select a \\nhypothesis from a certain class of hypotheses. The goal is that, with high probability (the “probably” \\npart), the selected hypothesis will have low generalization error (the “approximately correct” part). In \\nthis section we first give an informal definition of PAC-learnability. After introducing a few nore notions, \\nwe give a more formal, mathematically oriented, definition of PAC-learnability. At the end, we mention \\none of the applications of PAC-learnability. \\n \\nPAC-learnability \\nTo define PAC-learnability we require some specific terminology and related notations.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 25}, page_content='21 \\n \\n\\uf0b7 \\nLet X be a set called the instance space which may be finite or infinite. For example, X may be \\nthe set of all points in a plane. \\n\\uf0b7 \\nA concept class C for X is a family of functions c : X \\uf0e0 {0; 1}. A member of C is called a concept. \\nA concept can also be thought of as a subset of X. If C is a subset of X, it defines a unique \\nfunction µc : X \\uf0e0 {0; 1} as follows: \\n \\n \\n \\n\\uf0b7 \\nA hypothesis h is also a function h : X \\uf0e0 {0; 1}. So, as in the case of concepts, a hypothesis can \\nalso be thought of as a subset of X. H will denote a set of hypotheses. \\n\\uf0b7 \\nWe assume that F is an arbitrary, but fixed, probability distribution over X. \\n\\uf0b7 \\nTraining examples are obtained by taking random samples from X. We assume that the samples \\nare randomly generated from X according to the probability distribution F. \\n \\nNow, we give below an informal definition of PAC-learnability. \\n \\nDefinition (informal) \\nLet X be an instance space, C a concept class for X, h a hypothesis in C and F an arbitrary, but fixed, \\nprobability distribution. The concept class C is said to be PAC-learnable if there is an algorithm A which, \\nfor samples drawn with any probability distribution F and any concept c Є C, will with high probability \\nproduce a hypothesis h Є C whose error is small. \\n \\nExamples \\n \\nTo illustrate the definition of PAC-learnability, let us consider some concrete examples. \\n \\n \\nFigure : An axis-aligned rectangle in the Euclidean plane \\n \\nExample'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 26}, page_content='22 \\n \\n\\uf0b7 \\nLet the instance space be the set X of all points in the Euclidean plane. Each point is represented \\nby its coordinates (x; y). So, the dimension or length of the instances is 2.  \\n\\uf0b7 \\nLet the concept class C be the set of all “axis-aligned rectangles” in the plane; that is, the set of \\nall rectangles whose sides are parallel to the coordinate axes in the plane (see Figure).  \\n\\uf0b7 \\nSince an axis-aligned rectangle can be defined by a set of inequalities of the following form \\nhaving four parameters \\n \\na ≤ x ≤ b,    c ≤ y ≤ d \\n \\nthe size of a concept is 4. \\n\\uf0b7 \\nWe take the set H of all hypotheses to be equal to the set C of concepts, H = C. \\n \\nGiven a set of sample points labeled positive or negative, let L be the algorithm which outputs the \\nhypothesis defined by the axis-aligned rectangle which gives the tightest fit to the positive examples \\n(that is, that rectangle with the smallest area that includes all of the positive examples and none of the \\nnegative examples) (see Figure bleow). \\n \\n \\nFigure : Axis-aligned rectangle which gives the tightest fit to the positive examples \\n \\nIt can be shown that, in the notations introduced above, the concept class C is PAC-learnable by the \\nalgorithm L using the hypothesis space H of all axis-aligned rectangles. \\n \\n1.9 Vapnik-Chervonenkis (VC) dimension \\nThe concepts of Vapnik-Chervonenkis dimension (VC dimension) and probably approximate \\ncorrect (PAC) learning are two important concepts in the mathematical theory of learnability and hence \\nare mathematically oriented. The former is a measure of the capacity (complexity, expressive power, \\nrichness, or flexibility) of a space of functions that can be learned by a classification algorithm. It was \\noriginally defined by Vladimir Vapnik and Alexey Chervonenkis in 1971. The latter is a framework for the \\nmathematical analysis of learning algorithms. The goal is to check whether the probability for a selected \\nhypothesis to be approximately correct is very high. The notion of PAC \\nlearning was proposed by Leslie Valiant in 1984. \\n \\nV-C dimension \\nLet H be the hypothesis space for some machine learning problem. The Vapnik-Chervonenkis dimension \\nof H, also called the VC dimension of H, and denoted by V C(H), is a measure of the complexity (or, \\ncapacity, expressive power, richness, or flexibility) of the space H. To define the VC dimension we \\nrequire the notion of the shattering of a set of instances.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 27}, page_content='23 \\n \\n \\nShattering of a set \\nLet D be a dataset containing N examples for a binary classification problem with class labels 0 and 1. \\nLet H be a hypothesis space for the problem. Each hypothesis h in H partitions D into two disjoint \\nsubsets as follows: \\n \\n \\nSuch a partition of S is called a “dichotomy” in D. It can be shown that there are 2N possible dichotomies \\nin D. To each dichotomy of D there is a unique assignment of the labels “1” and “0” to the elements of \\nD. Conversely, if S is any subset of D then, S defines a unique hypothesis h as follows: \\n \\n \\nThus to specify a hypothesis h, we need only specify the set {x Є D |  h(x) = 1}. Figure 3.1 shows all \\npossible dichotomies of D if D has three elements. In the figure, we have shown only one of the two sets \\nin a dichotomy, namely the set {x Є D |  h(x) = 1}.The circles and ellipses represent such sets. \\n \\n \\n \\n \\nDefinition \\nA set of examples D is said to be shattered by a hypothesis space H if and only if for every dichotomy of \\nD there exists some hypothesis in H consistent with the dichotomy of D. \\n \\nThe following example illustrates the concept of Vapnik-Chervonenkis dimension. \\n \\nExample \\n \\nIn figure , we see that an axis-aligned rectangle can shatter four points in two dimensions. Then  VC(H), \\nwhen H is the hypothesis class of axis-aligned rectangles in two dimensions, is four. In calculating the VC \\ndimension, it is enough that we find four points that can be shattered; it is not necessary that we be \\nable to shatter any four points in two dimensions.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 28}, page_content='24 \\n \\n \\nFig: An axis-aligned rectangle can shattered four points. Only rectangle covering two points are shown. \\n \\nVC dimension may seem pessimistic. It tells us that using a rectangle as our hypothesis class, we can \\nlearn only datasets containing four points and not more. \\n \\n \\n \\n \\n \\n \\n \\nUnit II \\nSupervised and Unsupervised Learning \\n \\nTopics: Decision Trees: ID3, Classification and Regression Trees, Regression: Linear Regression, \\nMultiple Linear Regression, Logistic Regression, Neural Networks: Introduction, Perception, \\nMultilayer Perception, Support Vector Machines: Linear and Non-Linear, Kernel Functions, K \\nNearest Neighbors. Introduction to clustering, K-means clustering, K-Mode Clustering. \\n \\n2.1. Decision Tree \\nIntroduction Decision Trees are a type of Supervised Machine Learning (that is you explain what \\nthe input is and what the corresponding output is in the training data) where the data is continuously \\nsplit according to a certain parameter. The tree can be explained by two entities, namely decision \\nnodes and leaves. The leaves are the decisions or the final outcomes. And the decision nodes are where \\nthe data is split. \\n \\nAn example of a decision tree can be explained using above binary tree. Let’s say you want to predict \\nwhether a person is fit given their information like age, eating habit, and physical activity, etc. The'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 29}, page_content='25 \\n \\ndecision nodes here are questions like ‘What’s the age?’, ‘Does he exercise?’, and ‘Does he eat a lot of \\npizzas’? And the leaves, which are outcomes like either ‘fit’, or ‘unfit’. In this case this was a binary \\nclassification problem (a yes no type problem). There are two main types of Decision Trees: \\n1. Classification trees (Yes/No types) \\nWhat we have seen above is an example of classification tree, where the outcome was a variable like \\n‘fit’ or ‘unfit’. Here the decision variable is Categorical. \\n \\n2. Regression trees (Continuous data types) \\nHere the decision or the outcome variable is Continuous, e.g. a number like 123.  Working Now that we \\nknow what a Decision Tree is, we’ll see how it works internally. There are many algorithms out there \\nwhich construct Decision Trees, but one of the best is called as ID3 Algorithm. ID3 Stands for Iterative \\nDichotomiser 3. Before discussing the ID3 algorithm, we’ll go through few definitions. Entropy Entropy, \\nalso called as Shannon Entropy is denoted by H(S) for a finite set S, is the measure of the amount of \\nuncertainty or randomness in data. \\n \\nIntuitively, it tells us about the predictability of a certain event. Example, consider a coin toss whose \\nprobability of heads is 0.5 and probability of tails is 0.5. Here the entropy is the highest possible, since \\nthere’s no way of determining what the outcome might be. Alternatively, consider a coin which has \\nheads on both the sides, the entropy of such an event can be predicted perfectly since we know \\nbeforehand that it’ll always be heads. In other words, this event has no randomness hence it’s entropy \\nis zero. In particular, lower values imply less uncertainty while higher values imply high \\nuncertainty. Information Gain Information gain is also called as Kullback-Leibler divergence denoted by \\nIG(S,A) for a set S is the effective change in entropy after deciding on a particular attribute A. It \\nmeasures the relative change in entropy with respect to the independent variables \\n \\n\\uf028\\n\\uf029\\n\\uf028\\uf029\\n\\uf028\\n\\uf029\\nA\\nS\\nH\\nS\\nH\\nA\\nS\\nIG\\n,\\n,\\n\\uf02d\\n\\uf03d\\n \\nAlternatively, \\n \\n \\n \\nwhere IG(S, A) is the information gain by applying feature A. H(S) is the Entropy of the entire set, while \\nthe second term calculates the Entropy after applying the feature A, where P(x) is the probability of \\nevent x. Let’s understand this with the help of an example Consider a piece of data collected over the \\ncourse of 14 days where the features are Outlook, Temperature, Humidity, Wind and the outcome \\nvariable is whether Golf was played on the day. Now, our job is to build a predictive model which takes \\nin above 4 parameters and predicts whether Golf will be played on the day. We’ll build a decision tree \\nto do that using ID3 algorithm. \\n \\nDay \\nOutlook \\nTemperature \\nHumidity \\nWind \\nPlay Golf \\nD1 \\nSunny \\nHot \\nHigh \\nWeak \\nNo \\nD2 \\nSunny \\nHot \\nHigh \\nStrong \\nNo \\nD3 \\nOvercast \\nHot \\nHigh \\nWeak \\nYes \\n\\uf028\\n\\uf029\\n\\uf028\\uf029\\n\\uf028\\uf029\\n\\uf028\\uf029\\n\\uf0e5\\n\\uf03d\\n\\uf02a\\n\\uf02d\\n\\uf03d\\nn\\ni\\nx\\nH\\nx\\nP\\nS\\nH\\nA\\nS\\nIG\\n0\\n,'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 30}, page_content='26 \\n \\nD4 \\nRain \\nMild \\nHigh \\nWeak \\nYes \\nD5 \\nRain \\nCool \\nNormal \\nWeak \\nYes \\nD6 \\nRain \\nCool \\nNormal \\nStrong \\nNo \\nD7 \\nOvercast \\nCool \\nNormal \\nStrong \\nYes \\nD8 \\nSunny \\nMild \\nHigh \\nWeak \\nNo \\nD9 \\nSunny \\nCool \\nNormal \\nWeak \\nYes \\nD10 \\nRain \\nMild \\nNormal \\nWeak \\nYes \\nD11 \\nSunny \\nMild \\nNormal \\nStrong \\nYes \\nD12 \\nOvercast \\nMild \\nHigh \\nStrong \\nYes \\nD13 \\nOvercast \\nHot \\nNormal \\nWeak \\nYes \\nD14 \\nRain \\nMild \\nHigh \\nStrong \\nNo \\n \\n2.1.1 ID3 \\n ID3 Algorithm will perform following tasks recursively \\n \\n1. Create root node for the tree \\n2. If all examples are positive, return leaf node „positive‟ \\n3. Else if all examples are negative, return leaf node „negative‟ \\n4. Calculate the entropy of current state H(S) \\n5. For each attribute, calculate the entropy with respect to the attribute „x‟ denoted by H(S, x) \\n6. Select the attribute which has maximum value of IG(S, x) \\n7. Remove the attribute that offers highest IG from the set of attributes \\n8. Repeat until we run out of all attributes, or the decision tree has all leaf nodes. \\n \\nNow we‟ll go ahead and grow the decision tree. The initial step is to calculate H(S), the Entropy of the current state. \\nIn the above example, we can see in total there are 5 No‟s and 9 Yes‟s. \\n \\nYes \\nNo \\nTotal \\n9 \\n5 \\n14 \\n \\n \\nwhere „x‟ are the possible values for an attribute. Here, attribute „Wind‟ takes two possible values in the sample \\ndata, hence x = {Weak, Strong} we‟ll have to calculate: \\n \\n \\n \\nAmongst all the 14 examples we have 8 places where the wind is weak and 6 where the wind is Strong. \\n \\nWind = Weak \\nWind = Strong \\nTotal \\n8 \\n6 \\n14'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 31}, page_content='27 \\n \\n \\nNow out of the 8 Weak examples, 6 of them were „Yes‟ for Play Golf and 2 of them were „No‟ for „Play Golf‟. So, \\nwe have, \\n \\n \\nSimilarly, out of 6 Strong examples, we have 3 examples where the outcome was „Yes‟ for Play Golf and 3 \\nwhere we had „No‟ for Play Golf. \\n \\n \\n \\nRemember, here half items belong to one class while other half belong to other. Hence we have perfect randomness. \\nNow we have all the pieces required to calculate the Information Gain, \\n \\n \\n \\nWhich tells us the Information Gain by considering „Wind‟ as the feature and give us information gain of 0.048. \\nNow we must similarly calculate the Information Gain for all the features. \\n \\n \\nWe can clearly see that IG(S, Outlook) has the highest information gain of 0.246, hence we chose Outlook \\nattribute  as the root node. At this point, the decision tree looks like.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 32}, page_content='28 \\n \\nHere we observe that whenever the outlook is Overcast, Play Golf is always ‘Yes’, it’s no coincidence by \\nany chance, the simple tree resulted because of the highest information gain is given by the attribute \\nOutlook. Now how do we proceed from this point? We can simply apply recursion, you might want to \\nlook at the algorithm steps described earlier. Now that we’ve used Outlook, we’ve got three of them \\nremaining Humidity, Temperature, and Wind. And, we had three possible values of Outlook: Sunny, \\nOvercast, Rain. Where the Overcast node already ended up having leaf node ‘Yes’, so we’re left with \\ntwo subtrees to compute: Sunny and Rain. \\n \\nTable where the value of Outlook is Sunny looks like: \\nTemperature \\nHumidity \\nWind \\nPlay Golf \\nHot \\nHigh \\nWeak \\nNo \\nHot \\nHigh \\nStrong \\nNo \\nMild \\nHigh \\nWeak \\nNo \\nCool \\nNormal \\nWeak \\nYes \\nMild \\nNormal \\nStrong \\nYes \\n \\n \\nAs we can see the highest Information Gain is given by Humidity. Proceeding in the same way with \\n \\nwill give us Wind as the one with highest information gain. The final Decision Tree looks something like \\nthis. The final Decision Tree looks something like this. \\n \\n \\n \\n2.1.2. Classification and Regression Trees \\n2.1.2.1. Classification Trees \\nA classification tree is an algorithm where the target variable is fixed or categorical. The \\nalgorithm is then used to identify the “class” within which a target variable would most likely fall. \\nAn example of a classification-type problem would be determining who will or will not subscribe to a \\ndigital platform; or who will or will not graduate from high school. \\nThese are examples of simple binary classifications where the categorical dependent variable can \\nassume only one of two, mutually exclusive values. In other cases, you might have to predict among a \\nnumber of different variables. For instance, you may have to predict which type of smartphone a \\nconsumer may decide to purchase. \\nIn such cases, there are multiple values for the categorical dependent variable. Here’s what a classic \\nclassification tree looks like'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 33}, page_content='29 \\n \\n \\n2.1.2.2. Regression Trees \\nA regression tree refers to an algorithm where the target variable is and the algorithm is used to \\npredict it’s value. As an example of a regression type problem, you may want to predict the selling \\nprices of a residential house, which is a continuous dependent variable. \\nThis will depend on both continuous factors like square footage as well as categorical factors like the \\nstyle of home, area in which the property is located and so on. \\n \\n \\n \\nWhen to use Classification and Regression Trees \\nClassification trees are used when the dataset needs to be split into classes which belong to the \\nresponse variable. In many cases, the classes Yes or No. \\nIn other words, they are just two and mutually exclusive. In some cases, there may be more than two \\nclasses in which case a variant of the classification tree algorithm is used. \\nRegression trees, on the other hand, are used when the response variable is continuous. For instance, if \\nthe response variable is something like the price of a property or the temperature of the day, a \\nregression tree is used. \\nIn other words, regression trees are used for prediction-type problems while classification trees are \\nused for classification-type problems. \\n \\nHow Classification and Regression Trees Work \\nA classification tree splits the dataset based on the homogeneity of data. Say, for instance, \\nthere are two variables; income and age; which determine whether or not a consumer will buy a \\nparticular kind of phone. \\nIf the training data shows that 95% of people who are older than 30 bought the phone, the data gets \\nsplit there and age becomes a top node in the tree. This split makes the data “95% pure”. Measures of \\nimpurity like entropy or Gini index are used to quantify the homogeneity of the data when it comes to \\nclassification trees.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 34}, page_content='30 \\n \\nIn a regression tree, a regression model is fit to the target variable using each of the independent \\nvariables. After this, the data is split at several points for each independent variable. \\nAt each such point, the error between the predicted values and actual values is squared to get “A Sum \\nof Squared Errors” (SSE). The SSE is compared across the variables and the variable or point which has \\nthe lowest SSE is chosen as the split point. This process is continued recursively. \\n \\n \\nAdvantages of Classification and Regression Trees \\nThe purpose of the analysis conducted by any classification or regression tree is to create a set of if-else \\nconditions that allow for the accurate prediction or classification of a case. \\n(i) The Results are Simplistic \\nThe interpretation of results summarized in classification or regression trees is usually fairly simple. The \\nsimplicity of results helps in the following ways. \\n\\uf0b7 \\nIt allows for the rapid classification of new observations. That’s because it is much simpler to \\nevaluate just one or two logical conditions than to compute scores using complex nonlinear \\nequations for each group. \\n\\uf0b7 \\nIt can often result in a simpler model which explains why the observations are either classified \\nor predicted in a certain way. For instance, business problems are much easier to explain with \\nif-then statements than with complex nonlinear equations. \\n(ii) Classification and Regression Trees are Nonparametric & Nonlinear \\nThe results from classification and regression trees can be summarized in simplistic if-then conditions. \\nThis negates the need for the following implicit assumptions. \\n\\uf0b7 \\nThe predictor variables and the dependent variable are linear. \\n\\uf0b7 \\nThe predictor variables and the dependent variable follow some specific nonlinear link function. \\n\\uf0b7 \\nThe predictor variables and the dependent variable are monotonic. \\nSince there is no need for such implicit assumptions, classification and regression tree methods are well \\nsuited to data mining. This is because there is very little knowledge or assumptions that can be made \\nbeforehand about how the different variables are related. \\nAs a result, classification and regression trees can actually reveal relationships between these variables \\nthat would not have been possible using other techniques. \\n(iii) Classification and Regression Trees Implicitly Perform Feature Selection \\nFeature selection or variable screening is an important part of analytics. When we use decision trees, \\nthe top few nodes on which the tree is split are the most important variables within the set. As a result, \\nfeature selection gets performed automatically and we don’t need to do it again. \\nLimitations of Classification and Regression Trees'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 35}, page_content='31 \\n \\nClassification and regression tree tutorials, as well as classification and regression tree ppts, exist in \\nabundance. This is a testament to the popularity of these decision trees and how frequently they are \\nused. However, these decision trees are not without their disadvantages. \\nThere are many classification and regression trees examples where the use of a decision tree has not \\nled to the optimal result. Here are some of the limitations of classification and regression trees. \\n(i) Overfitting \\nOverfitting occurs when the tree takes into account a lot of noise that exists in the data and \\ncomes up with an inaccurate result. \\n(ii) High variance \\nIn this case, a small variance in the data can lead to a very high variance in the prediction, \\nthereby affecting the stability of the outcome. \\n(iii) Low bias \\nA decision tree that is very complex usually has a low bias. This makes it very difficult for the \\nmodel to incorporate any new data. \\n \\nWhat is a CART in Machine Learning? \\nA Classification and Regression Tree (CART) is a predictive algorithm used in machine learning. It \\nexplains how a target variable’s values can be predicted based on other values. \\nIt is a decision tree where each fork is a split in a predictor variable and each node at the end has a \\nprediction for the target variable. \\nThe CART algorithm is an important decision tree algorithm that lies at the foundation of machine \\nlearning. Moreover, it is also the basis for other powerful machine learning algorithms like bagged \\ndecision trees, random forest and boosted decision trees. \\nSumming up \\nThe Classification and regression tree (CART) methodology is one of the oldest and most fundamental \\nalgorithms. It is used to predict outcomes based on certain predictor variables. \\nThey are excellent for data mining tasks because they require very little data pre-processing. Decision \\ntree models are easy to understand and implement which gives them a strong advantage when \\ncompared to other analytical models. \\n \\n2.2. Regression \\nRegression Analysis in Machine learning \\nRegression analysis is a statistical method to model the relationship between a dependent \\n(target) and independent (predictor) variables with one or more independent variables. More \\nspecifically, Regression analysis helps us to understand how the value of the dependent variable is \\nchanging corresponding to an independent variable when other independent variables are held fixed. It \\npredicts continuous/real values such as temperature, age, salary, price, etc. \\n \\nWe can understand the concept of regression analysis using the below example: \\n \\nExample: Suppose there is a marketing company A, who does various advertisement every year and get \\nsales on that. The below list shows the advertisement made by the company in the last 5 years and the \\ncorresponding sales:'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 36}, page_content='32 \\n \\n \\n \\nNow, the company wants to do the advertisement of $200 in the year 2019 and wants to know the \\nprediction about the sales for this year. So to solve such type of prediction problems in machine \\nlearning, we need regression analysis. \\nRegression is a supervised learning technique which helps in finding the correlation between variables \\nand enables us to predict the continuous output variable based on the one or more predictor variables. \\nIt is mainly used for prediction, forecasting, time series modeling, and determining the causal-effect \\nrelationship between variables. \\nIn Regression, we plot a graph between the variables which best fits the given datapoints, using this \\nplot, the machine learning model can make predictions about the data. In simple words, \"Regression \\nshows a line or curve that passes through all the datapoints on target-predictor graph in such a way \\nthat the vertical distance between the datapoints and the regression line is minimum.\" The distance \\nbetween datapoints and line tells whether a model has captured a strong relationship or not. \\n \\nSome examples of regression can be as: \\no \\nPrediction of rain using temperature and other factors \\no \\nDetermining Market trends \\no \\nPrediction of road accidents due to rash driving. \\n \\nTerminologies Related to the Regression Analysis: \\no \\nDependent Variable: The main factor in Regression analysis which we want to predict or \\nunderstand is called the dependent variable. It is also called target variable. \\no \\nIndependent Variable: The factors which affect the dependent variables or which are used to \\npredict the values of the dependent variables are called independent variable, also called as \\na predictor. \\no \\nOutliers: Outlier is an observation which contains either very low value or very high value in \\ncomparison to other observed values. An outlier may hamper the result, so it should be \\navoided. \\no \\nMulticollinearity: If the independent variables are highly correlated with each other than other \\nvariables, then such condition is called Multicollinearity. It should not be present in the dataset, \\nbecause it creates problem while ranking the most affecting variable. \\no \\nUnderfitting and Overfitting: If our algorithm works well with the training dataset but not well \\nwith test dataset, then such problem is called Overfitting. And if our algorithm does not \\nperform well even with training dataset, then such problem is called underfitting.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 37}, page_content='33 \\n \\nWhy do we use Regression Analysis? \\nAs mentioned above, Regression analysis helps in the prediction of a continuous variable. There are \\nvarious scenarios in the real world where we need some future predictions such as weather condition, \\nsales prediction, marketing trends, etc., for such case we need some technology which can make \\npredictions more accurately. So for such case we need Regression analysis which is a statistical method \\nand used in machine learning and data science. Below are some other reasons for using Regression \\nanalysis: \\no \\nRegression estimates the relationship between the target and the independent variable. \\no \\nIt is used to find the trends in data. \\no \\nIt helps to predict real/continuous values. \\no \\nBy performing the regression, we can confidently determine the most important factor, the \\nleast important factor, and how each factor is affecting the other factors. \\n \\nTypes of Regression \\nThere are various types of regressions which are used in data science and machine learning. Each type \\nhas its own importance on different scenarios, but at the core, all the regression methods analyze the \\neffect of the independent variable on dependent variables. Here we are discussing some important \\ntypes of regression which are given below: \\no \\nLinear Regression \\no \\nLogistic Regression \\no \\nPolynomial Regression \\no \\nSupport Vector Regression \\no \\nDecision Tree Regression \\no \\nRandom Forest Regression \\no \\nRidge Regression \\no \\nLasso Regression \\n \\n \\n \\n2.2.1. Linear Regression: \\no \\nLinear regression is a statistical regression method which is used for predictive analysis. \\no \\nIt is one of the very simple and easy algorithms which works on regression and shows the relationship \\nbetween the continuous variables. \\no \\nIt is used for solving the regression problem in machine learning. \\no \\nLinear regression shows the linear relationship between the independent variable (X-axis) and the \\ndependent variable (Y-axis), hence called linear regression.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 38}, page_content='34 \\n \\no \\nIf there is only one input variable (x), then such linear regression is called simple linear regression. And if \\nthere is more than one input variable, then such linear regression is called multiple linear regression. \\no \\nThe relationship between variables in the linear regression model can be explained using the below image. \\nHere we are predicting the salary of an employee on the basis of the year of experience. \\n \\nBelow is the mathematical equation for Linear regression: \\nY= aX+b  \\n \\nHere, Y = dependent variables (target variables), \\nX= Independent variables (predictor variables), \\na and b are the linear coefficients \\n \\nSome popular applications of linear regression are: \\no \\nAnalyzing trends and sales estimates \\no \\nSalary forecasting \\no \\nReal estate prediction \\no \\nArriving at ETAs in traffic. \\n2.2.2. Logistic Regression: \\no \\nLogistic regression is another supervised learning algorithm which is used to solve the classification \\nproblems. In classification problems, we have dependent variables in a binary or discrete format such as 0 \\nor 1. \\no \\nLogistic regression algorithm works with the categorical variable such as 0 or 1, Yes or No, True or False, \\nSpam or not spam, etc. \\no \\nIt is a predictive analysis algorithm which works on the concept of probability. \\no \\nLogistic regression is a type of regression, but it is different from the linear regression algorithm in the \\nterm how they are used. \\no \\nLogistic regression uses sigmoid function or logistic function which is a complex cost function. This \\nsigmoid function is used to model the data in logistic regression. The function can be represented as:'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 39}, page_content='35 \\n \\n \\no \\nf(x)= Output between the 0 and 1 value. \\no \\nx= input to the function \\no \\ne= base of natural logarithm. \\nWhen we provide the input values (data) to the function, it gives the S-curve as follows: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\no \\nIt uses the concept of threshold levels, values above the threshold level are rounded up to 1, and values \\nbelow the threshold level are rounded up to 0. \\nThere are three types of logistic regression: \\no \\nBinary(0/1, pass/fail) \\no \\nMulti(cats, dogs, lions) \\no \\nOrdinal(low, medium, high) \\nLinear Regression in Machine Learning \\nLinear regression is one of the easiest and most popular Machine Learning algorithms. It is a statistical method that \\nis used for predictive analysis. Linear regression makes predictions for continuous/real or numeric variables such \\nas sales, salary, age, product price, etc. \\nLinear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) \\nvariables, hence called as linear regression. Since linear regression shows the linear relationship, which means it \\nfinds how the value of the dependent variable is changing according to the value of the independent variable. \\nThe linear regression model provides a sloped straight line representing the relationship between the variables. \\nConsider the below image:'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 40}, page_content='36 \\n \\n \\nMathematically, we can represent a linear regression as: \\n \\n \\n \\n \\n \\n \\n \\ny= a0+a1x+ ε \\n \\nHere, \\nY= Dependent Variable (Target Variable) \\nX= Independent Variable (predictor Variable) \\na0= intercept of the line (Gives an additional degree of freedom) \\na1 = Linear regression coefficient (scale factor to each input value). \\nε = random error \\nThe values for x and y variables are training datasets for Linear Regression model representation. \\n \\nTypes of Linear Regression \\n \\nLinear regression can be further divided into two types of the algorithm: \\no \\nSimple Linear Regression: \\nIf a single independent variable is used to predict the value of a numerical dependent variable, then such a \\nLinear Regression algorithm is called Simple Linear Regression. \\no \\nMultiple Linear regression: \\nIf more than one independent variable is used to predict the value of a numerical dependent variable, then \\nsuch a Linear Regression algorithm is called Multiple Linear Regression. \\nLinear Regression Line: \\nA linear line showing the relationship between the dependent and independent variables is called a regression line. \\nA regression line can show two types of relationship: \\no \\nPositive Linear Relationship: \\nIf the dependent variable increases on the Y-axis and independent variable increases on X-axis, then such a \\nrelationship is termed as a Positive linear relationship.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 41}, page_content='37 \\n \\n \\no \\nNegative Linear Relationship: \\nIf the dependent variable decreases on the Y-axis and independent variable increases on the X-axis, then \\nsuch a relationship is called a negative linear relationship. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFinding the best fit \\nline: \\n \\nWhen working with linear regression, our main goal is to find the best fit line that means the error between \\npredicted values and actual values should be minimized. The best fit line will have the least error. \\nThe different values for weights or the coefficient of lines (a0, a1) gives a different line of regression, so we \\nneed to calculate the best values for a0 and a1 to find the best fit line, so to calculate this we use cost function. \\nCost function- \\no \\nThe different values for weights or coefficient of lines (a0, a1) gives the different line of regression, and the \\ncost function is used to estimate the values of the coefficient for the best fit line. \\no \\nCost function optimizes the regression coefficients or weights. It measures how a linear regression model \\nis performing. \\no \\nWe can use the cost function to find the accuracy of the mapping function, which maps the input variable \\nto the output variable. This mapping function is also known as Hypothesis function. \\nFor Linear Regression, we use the Mean Squared Error (MSE) cost function, which is the average of \\nsquared error occurred between the predicted values and actual values. It can be written as:'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 42}, page_content='38 \\n \\nFor the above linear equation, MSE can be calculated as: \\n \\n \\n \\nWhere, \\nN=Total number of observation \\nYi = Actual value \\n(a1xi+a0)= Predicted value. \\n \\nResiduals: The distance between the actual value and predicted values is called residual. If the observed points are \\nfar from the regression line, then the residual will be high, and so cost function will high. If the scatter points are \\nclose to the regression line, then the residual will be small and hence the cost function. \\n \\nGradient Descent: \\no \\nGradient descent is used to minimize the MSE by calculating the gradient of the cost function. \\no \\nA regression model uses gradient descent to update the coefficients of the line by reducing the cost \\nfunction. \\no \\nIt is done by a random selection of values of coefficient and then iteratively update the values to reach the \\nminimum cost function. \\nModel Performance: \\nThe Goodness of fit determines how the line of regression fits the set of observations. The process of \\nfinding the best model out of various models is called optimization. It can be achieved by below method: \\n \\n1. R-squared method: \\no \\nR-squared is a statistical method that determines the goodness of fit. \\no \\nIt measures the strength of the relationship between the dependent and independent variables on a scale of \\n0-100%. \\no \\nThe high value of R-square determines the less difference between the predicted values and actual values \\nand hence represents a good model. \\no \\nIt is also called a coefficient of determination, or coefficient of multiple determination for multiple \\nregression. \\no \\nIt can be calculated from the below formula: \\n \\nAssumptions of Linear Regression \\nBelow are some important assumptions of Linear Regression. These are some formal checks while building a \\nLinear Regression model, which ensures to get the best possible result from the given dataset. \\n \\no \\nLinear relationship between the features and target: \\nLinear regression assumes the linear relationship between the dependent and independent variables. \\no \\nSmall or no multicollinearity between the features: \\nMulticollinearity means high-correlation between the independent variables. Due to multicollinearity, it \\nmay difficult to find the true relationship between the predictors and target variables. Or we can say, it is'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 43}, page_content='39 \\n \\ndifficult to determine which predictor variable is affecting the target variable and which is not. So, the \\nmodel assumes either little or no multicollinearity between the features or independent variables. \\no \\nHomoscedasticity Assumption: \\nHomoscedasticity is a situation when the error term is the same for all the values of independent variables. \\nWith homoscedasticity, there should be no clear pattern distribution of data in the scatter plot. \\no \\nNormal distribution of error terms: \\nLinear regression assumes that the error term should follow the normal distribution pattern. If error terms \\nare not normally distributed, then confidence intervals will become either too wide or too narrow, which \\nmay cause difficulties in finding coefficients. \\nIt can be checked using the q-q plot. If the plot shows a straight line without any deviation, which means \\nthe error is normally distributed. \\no \\nNo autocorrelations: \\nThe linear regression model assumes no autocorrelation in error terms. If there will be any correlation in \\nthe error term, then it will drastically reduce the accuracy of the model. Autocorrelation usually occurs if \\nthere is a dependency between residual errors. \\nSimple Linear Regression in Machine Learning \\nSimple Linear Regression is a type of Regression algorithms that models the relationship between a dependent \\nvariable and a single independent variable. The relationship shown by a Simple Linear Regression model is linear or \\na sloped straight line, hence it is called Simple Linear Regression. \\nThe key point in Simple Linear Regression is that the dependent variable must be a continuous/real value. \\nHowever, the independent variable can be measured on continuous or categorical values. \\nSimple Linear regression algorithm has mainly two objectives: \\no \\nModel the relationship between the two variables. Such as the relationship between Income and \\nexpenditure, experience and Salary, etc. \\no \\nForecasting new observations. Such as Weather forecasting according to temperature, Revenue of a \\ncompany according to the investments in a year, etc. \\nSimple Linear Regression Model: \\nThe Simple Linear Regression model can be represented using the below equation: \\ny= a0+a1x+ ε  \\n \\n \\nWhere, \\na0= It is the intercept of the Regression line (can be obtained putting x=0) \\na1= It is the slope of the regression line, which tells whether the line is increasing or decreasing. \\nε = The error term. (For a good model it will be negligible) \\n \\n2.2.3. Multiple Linear Regressions \\nIn the previous topic, we have learned about Simple Linear Regression, where a single \\nIndependent/Predictor(X) variable is used to model the response variable (Y). But there may be various cases in \\nwhich the response variable is affected by more than one predictor variable; for such cases, the Multiple Linear \\nRegression algorithm is used.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 44}, page_content='40 \\n \\n \\nMoreover, Multiple Linear Regression is an extension of Simple Linear regression as it takes more than one \\npredictor variable to predict the response variable. \\n \\n We can define it as: \\n“Multiple Linear Regression is one of the important regression algorithms which models the linear relationship \\nbetween a single dependent continuous variable and more than one independent variable.” \\n \\nExample: \\nPrediction of CO2 emission based on engine size and number of cylinders in a car. \\n \\nSome key points about MLR: \\no \\nFor MLR, the dependent or target variable(Y) must be the continuous/real, but the predictor or independent \\nvariable may be of continuous or categorical form. \\no \\nEach feature variable must model the linear relationship with the dependent variable. \\no \\nMLR tries to fit a regression line through a multidimensional space of data-points. \\nMLR equation: \\nIn Multiple Linear Regression, the target variable(Y) is a linear combination of multiple predictor variables \\nx1, x2, x3, ...,xn. Since it is an enhancement of Simple Linear Regression, so the same is applied for the multiple \\nlinear regression equation, the equation becomes: \\nY= b<sub>0</sub>+b<sub>1</sub>x<sub>1</sub>+ b<sub>2</sub>x<sub>2</sub>+ b<sub>3</sub>x<sub>\\n3</sub>+...... bnxn       ............... (a)  \\nWhere, \\nY= Output/Response variable \\nb0, b1, b2, b3 , bn....= Coefficients of the model. \\nx1, x2, x3, x4,...= Various Independent/feature variable \\nAssumptions for Multiple Linear Regression: \\no \\nA linear relationship should exist between the Target and predictor variables. \\no \\nThe regression residuals must be normally distributed. \\no \\nMLR assumes little or no multicollinearity (correlation between the independent variable) in data. \\n2.3. Neural Networks (ANN - Artificial Neural Network) \\n \\n2.3.1. Introduction \\nThe term \"Artificial Neural Network\" is derived from Biological neural networks that develop the structure \\nof a human brain. Similar to the human brain that has neurons interconnected to one another, artificial neural \\nnetworks also have neurons that are interconnected to one another in various layers of the networks. These neurons \\nare known as nodes.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 45}, page_content='41 \\n \\n \\n \\nThe given figure illustrates the typical diagram of Biological Neural Network. \\n \\nThe typical Artificial Neural Network looks something like the given figure. \\n \\n \\nDendrites from Biological Neural Network represent inputs in Artificial Neural Networks, cell nucleus represents \\nNodes, synapse represents Weights, and Axon represents Output. \\n \\nRelationship between Biological neural network and artificial neural network: \\n \\nBiological Neural Network \\nArtificial Neural Network \\nDendrites \\nInputs \\nCell nucleus \\nNodes \\nSynapse \\nWeights \\nAxon \\nOutput \\n \\nAn Artificial Neural Network in the field of Artificial intelligence where it attempts to mimic the network of \\nneurons makes up a human brain so that computers will have an option to understand things and make decisions in \\na human-like manner. The artificial neural network is designed by programming computers to behave simply like \\ninterconnected brain cells. \\n \\nThere are around 1000 billion neurons in the human brain. Each neuron has an association point somewhere in the \\nrange of 1,000 and 100,000. In the human brain, data is stored in such a manner as to be distributed, and we can'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 46}, page_content='42 \\n \\nextract more than one piece of this data when necessary from our memory parallelly. We can say that the human \\nbrain is made up of incredibly amazing parallel processors. \\n \\n \\nWe can understand the artificial neural network with an example, consider an example of a digital logic gate that \\ntakes an input and gives an output. \"OR\" gate, which takes two inputs. If one or both the inputs are \"On,\" then we \\nget \"On\" in output. If both the inputs are \"Off,\" then we get \"Off\" in output. Here the output depends upon input. \\nOur brain does not perform the same task. The outputs to inputs relationship keep changing because of the neurons \\nin our brain, which are \"learning.\" \\n \\nThe architecture of an artificial neural network: \\n \\n \\n \\nInput Layer: \\nAs the name suggests, it accepts inputs in several different formats provided by the programmer. \\n \\nHidden Layer: \\nThe hidden layer presents in-between input and output layers. It performs all the calculations to find \\nhidden features and patterns. \\n \\nOutput Layer: \\nThe input goes through a series of transformations using the hidden layer, which finally results in output \\nthat is conveyed using this layer. \\n \\nThe artificial neural network takes input and computes the weighted sum of the inputs and includes a bias. This \\ncomputation is represented in the form of a transfer function. \\n \\n \\n \\nIt determines weighted total is passed as an input to an activation function to produce the output. Activation \\nfunctions choose whether a node should fire or not. Only those who are fired make it to the output layer. There are \\ndistinctive activation functions available that can be applied upon the sort of task we are performing. \\n \\nAdvantages of Artificial Neural Network (ANN) \\n \\nParallel processing capability: \\nArtificial neural networks have a numerical value that can perform more than one task simultaneously. \\n \\nStoring data on the entire network: \\nData that is used in traditional programming is stored on the whole network, not on a database. The \\ndisappearance of a couple of pieces of data in one place doesn\\'t prevent the network from working. \\n \\nCapability to work with incomplete knowledge: \\nAfter ANN training, the information may produce output even with inadequate data. The loss of'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 47}, page_content=\"43 \\n \\nperformance here relies upon the significance of missing data. \\n \\nHaving a memory distribution: \\nFor ANN is to be able to adapt, it is important to determine the examples and to encourage the network \\naccording to the desired output by demonstrating these examples to the network. The succession of the network is \\ndirectly proportional to the chosen instances, and if the event can't appear to the network in all its aspects, it can \\nproduce false output. \\n \\nHaving fault tolerance: \\nExtortion of one or more cells of ANN does not prohibit it from generating output, and this feature makes \\nthe network fault-tolerance. \\n \\nDisadvantages of Artificial Neural Network: \\n \\nAssurance of proper network structure: \\nThere is no particular guideline for determining the structure of artificial neural networks. The appropriate \\nnetwork structure is accomplished through experience, trial, and error. \\n \\nUnrecognized behavior of the network: \\nIt is the most significant issue of ANN. When ANN produces a testing solution, it does not provide insight \\nconcerning why and how. It decreases trust in the network. \\n \\nHardware dependence: \\nArtificial neural networks need processors with parallel processing power, as per their structure. Therefore, \\nthe realization of the equipment is dependent. \\n \\nDifficulty of showing the issue to the network: \\nANNs can work with numerical data. Problems must be converted into numerical values before being \\nintroduced to ANN. The presentation mechanism to be resolved here will directly impact the performance of the \\nnetwork. It relies on the user's abilities. \\n \\nThe duration of the network is unknown: \\nThe network is reduced to a specific value of the error, and this value does not give us optimum results. \\n“Science artificial neural networks that have steeped into the world in the mid-20th century are exponentially \\ndeveloping. In the present time, we have investigated the pros of artificial neural networks and the issues \\nencountered in the course of their utilization. It should not be overlooked that the cons of ANN networks, which are \\na flourishing science branch, are eliminated individually, and their pros are increasing day by day. It means that \\nartificial neural networks will turn into an irreplaceable part of our lives progressively important.” \\n \\nHow do artificial neural networks work? \\nArtificial Neural Network can be best represented as a weighted directed graph, where the artificial \\nneurons form the nodes. The association between the neurons outputs and neuron inputs can be viewed as the \\ndirected edges with weights. The Artificial Neural Network receives the input signal from the external source in the \\nform of a pattern and image in the form of a vector. These inputs are then mathematically assigned by the notations \\nx(n) for every n number of inputs.\"),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 48}, page_content='44 \\n \\n \\n \\nAfterward, each of the input is multiplied by its corresponding weights ( these weights are the details \\nutilized by the artificial neural networks to solve a specific problem ). In general terms, these weights normally \\nrepresent the strength of the interconnection between neurons inside the artificial neural network. All the weighted \\ninputs are summarized inside the computing unit. \\n \\nIf the weighted sum is equal to zero, then bias is added to make the output non-zero or something else to \\nscale up to the system\\'s response. Bias has the same input, and weight equals to 1. Here the total of weighted inputs \\ncan be in the range of 0 to positive infinity. Here, to keep the response in the limits of the desired value, a certain \\nmaximum value is benchmarked, and the total of weighted inputs is passed through the activation function. \\nThe activation function refers to the set of transfer functions used to achieve the desired output. There is a \\ndifferent kind of the activation function, but primarily either linear or non-linear sets of functions. Some of the \\ncommonly used sets of activation functions are the Binary, linear, and Tan hyperbolic sigmoidal activation \\nfunctions. Let us take a look at each of them in details: \\n \\nBinary: \\nIn binary activation function, the output is either a one or a 0. Here, to accomplish this, there is a threshold \\nvalue set up. If the net weighted input of neurons is more than 1, then the final output of the activation function is \\nreturned as one or else the output is returned as 0. \\n \\nSigmoidal Hyperbolic: \\nThe Sigmoidal Hyperbola function is generally seen as an \"S\" shaped curve. Here the tan hyperbolic \\nfunction is used to approximate output from the actual net input. The function is defined as: \\nF(x) = (1/1 + exp(-????x)) \\nWhere ???? is considered the Steepness parameter. \\n \\nTypes of Artificial Neural Network: \\nThere are various types of Artificial Neural Networks (ANN) depending upon the human brain neuron and \\nnetwork functions, an artificial neural network similarly performs tasks. The majority of the artificial neural \\nnetworks will have some similarities with a more complex biological partner and are very effective at their expected \\ntasks. For example, segmentation or classification. \\n \\nFeedback ANN: \\nIn this type of ANN, the output returns into the network to accomplish the best-evolved results internally. \\nAs per the University of Massachusetts, Lowell Centre for Atmospheric Research. The feedback networks feed \\ninformation back into itself and are well suited to solve optimization issues. The Internal system error corrections \\nutilize feedback ANNs. \\n \\nFeed-Forward ANN: \\nA feed-forward network is a basic neural network comprising of an input layer, an output layer, and at least \\none layer of a neuron. Through assessment of its output by reviewing its input, the intensity of the network can be \\nnoticed based on group behavior of the associated neurons, and the output is decided. The primary advantage of this \\nnetwork is that it figures out how to evaluate and recognize input patterns.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 49}, page_content='45 \\n \\nPrerequisite \\nNo specific expertise is needed as a prerequisite before starting this tutorial. \\n \\nAudience \\nOur Artificial Neural Network Tutorial is developed for beginners as well as professionals, to help them \\nunderstand the basic concept of ANNs. \\n \\n2.3.2. PERCEPTRONS \\nOne type of ANN system is based on a unit called a perceptron, illustrated in below Figure: A \\nperceptron takes a vector of real-valued inputs, calculates a linear combination of these inputs, then \\noutputs a 1 if the result is greater than some threshold and -1 otherwise. More precisely, given inputs xl \\nthrough xn the output o(xl, . . . , xn) computed by the perceptron is \\n \\n \\n \\n \\n \\nwhere eachwi  is a real-valued constant, or weight, that determines the contribution of input xi  to \\nthe perceptron output. Notice the quantity \\n)\\n(\\n0\\nw\\n\\uf02d\\n  is a threshold that the weighted combination of \\ninputs \\nx\\nw\\nx\\nw\\nn\\nn\\n\\uf02b\\n\\uf02b....\\n1\\n1\\n must surpass in order for the perceptron to output a 1. \\n \\nTo simplify notation, we imagine an additional constant input\\n1\\n0 \\uf03d\\nx\\n, allowing us to write the above \\ninequality as\\n0\\n0\\n\\uf03e\\n\\uf0e5\\uf03d\\nn\\ni\\ni\\nix\\nw\\n, or in vector form as \\no\\nx\\nw \\uf03e\\n\\uf0ae\\n\\uf0ae\\n.\\n. For brevity, we will sometimes write the \\nperceptron function as \\n \\n  \\n \\nLearning a perceptron involves choosing values for the weights \\nw\\nw\\nn\\n,....,\\n0\\nTherefore, the space H of \\ncandidate hypotheses considered in perceptron learning is the set of all possible real-valued weight \\nvectors. \\n \\nRepresentational Power of Perceptrons: \\n \\nWe can view the perceptron as representing a hyperplane decision surface in the n--dimensional space \\nof instances (i.e., points). The perceptron outputs a 1 for instances lying on one side of the hyperplane'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 50}, page_content='46 \\n \\nand outputs a -1 for instances lying on the other side, as illustrated in Figure below The equation for this \\ndecision hyperplane is \\n0\\n.\\n\\uf03d\\n\\uf0ae\\n\\uf0ae\\nx\\nw\\n. Of course, some sets of positive and negative examples cannot be \\nseparated by any hyperplane. Those that can be separated are called linearly separable sets of \\nexamples.  \\n \\n \\nThe decision surface represented by a two-input perceptron. (a) A set of training examples and \\nthe decision surface of a perceptron that classifies them correctly. (b) A set of training examples that is \\nnot linearly separable (i.e., that cannot be correctly classified by any straight line). xl and x2 are the \\nPerceptron inputs. Positive examples are indicated by \"+\", negative by \"-\". The inputs are fed to multiple \\nunits, and the outputs of these units are then input to a second, final stage. One way is to represent the \\nBoolean function in disjunctive normal form (i.e., as the disjunction (OR) of a set of conjunctions (ANDs) \\nof the inputs and their negations). Note that the input to an AND perceptron can be negated simply by \\nchanging the sign of the corresponding input weight. Because networks of threshold units can represent \\na rich variety of functions and because single units alone cannot, we will generally be interested in \\nlearning multilayer networks of threshold units. \\n \\nThe Perceptron Training Rule \\nAlthough we are interested in learning networks of many interconnected units, let us begin by \\nunderstanding how to learn the weights for a single perceptron. Here the precise learning problem is to \\ndetermine a weight vector that causes the perceptron to produce the correct 1\\n\\uf0b1 output for each of the \\ngiven training examples. \\nSeveral algorithms are known to solve this learning problem. Here we consider two: the \\nperceptron rule and the delta rule. These two algorithms are guaranteed to converge to somewhat \\ndifferent acceptable hypotheses, under somewhat different conditions. They are important to ANNs \\nbecause they provide the basis for learning networks of many units. \\nOne way to learn an acceptable weight vector is to begin with random weights, then iteratively \\napply the perceptron to each training example, modifying the perceptron weights whenever it \\nmisclassifies an example. This process is repeated, iterating through the training examples as many \\ntimes as needed until \\nthe perceptron classifies all training examples correctly. Weights are modified at each step according to \\nthe perceptron training rule, which revises the weight wi associated with input xi according to the rule \\n \\n \\nHere t is the target output for the current training example, o is the output generated by the'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 51}, page_content='47 \\n \\nperceptron, and \\uf068is a positive constant called the learning rate. The role of the learning rate is to \\nmoderate the degree to which weights are changed at each step. It is usually set to some small value \\n(e.g., 0.1) and is sometimes made to decay as the number of weight-tuning iterations increases. \\nWhy should this update rule converge toward successful weight values? To get an intuitive feel, \\nconsider some specific cases. Suppose the training example is correctly classified already by the \\nperceptron. In this case, (t - o) is zero, making wi\\n\\uf044\\n zero, so that no weights are updated. Suppose the \\nperceptron outputs a -1, when the target output is +1. To make the perceptron output a+1 instead of -1 \\nin this case, the weights must be altered to increase the value of \\n\\uf0ae\\n\\uf0ae\\nx\\nw.\\n For example, if xi>0, then \\nincreasing wi will bring the perceptron closer to correctly classifying this example. Notice the training \\nrule will increase w, in this case, because (t - o),\\uf068, and xi are all positive. For example, if xi = .8, \\uf068 = 0.1, \\nt = 1, and o = - 1, then the weight update will be wi\\n\\uf044\\n = \\uf068 (t - o)xi = O.1(1 - (-1))0.8 = 0.16. On the \\nother hand, if t = -1 and o = 1, then weights associated with positive xi will be decreased rather than \\nincreased. \\n \\nIn fact, the above learning procedure can be proven to converge within a finite number of applications \\nof the perceptron training rule to a weight vector that correctly classifies all training examples, provided \\nthe training examples are linearly separable and provided a sufficiently small \\uf068 is used. If the data are \\nnot linearly separable, convergence is not assured. \\n \\nGradient Descent and the Delta Rule \\nAlthough the perceptron rule finds a successful weight vector when the training examples are \\nlinearly separable, it can fail to converge if the examples are not linearly separable. A second training \\nrule, called the delta rule, is designed to overcome this difficulty. If the training examples are not \\nlinearly separable, the delta rule converges toward a best-fit approximation to the target concept. The \\nkey idea behind the delta rule is to use gradient descent to search the hypothesis space of possible \\nweight vectors to find the weights that best fit the training examples. This rule is important because \\ngradient descent provides the basis for the BACKPROPAGATION algorithm, which can learn networks \\nwith many interconnected units. It is also important because gradient descent can serve as the basis for \\nlearning algorithms that must search through hypothesis spaces containing many different types of \\ncontinuously parameterized hypotheses.  \\nThe delta training rule is best understood by considering the task of training an unthresholded \\nperceptron; that is, a linear unit for which the output o is given by \\n \\n \\n \\nThus, a linear unit corresponds to the first stage of a perceptron, without the threshold. \\nIn order to derive a weight learning rule for linear units, let us begin by specifying a measure for the \\ntraining error of a hypothesis (weight vector), relative to the training examples. Although there are \\nmany ways to define this error, one common measure that will turn out to be especially convenient is \\n \\nwhere D is the set of training examples, td is the target output for training example d, and od is the \\noutput of the linear unit for training example d. By this definition, \\n\\uf028\\uf029\\n\\uf0ae\\nw\\nE\\n is simply half the squared'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 52}, page_content='48 \\n \\ndifference between the target output td and the hear unit output od, summed over all training \\nexamples. Here we characterize E as a function of \\uf028\\uf029\\n\\uf0ae\\nw , because the linear unit output o depends on this \\nweight vector. Of course E also depends on the particular set of training examples, but we assume these \\nare fixed during training, so we do not bother to write E as an explicit function of these. In particular, \\nthere we show that under certain conditions the hypothesis that minimizes E is also the most probable \\nhypothesis in H given the training data. \\n \\n2.3.2. Multi-layer Perceptron \\n \\nMulti-layer \\nPerceptron \\n(MLP) is \\na \\nsupervised \\nlearning \\nalgorithm \\nthat \\nlearns \\na \\nfunction f(⋅):Rm→Ro by training on a dataset, where m is the number of dimensions for input and o is the \\nnumber of dimensions for output. Given a set of features X=x1,x2,...,xm and a target y, it can learn a non-\\nlinear function approximator for either classification or regression. It is different from logistic \\nregression, in that between the input and the output layer, there can be one or more non-linear layers, \\ncalled hidden layers. Figure  shows a one hidden layer MLP with scalar output. \\n \\nThe leftmost layer, known as the input layer, consists of a set of neurons {xi|x1,x2,...,xm} representing the \\ninput features. Each neuron in the hidden layer transforms the values from the previous layer with a \\nweighted linear summation w1x1+w2x2+...+wmxm, followed by a non-linear activation function g(⋅):R→R - \\nlike the hyperbolic tan function. The output layer receives the values from the last hidden layer and \\ntransforms them into output values. \\nThe module contains the public attributes coefs_ and intercepts_. coefs_ is a list of weight matrices, \\nwhere weight matrix at index i represents the weights between layer i and layer i+1. intercepts_ is a list \\nof bias vectors, where the vector at index i represents the bias values added to layer i+1. \\nThe advantages of Multi-layer Perceptron are: \\n\\uf0b7 \\nCapability to learn non-linear models. \\n\\uf0b7 \\nCapability to learn models in real-time (on-line learning) using partial_fit. \\nThe disadvantages of Multi-layer Perceptron (MLP) include: \\n\\uf0b7 \\nMLP with hidden layers have a non-convex loss function where there exists more than one local \\nminimum. Therefore different random weight initializations can lead to different validation \\naccuracy. \\n\\uf0b7 \\nMLP requires tuning a number of hyperparameters such as the number of hidden neurons, \\nlayers, and iterations. \\n\\uf0b7 \\nMLP is sensitive to feature scaling.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 53}, page_content='49 \\n \\n2.4. Support Vector Machines  \\n \\nSupport Vector Machine or SVM is one of the most popular Supervised Learning algorithms, \\nwhich is used for Classification as well as Regression problems. However, primarily, it is used for \\nClassification problems in Machine Learning. The goal of the SVM algorithm is to create the best line or \\ndecision boundary that can segregate n-dimensional space into classes so that we can easily put the \\nnew data point in the correct category in the future. This best decision boundary is called a hyperplane. \\nSVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme cases are \\ncalled as support vectors, and hence algorithm is termed as Support Vector Machine. Consider the \\nbelow diagram in which there are two different categories that are classified using a decision boundary \\nor hyperplane:  \\n \\n \\nExample: SVM can be understood with the example that we have used in the KNN classifier. Suppose \\nwe see a strange cat that also has some features of dogs, so if we want a model that can accurately \\nidentify whether it is a cat or dog, so such a model can be created by using the SVM algorithm. We will \\nfirst train our model with lots of images of cats and dogs so that it can learn about different features of \\ncats and dogs, and then we test it with this strange creature. So as support vector creates a decision \\nboundary between these two data (cat and dog) and choose extreme cases (support vectors), it will see \\nthe extreme case of cat and dog. On the basis of the support vectors, it will classify it as a cat. Consider \\nthe below diagram: \\n \\n \\nSVM algorithm can be used for Face detection, image classification, text categorization, etc. \\nTypes of SVM \\nSVM can be of two types:'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 54}, page_content='50 \\n \\no \\nLinear SVM: Linear SVM is used for linearly separable data, which means if a dataset can be \\nclassified into two classes by using a single straight line, then such data is termed as linearly \\nseparable data, and classifier is used called as Linear SVM classifier. \\no \\nNon-linear SVM: Non-Linear SVM is used for non-linearly separated data, which means if a \\ndataset cannot be classified by using a straight line, then such data is termed as non-linear data \\nand classifier used is called as Non-linear SVM classifier. \\nHyperplane and Support Vectors in the SVM algorithm: \\n \\nHyperplane: There can be multiple lines/decision boundaries to segregate the classes in n-\\ndimensional space, but we need to find out the best decision boundary that helps to classify the data \\npoints. This best boundary is known as the hyperplane of SVM. \\nThe dimensions of the hyperplane depend on the features present in the dataset, which means if there \\nare 2 features (as shown in image), then hyperplane will be a straight line. And if there are 3 features, \\nthen hyperplane will be a 2-dimension plane. \\nWe always create a hyperplane that has a maximum margin, which means the maximum distance \\nbetween the data points. \\n \\nSupport Vectors: \\nThe data points or vectors that are the closest to the hyperplane and which affect the position of the \\nhyperplane are termed as Support Vector. Since these vectors support the hyperplane, hence called a \\nSupport vector. How does SVM works? \\n \\n2.4.1. Linear SVM: \\nThe working of the SVM algorithm can be understood by using an example. Suppose we have a dataset \\nthat has two tags (green and blue), and the dataset has two features x1 and x2. We want a classifier \\nthat can classify the pair(x1, x2) of coordinates in either green or blue. Consider the below image: \\n \\nSo as it is 2-d space so by just using a straight line, we can easily separate these two classes. But there \\ncan be multiple lines that can separate these classes. Consider the below image:'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 55}, page_content='51 \\n \\n \\n \\nHence, the SVM algorithm helps to find the best line or decision boundary; this best boundary or region \\nis called as a hyperplane. SVM algorithm finds the closest point of the lines from both the classes. These \\npoints are called support vectors. The distance between the vectors and the hyperplane is called \\nas margin. And the goal of SVM is to maximize this margin. The hyperplane with maximum margin is \\ncalled the optimal hyperplane.  \\n \\n2.4.2. Non-Linear SVM: \\n           If data is linearly arranged, then we can separate it by using a straight line, but for non-linear \\ndata, we cannot draw a single straight line. Consider the below image: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSo to separate these data points, we need to add one more dimension. For linear data, we have used two dimensions \\nx and y, so for non-linear data, we will add a third dimension z. It can be calculated as: \\nz=x2 +y2 \\nBy adding the third dimension, the sample space will become as below image:'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 56}, page_content='52 \\n \\n \\nSo now, SVM will divide the datasets into classes in the following way. Consider the below image: \\n \\n \\nSince we are in 3-d Space, hence it is looking like a plane parallel to the x-axis. If we convert it in 2d \\nspace with z=1, then it will become as: \\n \\n \\nHence we get a circumference of radius 1 in case of non-linear data. \\n \\n2.4.3. SVM Kernels \\nIn practice, SVM algorithm is implemented with kernel that transforms an input data space into'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 57}, page_content='53 \\n \\nthe required form. SVM uses a technique called the kernel trick in which kernel takes a low dimensional \\ninput space and transforms it into a higher dimensional space. In simple words, kernel converts non-\\nseparable problems into separable problems by adding more dimensions to it. It makes SVM more \\npowerful, flexible and accurate. The following are some of the types of kernels used by SVM. \\nLinear Kernel \\nIt can be used as a dot product between any two observations. The formula of linear kernel is as below  \\n \\nK(x,xi)=sum(x∗xi) \\n \\nFrom the above formula, we can see that the product between two vectors say 𝑥 & 𝑥𝑖 is the sum of the \\nmultiplication of each pair of input values. \\n \\n2.5. Unsupervised Machine Learning: \\n2.5.1. Introduction to clustering \\n \\nAs the name suggests, unsupervised learning is a machine learning technique in which models \\nare not supervised using training dataset. Instead, models itself find the hidden patterns and insights \\nfrom the given data. It can be compared to learning which takes place in the human brain while learning \\nnew things. It can be defined as: \\n \\n“Unsupervised learning is a type of machine learning in which models are trained using \\nunlabeled dataset and are allowed to act on that data without any supervision.” \\n \\nUnsupervised learning cannot be directly applied to a regression or classification problem \\nbecause unlike supervised learning, we have the input data but no corresponding output data. The goal \\nof unsupervised learning is to find the underlying structure of dataset, group that data according to \\nsimilarities, and represent that dataset in a compressed format \\n \\nExample: Suppose the unsupervised learning algorithm is given an input dataset containing images of \\ndifferent types of cats and dogs. The algorithm is never trained upon the given dataset, which means it \\ndoes not have any idea about the features of the dataset. The task of the unsupervised learning \\nalgorithm is to identify the image features on their own. Unsupervised learning algorithm will perform \\nthis task by clustering the image dataset into the groups according to similarities between images. \\n \\n \\nWhy use Unsupervised Learning?'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 58}, page_content='54 \\n \\nBelow are some main reasons which describe the importance of Unsupervised Learning: \\no \\nUnsupervised learning is helpful for finding useful insights from the data. \\no \\nUnsupervised learning is much similar as a human learns to think by their own experiences, \\nwhich makes it closer to the real AI. \\no \\nUnsupervised learning works on unlabeled and uncategorized data which make unsupervised \\nlearning more important. \\no \\nIn real-world, we do not always have input data with the corresponding output so to solve such \\ncases, we need unsupervised learning. \\nWorking of Unsupervised Learning \\n \\nWorking of unsupervised learning can be understood by the below diagram: \\n \\n \\nHere, we have taken an unlabeled input data, which means it is not categorized and \\ncorresponding outputs are also not given. Now, this unlabeled input data is fed to the machine learning \\nmodel in order to train it. Firstly, it will interpret the raw data to find the hidden patterns from the data \\nand then will apply suitable algorithms such as k-means clustering, Decision tree, etc. \\nOnce it applies the suitable algorithm, the algorithm divides the data objects into groups according to \\nthe similarities and difference between the objects. \\nTypes of Unsupervised Learning Algorithm: \\nThe unsupervised learning algorithm can be further categorized into two types of problems:'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'file_path': 'rag-dataset-main/machine-learning/MACHINE LEARNING(R17A0534).pdf', 'total_pages': 120, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 59}, page_content=\"55 \\n \\no \\nClustering: Clustering is a method of grouping the objects into clusters such that objects with \\nmost similarities remains into a group and has less or no similarities with the objects of another \\ngroup. Cluster analysis finds the commonalities between the data objects and categorizes them \\nas per the presence and absence of those commonalities. \\no \\nAssociation: An association rule is an unsupervised learning method which is used for finding \\nthe relationships between variables in the large database. It determines the set of items that \\noccurs together in the dataset. Association rule makes marketing strategy more effective. Such \\nas people who buy X item (suppose a bread) are also tend to purchase Y (Butter/Jam) item. A \\ntypical example of Association rule is Market Basket Analysis. \\n \\n \\nUnsupervised Learning algorithms: \\n \\nBelow is the list of some popular unsupervised learning algorithms: \\no \\nK-means clustering \\no \\nKNN (k-nearest neighbors) \\no \\nHierarchal clustering \\no \\nAnomaly detection \\no \\nNeural Networks \\no \\nPrinciple Component Analysis \\no \\nIndependent Component Analysis \\no \\nApriori algorithm \\no \\nSingular value decomposition \\nAdvantages of Unsupervised Learning \\no \\nUnsupervised learning is used for more complex tasks as compared to supervised learning \\nbecause, in unsupervised learning, we don't have labeled input data. \\no \\nUnsupervised learning is preferable as it is easy to get unlabeled data in comparison to labeled \\ndata. \\nDisadvantages of Unsupervised Learning \\no \\nUnsupervised learning is intrinsically more difficult than supervised learning as it does not have \\ncorresponding output. \\no \\nThe result of the unsupervised learning algorithm might be less accurate as input data is not \\nlabeled, and algorithms do not know the exact output in advance.\"),\n",
       " ...]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:43:25.989570Z",
     "start_time": "2025-07-05T17:42:24.738977Z"
    }
   },
   "cell_type": "code",
   "source": "ids = vector_store.add_documents(documents = chunks)",
   "id": "99e0bb7f4caee0a4",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:44:12.102008Z",
     "start_time": "2025-07-05T17:44:12.099674Z"
    }
   },
   "cell_type": "code",
   "source": "ids[:10], len(ids)",
   "id": "52c606d6efd5a763",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['f1de637a-c06c-47b6-b77e-a8ebabec940c',\n",
       "  'a3013ca1-a39b-4e0e-b031-bd1f66cf1bd0',\n",
       "  'c18c9f8b-198e-42cb-9c87-f982ecc3dcf0',\n",
       "  '1baa2d99-f6b7-4de7-8fe5-b8c42dbe387c',\n",
       "  'e08b49f5-b476-4e15-8b98-25170a3d40df',\n",
       "  '6ac379ba-dc6b-41e3-ac10-d60408b6b25b',\n",
       "  '59a89ebc-9e7c-4027-b179-f45dfdcf9b47',\n",
       "  '52c81183-560f-4f0e-9f6f-eee23adf6346',\n",
       "  'f8fdb64b-a9f7-4ecb-abd2-e53a0e262eef',\n",
       "  '70373b9c-1505-412c-aaac-c9ad030969af'],\n",
       " 2440)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:44:46.104326Z",
     "start_time": "2025-07-05T17:44:46.094144Z"
    }
   },
   "cell_type": "code",
   "source": "vector_store.index_to_docstore_id",
   "id": "3a577a56a8acd988",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'd58eca07-3e59-4672-bd2e-2c38e616d948',\n",
       " 1: '38afa4f9-a8b8-44de-8f0e-bbac52d71490',\n",
       " 2: '77d0580c-0576-49ab-b8a4-77ea162abb1e',\n",
       " 3: '6b4142d1-f1fb-4e9f-8e08-30a7cb5e04c3',\n",
       " 4: '5f81a121-5dba-4b42-b885-2876ca5ace30',\n",
       " 5: 'ab9f21a3-f874-475d-adfb-f02a3e1af660',\n",
       " 6: '1cb3b16c-4837-4b5b-88a0-1eeac6811a3c',\n",
       " 7: '0cb8c823-afcb-49b9-8484-f0ef7f32b4fc',\n",
       " 8: '26dad152-ed92-4a0c-a4a3-cbbcfc06feb2',\n",
       " 9: 'b1bfb554-b09d-493d-a729-5dc335da6d23',\n",
       " 10: '1edb36b3-e51d-4923-8484-a610fd53b9d4',\n",
       " 11: 'c5921a0f-5ebe-4e28-a0e0-37c987fae66c',\n",
       " 12: '035249e6-c482-40ce-a67a-8d325bd05317',\n",
       " 13: '0275d68e-4b9c-4fac-aba1-167a71bbc22d',\n",
       " 14: '9ff4568d-7aa8-4a94-b547-92b8ba2feda6',\n",
       " 15: 'c6732cc2-5e65-4012-a09a-35a568b9eee1',\n",
       " 16: 'd7607d5a-6f9b-4591-9d41-0c87ed42bb9e',\n",
       " 17: 'baf2a2ae-1da8-493a-b20a-0706a56159a0',\n",
       " 18: '24107200-d3b4-4537-a0de-75de3ac73ae2',\n",
       " 19: 'cb4722fc-d7b5-42a7-8bb1-57675e6a4b1c',\n",
       " 20: 'e232fcb1-879c-4517-8bc2-9a722945616f',\n",
       " 21: '5ba0ff87-b24d-485f-a926-5e9c11b3ae0b',\n",
       " 22: 'a2866a25-d516-421e-9580-d02f1b0337b1',\n",
       " 23: 'fbef20e9-198c-46e3-ade0-53f0470bcc86',\n",
       " 24: '1f2fcc0f-e080-4295-b257-1a12860e2638',\n",
       " 25: '01d28311-f808-44ca-ac30-0ae9546ed702',\n",
       " 26: '719ac25f-9264-4f14-a532-fb6bcfae8a7a',\n",
       " 27: '0beca944-06b8-429c-9823-39c38984aa14',\n",
       " 28: '7918079f-f936-4297-890b-3bcceaf3fbed',\n",
       " 29: 'f59d7fb8-d383-4295-a7ad-24117ff75052',\n",
       " 30: 'e095c579-237a-4d13-87bb-41fced3d9c86',\n",
       " 31: '015687b0-45e9-4baf-8dbe-3791a20b7fc8',\n",
       " 32: '0c63663d-0617-4938-b871-2bb4758ae0b4',\n",
       " 33: 'b3312020-45bd-450e-ba31-23567e99f4eb',\n",
       " 34: 'ed9de60c-f5fc-4637-8960-8398b905b9c8',\n",
       " 35: 'b1324f1a-cf9e-4ad7-b40d-57067798b525',\n",
       " 36: '1c5aea4a-cf55-4bef-9412-700d5b33fc87',\n",
       " 37: '4415be21-46b5-4084-b544-ab91c35b063e',\n",
       " 38: '679c6bbe-fb2b-4b22-95f0-159636595c27',\n",
       " 39: '7d5481db-89d2-4240-a5ef-444751bd00ab',\n",
       " 40: 'e1ea0591-4d2f-4121-873c-01413ce0ac31',\n",
       " 41: '094fe678-7d1f-4c5b-9375-7520a5006492',\n",
       " 42: '636c3ecf-06ff-43c3-a5ea-f70a152797b2',\n",
       " 43: '5b95d412-546e-4afb-a817-4c46e5429680',\n",
       " 44: 'c1263506-fb5a-435e-b41e-f63321abdbe2',\n",
       " 45: 'f353992d-90f1-4d83-bd85-fd5a5b9d24c6',\n",
       " 46: 'fafd35c1-a041-409d-bba0-44691d571b25',\n",
       " 47: '2c0c25cd-be4a-4c66-b351-4abea1ba63ef',\n",
       " 48: '0a5bd4ee-4896-4ff7-aed4-631c952c5192',\n",
       " 49: '0cee56d4-cc79-499e-82de-6885a599c4ad',\n",
       " 50: 'e368663c-ad2d-4cd2-bc77-744cf1a349f0',\n",
       " 51: '69ed5ac5-3138-40f3-88bf-d76835e55fae',\n",
       " 52: 'c3accb39-bafd-49f4-bc83-49dcf8d87ce7',\n",
       " 53: '834796ae-4160-4f59-a2f2-ae31fd2a6238',\n",
       " 54: 'ea7108c0-3fcb-431c-be82-81930cf962b4',\n",
       " 55: '2933528e-da8e-4205-8dab-95bfac4cca8d',\n",
       " 56: '0043f9c9-9be6-447f-81f5-9824f78d5425',\n",
       " 57: 'b4c986bd-a2e9-4701-a04e-bbbe4cedf161',\n",
       " 58: '0814239c-a09b-4e52-8099-faeca81d3f2f',\n",
       " 59: '5799e42b-60ee-4e2e-92d1-ec4daffe2384',\n",
       " 60: 'cc2aad6e-6267-41c1-91b2-e2925a4a76d7',\n",
       " 61: 'b3d7d4cc-92b8-467c-bf90-1594cf6e7486',\n",
       " 62: 'ac3a5b31-9d0e-4209-8072-68f85ce731ab',\n",
       " 63: '00a08262-ce61-4d8d-92ff-f309a3e7b95b',\n",
       " 64: 'e65182b8-a239-4d8b-8984-a608f0e36f5a',\n",
       " 65: '94b672e7-e8a0-4e05-bd00-44dd961913cb',\n",
       " 66: '3aa4cc29-78c5-4f01-ab86-1bc4e484ba6a',\n",
       " 67: 'f93dde74-079e-40f2-8c3a-18593662a8d7',\n",
       " 68: '6c8aefa8-071e-44a4-9413-12608f5a50cd',\n",
       " 69: '752cb7ac-e453-4a44-8efa-e1805234f75c',\n",
       " 70: 'c79906c3-c117-4df5-94c1-f4b0994f383d',\n",
       " 71: '4e8c38aa-c027-4623-9350-4fe548cec031',\n",
       " 72: '8a2fcfd3-dea8-422b-8cd6-6017ce529f0c',\n",
       " 73: 'b95dc541-ebda-48b3-933f-e56281601bb7',\n",
       " 74: '16a99691-91cb-49e0-989d-516ec1a61163',\n",
       " 75: '83f57021-5aba-4e9b-bbef-f1b1d870f757',\n",
       " 76: '85cdd75e-1661-4397-a789-75724134688a',\n",
       " 77: '6e4762d2-9e88-4c58-93a4-9eb1a2c8ca7b',\n",
       " 78: 'da59af76-034d-4b15-bfc3-0ca563248f8d',\n",
       " 79: 'dded685c-9a94-4283-8b1f-4d208affe987',\n",
       " 80: 'fab670e5-9ea9-4899-8659-7714ce73b111',\n",
       " 81: '7e14d398-a626-4a0b-9eac-97235d5d5b8f',\n",
       " 82: 'd7c97780-6dbc-459c-a0be-a34f494f3216',\n",
       " 83: '87d2f495-6cb2-4fd3-b417-7cc810cdee1b',\n",
       " 84: '00cd4eb1-2e04-4086-8a25-1bfa7d7405f1',\n",
       " 85: 'ce79f67f-3b01-47cd-aa86-3316cd5b048b',\n",
       " 86: 'ee2f38ab-3dbe-48ca-aef8-5b178cbb3f86',\n",
       " 87: 'ddf15e9a-95b5-4ba4-b0f4-09b086cf58ce',\n",
       " 88: '71f2ad01-cb7c-4123-a61e-e3f1f0aafb8e',\n",
       " 89: '426a5b0f-4d20-4775-84ab-9b51a450c516',\n",
       " 90: '6cfbd828-fccb-45ef-97d1-2766f0f73428',\n",
       " 91: '627a9166-dabb-41de-bafe-59a31d43b424',\n",
       " 92: '1efe0291-fe75-47a6-b072-10a72c2dad09',\n",
       " 93: '7e20a9ed-569a-4995-96dc-9b39189da86f',\n",
       " 94: '5234e836-56c1-4353-b41b-193b4f130126',\n",
       " 95: 'ccb6d81b-2c50-487e-931b-e0f7eb52a489',\n",
       " 96: '061d850a-8785-4ad8-987e-556bba25a78d',\n",
       " 97: '2a605584-2897-4184-9e31-ffb48b6bb23b',\n",
       " 98: '00e31e0d-1588-4a94-ae05-7fce6b984f6d',\n",
       " 99: '2703eaa1-c0b6-4cf8-97e4-a6b375092bdb',\n",
       " 100: '124a1eb7-fbf8-4f42-a86d-9a4f5605a65f',\n",
       " 101: 'e35de959-b583-45db-b389-5fdc34cff339',\n",
       " 102: '8abc6e36-2cd2-4f10-8c7f-84c714996b8b',\n",
       " 103: '9e348745-f20d-4d89-bde0-5fee5cefcf16',\n",
       " 104: '07353256-fd2f-4572-9f0c-6ccedf52980e',\n",
       " 105: 'e974928e-e9a8-433c-bfa5-8b5910cd9771',\n",
       " 106: '15dd0568-9142-46dd-8103-8424873a2aa8',\n",
       " 107: 'f056678e-925a-412b-864b-6d9e36bdf74b',\n",
       " 108: 'b5ef8156-1438-41d4-8f2f-56a197789378',\n",
       " 109: 'b25ad934-8bdd-4e5a-b7e5-249863c6ca6a',\n",
       " 110: '240907ca-0716-467c-af79-10ecf2e34d5d',\n",
       " 111: 'f592b42e-1030-4613-99f4-bb36911303fc',\n",
       " 112: '4b28e982-b083-44aa-b235-34771ac7ba05',\n",
       " 113: 'f2a29219-d38f-4a17-8a8f-0952e5b28431',\n",
       " 114: '466f7cec-9c26-4048-8011-1fd560135752',\n",
       " 115: '64d80ee9-7e64-450a-89c3-2723339a90a8',\n",
       " 116: 'f965d4f0-aa7c-4aa5-a6af-ae87caf5c9a8',\n",
       " 117: 'c2729a9f-2e10-4288-8e50-a9566509b479',\n",
       " 118: 'a7d51664-2dfb-4040-b000-d254802b93ed',\n",
       " 119: '41e41368-2c08-4f55-8258-bf87bf8f3585',\n",
       " 120: '05814425-d93d-4e1b-8e4e-aa18e5270fd7',\n",
       " 121: 'dc61f3b3-df2b-4db4-b92f-39d5f400ef71',\n",
       " 122: '1ed544b4-5320-474b-a4f3-72b2537c367f',\n",
       " 123: '2b8843c8-8c86-44b3-88e2-1cdb33ab8d95',\n",
       " 124: 'd9c01809-8554-4ff4-ab45-bd70e296410b',\n",
       " 125: 'b34b23b1-f0c4-46f5-accd-c992e3748918',\n",
       " 126: 'ed5a6d52-65db-4964-a8fc-cdeb6021bcbb',\n",
       " 127: '1e394237-57bf-49fd-ad4c-53a988c63528',\n",
       " 128: '5fceb85e-0e8c-404c-9d8c-f49b4164becd',\n",
       " 129: '056cd64a-1144-4476-84cc-c6a9a8b81cef',\n",
       " 130: 'b4edbf4f-5f87-4466-b6d7-974338cf29c1',\n",
       " 131: 'c4e184b5-2734-41bf-ba6d-608309b4cb0d',\n",
       " 132: '40608e0a-5fe1-40db-93dd-420161a6a84f',\n",
       " 133: 'ca7e4271-e523-4bff-860d-2356fde3ae19',\n",
       " 134: '9211077f-a2f7-4ec3-9e48-437965f28946',\n",
       " 135: '774c3cde-c997-48ab-bbfe-5bb0b2cf607c',\n",
       " 136: 'd4a1e25c-8f3a-4e15-a587-668b6aa8219f',\n",
       " 137: '88b2638c-859f-45d3-af3e-8151a1b465bd',\n",
       " 138: '0240035f-b917-42e8-bdac-fb0d0d832b1d',\n",
       " 139: '46a99c02-7f18-41f1-8da3-2a6c2c05e5f4',\n",
       " 140: '2c2b8ffc-15ef-485a-8398-de45af73618b',\n",
       " 141: 'ec40e556-bd31-4f94-b919-9ec086a34f98',\n",
       " 142: 'fabcc577-f85f-4b6d-87f5-45ef82d16273',\n",
       " 143: '67a32f82-f99a-492f-aaf8-9f1a049a5e37',\n",
       " 144: '48b8ec44-1cd8-49db-89cb-505c681424ca',\n",
       " 145: 'aca88f71-5f18-4a65-bc08-fb429ea9c791',\n",
       " 146: '22854773-4354-4587-b732-cc81480023c7',\n",
       " 147: '52bbe936-29db-43e0-886f-8f6e4a49db05',\n",
       " 148: '8d8eb054-7622-4886-ac7c-b634f53f3335',\n",
       " 149: 'db95fa25-2889-4822-b97d-5eb64ff0104b',\n",
       " 150: '0dd7b665-65a1-4f23-be60-1d43efe106a9',\n",
       " 151: 'd3b7ebd5-f430-48fc-b310-fc7aca312537',\n",
       " 152: '36d41916-83dd-4da1-b8ee-08c3a0afe289',\n",
       " 153: '3cffe6b2-f0e7-4305-bdc6-4a76eb32c55b',\n",
       " 154: '6cfda1a5-55df-4811-8e85-a8f91c9b1c16',\n",
       " 155: 'fb14cd8e-d899-49e0-81ac-68d665964cad',\n",
       " 156: 'eae745c7-d754-4488-887f-eb561474a751',\n",
       " 157: '7b565e7a-6d71-4ed3-aa72-d0912ddd5b03',\n",
       " 158: '807df677-9347-4d8d-906a-d490c5b45e11',\n",
       " 159: 'a9aeae7e-6fef-47a4-b074-58ca4b682db5',\n",
       " 160: '64c2b66e-0a23-42d6-9dc7-6faaded0e2c9',\n",
       " 161: 'b88c5367-2fad-4e01-b28f-79c78413f39c',\n",
       " 162: 'e254dd29-91a3-464d-84b6-2c2e6ecb6553',\n",
       " 163: 'ff0afe96-6f7c-48ac-8d58-fc2f12c9a201',\n",
       " 164: 'b5c82a92-eed6-41cf-a9cb-d4336b0e6e71',\n",
       " 165: '212c9092-5218-4d00-a41a-8f312371c3fb',\n",
       " 166: '8499fe2e-2073-4760-bd9b-6669531c6883',\n",
       " 167: 'd9f03d67-e43e-42db-8ab1-362db73a48ee',\n",
       " 168: '80e067c5-8a70-4e24-9e9c-b45f5a97ccac',\n",
       " 169: '9a11f5f5-3945-4974-8fb2-71c97bdc424f',\n",
       " 170: 'ee965fe4-0645-4b81-97b5-9ac1bfcc0d8d',\n",
       " 171: '96bbc3dd-b661-4733-8150-7e38db1396c6',\n",
       " 172: '914d6af4-e153-48be-9096-da7944c84276',\n",
       " 173: '9a0d6eae-117e-4387-bc3d-c9be7cd23ef8',\n",
       " 174: 'f3a85bb0-92dc-44e5-89ac-19b2e2400fdf',\n",
       " 175: 'd4def999-fc87-459e-a28c-01dedc3ee8ec',\n",
       " 176: '43acd902-8357-4700-b1f5-d608db9da13a',\n",
       " 177: 'b7f1ff67-b619-41b2-bbe0-7ff752f03c86',\n",
       " 178: '0b341267-b991-4494-a28b-06cb03b54182',\n",
       " 179: '8261a305-7068-4bea-b642-55739e4caa9a',\n",
       " 180: '4fce0d37-8d83-4513-9714-f7170469d7b9',\n",
       " 181: '0de815ae-9940-4469-8fa9-8a9db3ad8fcd',\n",
       " 182: '2ba2c2bc-27d8-4917-a999-c3bfc9160232',\n",
       " 183: '2c564679-8ef2-48bb-b5aa-09d6c5724c61',\n",
       " 184: 'db5e8ce7-f5a2-48cb-982d-2727108e7c75',\n",
       " 185: 'e7f8aa92-224b-4867-bd61-744ec333abbf',\n",
       " 186: 'f71f1d53-55fe-4f9c-95ad-05def55b7f02',\n",
       " 187: '7dc0266d-0e65-4d19-8e2a-08a8191b27cb',\n",
       " 188: 'bd58089a-0766-4d19-b6e2-e3f220ecf0d0',\n",
       " 189: '58a3896e-3e72-46d3-ab38-4dc19c9bb733',\n",
       " 190: '8fbfb8c8-60b8-45a0-9e29-d921ddb9f9b5',\n",
       " 191: '2f5cf1aa-bc8c-4797-92c2-58bfb3cd7e19',\n",
       " 192: '6c26ae4a-60b7-4067-a289-f355723b0d5b',\n",
       " 193: '05984fe4-655e-44de-ba19-150b2b3a39db',\n",
       " 194: 'bf89884e-908d-4696-9c41-8702b817c2c2',\n",
       " 195: 'a77c99e9-2250-4636-9384-26e2856cee89',\n",
       " 196: '9114d17c-e34b-40e2-8f57-d9c8d1ed2c23',\n",
       " 197: '141580a7-3081-4a6a-9c76-89a6bce01df0',\n",
       " 198: '4d0a4739-f477-4e7c-9cd0-5f9b3d777654',\n",
       " 199: '3cbbd36b-fb0a-4236-8a7d-da035a4572b5',\n",
       " 200: 'e02e8a9c-e92a-4ceb-9801-85e2441a3efc',\n",
       " 201: 'dcb49f5f-b901-4755-a02a-8c4c70916c2b',\n",
       " 202: '7b258fb2-f0ff-4a7e-8be0-1172489ac662',\n",
       " 203: '9ee5aa04-bc9c-42ab-beae-4964392565b6',\n",
       " 204: '6ea599b0-ffeb-483c-b02e-351ab17796ea',\n",
       " 205: '127f3d06-3b42-40ac-bed4-1b9bcf4834cb',\n",
       " 206: '60aadfda-955e-4b63-b576-975427cd14c1',\n",
       " 207: '7c2c4c98-9804-4fef-8d90-53c513795856',\n",
       " 208: 'd5b3c729-6072-4158-a46d-58595fd1baf0',\n",
       " 209: '08b9b7d9-1186-4794-bb59-34c51c7452de',\n",
       " 210: '12308a18-7188-4d45-8ae1-d9cd9a566921',\n",
       " 211: '420ed6b3-833a-4613-8bb0-3d20b4ce4266',\n",
       " 212: 'c38aa03f-b603-4fb3-ab15-b51adcb7d134',\n",
       " 213: '92d41fc3-7c89-4edc-85f6-dacd82548cf8',\n",
       " 214: '9514be46-dd49-4529-87d9-ceacdbbc01da',\n",
       " 215: '5292c9ff-543e-4e89-b85f-ebfa52aa7d48',\n",
       " 216: '84e44d1d-226d-49e3-af9c-f9375edac514',\n",
       " 217: '2a7b75ca-ee4c-4092-a175-449f3b4dd137',\n",
       " 218: '4a89538c-6c6a-4742-81f3-d7e12b5df4a1',\n",
       " 219: '19507bff-e32d-4aa9-8286-4ddac30e6127',\n",
       " 220: 'adbd24de-6ffb-4f00-acb7-99951cc1052a',\n",
       " 221: 'ed3574c9-b772-470f-a19c-63c6f4c0caed',\n",
       " 222: '841cee81-4954-426b-84a3-bb3a97d2d377',\n",
       " 223: '5b0d09c7-88ed-47b1-8460-0a18d8af108f',\n",
       " 224: '047d482e-077f-4c14-a914-51b83ff8a7de',\n",
       " 225: 'f9077f11-f61d-484b-ae4a-b8f23402df0a',\n",
       " 226: '6297e81e-2982-4e1f-af30-638f132e3810',\n",
       " 227: 'e7eaa38e-162b-4acd-bfe9-d5dfffe58125',\n",
       " 228: '1bb5f161-0d30-4914-bd62-bfd8370da588',\n",
       " 229: '5ad16f2a-a16f-4ffe-8f68-e332c47e40a4',\n",
       " 230: '4a7c15de-f4f0-4336-b23b-c519e821f3ed',\n",
       " 231: '31a6099b-dfd5-4dc0-a220-d6b5602193f1',\n",
       " 232: 'a18d9658-af39-462a-be8c-07f6a7ba2ddd',\n",
       " 233: '8c9b7ba9-068c-4594-8480-218d55e0ae65',\n",
       " 234: '09b568ba-f779-4653-b2d8-e88b42147b4f',\n",
       " 235: 'af8e7df8-a731-4de9-87c9-12ec51be7dd5',\n",
       " 236: '548c8cff-d2ea-479c-ad7e-b6c8fbac498b',\n",
       " 237: 'ae31d434-1d4a-475b-ad4a-fa5da1daad9d',\n",
       " 238: '7c818075-1f28-4e10-bee6-b0a8073eaeb6',\n",
       " 239: 'f7bc9358-202a-4e78-a5e9-da7a94190b94',\n",
       " 240: '4504547e-5fcf-4e17-a862-4219309aae26',\n",
       " 241: 'bc6ca762-81e5-4312-89fc-fa9dc8ef58a9',\n",
       " 242: '27958d37-b1fb-4669-96d1-2cedf10d7a45',\n",
       " 243: '1e7ceead-428d-41c6-b2dd-f55955a9a81f',\n",
       " 244: '85a646dc-2fdc-4470-9751-6aedfd60ecfd',\n",
       " 245: '7d4660ef-3786-48d9-8f62-80b1e28755e0',\n",
       " 246: '221e0f58-04ab-4441-a6c4-557ef3f8ceb0',\n",
       " 247: 'baacae93-f6d1-45f3-a16e-afc941adab33',\n",
       " 248: '9b28430a-ba3c-44fa-9d8b-ef9bcc66e54d',\n",
       " 249: '1d83c244-1c24-49c6-b47d-75074be3e454',\n",
       " 250: '39574e6f-5f75-4801-8c10-d59200b8776e',\n",
       " 251: '84553cd1-3fa4-49e1-80d8-ba72c1d87da4',\n",
       " 252: 'fd0b0221-bc65-40ed-b278-ff46b5028431',\n",
       " 253: '729b6fee-239d-43ab-a4b1-1d0041654804',\n",
       " 254: 'd3497dc0-0f84-4151-bc5c-11a10ca27e8d',\n",
       " 255: '6092abf6-c552-4012-adde-5f08ff251bf7',\n",
       " 256: '618d1f36-5eea-4a62-98e1-5a2e586ec58d',\n",
       " 257: 'bd2d47eb-df62-453f-a060-e888bb034871',\n",
       " 258: 'bacc873c-dcdd-4902-9c3c-189aee653519',\n",
       " 259: '8a46afa7-fd58-4680-92af-30e90d5a7876',\n",
       " 260: 'f78e0d94-8710-4bb7-aff8-3c374d04061c',\n",
       " 261: '65f4e1ed-cf7a-47f5-b680-2e2a3384137f',\n",
       " 262: '39b99b47-748c-425f-a278-83d0c902ef58',\n",
       " 263: '00e6d5a2-385e-4890-9508-535e6e20555f',\n",
       " 264: 'fd6fc45e-5535-45c1-bf7d-facb79415a1a',\n",
       " 265: '6115abc4-afd5-4727-b7af-c2c9e5221437',\n",
       " 266: '6b94d198-0226-4c5f-b4d2-3d1c40f4d1f7',\n",
       " 267: '331f470a-a472-438f-81bd-77637fee27ea',\n",
       " 268: '3e1ce666-40d6-4a05-a3f2-b6d173573ada',\n",
       " 269: '5e5d0386-c8e6-4bea-8936-30d6261d8914',\n",
       " 270: '83fa23c2-7b33-4ea3-ae90-f3669cc23d59',\n",
       " 271: '3b2f026c-e215-4c43-ae6e-b83c85912f3c',\n",
       " 272: 'be2558fc-fd75-4ff5-87eb-fc106dbf1cc7',\n",
       " 273: '9d89b05c-1124-4ede-9f5f-4081d3ab5800',\n",
       " 274: 'c22c9453-d640-462d-8ca9-1eb61e2c8eda',\n",
       " 275: '0212cc58-3a36-479a-956f-c7679630c78c',\n",
       " 276: 'bd742e7f-c0fa-4c21-95dc-430ac358b199',\n",
       " 277: '193db7d8-a93d-4c68-b707-60be9cd38ccc',\n",
       " 278: '7ad9b7ed-1c6f-4c1a-91dd-18c2da927e13',\n",
       " 279: '67ec9916-bd69-4c4a-b680-a80fa37bfe42',\n",
       " 280: '5165bdf2-a997-4840-9bb5-e3da5c367d5d',\n",
       " 281: 'c765f23d-98e4-4495-9330-94b4774d1524',\n",
       " 282: '5253efb8-99d1-47e3-941c-d73aebdaeec4',\n",
       " 283: 'b6f9ddc8-65f3-421d-9b23-dc5db0e1ed02',\n",
       " 284: '8a03e867-b093-4bf3-9b7e-297fa382e2e1',\n",
       " 285: '6417a1cb-ed30-4bcb-b0ad-e38d90e985c8',\n",
       " 286: '1bf43439-4557-40f8-867e-e848499f4ed8',\n",
       " 287: 'd4de459c-cfcb-4e1b-8863-3c9f1211ac48',\n",
       " 288: '87583e28-ca98-40d3-b95b-0b652f82602f',\n",
       " 289: 'c8f7efa8-ca35-42bc-b44e-8bbfd027b55a',\n",
       " 290: 'ea768411-27ae-4320-bf13-f37d23759872',\n",
       " 291: '27047072-9125-490a-b27d-3c8c4630262d',\n",
       " 292: 'abbe69b3-2034-4e94-a8ca-cd0fb2d9f11e',\n",
       " 293: 'b41410ec-05ca-4ade-8cbf-ed61c44d90fb',\n",
       " 294: '7eda24d7-0481-4871-bdd6-c4cc4b6250b5',\n",
       " 295: '19527e7f-cc94-4742-9b39-f284a3189768',\n",
       " 296: '03acc596-f224-419c-beb7-84c3330ecbef',\n",
       " 297: 'ec238a81-96c2-4238-b3c6-bb6d8762424e',\n",
       " 298: '2b31cb3d-c6b8-4055-b53b-b638180bdda0',\n",
       " 299: '81005634-a6ed-45c9-8b5e-c0b6854bd046',\n",
       " 300: 'f83d4525-4899-4c89-94ae-127de72dd91d',\n",
       " 301: '4c14512d-d0a3-4a01-993f-92c457b795dc',\n",
       " 302: '6b5dedb6-ffed-40d5-a0dc-f287a8c22b8b',\n",
       " 303: 'dfc614e3-defa-458e-bf63-433f9dc99f07',\n",
       " 304: 'd866406d-6964-46de-b34d-9e5bfe2447ca',\n",
       " 305: 'bf730cde-57bb-4b58-a830-b746c0ed6ebb',\n",
       " 306: '12b49c82-bd11-44de-a15a-4a4ade139f8a',\n",
       " 307: 'd6a2d43b-1b39-42d1-9238-4e83d774dcd0',\n",
       " 308: '94de9c5c-72cc-4eac-92b3-564df2ebfe5e',\n",
       " 309: '77f44172-e61e-4435-81ef-2c1f65581f55',\n",
       " 310: 'e9208346-4095-4d19-bfdb-5567698c6be2',\n",
       " 311: '49bb42ab-7261-4d7a-8a1e-63b162c3b0b8',\n",
       " 312: '41297d1d-648b-4353-9921-87ee11454e0b',\n",
       " 313: '68ad9317-e8b5-45ed-b641-7dd1bb095e75',\n",
       " 314: 'dcf54278-91ac-465e-8acd-b275fac29bbb',\n",
       " 315: '0e6e5485-b6dd-43ad-b5fb-3974e4c8dbc3',\n",
       " 316: '4c52da07-8b71-40a7-b598-d78ac39a7d0f',\n",
       " 317: '98000b84-b615-4a24-b2d8-5d00a0b6556a',\n",
       " 318: '1e6767a2-c14d-411d-b218-7ba45beb2c2a',\n",
       " 319: '10c40dc9-6301-491e-a5f7-d98f3df79f93',\n",
       " 320: '8dbdd532-8062-492c-be55-961b21b7eb5a',\n",
       " 321: 'aeaf9e8c-c28d-42f8-a590-23f828faa730',\n",
       " 322: 'e21f1b6b-deab-4cd8-af2b-ed033411bc56',\n",
       " 323: 'df7b3a0d-bacc-4a23-bd58-635cf800df3a',\n",
       " 324: '20b1b187-0c9b-4324-9569-460b27743cc3',\n",
       " 325: '55e89c38-4e5a-41b6-8990-1fdc55b6862d',\n",
       " 326: 'f4620279-c432-41ea-b542-a30943a36df5',\n",
       " 327: '25ac13ae-316d-4d21-9418-28f83cc6c2fb',\n",
       " 328: '8456d81b-bea0-41ab-bb0b-1f1d964df8ed',\n",
       " 329: '965c796a-6275-4a2f-b7ab-1c42b6a43070',\n",
       " 330: 'e24413d8-8a41-430d-aa47-fd549819445c',\n",
       " 331: 'c368036c-8e83-4dc8-bf10-db430b969368',\n",
       " 332: '9581d89c-d178-4ab0-81c7-9892f28be4ed',\n",
       " 333: '7fe702c8-cb6f-43f8-b8e6-74ad8e07a6be',\n",
       " 334: '55f8cf91-519b-4b67-9005-a8f35919c200',\n",
       " 335: '3f492616-dfb5-4817-a6d7-63818ffa5d42',\n",
       " 336: '79dd3196-5c51-4968-b6ee-fde0a098a827',\n",
       " 337: 'f79b0fc1-0643-4352-b9b7-2cb8d626d768',\n",
       " 338: '59b70c48-3b7d-40a4-b645-885214faf11d',\n",
       " 339: 'b9bd87ef-f678-4efd-9a9e-f221f0b13247',\n",
       " 340: 'f58f6e37-3d79-4368-95d7-98b3e36a3b10',\n",
       " 341: '31435463-799d-4419-848e-22a54548d9ac',\n",
       " 342: '91c56fb0-40b0-4149-a221-35bfeb545dc1',\n",
       " 343: '880debdf-4571-4725-bf1d-a759a9035d2c',\n",
       " 344: '8b40a599-b99f-4b37-84e9-a75b2c3ad697',\n",
       " 345: '4789b7ee-ca5a-4f91-88e0-74ee0d758e87',\n",
       " 346: 'ef91d771-6694-4361-8223-a8e47e604e0e',\n",
       " 347: '2cd65efa-b3d3-4ae7-8692-326ce57c2b0b',\n",
       " 348: '9872ec3c-0630-4446-9597-66130fe76405',\n",
       " 349: '7ff41cbc-258f-4cf8-b013-eb36607f7c41',\n",
       " 350: '248460c2-b15c-4b31-ac55-a9417d1c3827',\n",
       " 351: '9600e9be-695a-4851-b708-80ca26426843',\n",
       " 352: '638bbe23-5d95-416d-8b38-82acffc60e38',\n",
       " 353: '0a436c6b-44bb-42d9-a997-5e3f059f7629',\n",
       " 354: '9b93bc47-d2c9-44bd-a77c-b73d423a696a',\n",
       " 355: '5c1d2e94-8b0e-4f20-9351-cf7902db722d',\n",
       " 356: '17158291-a560-4199-a00c-f5aea9134d05',\n",
       " 357: 'a7688107-fd73-46d0-aa51-446788d95e5b',\n",
       " 358: '6596bdae-3feb-4981-910f-c36e3c9fd1ac',\n",
       " 359: '45192d79-f163-427f-92c5-9b92960a8262',\n",
       " 360: 'b1c5a892-ebc8-4eee-9049-e03bc693b388',\n",
       " 361: '94619d50-60f1-4882-8935-5b1301f70c71',\n",
       " 362: 'e8f37d34-68c0-477b-9167-485d68800c6b',\n",
       " 363: '08584d16-e72e-496b-801a-f2015e9c85d8',\n",
       " 364: 'ad113cad-fa74-437f-a457-da1737a47b63',\n",
       " 365: 'd7907bed-8282-49f7-b055-8cd1a6a65630',\n",
       " 366: '0d1b8d16-6383-4a6a-b53c-4d6fb623255f',\n",
       " 367: '090ae0b0-b7dc-4092-b910-ee5a3261b690',\n",
       " 368: 'b3a37e6a-c28b-4fc8-bb8a-677e3e28e2a2',\n",
       " 369: 'b5fa3c82-7e9f-4369-b839-ac214db4ea8c',\n",
       " 370: '33981a56-292c-4eea-95fe-394f9ac719e7',\n",
       " 371: '51b8a3f0-c52e-4edc-b328-bf1305195289',\n",
       " 372: '7582a5c9-833f-4ed2-a61d-6b7f6bf61dac',\n",
       " 373: 'e526d3e0-f899-4e42-8a4a-b4290012a224',\n",
       " 374: '7317bf80-f361-4a39-947c-51e1197ee6fe',\n",
       " 375: 'ec364296-4b2c-4d29-9ff8-db37f3d634cd',\n",
       " 376: 'f928da44-83d1-41e4-843d-f0f602175627',\n",
       " 377: 'db889067-065b-44b5-8759-c4c35a8ba521',\n",
       " 378: '6cbe4570-af9b-423e-9b19-7c216bbe85ce',\n",
       " 379: '4b9d625f-b949-448a-ab11-c59a8e71b79c',\n",
       " 380: '3eed087d-1112-463f-8699-629ee22c92ce',\n",
       " 381: 'c3f21720-6f97-47a1-a5cb-c20b0ed5f397',\n",
       " 382: 'b33cb244-a724-4221-bba8-c5cd451bb2f0',\n",
       " 383: 'a2464e1a-5463-489e-8f15-a1c6b63c9f9b',\n",
       " 384: '636aef70-de46-4615-8084-77fb0cfc8bc1',\n",
       " 385: '69c9d975-a4a5-4ce6-85ba-c3e476b099a4',\n",
       " 386: 'b75ec049-a746-4ea7-852c-960513b1ae83',\n",
       " 387: '0dbd65b4-72e0-45a9-b16c-2a52102b1ce5',\n",
       " 388: '94f29851-58f2-408a-89c7-a7c148e8eaee',\n",
       " 389: '63be4c29-2dda-49e2-ae3d-efa8bb4acf99',\n",
       " 390: 'eef771b6-11f4-42fb-9b76-1d5f8cf6a559',\n",
       " 391: '4d29e936-1364-47ca-8343-b43ab23762d3',\n",
       " 392: '6ab015d8-7ceb-473b-943c-2b0f416461d0',\n",
       " 393: 'd60c568f-31fb-43fb-b429-ac33d01884e4',\n",
       " 394: 'b3c0f2c0-8478-4a67-bdc9-2661a6f0b0de',\n",
       " 395: 'e695fdd4-1801-4884-8994-48377104f4c5',\n",
       " 396: '96b0aaba-64bb-4804-8c95-bc7ef96d5b92',\n",
       " 397: '0b13ad36-efdb-4eac-a141-cf861f011671',\n",
       " 398: 'e265a22e-9c30-4a1f-b5da-55ef50d28520',\n",
       " 399: '51dcb9cc-28e9-4400-b9e9-4d1ad4557279',\n",
       " 400: '2d7e053f-20a4-4b42-bd97-d91bd1e50a67',\n",
       " 401: 'e96853a6-e479-439a-900f-3f63d7e7497a',\n",
       " 402: 'd7946523-8e15-4ee2-968e-ba6744d0feb8',\n",
       " 403: '20c98c11-706a-49fd-a2af-ed91d967e3c4',\n",
       " 404: '479b65e6-aa1f-4849-8e76-4bab23c8025d',\n",
       " 405: 'cb6d8512-106c-40a3-93af-a3b83c4e50d8',\n",
       " 406: '063b9812-586a-485c-8936-998aa8d8c03f',\n",
       " 407: '27375d5f-4a3d-4083-8942-4ededea74d1e',\n",
       " 408: '47c1f711-751e-4281-af35-5e88015b3e56',\n",
       " 409: 'f6cad904-3300-47e6-ac18-0e2440faa635',\n",
       " 410: '9c103331-db0a-4767-97e7-ede9859d8c20',\n",
       " 411: 'd6d842fe-792a-4e3e-b996-a4944a394f39',\n",
       " 412: '3725d516-5cea-440b-8bed-3242756a9385',\n",
       " 413: '82779348-a32b-4efb-a34b-951d59853b10',\n",
       " 414: '59706165-2e16-4124-be53-c4c4245a376c',\n",
       " 415: '6ec0e3c1-e704-437d-afa3-b253e758c01e',\n",
       " 416: 'f58a4f65-8695-4606-b402-b5f75a92e127',\n",
       " 417: '34be4725-3fb8-4124-8e5c-acbaebed96c1',\n",
       " 418: 'd21c71fb-66f9-485a-a2c0-238e346ff7f8',\n",
       " 419: 'd5ae54c9-0858-40c1-a234-eb25d78fc165',\n",
       " 420: '89b170ff-767f-46cd-9bfc-544204d83a7a',\n",
       " 421: 'a9816468-bf80-4063-bcc2-3c2c8ca8de78',\n",
       " 422: 'a6c6af7c-aae4-4ba2-843e-774b6b10e7de',\n",
       " 423: 'e3c5ba3c-0dd1-4b71-92d9-00f6aa9bf4cb',\n",
       " 424: '7ca2941c-486f-43af-abce-f531ce0dfce5',\n",
       " 425: '62c1ead0-1734-43eb-8e69-1315a3c49fce',\n",
       " 426: '0e1d3db2-8c90-492b-a818-b234f0535a06',\n",
       " 427: 'b1578c59-0faf-475c-b79d-4bea1a3de5af',\n",
       " 428: 'a3e99fa8-6a0b-48c6-b2dd-e0d4a043bdd0',\n",
       " 429: 'b65bb833-ccd3-4655-99af-978d3baf78a3',\n",
       " 430: 'ad9785b5-974e-4a25-aa47-df1b72746909',\n",
       " 431: '69717ea2-1f8b-4f1f-a96a-116020a71bdc',\n",
       " 432: 'b6f5ff3a-5013-40b7-a28d-35e1c6af03cf',\n",
       " 433: '500d55cb-476b-4d78-ba82-c5e06e12f7f3',\n",
       " 434: '19aeadd2-0d14-4a37-95a4-51801e2b7a43',\n",
       " 435: '053f2849-cd26-4c6b-bb18-56031856aef2',\n",
       " 436: '1be96ed7-5a91-4cd9-9053-4eb6430cfb0d',\n",
       " 437: 'cde57402-84c7-4418-a23b-ecbb86de3630',\n",
       " 438: '4671fafa-69d0-45f9-98e3-301ee59ea0d3',\n",
       " 439: 'f1dc77c4-e125-41b8-aeae-bcbc6ff8e180',\n",
       " 440: '699a1f1e-cb1c-478f-9cf9-3cc0c15275cc',\n",
       " 441: '59198b79-a16b-4d9f-88e9-078b1702de3f',\n",
       " 442: '5acc2f0b-3cb4-48e1-9c0d-f74614153cf9',\n",
       " 443: '5cff9c5a-0a64-4419-81b4-4e9b725c16c8',\n",
       " 444: '932873c7-b151-41f0-ade3-42bbbda9862e',\n",
       " 445: 'f46bc76c-41cd-4c32-99be-6f45e9d9123f',\n",
       " 446: 'ea1e2de1-11ed-4edf-82ce-dc1633f953bb',\n",
       " 447: '65eb1610-b697-4e7b-9cea-291078fe9219',\n",
       " 448: '45dc66b4-a098-4451-a946-307dc3acc363',\n",
       " 449: '1ac896f0-4efa-4647-9e79-ddb14ec5344f',\n",
       " 450: '1b7f4134-5e7b-4231-a891-c3e78de4d104',\n",
       " 451: '53280567-c114-4caa-b7d1-768302e47adb',\n",
       " 452: '572a800c-b70a-4531-b055-b6403134e042',\n",
       " 453: '3159563e-0886-45af-9907-59985513d0a7',\n",
       " 454: '7d9fd13a-dd43-4ad7-9f1f-5aca049f50b8',\n",
       " 455: 'b7111bda-061c-459e-b302-fc03adf55b2a',\n",
       " 456: 'b536b8af-1c0d-4f62-8f35-6d5871fedfc4',\n",
       " 457: 'a5bfdbd2-4ce9-4eb6-8852-5f7e4282360e',\n",
       " 458: 'a5e41498-15f7-43f4-b326-c28de74c7eb9',\n",
       " 459: 'eb4a4331-17a9-4762-8cc4-3b15220c6a90',\n",
       " 460: '1260c103-07a3-41bb-8124-c2796416ea25',\n",
       " 461: 'a44411d9-e723-45ac-94e4-aec4e0cc5bfa',\n",
       " 462: '8fa4d6c8-0f81-4f97-a5de-685b6bc4dd73',\n",
       " 463: '571492b2-48f8-4131-984c-c25780d4d9d2',\n",
       " 464: '4279995d-10a1-49eb-a037-b5e52ce68d28',\n",
       " 465: '109a0c43-d5e7-4ce3-9c2a-f9e68a8179b7',\n",
       " 466: '92bfabb3-207c-46c2-b113-0de03654ebd7',\n",
       " 467: 'ed763c24-7a11-4447-8bb3-74e63dd7c0bd',\n",
       " 468: 'd11cf40e-9550-45c6-9399-afa487d48381',\n",
       " 469: 'bd16f97d-5cf5-4bd9-b3ec-6ee7aa854748',\n",
       " 470: 'ee9bdc25-cf52-47e9-85b0-a05b8fbbb5b4',\n",
       " 471: '1626c0ee-4b98-4375-8315-ca153ef87bbf',\n",
       " 472: 'b98b9c4d-c4c6-4f33-ac72-c0c5756ae171',\n",
       " 473: 'b9f48903-12af-408b-a8c3-211dd3b423b2',\n",
       " 474: '663d1176-2fed-43ed-82e4-ed2c101a6ff5',\n",
       " 475: 'a725e6c6-e199-4119-afed-916310050036',\n",
       " 476: '9a05607b-e4f9-4536-aed5-20cf0dd626d9',\n",
       " 477: 'ea7f3eb2-9030-486d-9f25-bbde8366c86d',\n",
       " 478: 'c9900e09-dacb-4f2e-a241-24309fb11b31',\n",
       " 479: 'a0476981-a4f0-4ae1-90f0-53b53c3a4245',\n",
       " 480: '800e0d4e-fcf1-4cac-8848-df163a68b067',\n",
       " 481: '35ef2a63-7525-4772-9772-49ada87a6b1d',\n",
       " 482: '0745c587-8aaa-4042-a925-84ab517b2d2f',\n",
       " 483: '738ff958-1830-4c57-8027-43b57da2fde2',\n",
       " 484: '2fcd07b8-b1a1-4462-998b-acdba9b0c6cd',\n",
       " 485: '6bc91a4f-743b-4fd9-adb6-3b5aba7af4ee',\n",
       " 486: 'f2469075-7a3a-4477-84aa-94ca5402a3c6',\n",
       " 487: 'f1efb8f7-d818-4658-836a-08a48b2434c0',\n",
       " 488: '388ffda2-e50f-4099-aa6e-fdc78651627e',\n",
       " 489: 'aad07d94-d307-44ad-9465-8f4e067b9014',\n",
       " 490: 'd4c59984-888c-4861-9105-676e0b2531a9',\n",
       " 491: '2e38fb6a-e49a-4d5b-ab54-f35367f57f55',\n",
       " 492: '4fb28be6-53af-4b53-84ee-02a027d10bbd',\n",
       " 493: 'f5bdb103-5695-4e77-86c4-d387f9ab3d98',\n",
       " 494: 'e156b6a1-98ac-43f6-a1a3-3037821595b4',\n",
       " 495: '7e441c81-cd3c-49ec-a282-08be5c4010ef',\n",
       " 496: 'db671cdd-8b55-497a-b432-df38e5eb119e',\n",
       " 497: 'f19db184-f5e6-4dd0-8b78-15379d880ac0',\n",
       " 498: '83af354f-7782-46c1-b135-ed8d3313cc04',\n",
       " 499: '262e88e7-b9a2-4515-9f57-5bb696b96a2e',\n",
       " 500: '0f8a39fa-2210-4c34-8f8b-5e02cc4e0262',\n",
       " 501: '199a2539-be46-4fa5-85ba-874f4cfaefdd',\n",
       " 502: '0e0e35c7-a7f6-48b5-8c70-16e97b17831f',\n",
       " 503: '8aa4541b-f684-4e46-bc87-e87762717264',\n",
       " 504: '19edb9ff-9321-4c2a-88be-4b4a8be2d032',\n",
       " 505: 'c4755a2e-97f0-401c-a67e-b64908ab6632',\n",
       " 506: 'af875175-bb57-4de3-b4fa-b5eb5aca5810',\n",
       " 507: '8b9cc15f-d01b-49a1-ab2a-b16948878b0a',\n",
       " 508: '771abfa4-0f17-478f-964e-2cdc9092f173',\n",
       " 509: 'eeb75bee-e691-4d18-9c61-6eaab591e6d4',\n",
       " 510: 'cb549da0-7754-4785-ad3e-ff8883c25355',\n",
       " 511: '17247748-729a-4e75-b5c7-dad93b0ed93b',\n",
       " 512: '0c755f76-b5ad-440c-ac4b-3c0574e13750',\n",
       " 513: 'fed44640-e665-4776-b0a9-623320e6f23d',\n",
       " 514: 'c9e07cd8-44d7-4e54-a4ac-833b69d5d592',\n",
       " 515: 'c80569f4-aed1-4d62-8f9d-6f23fca90991',\n",
       " 516: '8c92ad81-8863-4163-a3ed-0e9e7fdeff1a',\n",
       " 517: 'f66385d8-8416-4daa-9067-66f509d256bd',\n",
       " 518: '83ca6b58-f766-44ba-8461-b9636f4e59d3',\n",
       " 519: 'f9642f37-ab3e-4580-be65-5475a730796b',\n",
       " 520: '244be9ab-0ac7-4e9d-a695-6098086334f8',\n",
       " 521: '924a0b82-69d9-46bd-8af1-224807e65f7f',\n",
       " 522: '369f462d-a65b-4594-8b9e-8da2395f65f9',\n",
       " 523: '490e6c4b-cc02-4fd1-9e46-293d8b80922a',\n",
       " 524: 'b8166384-a95f-48c7-9394-2e305bf4dd68',\n",
       " 525: '43ddb9a4-ab5c-4640-8358-65c118b0b331',\n",
       " 526: 'eb04ca99-9c8f-4287-9da9-259a915b04d7',\n",
       " 527: '58d95abb-f8fd-4fde-b74c-6009cb2bfbdf',\n",
       " 528: 'bcfbe959-c479-410d-ac1a-33bb6146ea66',\n",
       " 529: '1a4d100d-ca83-4de3-a404-6ff37ec83f0c',\n",
       " 530: '7e977667-e182-4fc5-b667-5a3acd829a4d',\n",
       " 531: '5e7b973c-a7de-4d77-8cb7-cdd983e68c76',\n",
       " 532: 'b8a4118c-600b-426c-804a-48b5364f8d78',\n",
       " 533: '96180f83-2c29-434d-a4cd-fcf40b63e8dd',\n",
       " 534: 'eba1c934-1e74-430c-b0ec-26b37f878dc3',\n",
       " 535: 'ee45fdd1-a848-4b0b-bd65-b3c548e0c325',\n",
       " 536: '6d5411fe-51ec-48bc-823a-12b17033a642',\n",
       " 537: '13cb378d-b0b6-4132-a2b7-f42fe6eedc69',\n",
       " 538: 'cd4fba25-ac49-48bd-8684-9be5bb343b9c',\n",
       " 539: 'e9d18c67-b997-48a4-9f9f-211b183a48b2',\n",
       " 540: '0b140af0-4437-4215-b2ce-2d230317f09d',\n",
       " 541: 'adcc48c3-d9a8-4de5-b266-3c25d056dbb2',\n",
       " 542: 'aac26cc2-854f-4349-b81b-c6e1622f5c9d',\n",
       " 543: 'bc34b154-2600-4947-8c63-18d93be5684f',\n",
       " 544: 'a2bda818-4dfd-4b31-a13f-1803d513162e',\n",
       " 545: 'a114aad6-25c0-4f00-9e9c-3315f7324064',\n",
       " 546: '25607fd3-c12f-47c6-8c43-70c9b0e33107',\n",
       " 547: '5cbb63d1-f8d3-49f7-93cd-ced4b325859d',\n",
       " 548: '157a081a-2568-49f0-bea2-7d32c85b2841',\n",
       " 549: 'fcbcd6b0-07b9-445d-a770-c130ac1f9d11',\n",
       " 550: '9f0a5ff3-e2b2-40e9-afa4-05d2624d0750',\n",
       " 551: 'd5722f4b-db55-42b0-9a4a-84d7b8414e5f',\n",
       " 552: '0ba9eed0-4973-4c3e-b19c-e2fcdfafd88b',\n",
       " 553: 'e8edad72-6dd5-424e-aedb-f1938e25af9e',\n",
       " 554: '5a5df333-2140-48a9-83c3-44d0dd975cc3',\n",
       " 555: 'c343a55f-74aa-4d03-8848-b61faa8bdac1',\n",
       " 556: '254da840-7a38-44ca-af7c-15bc9f060619',\n",
       " 557: 'cc713d01-e648-4d35-9f26-86aeeb0ae46c',\n",
       " 558: '2be4e4ee-147c-4d9c-85b5-bbc2f3c49a4a',\n",
       " 559: '0c870c38-b27b-4296-bf76-4cea9eb48000',\n",
       " 560: 'd76c6d4b-94be-475d-bcaf-26e59b2ad7d7',\n",
       " 561: '47a54e94-136d-4759-99af-b5b146201a21',\n",
       " 562: 'da5c3117-d33d-4e90-a21d-86a6baef9743',\n",
       " 563: 'cfd48249-3690-4af4-aa0a-c0a26c5962ea',\n",
       " 564: '9fecfc43-b0f3-4a74-a14a-ff411aef79ac',\n",
       " 565: 'ecd17f3f-af29-4f40-8192-28ce6e8f5d80',\n",
       " 566: 'bddab1de-8066-40ff-94e0-c4bd72f815ca',\n",
       " 567: '7c8dbab4-6f5f-4da8-a5e7-ea22dc015e4f',\n",
       " 568: '2a070e76-7a40-4527-8e70-0cd3042d1261',\n",
       " 569: 'db7b0a34-5f36-4261-ade5-df260a196de2',\n",
       " 570: 'f23532d7-c6a4-4807-9e53-0303720a38b7',\n",
       " 571: '47cfd8d2-8db6-4d33-8396-2d64a730b214',\n",
       " 572: '34242ca5-f9a2-4242-baac-1d8aef7e3b59',\n",
       " 573: '7654c2f9-2bef-4d22-bf06-23c10f1cb024',\n",
       " 574: '352ee25d-d3aa-4432-916b-a37aa02f3a73',\n",
       " 575: 'e13f4e52-82a4-4f86-a37f-6847a50c5390',\n",
       " 576: 'e63f31f3-66a8-43c8-bf9d-de1cd197c44e',\n",
       " 577: 'f208787e-e76a-4831-aadc-51adbf102c8f',\n",
       " 578: 'e00a4dc4-c3d7-4b3d-88c0-08c84291ce93',\n",
       " 579: 'b436dca9-be31-4101-84e2-e228186182dd',\n",
       " 580: '2428ad0d-4253-449e-8c51-4169411cdc96',\n",
       " 581: '5aec5557-e365-43b9-89f6-a2b9a2699794',\n",
       " 582: '9517292b-6168-45c1-97ea-e9a24de37237',\n",
       " 583: 'dec5b00b-2480-4b76-887a-ea83530fcab6',\n",
       " 584: '7dd42ce2-8b63-4e09-96e5-3ac239199e05',\n",
       " 585: '9235c827-190f-4374-b3b0-97c001d82d13',\n",
       " 586: 'd9decaa5-f772-4948-9da4-04c2f8f602b3',\n",
       " 587: '2e04dbb8-a8bf-4a63-a7d0-3bea94a77063',\n",
       " 588: 'f884864e-5594-4f3a-994e-0e080635b32e',\n",
       " 589: '6cdefd9d-d56d-4041-8c5d-efe576343c0d',\n",
       " 590: '0139f3a5-2f16-4b38-bd63-5942a4880556',\n",
       " 591: '0d176a3e-839a-4e45-a3fc-e1378ce541ca',\n",
       " 592: '03148d4c-97bd-4475-8a94-aadc7329f930',\n",
       " 593: '628aa8de-87f4-49e7-ba53-85df4051056a',\n",
       " 594: 'd69935b1-1d89-44bd-abf1-546884a35cf1',\n",
       " 595: 'fae6d9f2-b5bd-4cb1-9e04-b028e0124650',\n",
       " 596: '03bfb97e-3da5-421b-8534-59d7ecc6e50f',\n",
       " 597: '9c2bc51d-8893-4443-8002-52267368210e',\n",
       " 598: '405df2d7-1ef5-4393-8efa-f86348e0a170',\n",
       " 599: '9b58f585-51b1-4aea-ba72-0c4aa6e6d685',\n",
       " 600: 'd0dc7a9e-fa5b-49ea-99a8-ca532c6639d7',\n",
       " 601: 'b2487279-9821-49f9-a429-51ebbbafd781',\n",
       " 602: 'b524db7e-871c-4eb3-9a92-6f0be6dd16c6',\n",
       " 603: '030b4e97-38b7-45d6-9d51-0bba68373c98',\n",
       " 604: '13b3d7d2-9ec7-41fe-80b8-cecec271dc91',\n",
       " 605: 'd84c3235-7cc4-42fa-ac98-151a958b1994',\n",
       " 606: '8818fa43-5ed5-4899-9cc0-b8950a8d8327',\n",
       " 607: '0c275c3b-c9b0-4f24-ae0c-ff6ed042d6fc',\n",
       " 608: '2ecd8915-c91f-454a-8a49-ff890f061118',\n",
       " 609: '77413e02-d818-4c13-9f8e-d3bab15412db',\n",
       " 610: '9b1ba498-f72c-4551-a035-467dc9b61934',\n",
       " 611: '85b5f93c-34e5-439f-970f-d7a82c168473',\n",
       " 612: 'a99d1b47-750c-41d5-8040-a0b4b718949e',\n",
       " 613: '50f4583f-174c-460b-bfc2-b54281ca37fb',\n",
       " 614: 'b46b1ae4-b0a0-4f14-8ea7-8a1dc3b9b0c0',\n",
       " 615: '8db615a7-6985-43f4-ac7e-76028667d771',\n",
       " 616: '3f145898-7847-4796-8174-692cef6883b0',\n",
       " 617: '53603f5f-7600-417c-bf87-745f57cddeec',\n",
       " 618: '9f09d1b5-6f64-414a-ab10-dd529364a2be',\n",
       " 619: '61f30330-0776-484e-836b-c639add6e2ba',\n",
       " 620: '6584041a-91ee-4f6a-af64-419a41c29e5a',\n",
       " 621: '9ee4db4e-2708-400b-9a17-6c2132671127',\n",
       " 622: '1fb4c5e7-c63c-4593-a29e-1cdbdd4ca869',\n",
       " 623: '633189c6-886d-4a34-8381-ce164699db62',\n",
       " 624: '48e4f56a-e157-4a84-ab43-37d5718c859c',\n",
       " 625: '9ff3644d-c26b-40ff-b180-3f36bd0af959',\n",
       " 626: 'a2f05304-ddaa-45cc-b12e-6435e01110f4',\n",
       " 627: '8538118c-adb2-49c7-a021-5963759a3d7f',\n",
       " 628: 'cb4d20f8-00e2-49cc-a2c7-860e5546aea4',\n",
       " 629: 'c87689c9-2907-4050-afd3-122958d15483',\n",
       " 630: '2ab3651f-aee1-4c89-a85a-d7a4dbc044b4',\n",
       " 631: '7034b39e-bf90-42cc-a3a5-f94c8d9073d9',\n",
       " 632: 'b3e2e2e6-14ba-4642-ab6d-97bc940abed9',\n",
       " 633: 'b4565104-57e7-4180-9310-8b88f2b35433',\n",
       " 634: '52344b46-9896-419c-9fdb-c199f44baa21',\n",
       " 635: 'a1fbdbdf-9eab-4200-9595-c2b7eb66a351',\n",
       " 636: 'ebd4026b-11a8-48a2-8821-f7e5366a997a',\n",
       " 637: 'fbd4c173-dd5e-4880-9699-efd4eeef0f9d',\n",
       " 638: 'c557ac34-e898-42d6-9bb5-68b8af93c53e',\n",
       " 639: '0a6cb763-8c8a-4d14-8867-e1ce0e9f6705',\n",
       " 640: 'e67be06c-8920-49ad-bee7-ab21ca14eb18',\n",
       " 641: 'af523445-1622-41c7-80cd-d5c1f5e2c8d2',\n",
       " 642: '246bf5d5-fa6e-486a-9cc0-0167bc10eba3',\n",
       " 643: '2df28e02-e7bb-4c1d-b0fb-b7e9f5a562b2',\n",
       " 644: 'f21316c1-9d86-4548-a1a0-a27d5e2c0321',\n",
       " 645: '0080c0f4-10c2-4411-9436-13fb4dfb4798',\n",
       " 646: '85b8187b-90b3-4de3-8a86-7ed33fae0b34',\n",
       " 647: '327dda78-833b-4e50-a49e-c3ca79f60862',\n",
       " 648: '0f36131a-50d7-48e2-abe3-85cd4b730d0b',\n",
       " 649: 'ca9125b5-7e28-463d-9bad-228e790e911e',\n",
       " 650: '714cf3d4-58c1-4680-8c7f-b3cb02ca5438',\n",
       " 651: 'c7abd66b-ac5d-48bc-a415-35e0e5802cd0',\n",
       " 652: 'cb426dbd-30d2-469e-af7b-ae87b39affa4',\n",
       " 653: '0428b90e-e4f8-4eff-b6fc-190fa314e406',\n",
       " 654: 'fa33ee74-42f1-4fdf-af62-88996a6a2b53',\n",
       " 655: '756b2eea-15b8-4f99-96f6-87f026b277db',\n",
       " 656: '02381bee-57bd-4506-8601-e8df402189eb',\n",
       " 657: 'b41daef4-30b7-4558-8c91-855f47f164c2',\n",
       " 658: 'adbcab8a-f88e-4076-b859-0c403725f7a7',\n",
       " 659: 'dfa344f2-8bdf-4f50-a384-5cdbf3332f34',\n",
       " 660: '84098f2b-2907-4911-a30f-4a1021fead95',\n",
       " 661: '9edbbc6d-534b-46ac-85c1-4998011033ef',\n",
       " 662: '520d0f6c-abee-4820-8627-a2beb4fba945',\n",
       " 663: 'f59b5979-58d7-44c4-9fe5-f24f9d4b5832',\n",
       " 664: '629bd255-d04f-4935-8363-57a51dc450a9',\n",
       " 665: '958dd9ef-a20c-4ab6-b18d-3a6f1111f6c4',\n",
       " 666: '2ad1256c-09cc-4672-8ea9-acb54da9fcb0',\n",
       " 667: '057ab4a9-1eea-4996-ab05-0a98e34e5148',\n",
       " 668: '7f3ca12b-3f8d-4b73-8e0d-3dce62310bc3',\n",
       " 669: '88d9c02e-2cc6-41f4-b611-f1985b7f8ac5',\n",
       " 670: '89f82f16-1069-496c-85b2-4ece24b9321f',\n",
       " 671: 'f7e480b5-3f3b-4ed4-ba91-90fee6ccdef9',\n",
       " 672: '098ce480-d122-487d-a5b8-07724304436a',\n",
       " 673: 'b7934781-2885-471d-96c3-ddfe3e8b9e9e',\n",
       " 674: '54058134-3c37-465c-b168-0ebb05e2e693',\n",
       " 675: 'bde5d967-ba95-4fb1-a5d5-3ab4ac5ff23d',\n",
       " 676: '886633a9-63cb-49e2-9654-206ba5b71f6f',\n",
       " 677: '90306aa4-a2a9-4fca-a146-aac2d869c915',\n",
       " 678: '092882ad-f680-48ad-b165-479ce5b1bae3',\n",
       " 679: '58d1964c-c93c-42d4-9470-1de40f69f8a0',\n",
       " 680: 'abc0d802-c307-47b2-9fe3-f56105d9f03a',\n",
       " 681: 'ae0612a7-4de9-4d35-8b54-88a73829c258',\n",
       " 682: '2a0f2a36-43a8-4f35-a356-faccd51a175c',\n",
       " 683: '938d9e3f-a40f-4fc6-834a-84691b687ff7',\n",
       " 684: 'b8ecc978-95ef-4354-a89a-dfecba7c0735',\n",
       " 685: '56531c10-995e-4ffd-89b0-45e7f102f2eb',\n",
       " 686: 'df479bd4-ae60-4b84-bb91-f6c8c1b39410',\n",
       " 687: '301aa5cf-7994-4add-8a5f-2807c160a4d6',\n",
       " 688: '85f22392-e3b2-44bd-b064-a51493856dc5',\n",
       " 689: '80a5d640-6141-4655-ae3e-447c37a55622',\n",
       " 690: '2bd1b5d5-e545-4c7b-a84e-6a832ac4b562',\n",
       " 691: 'a361ac22-7633-46f2-b645-f9388a72f41f',\n",
       " 692: '1e1df1b0-ed95-402b-9673-c8d6ca34394c',\n",
       " 693: 'f77b2396-0170-459c-a598-db4687ecd197',\n",
       " 694: 'b0867418-5960-4f3b-887d-7a121ddad5d9',\n",
       " 695: 'ca04d47a-194e-4fea-9161-ca85ab81f8ad',\n",
       " 696: 'c66b76bd-4c59-4218-a4c3-f01dab2b3478',\n",
       " 697: 'c9acdbfb-f2ab-45ea-b258-7f2379842e6d',\n",
       " 698: '8ca9bb07-85d5-4d25-af3d-2852a63fc475',\n",
       " 699: '8748eb3b-a17f-4752-a956-2c8adc74feef',\n",
       " 700: 'f4cff18f-3142-41c3-8466-2536f8f8e4d5',\n",
       " 701: '2246a96d-1422-4700-86a1-e4a9229e197b',\n",
       " 702: '77c6d9c6-ed07-4a4d-b57c-45b614b68049',\n",
       " 703: 'fdf35a6b-59a2-4237-bbe6-6a1d3e77196b',\n",
       " 704: '3c3d9776-b156-4e6a-b60f-923133ff3e5c',\n",
       " 705: '50fca403-fba8-49d9-8e14-8ed30141f79c',\n",
       " 706: 'f582662a-66b6-4300-a3d8-9a31e6970208',\n",
       " 707: '12f0b926-a6a5-42d3-adac-ab2f9604035a',\n",
       " 708: '0ce00cd3-0223-4b9d-92b6-7adeea81d9ea',\n",
       " 709: '9543cdbe-c6fb-4f29-8d13-0f7df21114cc',\n",
       " 710: 'eba7aaa6-0a97-4264-8e81-d7fc6d7e8613',\n",
       " 711: '0dfc21c6-cd29-4265-992b-b14249254913',\n",
       " 712: '53557dd9-d2f5-416b-9335-134c9884d8a3',\n",
       " 713: '0a1dada0-0068-4f7e-a187-64b8fc4493f1',\n",
       " 714: '264b9f61-053a-4b69-a1cd-0d07897b2bff',\n",
       " 715: '6f58b2a8-f6c6-4885-8f09-91423a9f4f84',\n",
       " 716: '47a52b99-9d1a-4c31-8878-4dd4fef9a324',\n",
       " 717: '6a9650ce-1021-4f5c-90a3-bad771c11b3f',\n",
       " 718: 'a65cdbd6-9c72-4435-8365-730dd51702a9',\n",
       " 719: '8bb567f6-b80c-42de-a8f3-52983b943462',\n",
       " 720: '72c4ce9c-2e74-463c-879e-6af51acf4fe5',\n",
       " 721: 'fd67717e-02e9-465e-807c-5c4992de3e63',\n",
       " 722: '69a7066c-d2c6-4f1f-a094-19d51580e87a',\n",
       " 723: '2cd0b03e-eaed-47ac-8f0b-95525935c1cb',\n",
       " 724: '8d39e817-54c9-4ffb-9e2c-b98996bba1d5',\n",
       " 725: 'f4ca54be-c4e7-42c1-881c-8bb7acd19fcc',\n",
       " 726: '255675ab-5e5a-426a-a703-22c815c4c945',\n",
       " 727: '8250b66e-3c23-40ca-a6a3-08311a15e453',\n",
       " 728: '2e72d257-0621-45c3-9f3d-66bfd55fd776',\n",
       " 729: 'f919b81c-3025-47e7-8f32-f11f50f4314f',\n",
       " 730: 'b53cb2ea-2831-4091-8c3f-e3128d2e49ca',\n",
       " 731: 'eb724143-65d6-4641-bf4d-f62428018e46',\n",
       " 732: '0302f367-15c2-48de-bb6f-3a748a571e4e',\n",
       " 733: 'a4acc000-c431-401a-aada-3d45ee916422',\n",
       " 734: 'ca9b4903-da10-483f-a9d1-e5b1b2535796',\n",
       " 735: '7ed615f0-3121-41c5-9077-7e1eb6bd8ef6',\n",
       " 736: '678254df-42e1-4d51-b325-e4808430d7d1',\n",
       " 737: '0ee8c266-e27b-49ec-95a5-4fb93ba2d2c4',\n",
       " 738: '4bcacafe-4864-4493-af4b-be45e1923834',\n",
       " 739: '648842ed-9df7-4a86-8fae-a5b4c1cd11bd',\n",
       " 740: 'b082edd7-caf8-48a0-91ee-82202c6194f5',\n",
       " 741: '0039f31e-cefe-40a7-a915-e14fc390187b',\n",
       " 742: '281672df-031b-4d65-aae6-61d5b7a69e1f',\n",
       " 743: 'e933200b-b123-4ff8-81e7-8505671da7c4',\n",
       " 744: 'c31ad6a3-2981-4ac8-8270-e9814db0d48d',\n",
       " 745: 'eac88a17-3397-4d96-8f31-ea41a4d211af',\n",
       " 746: '985f0706-7419-4294-9f67-93c95bad6723',\n",
       " 747: '3aa97c1a-9932-4cdf-a3e5-c8ae187f8e6c',\n",
       " 748: '52c550c3-ce2e-4e7c-826d-538c37ac2073',\n",
       " 749: 'b3dd537a-d890-4228-8dc5-672e7add4e18',\n",
       " 750: '10dd97b0-4589-4c8a-8dcb-da5da4f8e4f2',\n",
       " 751: 'd95b0b4b-3dd0-4449-9add-74bd3f576cf9',\n",
       " 752: 'bcfa03f5-9ced-4b0d-bf2a-839f9e21d80b',\n",
       " 753: '5700b688-5a6d-4b57-9ef7-85190cb9177b',\n",
       " 754: '0cb51f16-dbe6-4a43-9d0f-c8738aa81b64',\n",
       " 755: '6a02ff17-a133-40ad-9ea9-069f54c2d341',\n",
       " 756: '5b7d119e-f2f7-49d6-ac90-759e67c83e81',\n",
       " 757: 'fe668c08-f3cd-45af-8e14-e0d99900b836',\n",
       " 758: '22c88365-ac89-4126-ae76-8b015ea95094',\n",
       " 759: 'aea8dc10-e778-4f51-9a98-97c8f43a3b99',\n",
       " 760: '9281f893-5c0d-4a36-a094-695c3645803a',\n",
       " 761: '6360fd12-1f91-4934-b164-68007dd0c94d',\n",
       " 762: 'b36d6d92-f7e2-4352-abea-bccfd4a351bc',\n",
       " 763: 'bebe7994-834e-42f1-a18f-ff4763f735f1',\n",
       " 764: '961aa2ae-3eaf-4398-95de-90dfd58f0ad2',\n",
       " 765: 'eed29bb8-3753-401e-9a73-27ad00ebfa49',\n",
       " 766: '0035ff3e-1ef8-4973-9775-2fff84fbcfcb',\n",
       " 767: 'bdc86fe0-1566-493b-985b-3df0961c4de9',\n",
       " 768: 'ae1ebc33-2246-4a4f-b1c1-f75fc87371d1',\n",
       " 769: 'fbbd42e1-0dce-49e8-a526-6df666df7247',\n",
       " 770: '7506d171-cfd6-43c8-a59b-0a31086ed610',\n",
       " 771: 'c2d22336-08e6-4b70-8fa0-1915b470714b',\n",
       " 772: 'b4d0a14f-bdc8-4e16-a40c-b9632c14ae59',\n",
       " 773: '9795324c-4163-46d9-b472-f97710a76dde',\n",
       " 774: 'ba32e0ee-d75c-4072-8a33-b656901c3b39',\n",
       " 775: 'f98b26ad-fbeb-48a1-adb5-96c728253b23',\n",
       " 776: '33813eee-3fe5-43c3-af0e-24bc847d5c4e',\n",
       " 777: '9e25e625-10a7-4594-b08a-4991f29551c4',\n",
       " 778: 'e4af4eda-c095-46ea-b073-da53ce70cde9',\n",
       " 779: '9d845d43-166a-4d2b-bec8-f6bd574ef2e0',\n",
       " 780: '6eadbf4c-958c-49ae-9471-b348920708b1',\n",
       " 781: 'f6e110d9-9f08-453e-a8d2-751a214201b1',\n",
       " 782: '76395b29-44fa-4f6c-b66c-13dde6230ba5',\n",
       " 783: '335d4d56-c6e5-4a94-9dbd-40cb52c7fb5c',\n",
       " 784: '6148d66c-7da1-4764-bfad-ba27cf66f2b9',\n",
       " 785: '9d090e62-fbc6-4739-97d1-99df285ae2a8',\n",
       " 786: '03ba55d6-cc9d-46c8-9097-644c8482a666',\n",
       " 787: '741bdecb-3e4e-4dc1-a116-4b31cc3141b3',\n",
       " 788: 'f861ac98-a829-400c-9c71-9fe54336ce61',\n",
       " 789: '57278d87-3752-4946-9e33-64498cac9afe',\n",
       " 790: 'd07c9a9f-8309-4a03-9ed8-34f3d3c46d8f',\n",
       " 791: '31f4e9bd-3cf7-41fa-b991-02c4244e88a2',\n",
       " 792: '8a8a38d0-36b3-4910-b72e-69d008d6f113',\n",
       " 793: '4d301b18-4553-43ca-98f5-b47043f1b29e',\n",
       " 794: '5d129ed4-fef4-4249-9911-926179fdaf44',\n",
       " 795: '78ac4840-f5fb-455e-96fb-55d1b4e32e88',\n",
       " 796: '164ac2bb-3acc-44cf-9844-aa6db4570f82',\n",
       " 797: 'f5e299a9-8c68-4964-8008-b2e459e172ff',\n",
       " 798: '7cf4edd9-2758-43b1-8d04-62a22e011b02',\n",
       " 799: '052f4261-46ba-4e77-91b3-875ddb94f135',\n",
       " 800: '46a0d1e7-f7a7-46e4-915e-4b8cba835e95',\n",
       " 801: 'ac63e407-f98e-40d5-b80f-bbfd7be780da',\n",
       " 802: 'ea290228-d019-4159-812c-7ee3da1b4086',\n",
       " 803: '85319ac9-61dc-453a-a0d9-31fc7837400c',\n",
       " 804: '5386ced6-1b4d-491d-8062-1774871f270d',\n",
       " 805: '8d1dceba-3e79-4614-bf45-5a1389264884',\n",
       " 806: 'a4c7080a-67bc-4849-a472-9c2b4348c393',\n",
       " 807: '7f94d444-b470-4e4c-bd79-122c4d4d22b6',\n",
       " 808: 'b34bf498-385a-4274-b560-03f22a916027',\n",
       " 809: '107bd632-b707-4d2b-a76f-829a4776d7e5',\n",
       " 810: '2a88e9b9-302f-4734-809c-2bd1f47cc2d0',\n",
       " 811: 'b379a92a-f545-4595-bd34-f608cdff1c97',\n",
       " 812: 'f359fc2b-4fa5-4f8b-bf21-2caf5a5b680e',\n",
       " 813: '4604929f-e142-4a87-8e88-3a002e1c7a87',\n",
       " 814: '71bbae63-6229-492e-80ba-0fca57637e56',\n",
       " 815: '0f89d1a6-77eb-4a42-9d09-85917c3aa780',\n",
       " 816: '04cfc5ef-d789-4536-a5d2-36c0ee858ddd',\n",
       " 817: '7f2ae00f-4fa4-492a-8c36-27f9c7ae9e8d',\n",
       " 818: '1548dc17-ee8c-4578-93a5-1594ec1745ee',\n",
       " 819: '79744798-ea75-4a4d-b446-3c00067054b8',\n",
       " 820: '17767fb6-f4cf-471d-b733-f851f5e839be',\n",
       " 821: '8816c8cc-5add-4f55-b985-48a175050713',\n",
       " 822: '90252904-4b2d-4bc1-9119-4b2cfcbcef45',\n",
       " 823: 'cd248b15-46a4-4fc1-a6bf-e419730c771c',\n",
       " 824: '247c37a6-fbc1-4a26-8e24-e9606abec6e8',\n",
       " 825: 'c16d9f27-38e0-4a3e-92b3-171d458b11e7',\n",
       " 826: '408e3137-4cb6-439b-8e60-2f0846044708',\n",
       " 827: 'c6f89712-8d33-4ad4-989a-baa23d60dc2d',\n",
       " 828: 'f416e978-182d-46bc-a5fe-1a235d13a6d9',\n",
       " 829: 'fdf71626-98cc-4940-88cc-286fa9b6d856',\n",
       " 830: '9c701b12-d232-4f86-93b9-2a7634005bd3',\n",
       " 831: 'd8936388-7880-468e-b60b-824bf3005111',\n",
       " 832: '5f99b22d-5efe-4812-a934-b75c6842b8de',\n",
       " 833: '670f3a01-817c-4b1f-a987-e90ee2129114',\n",
       " 834: '927c47f2-13d5-4760-8545-7027090c7228',\n",
       " 835: '7365dfa4-fc62-40a7-8885-d0714abf0322',\n",
       " 836: '6274b3b8-4a49-4e6e-9ccc-091b2ea85b2f',\n",
       " 837: 'add5a6ad-eb56-4f83-9355-51a8d3ffa4ed',\n",
       " 838: 'dfec4cae-61d7-4628-a432-791451c1de3c',\n",
       " 839: '89aa8848-ae26-4f40-8146-7409a0801733',\n",
       " 840: 'fbb158b8-e1f5-41ed-bd5a-433e19cd366a',\n",
       " 841: '2b04e58f-4203-4599-a9b1-2b20243591b0',\n",
       " 842: '52fa7027-d75d-4fc9-9426-5e6012ec9cf4',\n",
       " 843: '25fb06f1-f114-4b87-8e86-5aa5075f8914',\n",
       " 844: 'adb09a51-5941-48e7-a03e-51ba3a1587ba',\n",
       " 845: 'beeb70bb-38bb-4685-a0a3-e17fd2205e69',\n",
       " 846: '9aaeeb30-da84-4e2a-b03e-38258daa2354',\n",
       " 847: 'bd0b2d5d-6088-4d3e-ab7f-29506b9fc31f',\n",
       " 848: '409c24b1-f3f8-423d-a37c-74373bd234d4',\n",
       " 849: '8dbc7490-5ff0-4bf9-9564-b51cd9011c86',\n",
       " 850: '01d0ff3c-6936-40ff-a570-c80aa66e764c',\n",
       " 851: '0a0effcd-0863-4cfc-b074-474f35d7573b',\n",
       " 852: 'efdcb000-7428-4b50-b3ee-fdecf2389195',\n",
       " 853: '01ea3814-4701-4e26-a932-72c6618edfd1',\n",
       " 854: '86a8f30c-0292-45cd-8a80-286ca2df649b',\n",
       " 855: 'a69b66e7-c96d-4b67-bd03-f177ed4f8e9c',\n",
       " 856: 'e26aeaed-1890-4cd3-83c9-e585b2a9b1a7',\n",
       " 857: '683ebec0-92cc-43ea-b326-b76e2c2509bc',\n",
       " 858: '5a669e1c-4aa8-493a-863c-cbcd1400a170',\n",
       " 859: 'fbbbd7ed-c624-47ec-83d1-8debf02bc190',\n",
       " 860: '35fcf649-473e-4b35-88f8-2b2de6060c31',\n",
       " 861: 'd0ecb514-7570-4c67-8284-0a2eb5b42651',\n",
       " 862: '5e811124-23aa-48f7-9357-469e9d3da28c',\n",
       " 863: 'd4a94ede-7512-4421-8d7c-a3d720f6d944',\n",
       " 864: '7dc0da43-d271-49f1-8e17-8b4aa4ad296c',\n",
       " 865: '453f8f39-b3eb-4986-b2bc-01b0367d9a5f',\n",
       " 866: '968d0586-d44e-4b9d-9f40-7f2b29aca9e5',\n",
       " 867: '7b7f5038-fccd-42af-8e8b-f885284692f4',\n",
       " 868: 'f7baf608-60f2-4c01-8a45-7603540e3f02',\n",
       " 869: '46736028-3ca8-4203-88ff-1a77a743bf7b',\n",
       " 870: 'fd926790-4a45-444e-9ddb-ebedb73729a6',\n",
       " 871: '86ad088d-5066-4ee1-bed3-7b5cf1ee5155',\n",
       " 872: 'd9ee047b-9827-4fa1-993d-d41efec70973',\n",
       " 873: '9bcd5cc6-6abb-4104-9faa-c56cb0ebaf64',\n",
       " 874: '50e35463-f648-4609-89cb-ec276a35912f',\n",
       " 875: 'bff92922-7ada-4312-bdc4-4ca8d676ab60',\n",
       " 876: 'b578454e-e768-4836-8162-1129bb547665',\n",
       " 877: '962fbcfd-1a32-4656-84c7-f640b424f194',\n",
       " 878: 'cc64ec68-1e46-40e8-811c-4aa790ed0e88',\n",
       " 879: '01e5a044-0fac-4403-b124-6ad9c632ffbf',\n",
       " 880: '37d62c58-160c-4170-879a-80161d508421',\n",
       " 881: '84477bee-f391-4e3b-8ee9-1d6298da9d38',\n",
       " 882: '46447776-9796-4d7b-aa7b-6e95c211410e',\n",
       " 883: 'fdaf84cb-53b9-48f2-91a2-165b27947c89',\n",
       " 884: '65b32301-7ec6-4a4d-84b3-55a70ec81f3e',\n",
       " 885: 'fe0e1ea8-dedc-46c7-a601-c26f2e3b153b',\n",
       " 886: '55987cd9-54c5-4f53-9b9a-8e9b84c5fcaa',\n",
       " 887: '5b83fea9-bc59-4500-8e63-1089c93ace02',\n",
       " 888: '43850f51-6f7e-447a-826b-efb35bae9e25',\n",
       " 889: '1d68b1cb-74af-4770-a93d-03c2d7c763a6',\n",
       " 890: '23b4d8a5-08ed-4647-adf2-056c7bd73817',\n",
       " 891: '912128bc-b304-4dd0-a19b-3038868305a9',\n",
       " 892: '8365fd78-fc8e-4b26-b42f-6ed70df4c38c',\n",
       " 893: '12f530e1-3327-42f9-a0b6-812ddcf9a877',\n",
       " 894: '8b4b89d8-4449-4015-ac07-bf3967887d13',\n",
       " 895: 'dad0c550-abb1-4f23-846f-8860e3bd75b4',\n",
       " 896: 'ae3a6d37-46bd-43fc-853b-c3edc5623e3d',\n",
       " 897: 'ac7228c4-fa59-4f29-af0c-0f083a886885',\n",
       " 898: 'b6073d9e-5ce9-46f7-a390-42c7a89b1ff6',\n",
       " 899: '6fa24b6b-c872-4340-87ae-cf20e8696c62',\n",
       " 900: '0903da6a-c279-4135-b607-6a002d994048',\n",
       " 901: '229309fe-d354-4801-8762-0d4934950de5',\n",
       " 902: '0c006e6b-2f52-4f78-812b-7185c2cabded',\n",
       " 903: '0e85e8a4-54cc-4de2-9a2b-42ca5117eec1',\n",
       " 904: 'fc805e54-8775-4a45-b467-8b68d862470d',\n",
       " 905: 'ca28b617-66f2-4bc7-8329-04628555816e',\n",
       " 906: '31899acd-5425-494a-91c3-ed0ea849a3b6',\n",
       " 907: '06638515-182c-4277-9db2-48f0094d26be',\n",
       " 908: '4dfe9843-eac4-4b9e-8df0-6f69a5d1978e',\n",
       " 909: '479d0143-a070-4a34-9f8e-af1b514aba01',\n",
       " 910: 'ede8c8ed-21a2-4698-b18a-0f5f405384ea',\n",
       " 911: '5f2c7d07-6835-41f2-80e5-3051e525845f',\n",
       " 912: 'ddd3fb00-94f4-4f31-9f23-c431d6ccf5e1',\n",
       " 913: 'c40ea850-d7c2-4de8-b609-1f8d0af8e2a4',\n",
       " 914: '99542bc2-4bbf-49ba-a46a-844def40545e',\n",
       " 915: 'af797863-1923-4753-a264-406fb904e3c2',\n",
       " 916: '10f0228c-ba27-4539-8c9a-0732fbe2b099',\n",
       " 917: 'a852d8ec-2ead-4ebc-bde1-6ca9eb396da9',\n",
       " 918: '255acb8d-e6ff-43c1-8078-1b9529d3eda2',\n",
       " 919: '42fc7c18-a3c2-4757-925f-b7c56068c8fa',\n",
       " 920: '38b64388-ff3f-4dc2-a155-ac71ed9b7054',\n",
       " 921: '159f9204-1f97-4714-8b4b-7551dc1459ec',\n",
       " 922: '5eed8f03-ca3b-48c3-86ce-40a4a44db9c9',\n",
       " 923: 'b870d44e-28b9-4afa-803b-4c693e6816b0',\n",
       " 924: '33f434db-8f7d-4588-94f3-b036bd0fe6dc',\n",
       " 925: '8c8db737-1dae-42c4-8f8b-66ef1829f3bd',\n",
       " 926: '3182b380-8cce-47d5-ba97-4549ffbe4d2d',\n",
       " 927: 'd9cfc473-82f5-4878-939f-4987a46d6ce5',\n",
       " 928: '684a1158-4909-4b74-955e-9c32b8a09fee',\n",
       " 929: 'ca9127e3-79ca-4817-8066-b0918ada52fa',\n",
       " 930: '2443546a-4f56-4343-9685-ed27e14e14f5',\n",
       " 931: '00c349bf-b7da-4a2e-854c-18cb120dc5f0',\n",
       " 932: 'b506e571-36a1-4501-8f80-d4e05ae986b5',\n",
       " 933: '2772edd5-a212-45c8-a94e-6fc3ddb329b6',\n",
       " 934: '7fbc1c3b-c6bc-4f08-a49e-5c84f9099385',\n",
       " 935: '0046764d-6198-47f7-95f8-33a826bd5f0e',\n",
       " 936: '4971bf08-73ac-4778-bd28-9718d4e558bb',\n",
       " 937: '96a2a661-59ca-4047-a655-62bebff7aaa2',\n",
       " 938: 'b7656df9-b6e9-498a-acf5-feb91b4df3a3',\n",
       " 939: 'f38a4f6c-e5d3-4b29-b152-12843586933b',\n",
       " 940: '6a6beace-cf76-4dd4-ac35-34127da2d6c9',\n",
       " 941: '3d71b56e-fe31-4499-85b2-91d16f63549f',\n",
       " 942: 'bfdab3d9-7abd-4af2-8f89-61e7525917b8',\n",
       " 943: '70f8043a-97eb-4118-aad5-558c87f85a58',\n",
       " 944: '3519ce3b-d91a-4494-8689-3343ea7ffae3',\n",
       " 945: '1dc07002-2195-4b2c-8fa3-d67a1bbe0c0e',\n",
       " 946: '9a2c9bc0-2c82-49ef-87be-0098964570b1',\n",
       " 947: '438afdd9-f166-47da-9b5e-5a2f1d0aebf7',\n",
       " 948: '7e364526-cc34-49b3-9024-9ebdc488d747',\n",
       " 949: '74d9c343-d8f7-4c50-bd85-6856e43a20af',\n",
       " 950: '4c8c0b59-de36-4b1b-994a-b243b500e394',\n",
       " 951: '66efdd95-a77d-44be-a27b-0c089b00cc0c',\n",
       " 952: 'cef90b71-bf05-4e10-8926-b8897603c562',\n",
       " 953: '4d75cc31-c18c-4f99-861b-98b282e15530',\n",
       " 954: 'fcdd6785-abd2-4974-8621-aa3c782f5d71',\n",
       " 955: 'cf7cd2e4-4a61-4c39-afa6-fa36145596d9',\n",
       " 956: '77507d51-d688-499f-a142-d4fa89182589',\n",
       " 957: 'b3625bc4-c5ab-4420-b058-9acfaf56f2e5',\n",
       " 958: 'af74520f-f6b2-4ffd-9502-e126f3a24f6e',\n",
       " 959: 'e9e35285-d932-4815-92d3-3c7110748e82',\n",
       " 960: '37f4a83d-8544-4fd2-8edc-9234e132e5ec',\n",
       " 961: 'e3b49b9c-5e44-438e-94f6-c06d4f266ab7',\n",
       " 962: 'ee4967eb-d8b6-4220-ad39-a626098dfb96',\n",
       " 963: '21a505b2-337b-4afc-b2dc-3b45a1017b06',\n",
       " 964: 'a764f217-613b-4a07-adf2-3746852ad2d6',\n",
       " 965: '934c4138-9556-41b5-8522-3dc5a0ddda1a',\n",
       " 966: '25e76db8-34e5-48bf-8714-43fc449bb94d',\n",
       " 967: '21086bdd-e8b3-42c3-a7fe-dc493a889f8a',\n",
       " 968: '7ebcb4df-e3bc-42aa-8872-3e4c3edf9c7d',\n",
       " 969: '4e18fb9c-53a1-4a1c-b0d7-75b22c5597cc',\n",
       " 970: 'eee4a64b-811e-4997-950b-be6a564d14a8',\n",
       " 971: 'd37307ba-eda2-40e6-873d-fa03d20aa635',\n",
       " 972: 'c2117569-b381-4252-915a-c11d99cd552e',\n",
       " 973: '02b55660-2253-4bc6-ac62-fc8fc66132e5',\n",
       " 974: 'b7fbbc06-5969-4961-b682-f63483869f80',\n",
       " 975: '5066dc4c-197a-4deb-ac79-7547806f9423',\n",
       " 976: 'bed33ee7-b80d-42b4-879d-34be67636213',\n",
       " 977: '650afbd2-2c13-4fa0-ac00-cf81e641e285',\n",
       " 978: '45c1dccd-6937-4bfe-82c0-0be227eea726',\n",
       " 979: 'ac2d9ca3-0575-44b8-9a6e-3a93bae77fd0',\n",
       " 980: '770aace2-5d94-442b-b39c-163c3c1a0389',\n",
       " 981: '088ad680-dcc4-4b7b-87b1-35a8410e4bde',\n",
       " 982: 'a240ee41-0f69-4fd2-b7bf-b58e56de0079',\n",
       " 983: '57c3184d-b643-47cc-a3f3-b1562e632859',\n",
       " 984: 'ee31ff64-f55c-4bbf-84b5-84fbba97ffd1',\n",
       " 985: '5353a454-e36c-4a30-9477-f5eafa93fef7',\n",
       " 986: '5ece8b62-48b3-4297-836c-b5d196b85790',\n",
       " 987: 'b0241167-29c3-456a-abb2-5fb14d249469',\n",
       " 988: 'abe34432-abd6-485c-9a39-548969e82ed5',\n",
       " 989: '67785a23-ee60-4023-9f25-77eda0b796c2',\n",
       " 990: '4f146843-7275-47ba-94c6-9c2db54198c8',\n",
       " 991: '3712e898-6e84-453e-82d9-0c24abece8d9',\n",
       " 992: 'a6ea29e5-604a-4f5e-864f-e1db683d0ae2',\n",
       " 993: '878aac48-532e-47e5-8c07-6fd78e515088',\n",
       " 994: '2db38042-710a-41a9-9032-b41db776d9a0',\n",
       " 995: 'e8a418be-e309-4b45-a516-71e67a4ca9b7',\n",
       " 996: 'f9ea299d-9549-4fd0-bbdb-7b3a1fbd5895',\n",
       " 997: '7dd0c6e9-4144-4507-be87-5c84d7a1d96e',\n",
       " 998: '24ffb4bc-928c-4ba0-a43d-bf33aefb7bf8',\n",
       " 999: '0e60dc16-a9c3-404a-a295-68c64739e886',\n",
       " ...}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:56:08.164009Z",
     "start_time": "2025-07-05T17:56:08.116580Z"
    }
   },
   "cell_type": "code",
   "source": "vector_store.save_local('machine_learning_vdb')",
   "id": "670c5fc9afc63261",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:57:24.461396Z",
     "start_time": "2025-07-05T17:57:24.342661Z"
    }
   },
   "cell_type": "code",
   "source": "new_vec_store = FAISS.load_local('machine_learning_vdb', embeddings = embeddings, allow_dangerous_deserialization = True)",
   "id": "a009572d1525f7f4",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "690016bc062a4154"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
